Key,Item Type,Publication Year,Author,Title,Publication Title,ISBN,ISSN,DOI,Url,Abstract Note,Date
XXX,journalArticle,2013,"Carl G. Wagner, Bruce Tonn",Constructing Lower Probabilities,,,,,http://arxiv.org/abs/1303.1516v1,"An elaboration of Dempster's method of constructing belief functions suggests
a broadly applicable strategy for constructing lower probabilities under a
variety of evidentiary constraints.",2022-11-13 15:58:06
XXX,journalArticle,2009,Ji Han,A Rational Decision Maker with Ordinal Utility under Uncertainty: Optimism and Pessimism,,,,,http://arxiv.org/abs/0912.5073v2,"In game theory and artificial intelligence, decision making models often
involve maximizing expected utility, which does not respect ordinal invariance.
In this paper, the author discusses the possibility of preserving ordinal
invariance and still making a rational decision under uncertainty.",2022-11-13 15:58:06
XXX,journalArticle,2011,"Tor Lattimore, Marcus Hutter",Asymptotically Optimal Agents,"Proc. 22nd International Conf. on Algorithmic Learning Theory
  (ALT-2011) pages 368-382",,,,http://arxiv.org/abs/1107.5537v1,"Artificial general intelligence aims to create agents capable of learning to
solve arbitrary interesting problems. We define two versions of asymptotic
optimality and prove that no agent can satisfy the strong version while in some
cases, depending on discounting, there does exist a non-computable weak
asymptotically optimal agent.",2022-11-13 15:58:07
XXX,journalArticle,2013,"Paul E. Lehner, Azar Sadigh",Two Procedures for Compiling Influence Diagrams,,,,,http://arxiv.org/abs/1303.1494v1,"Two algorithms are presented for ""compiling"" influence diagrams into a set of
simple decision rules. These decision rules define simple-to-execute, complete,
consistent, and near-optimal decision procedures. These compilation algorithms
can be used to derive decision procedures for human teams solving time
constrained decision problems.",2022-11-13 15:58:07
XXX,journalArticle,2013,"Ross D. Shachter, David Heckerman",A Backwards View for Assessment,,,,,http://arxiv.org/abs/1304.3107v1,"Much artificial intelligence research focuses on the problem of deducing the
validity of unobservable propositions or hypotheses from observable evidence.!
Many of the knowledge representation techniques designed for this problem
encode the relationship between evidence and hypothesis in a directed manner.
Moreover, the direction in which evidence is stored is typically from evidence
to hypothesis.",2022-11-13 15:58:08
XXX,journalArticle,2015,George Konidaris,Constructing Abstraction Hierarchies Using a Skill-Symbol Loop,,,,,http://arxiv.org/abs/1509.07582v1,"We describe a framework for building abstraction hierarchies whereby an agent
alternates skill- and representation-acquisition phases to construct a sequence
of increasingly abstract Markov decision processes. Our formulation builds on
recent results showing that the appropriate abstract representation of a
problem is specified by the agent's skills. We describe how such a hierarchy
can be used for fast planning, and illustrate the construction of an
appropriate hierarchy for the Taxi domain.",2022-11-13 15:58:08
XXX,journalArticle,2015,Toby Walsh,Turing's Red Flag,,,,,http://arxiv.org/abs/1510.09033v1,"Sometime in the future we will have to deal with the impact of AI's being
mistaken for humans. For this reason, I propose that any autonomous system
should be designed so that it is unlikely to be mistaken for anything besides
an autonomous sysem, and should identify itself at the start of any interaction
with another agent.",2022-11-13 15:58:09
XXX,journalArticle,2017,Roman V. Yampolskiy,The Singularity May Be Near,,,,,http://arxiv.org/abs/1706.01303v1,"Toby Walsh in 'The Singularity May Never Be Near' gives six arguments to
support his point of view that technological singularity may happen but that it
is unlikely. In this paper, we provide analysis of each one of his arguments
and arrive at similar conclusions, but with more weight given to the 'likely to
happen' probability.",2022-11-13 15:58:09
XXX,journalArticle,2019,"James D. Miller, Roman Yampolskiy",An AGI with Time-Inconsistent Preferences,,,,,http://arxiv.org/abs/1906.10536v1,"This paper reveals a trap for artificial general intelligence (AGI) theorists
who use economists' standard method of discounting. This trap is implicitly and
falsely assuming that a rational AGI would have time-consistent preferences. An
agent with time-inconsistent preferences knows that its future self will
disagree with its current self concerning intertemporal decision making. Such
an agent cannot automatically trust its future self to carry out plans that its
current self considers optimal.",2022-11-13 15:58:10
XXX,journalArticle,2019,"Guillaume Escamocher, Barry O'Sullivan",Solving Logic Grid Puzzles with an Algorithm that Imitates Human Behavior,,,,,http://arxiv.org/abs/1910.06636v1,"We present in this paper our solver for logic grid puzzles. The approach used
by our algorithm mimics the way a human would try to solve the same problem.
Every progress made during the solving process is accompanied by a detailed
explanation of our program's reasoning. Since this reasoning is based on the
same heuristics that a human would employ, the user can easily follow the given
explanation.",2022-11-13 15:58:11
XXX,journalArticle,2020,Adnan Darwiche,Three Modern Roles for Logic in AI,,,,10.1145/3375395.3389131,http://arxiv.org/abs/2004.08599v1,"We consider three modern roles for logic in artificial intelligence, which
are based on the theory of tractable Boolean circuits: (1) logic as a basis for
computation, (2) logic for learning from a combination of data and knowledge,
and (3) logic for reasoning about the behavior of machine learning systems.",2022-11-13 15:58:11
XXX,journalArticle,2021,Roman V. Yampolskiy,AI Risk Skepticism,,,,,http://arxiv.org/abs/2105.02704v3,"In this work, we survey skepticism regarding AI risk and show parallels with
other types of scientific skepticism. We start by classifying different types
of AI Risk skepticism and analyze their root causes. We conclude by suggesting
some intervention approaches, which may be successful in reducing AI risk
skepticism, at least amongst artificial intelligence researchers.",2022-11-13 15:58:12
XXX,journalArticle,2021,"Ajay Krishnan, Niranj Jyothish, Xun Jia",Using reinforcement learning to design an AI assistantfor a satisfying co-op experience,,,,,http://arxiv.org/abs/2105.03414v1,"In this project, we designed an intelligent assistant player for the
single-player game Space Invaders with the aim to provide a satisfying co-op
experience. The agent behaviour was designed using reinforcement learning
techniques and evaluated based on several criteria. We validate the hypothesis
that an AI-driven computer player can provide a satisfying co-op experience.",2022-11-13 15:58:12
XXX,journalArticle,2021,"Domonkos Czifra, Endre Csóka, Zsolt Zombori, Géza Makay",Towards solving the 7-in-a-row game,,,,,http://arxiv.org/abs/2107.05363v1,"Our paper explores the game theoretic value of the 7-in-a-row game. We reduce
the problem to solving a finite board game, which we target using Proof Number
Search. We present a number of heuristic improvements to Proof Number Search
and examine their effect within the context of this particular game. Although
our paper does not solve the 7-in-a-row game, our experiments indicate that we
have made significant progress towards it.",2022-11-13 15:58:13
XXX,journalArticle,2022,"Jinxin Ding, Yuxin Huang, Keyang Ni, Xueyao Wang, Yinxiao Wang, Yucheng Wang",Intellectual Property Evaluation Utilizing Machine Learning,,,,,http://arxiv.org/abs/2208.08611v1,"Intellectual properties is increasingly important in the economic
development. To solve the pain points by traditional methods in IP evaluation,
we are developing a new technology with machine learning as the core. We have
built an online platform and will expand our business in the Greater Bay Area
with plans.",2022-11-13 15:58:13
XXX,journalArticle,1995,"C. G. Giraud-Carrier, T. R. Martinez",An Integrated Framework for Learning and Reasoning,"Journal of Artificial Intelligence Research, Vol 3, (1995),
  147-185",,,,http://arxiv.org/abs/cs/9508102v1,"Learning and reasoning are both aspects of what is considered to be
intelligence. Their studies within AI have been separated historically,
learning being the topic of machine learning and neural networks, and reasoning
falling under classical (or symbolic) AI. However, learning and reasoning are
in many ways interdependent. This paper discusses the nature of some of these
interdependencies and proposes a general framework called FLARE, that combines
inductive learning using prior knowledge together with reasoning in a
propositional setting. Several examples that test the framework are presented,
including classical induction, many important reasoning protocols and two
simple expert systems.",2022-11-13 15:58:14
XXX,journalArticle,2003,"Ulla Bergsten, Johan Schubert, Per Svensson",Beslutstödssystemet Dezzy - en översikt,"in Dokumentation 7 juni av Seminarium och fackutst\""allning om
  samband, sensorer och datorer f\""or ledningssystem till f\""orsvaret
  (MILINF'89), pp. 07B2:19-31, Enk\""oping, June 1989, Telub AB, V\""axj\""o, 1989",,,,http://arxiv.org/abs/cs/0305033v1,"Within the scope of the three-year ANTI-SUBMARINE WARFARE project of the
National Defence Research Establishment, the INFORMATION SYSTEMS subproject has
developed the demonstration prototype Dezzy for handling and analysis of
intelligence reports concerning foreign underwater activities.
  -----
  Inom ramen f\""or FOA:s tre{\aa}riga huvudprojekt UB{\AA}TSSKYDD har
delprojekt INFORMATIONSSYSTEM utvecklat demonstrationsprototypen Dezzy till ett
beslutsst\""odsystem f\""or hantering och analys av underr\""attelser om
fr\""ammande undervattensverksamhet.",2022-11-13 15:58:14
XXX,journalArticle,2003,"Joseph Y. Halpern, Riccardo Pucella",A logic for reasoning about upper probabilities,"Journal of AI Research 17, 2001, pp. 57-81",,,,http://arxiv.org/abs/cs/0307069v1,"We present a propositional logic %which can be used to reason about the
uncertainty of events, where the uncertainty is modeled by a set of probability
measures assigning an interval of probability to each event. We give a sound
and complete axiomatization for the logic, and show that the satisfiability
problem is NP-complete, no harder than satisfiability for propositional logic.",2022-11-13 15:58:15
XXX,journalArticle,2003,"Marcello Balduccini, Michael Gelfond",Diagnostic reasoning with A-Prolog,TPLP Vol 3(4&5) (2003) 425-461,,,,http://arxiv.org/abs/cs/0312040v1,"In this paper we suggest an architecture for a software agent which operates
a physical device and is capable of making observations and of testing and
repairing the device's components. We present simplified definitions of the
notions of symptom, candidate diagnosis, and diagnosis which are based on the
theory of action language ${\cal AL}$. The definitions allow one to give a
simple account of the agent's behavior in which many of the agent's tasks are
reduced to computing stable models of logic programs.",2022-11-13 15:58:15
XXX,journalArticle,2006,"Shane Legg, Marcus Hutter",A Formal Measure of Machine Intelligence,"Proc. 15th Annual Machine Learning Conference of {B}elgium and The
  Netherlands (Benelearn 2006) pages 73-80",,,,http://arxiv.org/abs/cs/0605024v1,"A fundamental problem in artificial intelligence is that nobody really knows
what intelligence is. The problem is especially acute when we need to consider
artificial systems which are significantly different to humans. In this paper
we approach this problem in the following way: We take a number of well known
informal definitions of human intelligence that have been given by experts, and
extract their essential features. These are then mathematically formalised to
produce a general measure of intelligence for arbitrary machines. We believe
that this measure formally captures the concept of machine intelligence in the
broadest reasonable sense.",2022-11-13 15:58:16
XXX,journalArticle,2009,"Eugen Staab, Martin Caminada",Assessing the Impact of Informedness on a Consultant's Profit,,,,,http://arxiv.org/abs/0909.0901v1,"We study the notion of informedness in a client-consultant setting. Using a
software simulator, we examine the extent to which it pays off for consultants
to provide their clients with advice that is well-informed, or with advice that
is merely meant to appear to be well-informed. The latter strategy is
beneficial in that it costs less resources to keep up-to-date, but carries the
risk of a decreased reputation if the clients discover the low level of
informedness of the consultant. Our experimental results indicate that under
different circumstances, different strategies yield the optimal results (net
profit) for the consultants.",2022-11-13 15:58:16
XXX,journalArticle,2011,"J. E. Laird, R. E. Wray",An Architectural Approach to Ensuring Consistency in Hierarchical Execution,"Journal Of Artificial Intelligence Research, Volume 19, pages
  355-398, 2003",,,10.1613/jair.1142,http://arxiv.org/abs/1106.4871v1,"Hierarchical task decomposition is a method used in many agent systems to
organize agent knowledge. This work shows how the combination of a hierarchy
and persistent assertions of knowledge can lead to difficulty in maintaining
logical consistency in asserted knowledge. We explore the problematic
consequences of persistent assumptions in the reasoning process and introduce
novel potential solutions. Having implemented one of the possible solutions,
Dynamic Hierarchical Justification, its effectiveness is demonstrated with an
empirical analysis.",2022-11-13 15:58:17
XXX,journalArticle,2012,"Mauricio Araya, Olivier Buffet, Vincent Thomas",Near-Optimal BRL using Optimistic Local Transitions,,,,,http://arxiv.org/abs/1206.4613v1,"Model-based Bayesian Reinforcement Learning (BRL) allows a found
formalization of the problem of acting optimally while facing an unknown
environment, i.e., avoiding the exploration-exploitation dilemma. However,
algorithms explicitly addressing BRL suffer from such a combinatorial explosion
that a large body of work relies on heuristic algorithms. This paper introduces
BOLT, a simple and (almost) deterministic heuristic algorithm for BRL which is
optimistic about the transition function. We analyze BOLT's sample complexity,
and show that under certain parameters, the algorithm is near-optimal in the
Bayesian sense with high probability. Then, experimental results highlight the
key differences of this method compared to previous work.",2022-11-13 15:58:17
XXX,journalArticle,2012,"Nitin Yadav, Sebastian Sardina",Reasoning about Agent Programs using ATL-like Logics,"In Proceedings of the European Conference on Logics in Artificial
  Intelligence (JELIA), volume 7519 of LNCS, pages 437-449, 2012",,,,http://arxiv.org/abs/1207.3874v1,"We propose a variant of Alternating-time Temporal Logic (ATL) grounded in the
agents' operational know-how, as defined by their libraries of abstract plans.
Inspired by ATLES, a variant itself of ATL, it is possible in our logic to
explicitly refer to ""rational"" strategies for agents developed under the
Belief-Desire-Intention agent programming paradigm. This allows us to express
and verify properties of BDI systems using ATL-type logical frameworks.",2022-11-13 15:58:18
XXX,journalArticle,2012,Michael Maher,Relative Expressiveness of Defeasible Logics,"Theory and Practice of Logic Programming 12 (4-5), 793--810, 2012",,,,http://arxiv.org/abs/1210.1785v1,"We address the relative expressiveness of defeasible logics in the framework
DL. Relative expressiveness is formulated as the ability to simulate the
reasoning of one logic within another logic. We show that such simulations must
be modular, in the sense that they also work if applied only to part of a
theory, in order to achieve a useful notion of relative expressiveness. We
present simulations showing that logics in DL with and without the capability
of team defeat are equally expressive. We also show that logics that handle
ambiguity differently -- ambiguity blocking versus ambiguity propagating --
have distinct expressiveness, with neither able to simulate the other under a
different formulation of expressiveness.",2022-11-13 15:58:19
XXX,journalArticle,2012,"Antonio Pisasale, Domenico Cantone",An Experiment on the Connection between the DLs' Family DL<ForAllPiZero> and the Real World,,,,,http://arxiv.org/abs/1211.4957v2,"This paper describes the analysis of a selected testbed of Semantic Web
ontologies, by a SPARQL query, which determines those ontologies that can be
related to the description logic DL<ForAllPiZero>, introduced in [4] and
studied in [9]. We will see that a reasonable number of them is expressible
within such computationally efficient language. We expect that, in a long-term
view, a temporalization of description logics, and consequently, of OWL(2), can
open new perspectives for the inclusion in this language of a greater number of
ontologies of the testbed and, hopefully, of the ""real world"".",2022-11-13 15:58:20
XXX,journalArticle,2013,"Paul J. Krause, John Fox, Philip Judson",Is There a Role for Qualitative Risk Assessment?,,,,,http://arxiv.org/abs/1302.4970v1,"Classically, risk is characterized by a point value probability indicating
the likelihood of occurrence of an adverse effect. However, there are domains
where the attainability of objective numerical risk characterizations is
increasingly being questioned. This paper reviews the arguments in favour of
extending classical techniques of risk assessment to incorporate meaningful
qualitative and weak quantitative risk characterizations. A technique in which
linguistic uncertainty terms are defined in terms of patterns of argument is
then proposed. The technique is demonstrated using a prototype computer-based
system for predicting the carcinogenic risk due to novel chemical compounds.",2022-11-13 15:58:20
XXX,journalArticle,2013,Michael Pittarelli,Anytime Decision Making with Imprecise Probabilities,,,,,http://arxiv.org/abs/1302.6837v1,"This paper examines methods of decision making that are able to accommodate
limitations on both the form in which uncertainty pertaining to a decision
problem can be realistically represented and the amount of computing time
available before a decision must be made. The methods are anytime algorithms in
the sense of Boddy and Dean 1991. Techniques are presented for use with Frisch
and Haddawy's [1992] anytime deduction system, with an anytime adaptation of
Nilsson's [1986] probabilistic logic, and with a probabilistic database model.",2022-11-13 15:58:21
XXX,journalArticle,2013,"Lisa J. Burnell, Eric J. Horvitz",A Synthesis of Logical and Probabilistic Reasoning for Program Understanding and Debugging,,,,,http://arxiv.org/abs/1303.1488v1,"We describe the integration of logical and uncertain reasoning methods to
identify the likely source and location of software problems. To date, software
engineers have had few tools for identifying the sources of error in complex
software packages. We describe a method for diagnosing software problems
through combining logical and uncertain reasoning analyses. Our preliminary
results suggest that such methods can be of value in directing the attention of
software engineers to paths of an algorithm that have the highest likelihood of
harboring a programming error.",2022-11-13 15:58:21
XXX,journalArticle,2013,Paul E. Lehner,Inference Policies,,,,,http://arxiv.org/abs/1304.1516v1,"It is suggested that an AI inference system should reflect an inference
policy that is tailored to the domain of problems to which it is applied -- and
furthermore that an inference policy need not conform to any general theory of
rational inference or induction. We note, for instance, that Bayesian reasoning
about the probabilistic characteristics of an inference domain may result in
the specification of an nonBayesian procedure for reasoning within the
inference domain. In this paper, the idea of an inference policy is explored in
some detail. To support this exploration, the characteristics of some standard
and nonstandard inference policies are examined.",2022-11-13 15:58:22
XXX,journalArticle,2013,David Heckerman,An Empirical Comparison of Three Inference Methods,,,,,http://arxiv.org/abs/1304.2357v2,"In this paper, an empirical evaluation of three inference methods for
uncertain reasoning is presented in the context of Pathfinder, a large expert
system for the diagnosis of lymph-node pathology. The inference procedures
evaluated are (1) Bayes' theorem, assuming evidence is conditionally
independent given each hypothesis; (2) odds-likelihood updating, assuming
evidence is conditionally independent given each hypothesis and given the
negation of each hypothesis; and (3) a inference method related to the
Dempster-Shafer theory of belief. Both expert-rating and decision-theoretic
metrics are used to compare the diagnostic accuracy of the inference methods.",2022-11-13 15:58:23
XXX,journalArticle,2013,Daniel Hunter,Dempster-Shafer vs. Probabilistic Logic,,,,,http://arxiv.org/abs/1304.2713v1,"The combination of evidence in Dempster-Shafer theory is compared with the
combination of evidence in probabilistic logic. Sufficient conditions are
stated for these two methods to agree. It is then shown that these conditions
are minimal in the sense that disagreement can occur when any one of them is
removed. An example is given in which the traditional assumption of conditional
independence of evidence on hypotheses holds and a uniform prior is assumed,
but probabilistic logic and Dempster's rule give radically different results
for the combination of two evidence events.",2022-11-13 15:58:23
XXX,journalArticle,2013,"John S. Breese, Edison Tse",Integrating Logical and Probabilistic Reasoning for Decision Making,,,,,http://arxiv.org/abs/1304.2751v1,"We describe a representation and a set of inference methods that combine
logic programming techniques with probabilistic network representations for
uncertainty (influence diagrams). The techniques emphasize the dynamic
construction and solution of probabilistic and decision-theoretic models for
complex and uncertain domains. Given a query, a logical proof is produced if
possible; if not, an influence diagram based on the query and the knowledge of
the decision domain is produced and subsequently solved. A uniform declarative,
first-order, knowledge representation is combined with a set of integrated
inference procedures for logical, probabilistic, and decision-theoretic
reasoning.",2022-11-13 15:58:24
XXX,journalArticle,2013,Dimiter Dobrev,Giving the AI definition a form suitable for the engineer,,,,,http://arxiv.org/abs/1312.5713v2,"Artificial Intelligence - what is this? That is the question! In earlier
papers we already gave a formal definition for AI, but if one desires to build
an actual AI implementation, the following issues require attention and are
treated here: the data format to be used, the idea of Undef and Nothing
symbols, various ways for defining the ""meaning of life"", and finally, a new
notion of ""incorrect move"". These questions are of minor importance in the
theoretical discussion, but we already know the answer of the question ""Does AI
exist?"" Now we want to make the next step and to create this program.",2022-11-13 15:58:24
XXX,journalArticle,2014,"Joseph Y. Halpern, Riccardo Pucella",A Logic for Reasoning about Upper Probabilities,,,,,http://arxiv.org/abs/1408.1485v1,"We present a propositional logic to reason about the uncertainty of events,
where the uncertainty is modeled by a set of probability measures assigning an
interval of probability to each event. We give a sound and complete
axiomatization for the logic, and show that the satisfiability problem is
NP-complete, no harder than satisfiability for propositional logic.",2022-11-13 15:58:25
XXX,journalArticle,2015,"Nate Soares, Benja Fallenstein",Toward Idealized Decision Theory,,,,,http://arxiv.org/abs/1507.01986v1,"This paper motivates the study of decision theory as necessary for aligning
smarter-than-human artificial systems with human interests. We discuss the
shortcomings of two standard formulations of decision theory, and demonstrate
that they cannot be used to describe an idealized decision procedure suitable
for approximation by artificial systems. We then explore the notions of policy
selection and logical counterfactuals, two recent insights into decision theory
that point the way toward promising paths for future research.",2022-11-13 15:58:26
XXX,journalArticle,2016,Amnon H. Eden,"The Singularity Controversy, Part I: Lessons Learned and Open Questions: Conclusions from the Battle on the Legitimacy of the Debate",,,,10.13140/RG.2.1.3416.6809,http://arxiv.org/abs/1601.05977v2,"This report seeks to inform policy makers on the nature and the merit of the
arguments for and against the concerns associated with a potential
technological singularity.
  Part I describes the lessons learned from our investigation of the subject,
separating the argu-ments of merit from the fallacies and misconceptions that
confuse the debate and undermine its rational resolution.",2022-11-13 15:58:26
XXX,journalArticle,2016,"Stuart Russell, Daniel Dewey, Max Tegmark",Research Priorities for Robust and Beneficial Artificial Intelligence,AI Magazine 36:4 (2015),,,,http://arxiv.org/abs/1602.03506v1,"Success in the quest for artificial intelligence has the potential to bring
unprecedented benefits to humanity, and it is therefore worthwhile to
investigate how to maximize these benefits while avoiding potential pitfalls.
This article gives numerous examples (which should by no means be construed as
an exhaustive list) of such worthwhile research aimed at ensuring that AI
remains robust and beneficial.",2022-11-13 15:58:27
XXX,journalArticle,2016,"Carissa Schoenick, Peter Clark, Oyvind Tafjord, Peter Turney, Oren Etzioni",Moving Beyond the Turing Test with the Allen AI Science Challenge,,,,,http://arxiv.org/abs/1604.04315v3,"Given recent successes in AI (e.g., AlphaGo's victory against Lee Sedol in
the game of GO), it's become increasingly important to assess: how close are AI
systems to human-level intelligence? This paper describes the Allen AI Science
Challenge---an approach towards that goal which led to a unique Kaggle
Competition, its results, the lessons learned, and our next steps.",2022-11-13 15:58:28
XXX,journalArticle,2016,"Tom Everitt, Marcus Hutter",Avoiding Wireheading with Value Reinforcement Learning,,,,,http://arxiv.org/abs/1605.03143v1,"How can we design good goals for arbitrarily intelligent agents?
Reinforcement learning (RL) is a natural approach. Unfortunately, RL does not
work well for generally intelligent agents, as RL agents are incentivised to
shortcut the reward sensor for maximum reward -- the so-called wireheading
problem. In this paper we suggest an alternative to RL called value
reinforcement learning (VRL). In VRL, agents use the reward signal to learn a
utility function. The VRL setup allows us to remove the incentive to wirehead
by placing a constraint on the agent's actions. The constraint is defined in
terms of the agent's belief distributions, and does not require an explicit
specification of which actions constitute wireheading.",2022-11-13 15:58:28
XXX,journalArticle,2017,Sebastian Benthall,Don't Fear the Reaper: Refuting Bostrom's Superintelligence Argument,,,,,http://arxiv.org/abs/1702.08495v2,"In recent years prominent intellectuals have raised ethical concerns about
the consequences of artificial intelligence. One concern is that an autonomous
agent might modify itself to become ""superintelligent"" and, in supremely
effective pursuit of poorly specified goals, destroy all of humanity. This
paper considers and rejects the possibility of this outcome. We argue that this
scenario depends on an agent's ability to rapidly improve its ability to
predict its environment through self-modification. Using a Bayesian model of a
reasoning agent, we show that there are important limitations to how an agent
may improve its predictive ability through self-modification alone. We conclude
that concern about this artificial intelligence outcome is misplaced and better
directed at policy questions around data access and storage.",2022-11-13 15:58:29
XXX,journalArticle,2017,Christopher A. Tucker,A proposal for ethically traceable artificial intelligence,,,,,http://arxiv.org/abs/1703.01908v2,"Although the problem of a critique of robotic behavior in near-unanimous
agreement to human norms seems intractable, a starting point of such an
ambition is a framework of the collection of knowledge a priori and experience
a posteriori categorized as a set of synthetical judgments available to the
intelligence, translated into computer code. If such a proposal were
successful, an algorithm with ethically traceable behavior and cogent
equivalence to human cognition is established. This paper will propose the
application of Kant's critique of reason to current programming constructs of
an autonomous intelligent system.",2022-11-13 15:58:29
XXX,journalArticle,2017,"Stuart Armstrong, Benjamin Levinstein",Low Impact Artificial Intelligences,,,,,http://arxiv.org/abs/1705.10720v1,"There are many goals for an AI that could become dangerous if the AI becomes
superintelligent or otherwise powerful. Much work on the AI control problem has
been focused on constructing AI goals that are safe even for such AIs. This
paper looks at an alternative approach: defining a general concept of `low
impact'. The aim is to ensure that a powerful AI which implements low impact
will not modify the world extensively, even if it is given a simple or
dangerous goal. The paper proposes various ways of defining and grounding low
impact, and discusses methods for ensuring that the AI can still be allowed to
have a (desired) impact despite the restriction. The end of the paper addresses
known issues with this approach and avenues for future research.",2022-11-13 15:58:30
XXX,journalArticle,2017,Virginia Dignum,Responsible Autonomy,,,,,http://arxiv.org/abs/1706.02513v1,"As intelligent systems are increasingly making decisions that directly affect
society, perhaps the most important upcoming research direction in AI is to
rethink the ethical implications of their actions. Means are needed to
integrate moral, societal and legal values with technological developments in
AI, both during the design process as well as part of the deliberation
algorithms employed by these systems. In this paper, we describe leading ethics
theories and propose alternative ways to ensure ethical behavior by artificial
systems. Given that ethics are dependent on the socio-cultural context and are
often only implicit in deliberation processes, methodologies are needed to
elicit the values held by designers and stakeholders, and to make these
explicit leading to better understanding and trust on artificial autonomous
systems.",2022-11-13 15:58:30
XXX,journalArticle,2017,"James Babcock, Janos Kramar, Roman V. Yampolskiy",Guidelines for Artificial Intelligence Containment,,,,,http://arxiv.org/abs/1707.08476v1,"With almost daily improvements in capabilities of artificial intelligence it
is more important than ever to develop safety software for use by the AI
research community. Building on our previous work on AI Containment Problem we
propose a number of guidelines which should help AI safety researchers to
develop reliable sandboxing software for intelligent programs of all levels.
Such safety container software will make it possible to study and analyze
intelligent artificial agent while maintaining certain level of safety against
information leakage, social engineering attacks and cyberattacks from within
the container.",2022-11-13 15:58:31
XXX,journalArticle,2017,"Pavel Naumov, Jia Tao",Together We Know How to Achieve: An Epistemic Logic of Know-How (Extended Abstract),"EPTCS 251, 2017, pp. 441-453",,,10.4204/EPTCS.251.32,http://arxiv.org/abs/1707.08759v1,"The existence of a coalition strategy to achieve a goal does not necessarily
mean that the coalition has enough information to know how to follow the
strategy. Neither does it mean that the coalition knows that such a strategy
exists. The paper studies an interplay between the distributed knowledge,
coalition strategies, and coalition ""know-how"" strategies. The main technical
result is a sound and complete trimodal logical system that describes the
properties of this interplay.",2022-11-13 15:58:31
XXX,journalArticle,2017,"Robert B. Allen, Eunsang Yang, Tatsawan Timakum",A Foundry of Human Activities and Infrastructures,,,,,http://arxiv.org/abs/1711.01927v1,"Direct representation knowledgebases can enhance and even provide an
alternative to document-centered digital libraries. Here we consider realist
semantic modeling of everyday activities and infrastructures in such
knowledgebases. Because we want to integrate a wide variety of topics, a
collection of ontologies (a foundry) and a range of other knowledge resources
are needed. We first consider modeling the routine procedures that support
human activities and technologies. Next, we examine the interactions of
technologies with aspects of social organization. Then, we consider approaches
and issues for developing and validating explanations of the relationships
among various entities.",2022-11-13 15:58:32
XXX,journalArticle,2017,"Yueh-Hua Wu, Shou-De Lin",A Low-Cost Ethics Shaping Approach for Designing Reinforcement Learning Agents,,,,,http://arxiv.org/abs/1712.04172v2,"This paper proposes a low-cost, easily realizable strategy to equip a
reinforcement learning (RL) agent the capability of behaving ethically. Our
model allows the designers of RL agents to solely focus on the task to achieve,
without having to worry about the implementation of multiple trivial ethical
patterns to follow. Based on the assumption that the majority of human
behavior, regardless which goals they are achieving, is ethical, our design
integrates human policy with the RL policy to achieve the target objective with
less chance of violating the ethical code that human beings normally obey.",2022-11-13 15:58:32
XXX,journalArticle,2018,Atrisha Sarkar,A Brandom-ian view of Reinforcement Learning towards strong-AI,,,,,http://arxiv.org/abs/1803.02912v1,"The analytic philosophy of Robert Brandom, based on the ideas of pragmatism,
paints a picture of sapience, through inferentialism. In this paper, we present
a theory, that utilizes essential elements of Brandom's philosophy, towards the
objective of achieving strong-AI. We do this by connecting the constitutive
elements of reinforcement learning and the Game Of Giving and Asking For
Reasons. Further, following Brandom's prescriptive thoughts, we restructure the
popular reinforcement learning algorithm A3C, and show that RL algorithms can
be tuned towards the objective of strong-AI.",2022-11-13 15:58:33
XXX,journalArticle,2018,"Christoph Benzmüller, Xavier Parent",First Experiments with a Flexible Infrastructure for Normative Reasoning,,,,,http://arxiv.org/abs/1804.02929v1,"A flexible infrastructure for normative reasoning is outlined. A small-scale
demonstrator version of the envisioned system has been implemented in the proof
assistant Isabelle/HOL by utilising the first authors universal logical
reasoning approach based on shallow semantical embeddings in meta-logic HOL.
The need for such a flexible reasoning infrastructure is motivated and
illustrated with a contrary-to-duty example scenario selected from the General
Data Protection Regulation.",2022-11-13 15:58:34
XXX,journalArticle,2018,Dimiter Dobrev,The IQ of Artificial Intelligence,"Serdica Journal of Computing, Vol. 13, Number 1-2, 2019, pp.41-70",,,,http://arxiv.org/abs/1806.04915v1,"All it takes to identify the computer programs which are Artificial
Intelligence is to give them a test and award AI to those that pass the test.
Let us say that the scores they earn at the test will be called IQ. We cannot
pinpoint a minimum IQ threshold that a program has to cover in order to be AI,
however, we will choose a certain value. Thus, our definition for AI will be
any program the IQ of which is above the chosen value. While this idea has
already been implemented in [3], here we will revisit this construct in order
to introduce certain improvements.",2022-11-13 15:58:34
XXX,journalArticle,2018,Daniele Funaro,Understanding the Meaning of Understanding,,,,,http://arxiv.org/abs/1806.05234v2,"Can we train a machine to detect if another machine has understood a concept?
In principle, this is possible by conducting tests on the subject of that
concept. However we want this procedure to be done by avoiding direct
questions. In other words, we would like to isolate the absolute meaning of an
abstract idea by putting it into a class of equivalence, hence without adopting
straight definitions or showing how this idea ""works"" in practice. We discuss
the metaphysical implications hidden in the above question, with the aim of
providing a plausible reference framework.",2022-11-13 15:58:35
XXX,journalArticle,2018,John Hooker,Truly Autonomous Machines Are Ethical,,,,,http://arxiv.org/abs/1812.02217v1,"While many see the prospect of autonomous machines as threatening, autonomy
may be exactly what we want in a superintelligent machine. There is a sense of
autonomy, deeply rooted in the ethical literature, in which an autonomous
machine is necessarily an ethical one. Development of the theory underlying
this idea not only reveals the advantages of autonomy, but it sheds light on a
number of issues in the ethics of artificial intelligence. It helps us to
understand what sort of obligations we owe to machines, and what obligations
they owe to us. It clears up the issue of assigning responsibility to machines
or their creators. More generally, a concept of autonomy that is adequate to
both human and artificial intelligence can lead to a more adequate ethical
theory for both.",2022-11-13 15:58:35
XXX,journalArticle,2018,Tshilidzi Marwala,The limit of artificial intelligence: Can machines be rational?,,,,,http://arxiv.org/abs/1812.06510v1,"This paper studies the question on whether machines can be rational. It
observes the existing reasons why humans are not rational which is due to
imperfect and limited information, limited and inconsistent processing power
through the brain and the inability to optimize decisions and achieve maximum
utility. It studies whether these limitations of humans are transferred to the
limitations of machines. The conclusion reached is that even though machines
are not rational advances in technological developments make these machines
more rational. It also concludes that machines can be more rational than
humans.",2022-11-13 15:58:36
XXX,journalArticle,2019,"Jobst Landgrebe, Barry Smith",Making AI meaningful again,,,,,http://arxiv.org/abs/1901.02918v3,"Artificial intelligence (AI) research enjoyed an initial period of enthusiasm
in the 1970s and 80s. But this enthusiasm was tempered by a long interlude of
frustration when genuinely useful AI applications failed to be forthcoming.
Today, we are experiencing once again a period of enthusiasm, fired above all
by the successes of the technology of deep neural networks or deep machine
learning. In this paper we draw attention to what we take to be serious
problems underlying current views of artificial intelligence encouraged by
these successes, especially in the domain of language processing. We then show
an alternative approach to language-centric AI, in which we identify a role for
philosophy.",2022-11-13 15:58:37
XXX,journalArticle,2019,"Risto Miikkulainen, Bret Greenstein, Babak Hodjat, Jerry Smith",Better Future through AI: Avoiding Pitfalls and Guiding AI Towards its Full Potential,,,,,http://arxiv.org/abs/1905.13178v1,"Artificial Intelligence (AI) technology is rapidly changing many areas of
society. While there is tremendous potential in this transition, there are
several pitfalls as well. Using the history of computing and the world-wide web
as a guide, in this article we identify those pitfalls and actions that lead AI
development to its full potential. If done right, AI will be instrumental in
achieving the goals we set for economy, society, and the world in general.",2022-11-13 15:58:37
XXX,journalArticle,2019,"Arthur Szlam, Jonathan Gray, Kavya Srinet, Yacine Jernite, Armand Joulin, Gabriel Synnaeve, Douwe Kiela, Haonan Yu, Zhuoyuan Chen, Siddharth Goyal, Demi Guo, Danielle Rothermel, C. Lawrence Zitnick, Jason Weston",Why Build an Assistant in Minecraft?,,,,,http://arxiv.org/abs/1907.09273v2,"In this document we describe a rationale for a research program aimed at
building an open ""assistant"" in the game Minecraft, in order to make progress
on the problems of natural language understanding and learning from dialogue.",2022-11-13 15:58:38
XXX,journalArticle,2019,Vaishak Belle,The Quest for Interpretable and Responsible Artificial Intelligence,,,,,http://arxiv.org/abs/1910.04527v1,"Artificial Intelligence (AI) provides many opportunities to improve private
and public life. Discovering patterns and structures in large troves of data in
an automated manner is a core component of data science, and currently drives
applications in computational biology, finance, law and robotics. However, such
a highly positive impact is coupled with significant challenges: How do we
understand the decisions suggested by these systems in order that we can trust
them? How can they be held accountable for those decisions?
  In this short survey, we cover some of the motivations and trends in the area
that attempt to address such questions.",2022-11-13 15:58:38
XXX,journalArticle,2020,"Constantine Goulimis, Gastón Simone",Can ML predict the solution value for a difficult combinatorial problem?,,,,,http://arxiv.org/abs/2003.03181v1,"We look at whether machine learning can predict the final objective function
value of a difficult combinatorial optimisation problem from the input. Our
context is the pattern reduction problem, one industrially important but
difficult aspect of the cutting stock problem. Machine learning appears to have
higher prediction accuracy than a na\""ive model, reducing mean absolute
percentage error (MAPE) from 12.0% to 8.7%.",2022-11-13 15:58:39
XXX,journalArticle,2020,"Evan Piermont, Peio Zuazo-Garin",Failures of Contingent Thinking,,,,,http://arxiv.org/abs/2007.07703v1,"In this paper, we provide a theoretical framework to analyze an agent who
misinterprets or misperceives the true decision problem she faces. Within this
framework, we show that a wide range of behavior observed in experimental
settings manifest as failures to perceive implications, in other words, to
properly account for the logical relationships between various payoff relevant
contingencies. We present behavioral characterizations corresponding to several
benchmarks of logical sophistication and show how it is possible to identify
which implications the agent fails to perceive. Thus, our framework delivers
both a methodology for assessing an agent's level of contingent thinking and a
strategy for identifying her beliefs in the absence full rationality.",2022-11-13 15:58:39
XXX,journalArticle,2020,"Vinod Muthusamy, Merve Unuvar, Hagen Völzer, Justin D. Weisz",Do's and Don'ts for Human and Digital Worker Integration,,,,,http://arxiv.org/abs/2010.07738v1,"Robotic process automation (RPA) and its next evolutionary stage, intelligent
process automation, promise to drive improvements in efficiencies and process
outcomes. However, how can business leaders evaluate how to integrate
intelligent automation into business processes? What is an appropriate division
of labor between humans and machines? How should combined human-AI teams be
evaluated? For RPA, often the human labor cost and the robotic labor cost are
directly compared to make an automation decision. In this position paper, we
argue for a broader view that incorporates the potential for multiple levels of
autonomy and human involvement, as well as a wider range of metrics beyond
productivity when integrating digital workers into a business process",2022-11-13 15:58:40
XXX,journalArticle,2021,Joar Skalse,A General Counterexample to Any Decision Theory and Some Responses,,,,,http://arxiv.org/abs/2101.00280v1,"In this paper I present an argument and a general schema which can be used to
construct a problem case for any decision theory, in a way that could be taken
to show that one cannot formulate a decision theory that is never outperformed
by any other decision theory. I also present and discuss a number of possible
responses to this argument. One of these responses raises the question of what
it means for two decision problems to be ""equivalent"" in the relevant sense,
and gives an answer to this question which would invalidate the first argument.
However, this position would have further consequences for how we compare
different decision theories in decision problems already discussed in the
literature (including e.g. Newcomb's problem).",2022-11-13 15:58:41
XXX,journalArticle,2021,"Zachary Kenton, Tom Everitt, Laura Weidinger, Iason Gabriel, Vladimir Mikulik, Geoffrey Irving",Alignment of Language Agents,,,,,http://arxiv.org/abs/2103.14659v1,"For artificial intelligence to be beneficial to humans the behaviour of AI
agents needs to be aligned with what humans want. In this paper we discuss some
behavioural issues for language agents, arising from accidental
misspecification by the system designer. We highlight some ways that
misspecification can occur and discuss some behavioural issues that could arise
from misspecification, including deceptive or manipulative language, and review
some approaches for avoiding these issues.",2022-11-13 15:58:41
XXX,journalArticle,2021,Bin Liu,"""Weak AI"" is Likely to Never Become ""Strong AI"", So What is its Greatest Value for us?",,,,,http://arxiv.org/abs/2103.15294v1,"AI has surpassed humans across a variety of tasks such as image
classification, playing games (e.g., go, ""Starcraft"" and poker), and protein
structure prediction. However, at the same time, AI is also bearing serious
controversies. Many researchers argue that little substantial progress has been
made for AI in recent decades. In this paper, the author (1) explains why
controversies about AI exist; (2) discriminates two paradigms of AI research,
termed ""weak AI"" and ""strong AI"" (a.k.a. artificial general intelligence); (3)
clarifies how to judge which paradigm a research work should be classified
into; (4) discusses what is the greatest value of ""weak AI"" if it has no chance
to develop into ""strong AI"".",2022-11-13 15:58:42
XXX,journalArticle,2021,"Lu Cheng, Ahmadreza Mosallanezhad, Paras Sheth, Huan Liu",Causal Learning for Socially Responsible AI,,,,,http://arxiv.org/abs/2104.12278v2,"There have been increasing concerns about Artificial Intelligence (AI) due to
its unfathomable potential power. To make AI address ethical challenges and
shun undesirable outcomes, researchers proposed to develop socially responsible
AI (SRAI). One of these approaches is causal learning (CL). We survey
state-of-the-art methods of CL for SRAI. We begin by examining the seven CL
tools to enhance the social responsibility of AI, then review how existing
works have succeeded using these tools to tackle issues in developing SRAI such
as fairness. The goal of this survey is to bring forefront the potentials and
promises of CL for SRAI.",2022-11-13 15:58:42
XXX,journalArticle,2021,Leopoldo Bertossi,"Reasoning about Counterfactuals and Explanations: Problems, Results and Directions",,,,,http://arxiv.org/abs/2108.11004v1,"There are some recent approaches and results about the use of answer-set
programming for specifying counterfactual interventions on entities under
classification, and reasoning about them. These approaches are flexible and
modular in that they allow the seamless addition of domain knowledge. Reasoning
is enabled by query answering from the answer-set program. The programs can be
used to specify and compute responsibility-based numerical scores as
attributive explanations for classification results.",2022-11-13 15:58:43
XXX,journalArticle,2022,"Florian Ellsaesser, Guido Fioretti",Deciding Not To Decide,,,,,http://arxiv.org/abs/2201.05818v1,"Sometimes unexpected, novel, unconceivable events enter our lives. The
cause-effect mappings that usually guide our behaviour are destroyed. Surprised
and shocked by possibilities that we had never imagined, we are unable to make
any decision beyond mere routine. Among them there are decisions, such as
making investments, that are essential for the long-term survival of businesses
as well as the economy at large. We submit that the standard machinery of
utility maximization does not apply, but we propose measures inspired by
scenario planning and graph analysis, pointing to solutions being explored in
machine learning.",2022-11-13 15:58:43
XXX,journalArticle,2022,"Harald Rueß, Simon Burton",Safe AI -- How is this Possible?,,,,,http://arxiv.org/abs/2201.10436v2,"Ttraditional safety engineering is coming to a turning point moving from
deterministic, non-evolving systems operating in well-defined contexts to
increasingly autonomous and learning-enabled AI systems which are acting in
largely unpredictable operating contexts. We outline some of underlying
challenges of safe AI and suggest a rigorous engineering framework for
minimizing uncertainty, thereby increasing confidence, up to tolerable levels,
in the safe behavior of AI systems.",2022-11-13 15:58:44
XXX,journalArticle,2022,"Olivia Brown, Brad Dillman",Proceedings of the Robust Artificial Intelligence System Assurance (RAISA) Workshop 2022,,,,,http://arxiv.org/abs/2202.04787v1,"The Robust Artificial Intelligence System Assurance (RAISA) workshop will
focus on research, development and application of robust artificial
intelligence (AI) and machine learning (ML) systems. Rather than studying
robustness with respect to particular ML algorithms, our approach will be to
explore robustness assurance at the system architecture level, during both
development and deployment, and within the human-machine teaming context. While
the research community is converging on robust solutions for individual AI
models in specific scenarios, the problem of evaluating and assuring the
robustness of an AI system across its entire life cycle is much more complex.
Moreover, the operational context in which AI systems are deployed necessitates
consideration of robustness and its relation to principles of fairness,
privacy, and explainability.",2022-11-13 15:58:44
XXX,journalArticle,2022,"Sebastian Farquhar, Ryan Carey, Tom Everitt",Path-Specific Objectives for Safer Agent Incentives,,,,,http://arxiv.org/abs/2204.10018v1,"We present a general framework for training safe agents whose naive
incentives are unsafe. As an example, manipulative or deceptive behaviour can
improve rewards but should be avoided. Most approaches fail here: agents
maximize expected return by any means necessary. We formally describe settings
with 'delicate' parts of the state which should not be used as a means to an
end. We then train agents to maximize the causal effect of actions on the
expected return which is not mediated by the delicate parts of state, using
Causal Influence Diagram analysis. The resulting agents have no incentive to
control the delicate state. We further show how our framework unifies and
generalizes existing proposals.",2022-11-13 15:58:45
XXX,journalArticle,2022,Travis LaCroix,"The Linguistic Blind Spot of Value-Aligned Agency, Natural and Artificial",,,,,http://arxiv.org/abs/2207.00868v1,"The value-alignment problem for artificial intelligence (AI) asks how we can
ensure that the 'values' (i.e., objective functions) of artificial systems are
aligned with the values of humanity. In this paper, I argue that linguistic
communication (natural language) is a necessary condition for robust value
alignment. I discuss the consequences that the truth of this claim would have
for research programmes that attempt to ensure value alignment for AI systems;
or, more loftily, designing robustly beneficial or ethical artificial agents.",2022-11-13 15:58:46
XXX,journalArticle,2022,"Susmit Jha, John Rushby",Inferring and Conveying Intentionality: Beyond Numerical Rewards to Logical Intentions,,,,,http://arxiv.org/abs/2207.05058v2,"Shared intentionality is a critical component in developing conscious AI
agents capable of collaboration, self-reflection, deliberation, and reasoning.
We formulate inference of shared intentionality as an inverse reinforcement
learning problem with logical reward specifications. We show how the approach
can infer task descriptions from demonstrations. We also extend our approach to
actively convey intentionality. We demonstrate the approach on a simple
grid-world example.",2022-11-13 15:58:47
XXX,journalArticle,2020,Kyle Dent,Ethical Considerations for AI Researchers,,,,,http://arxiv.org/abs/2006.07558v1,"Use of artificial intelligence is growing and expanding into applications
that impact people's lives. People trust their technology without really
understanding it or its limitations. There is the potential for harm and we are
already seeing examples of that in the world. AI researchers have an obligation
to consider the impact of intelligent applications they work on. While the
ethics of AI is not clear-cut, there are guidelines we can consider to minimize
the harm we might introduce.",2022-11-13 15:58:47
XXX,journalArticle,2000,"Joseph Y. Halpern, Gerhard Lakemeyer",Multi-Agent Only Knowing,,,,,http://arxiv.org/abs/cs/0001015v1,"Levesque introduced a notion of ``only knowing'', with the goal of capturing
certain types of nonmonotonic reasoning. Levesque's logic dealt with only the
case of a single agent. Recently, both Halpern and Lakemeyer independently
attempted to extend Levesque's logic to the multi-agent case. Although there
are a number of similarities in their approaches, there are some significant
differences. In this paper, we reexamine the notion of only knowing, going back
to first principles. In the process, we simplify Levesque's completeness proof,
and point out some problems with the earlier definitions. This leads us to
reconsider what the properties of only knowing ought to be. We provide an axiom
system that captures our desiderata, and show that it has a semantics that
corresponds to it. The axiom system has an added feature of interest: it
includes a modal operator for satisfiability, and thus provides a complete
axiomatization for satisfiability in the logic K45.",2022-11-13 15:58:48
XXX,journalArticle,2000,Marcus Hutter,A Theory of Universal Artificial Intelligence based on Algorithmic Complexity,,,,,http://arxiv.org/abs/cs/0004001v1,"Decision theory formally solves the problem of rational agents in uncertain
worlds if the true environmental prior probability distribution is known.
Solomonoff's theory of universal induction formally solves the problem of
sequence prediction for unknown prior distribution. We combine both ideas and
get a parameterless theory of universal Artificial Intelligence. We give strong
arguments that the resulting AIXI model is the most intelligent unbiased agent
possible. We outline for a number of problem classes, including sequence
prediction, strategic games, function minimization, reinforcement and
supervised learning, how the AIXI model can formally solve them. The major
drawback of the AIXI model is that it is uncomputable. To overcome this
problem, we construct a modified algorithm AIXI-tl, which is still effectively
more intelligent than any other time t and space l bounded agent. The
computation time of AIXI-tl is of the order tx2^l. Other discussed topics are
formal definitions of intelligence order relations, the horizon problem and
relations of the AIXI theory to other AI approaches.",2022-11-13 15:58:48
XXX,journalArticle,2002,"Pedrito Maynard-Reid II, Daniel Lehmann",Representing and Aggregating Conflicting Beliefs,"Proceedings of the Seventh International Conference on Principles
  of Knowledge Representation and Reasoning (KR 2000), April 2000, pp. 153-164",,,,http://arxiv.org/abs/cs/0203013v1,"We consider the two-fold problem of representing collective beliefs and
aggregating these beliefs. We propose modular, transitive relations for
collective beliefs. They allow us to represent conflicting opinions and they
have a clear semantics. We compare them with the quasi-transitive relations
often used in Social Choice. Then, we describe a way to construct the belief
state of an agent informed by a set of sources of varying degrees of
reliability. This construction circumvents Arrow's Impossibility Theorem in a
satisfactory manner. Finally, we give a simple set-theory-based operator for
combining the information of multiple agents. We show that this operator
satisfies the desirable invariants of idempotence, commutativity, and
associativity, and, thus, is well-behaved when iterated, and we describe a
computationally effective way of computing the resulting belief state.",2022-11-13 15:58:49
XXX,journalArticle,2002,Larry Wos,A Spectrum of Applications of Automated Reasoning,,,,,http://arxiv.org/abs/cs/0205078v1,"The likelihood of an automated reasoning program being of substantial
assistance for a wide spectrum of applications rests with the nature of the
options and parameters it offers on which to base needed strategies and
methodologies. This article focuses on such a spectrum, featuring W. McCune's
program OTTER, discussing widely varied successes in answering open questions,
and touching on some of the strategies and methodologies that played a key
role. The applications include finding a first proof, discovering single
axioms, locating improved axiom systems, and simplifying existing proofs. The
last application is directly pertinent to the recently found (by R. Thiele)
Hilbert's twenty-fourth problem--which is extremely amenable to attack with the
appropriate automated reasoning program--a problem concerned with proof
simplification. The methodologies include those for seeking shorter proofs and
for finding proofs that avoid unwanted lemmas or classes of term, a specific
option for seeking proofs with smaller equational or formula complexity, and a
different option to address the variable richness of a proof. The type of proof
one obtains with the use of OTTER is Hilbert-style axiomatic, including details
that permit one sometimes to gain new insights. We include questions still open
and challenges that merit consideration.",2022-11-13 15:58:49
XXX,journalArticle,2005,"Joseph Y. Halpern, Riccardo Pucella",Evidence with Uncertain Likelihoods,,,,,http://arxiv.org/abs/cs/0510079v2,"An agent often has a number of hypotheses, and must choose among them based
on observations, or outcomes of experiments. Each of these observations can be
viewed as providing evidence for or against various hypotheses. All the
attempts to formalize this intuition up to now have assumed that associated
with each hypothesis h there is a likelihood function \mu_h, which is a
probability measure that intuitively describes how likely each observation is,
conditional on h being the correct hypothesis. We consider an extension of this
framework where there is uncertainty as to which of a number of likelihood
functions is appropriate, and discuss how one formal approach to defining
evidence, which views evidence as a function from priors to posteriors, can be
generalized to accommodate this uncertainty.",2022-11-13 15:58:50
XXX,journalArticle,2007,Marcus Hutter,Universal Algorithmic Intelligence: A mathematical top->down approach,"In Artificial General Intelligence, Springer (2007) 227-290",,,,http://arxiv.org/abs/cs/0701125v1,"Sequential decision theory formally solves the problem of rational agents in
uncertain worlds if the true environmental prior probability distribution is
known. Solomonoff's theory of universal induction formally solves the problem
of sequence prediction for unknown prior distribution. We combine both ideas
and get a parameter-free theory of universal Artificial Intelligence. We give
strong arguments that the resulting AIXI model is the most intelligent unbiased
agent possible. We outline how the AIXI model can formally solve a number of
problem classes, including sequence prediction, strategic games, function
minimization, reinforcement and supervised learning. The major drawback of the
AIXI model is that it is uncomputable. To overcome this problem, we construct a
modified algorithm AIXItl that is still effectively more intelligent than any
other time t and length l bounded agent. The computation time of AIXItl is of
the order t x 2^l. The discussion includes formal definitions of intelligence
order relations, the horizon problem and relations of the AIXI theory to other
AI approaches.",2022-11-13 15:58:50
XXX,journalArticle,2009,"Joseph Y. Halpern, Leandro Rego",Reasoning About Knowledge of Unawareness Revisited,,,,,http://arxiv.org/abs/0906.4321v1,"In earlier work, we proposed a logic that extends the Logic of General
Awareness of Fagin and Halpern [1988] by allowing quantification over primitive
propositions. This makes it possible to express the fact that an agent knows
that there are some facts of which he is unaware. In that logic, it is not
possible to model an agent who is uncertain about whether he is aware of all
formulas. To overcome this problem, we keep the syntax of the earlier paper,
but allow models where, with each world, a possibly different language is
associated. We provide a sound and complete axiomatization for this logic and
show that, under natural assumptions, the quantifier-free fragment of the logic
is characterized by exactly the same axioms as the logic of Heifetz, Meier, and
Schipper [2008].",2022-11-13 15:58:51
XXX,journalArticle,2010,"David Tolpin, Solomon Eyal Shimony",Rational Value of Information Estimation for Measurement Selection,,,,,http://arxiv.org/abs/1003.5305v2,"Computing value of information (VOI) is a crucial task in various aspects of
decision-making under uncertainty, such as in meta-reasoning for search; in
selecting measurements to make, prior to choosing a course of action; and in
managing the exploration vs. exploitation tradeoff. Since such applications
typically require numerous VOI computations during a single run, it is
essential that VOI be computed efficiently. We examine the issue of anytime
estimation of VOI, as frequently it suffices to get a crude estimate of the
VOI, thus saving considerable computational resources. As a case study, we
examine VOI estimation in the measurement selection problem. Empirical
evaluation of the proposed scheme in this domain shows that computational
resources can indeed be significantly reduced, at little cost in expected
rewards achieved in the overall decision problem.",2022-11-13 15:58:51
XXX,journalArticle,2010,"Jan Feyereisl, Uwe Aickelin",ToLeRating UR-STD,"Proceedings of the 2nd International Conference on Emerging
  Security Information, Systems and Technologies, Cap Esterel, France, p
  287-293, 2008",,,,http://arxiv.org/abs/1006.1563v1,"A new emerging paradigm of Uncertain Risk of Suspicion, Threat and Danger,
observed across the field of information security, is described. Based on this
paradigm a novel approach to anomaly detection is presented. Our approach is
based on a simple yet powerful analogy from the innate part of the human immune
system, the Toll-Like Receptors. We argue that such receptors incorporated as
part of an anomaly detector enhance the detector's ability to distinguish
normal and anomalous behaviour. In addition we propose that Toll-Like Receptors
enable the classification of detected anomalies based on the types of attacks
that perpetrate the anomalous behaviour. Classification of such type is either
missing in existing literature or is not fit for the purpose of reducing the
burden of an administrator of an intrusion detection system. For our model to
work, we propose the creation of a taxonomy of the digital Acytota, based on
which our receptors are created.",2022-11-13 15:58:52
XXX,journalArticle,2010,"Alejandra Gonzalez-Beltran, Ben Tagger, Anthony Finkelstein",Ontology-based Queries over Cancer Data,,,,,http://arxiv.org/abs/1012.5506v1,"The ever-increasing amount of data in biomedical research, and in cancer
research in particular, needs to be managed to support efficient data access,
exchange and integration. Existing software infrastructures, such caGrid,
support access to distributed information annotated with a domain ontology.
However, caGrid's current querying functionality depends on the structure of
individual data resources without exploiting the semantic annotations. In this
paper, we present the design and development of an ontology-based querying
functionality that consists of: the generation of OWL2 ontologies from the
underlying data resources metadata and a query rewriting and translation
process based on reasoning, which converts a query at the domain ontology level
into queries at the software infrastructure level. We present a detailed
analysis of our approach as well as an extensive performance evaluation. While
the implementation and evaluation was performed for the caGrid infrastructure,
the approach could be applicable to other model and metadata-driven
environments for data sharing.",2022-11-13 15:58:52
XXX,journalArticle,2010,Wan Ahmad Tajuddin Wan Abdullah,Looking for plausibility,,,,,http://arxiv.org/abs/1012.5705v1,"In the interpretation of experimental data, one is actually looking for
plausible explanations. We look for a measure of plausibility, with which we
can compare different possible explanations, and which can be combined when
there are different sets of data. This is contrasted to the conventional
measure for probabilities as well as to the proposed measure of possibilities.
We define what characteristics this measure of plausibility should have.
  In getting to the conception of this measure, we explore the relation of
plausibility to abductive reasoning, and to Bayesian probabilities. We also
compare with the Dempster-Schaefer theory of evidence, which also has its own
definition for plausibility. Abduction can be associated with biconditionality
in inference rules, and this provides a platform to relate to the
Collins-Michalski theory of plausibility. Finally, using a formalism for wiring
logic onto Hopfield neural networks, we ask if this is relevant in obtaining
this measure.",2022-11-13 15:58:53
XXX,journalArticle,2011,Peter de Blanc,Ontological Crises in Artificial Agents' Value Systems,,,,,http://arxiv.org/abs/1105.3821v1,"Decision-theoretic agents predict and evaluate the results of their actions
using a model, or ontology, of their environment. An agent's goal, or utility
function, may also be specified in terms of the states of, or entities within,
its ontology. If the agent may upgrade or replace its ontology, it faces a
crisis: the agent's original goal may not be well-defined with respect to its
new ontology. This crisis must be resolved before the agent can make plans
towards achieving its goals.
  We discuss in this paper which sorts of agents will undergo ontological
crises and why we may want to create such agents. We present some concrete
examples, and argue that a well-defined procedure for resolving ontological
crises is needed. We point to some possible approaches to solving this problem,
and evaluate these methods on our examples.",2022-11-13 15:58:54
XXX,journalArticle,2011,M. A. Walker,An Application of Reinforcement Learning to Dialogue Strategy Selection in a Spoken Dialogue System for Email,"Journal Of Artificial Intelligence Research, Volume 12, pages
  387-416, 2000",,,10.1613/jair.713,http://arxiv.org/abs/1106.0241v1,"This paper describes a novel method by which a spoken dialogue system can
learn to choose an optimal dialogue strategy from its experience interacting
with human users. The method is based on a combination of reinforcement
learning and performance modeling of spoken dialogue systems. The reinforcement
learning component applies Q-learning (Watkins, 1989), while the performance
modeling component applies the PARADISE evaluation framework (Walker et al.,
1997) to learn the performance function (reward) used in reinforcement
learning. We illustrate the method with a spoken dialogue system named ELVIS
(EmaiL Voice Interactive System), that supports access to email over the phone.
We conduct a set of experiments for training an optimal dialogue strategy on a
corpus of 219 dialogues in which human users interact with ELVIS over the
phone. We then test that strategy on a corpus of 18 dialogues. We show that
ELVIS can learn to optimize its strategy selection for agent initiative, for
reading messages, and for summarizing email folders.",2022-11-13 15:58:54
XXX,journalArticle,2012,"Joseph Y. Halpern, Willemien Kets",Ambiguous Language and Differences in Beliefs,,,,,http://arxiv.org/abs/1203.0699v1,"Standard models of multi-agent modal logic do not capture the fact that
information is often ambiguous, and may be interpreted in different ways by
different agents. We propose a framework that can model this, and consider
different semantics that capture different assumptions about the agents'
beliefs regarding whether or not there is ambiguity. We consider the impact of
ambiguity on a seminal result in economics: Aumann's result saying that agents
with a common prior cannot agree to disagree. This result is known not to hold
if agents do not have a common prior; we show that it also does not hold in the
presence of ambiguity. We then consider the tradeoff between assuming a common
interpretation (i.e., no ambiguity) and a common prior (i.e., shared initial
beliefs).",2022-11-13 15:58:55
XXX,journalArticle,2012,Dimiter Dobrev,Formal Definition of AI,"International Journal ""Information Theories & Applications"",
  vol.12, Number 3, 2005, pp.277-285",,,,http://arxiv.org/abs/1209.4838v1,"A definition of Artificial Intelligence was proposed in [1] but this
definition was not absolutely formal at least because the word ""Human"" was
used. In this paper we will formalize the definition from [1]. The biggest
problem in this definition was that the level of intelligence of AI is compared
to the intelligence of a human being. In order to change this we will introduce
some parameters to which AI will depend. One of this parameters will be the
level of intelligence and we will define one AI to each level of intelligence.
We assume that for some level of intelligence the respective AI will be more
intelligent than a human being. Nevertheless, we cannot say which is this level
because we cannot calculate its exact value.",2022-11-13 15:58:55
XXX,journalArticle,2013,"Eric J. Horvitz, Andy Jacobs, David Hovel",Attention-Sensitive Alerting,,,,,http://arxiv.org/abs/1301.6707v1,"We introduce utility-directed procedures for mediating the flow of
potentially distracting alerts and communications to computer users. We present
models and inference procedures that balance the context-sensitive costs of
deferring alerts with the cost of interruption. We describe the challenge of
reasoning about such costs under uncertainty via an analysis of user activity
and the content of notifications. After introducing principles of
attention-sensitive alerting, we focus on the problem of guiding alerts about
email messages. We dwell on the problem of inferring the expected criticality
of email and discuss work on the Priorities system, centering on prioritizing
email by criticality and modulating the communication of notifications to users
about the presence and nature of incoming email.",2022-11-13 15:58:56
XXX,journalArticle,2013,Lonnie Chrisman,Independence with Lower and Upper Probabilities,,,,,http://arxiv.org/abs/1302.3568v1,"It is shown that the ability of the interval probability representation to
capture epistemological independence is severely limited. Two events are
epistemologically independent if knowledge of the first event does not alter
belief (i.e., probability bounds) about the second. However, independence in
this form can only exist in a 2-monotone probability function in degenerate
cases i.e., if the prior bounds are either point probabilities or entirely
vacuous. Additional limitations are characterized for other classes of lower
probabilities as well. It is argued that these phenomena are simply a matter of
interpretation. They appear to be limitations when one interprets probability
bounds as a measure of epistemological indeterminacy (i.e., uncertainty arising
from a lack of knowledge), but are exactly as one would expect when probability
intervals are interpreted as representations of ontological indeterminacy
(indeterminacy introduced by structural approximations). The ontological
interpretation is introduced and discussed.",2022-11-13 15:58:56
XXX,journalArticle,2013,David L. Poole,Exploiting the Rule Structure for Decision Making within the Independent Choice Logic,,,,,http://arxiv.org/abs/1302.4978v1,"This paper introduces the independent choice logic, and in particular the
""single agent with nature"" instance of the independent choice logic, namely
ICLdt. This is a logical framework for decision making uncertainty that extends
both logic programming and stochastic models such as influence diagrams. This
paper shows how the representation of a decision problem within the independent
choice logic can be exploited to cut down the combinatorics of dynamic
programming. One of the main problems with influence diagram evaluation
techniques is the need to optimise a decision for all values of the 'parents'
of a decision variable. In this paper we show how the rule based nature of the
ICLdt can be exploited so that we only make distinctions in the values of the
information available for a decision that will make a difference to utility.",2022-11-13 15:58:57
XXX,journalArticle,2013,Gregory M. Provan,Tradeoffs in Constructing and Evaluating Temporal Influence Diagrams,,,,,http://arxiv.org/abs/1303.1458v1,"This paper addresses the tradeoffs which need to be considered in reasoning
using probabilistic network representations, such as Influence Diagrams (IDs).
In particular, we examine the tradeoffs entailed in using Temporal Influence
Diagrams (TIDs) which adequately capture the temporal evolution of a dynamic
system without prohibitive data and computational requirements. Three
approaches for TID construction which make different tradeoffs are examined:
(1) tailoring the network at each time interval to the data available (rather
then just copying the original Bayes Network for all time intervals); (2)
modeling the evolution of a parsimonious subset of variables (rather than all
variables); and (3) model selection approaches, which seek to minimize some
measure of the predictive accuracy of the model without introducing too many
parameters, which might cause ""overfitting"" of the model. Methods of evaluating
the accuracy/efficiency of the tradeoffs are proposed.",2022-11-13 15:58:57
XXX,journalArticle,2013,"Adam J. Grove, Daphne Koller",Probability Estimation in Face of Irrelevant Information,,,,,http://arxiv.org/abs/1303.5719v1,"In this paper, we consider one aspect of the problem of applying decision
theory to the design of agents that learn how to make decisions under
uncertainty. This aspect concerns how an agent can estimate probabilities for
the possible states of the world, given that it only makes limited observations
before committing to a decision. We show that the naive application of
statistical tools can be improved upon if the agent can determine which of his
observations are truly relevant to the estimation problem at hand. We give a
framework in which such determinations can be made, and define an estimation
procedure to use them. Our framework also suggests several extensions, which
show how additional knowledge can be used to improve tile estimation procedure
still further.",2022-11-13 15:58:58
XXX,journalArticle,2013,"David Heckerman, Eric J. Horvitz, Blackford Middleton",An Approximate Nonmyopic Computation for Value of Information,,,,,http://arxiv.org/abs/1303.5720v2,"Value-of-information analyses provide a straightforward means for selecting
the best next observation to make, and for determining whether it is better to
gather additional information or to act immediately. Determining the next best
test to perform, given a state of uncertainty about the world, requires a
consideration of the value of making all possible sequences of observations. In
practice, decision analysts and expert-system designers have avoided the
intractability of exact computation of the value of information by relying on a
myopic approximation. Myopic analyses are based on the assumption that only one
additional test will be performed, even when there is an opportunity to make a
large number of observations. We present a nonmyopic approximation for value of
information that bypasses the traditional myopic analyses by exploiting the
statistical properties of large samples.",2022-11-13 15:58:59
XXX,journalArticle,2013,"Paul E. Lehner, Theresa M. Mullin, Marvin S. Cohen",When Should a Decision Maker Ignore the Advice of a Decision Aid?,,,,,http://arxiv.org/abs/1304.1515v1,"This paper argues that the principal difference between decision aids and
most other types of information systems is the greater reliance of decision
aids on fallible algorithms--algorithms that sometimes generate incorrect
advice. It is shown that interactive problem solving with a decision aid that
is based on a fallible algorithm can easily result in aided performance which
is poorer than unaided performance, even if the algorithm, by itself, performs
significantly better than the unaided decision maker. This suggests that unless
certain conditions are satisfied, using a decision aid as an aid is
counterproductive. Some conditions under which a decision aid is best used as
an aid are derived.",2022-11-13 15:58:59
XXX,journalArticle,2013,Spencer Star,Generating Decision Structures and Causal Explanations for Decision Making,,,,,http://arxiv.org/abs/1304.2376v1,"This paper examines two related problems that are central to developing an
autonomous decision-making agent, such as a robot. Both problems require
generating structured representafions from a database of unstructured
declarative knowledge that includes many facts and rules that are irrelevant in
the problem context. The first problem is how to generate a well structured
decision problem from such a database. The second problem is how to generate,
from the same database, a well-structured explanation of why some possible
world occurred. In this paper it is shown that the problem of generating the
appropriate decision structure or explanation is intractable without
introducing further constraints on the knowledge in the database. The paper
proposes that the problem search space can be constrained by adding knowledge
to the database about causal relafions between events. In order to determine
the causal knowledge that would be most useful, causal theories for
deterministic and indeterministic universes are proposed. A program that uses
some of these causal constraints has been used to generate explanations about
faulty plans. The program shows the expected increase in efficiency as the
causal constraints are introduced.",2022-11-13 15:59:00
XXX,journalArticle,2013,Eric J. Horvitz,Reasoning About Beliefs and Actions Under Computational Resource Constraints,,,,,http://arxiv.org/abs/1304.2759v1,"Although many investigators affirm a desire to build reasoning systems that
behave consistently with the axiomatic basis defined by probability theory and
utility theory, limited resources for engineering and computation can make a
complete normative analysis impossible. We attempt to move discussion beyond
the debate over the scope of problems that can be handled effectively to cases
where it is clear that there are insufficient computational resources to
perform an analysis deemed as complete. Under these conditions, we stress the
importance of considering the expected costs and benefits of applying
alternative approximation procedures and heuristics for computation and
knowledge acquisition. We discuss how knowledge about the structure of user
utility can be used to control value tradeoffs for tailoring inference to
alternative contexts. We address the notion of real-time rationality, focusing
on the application of knowledge about the expected timewise-refinement
abilities of reasoning strategies to balance the benefits of additional
computation with the costs of acting with a partial result. We discuss the
benefits of applying decision theory to control the solution of difficult
problems given limitations and uncertainty in reasoning resources.",2022-11-13 15:59:00
XXX,journalArticle,2013,"Trong Nghia Hoang, Kian Hsiang Low",Interactive POMDP Lite: Towards Practical Planning to Predict and Exploit Intentions for Interacting with Self-Interested Agents,,,,,http://arxiv.org/abs/1304.5159v1,"A key challenge in non-cooperative multi-agent systems is that of developing
efficient planning algorithms for intelligent agents to interact and perform
effectively among boundedly rational, self-interested agents (e.g., humans).
The practicality of existing works addressing this challenge is being
undermined due to either the restrictive assumptions of the other agents'
behavior, the failure in accounting for their rationality, or the prohibitively
expensive cost of modeling and predicting their intentions. To boost the
practicality of research in this field, we investigate how intention prediction
can be efficiently exploited and made practical in planning, thereby leading to
efficient intention-aware planning frameworks capable of predicting the
intentions of other agents and acting optimally with respect to their predicted
intentions. We show that the performance losses incurred by the resulting
planning policies are linearly bounded by the error of intention prediction.
Empirical evaluations through a series of stochastic games demonstrate that our
policies can achieve better and more robust performance than the
state-of-the-art algorithms.",2022-11-13 15:59:01
XXX,journalArticle,2013,Ernest Davis,The Relevance of Proofs of the Rationality of Probability Theory to Automated Reasoning and Cognitive Models,,,,,http://arxiv.org/abs/1310.1328v1,"A number of well-known theorems, such as Cox's theorem and de Finetti's
theorem. prove that any model of reasoning with uncertain information that
satisfies specified conditions of ""rationality"" must satisfy the axioms of
probability theory. I argue here that these theorems do not in themselves
demonstrate that probabilistic models are in fact suitable for any specific
task in automated reasoning or plausible for cognitive models. First, the
theorems only establish that there exists some probabilistic model; they do not
establish that there exists a useful probabilistic model, i.e. one with a
tractably small number of numerical parameters and a large number of
independence assumptions. Second, there are in general many different
probabilistic models for a given situation, many of which may be far more
irrational, in the usual sense of the term, than a model that violates the
axioms of probability theory. I illustrate this second point with an extended
examples of two tasks of induction, of a similar structure, where the
reasonable probabilistic models are very different.",2022-11-13 15:59:01
XXX,journalArticle,2013,"Christoph Salge, Cornelius Glackin, Daniel Polani",Empowerment -- an Introduction,,,,,http://arxiv.org/abs/1310.1863v2,"This book chapter is an introduction to and an overview of the
information-theoretic, task independent utility function ""Empowerment"", which
is defined as the channel capacity between an agent's actions and an agent's
sensors. It quantifies how much influence and control an agent has over the
world it can perceive. This book chapter discusses the general idea behind
empowerment as an intrinsic motivation and showcases several previous
applications of empowerment to demonstrate how empowerment can be applied to
different sensor-motor configuration, and how the same formalism can lead to
different observed behaviors. Furthermore, we also present a fast approximation
for empowerment in the continuous domain.",2022-11-13 15:59:02
XXX,journalArticle,2013,"Jie Fan, Yanjing Wang, Hans van Ditmarsch",Knowing Whether,,,,,http://arxiv.org/abs/1312.0144v3,"Knowing whether a proposition is true means knowing that it is true or
knowing that it is false. In this paper, we study logics with a modal operator
Kw for knowing whether but without a modal operator K for knowing that. This
logic is not a normal modal logic, because we do not have Kw (phi -> psi) ->
(Kw phi -> Kw psi). Knowing whether logic cannot define many common frame
properties, and its expressive power less than that of basic modal logic over
classes of models without reflexivity. These features make axiomatizing knowing
whether logics non-trivial. We axiomatize knowing whether logic over various
frame classes. We also present an extension of knowing whether logic with
public announcement operators and we give corresponding reduction axioms for
that. We compare our work in detail to two recent similar proposals.",2022-11-13 15:59:02
XXX,journalArticle,2014,"Dongmo Zhang, Michael Thielsher",Representing and Reasoning about Game Strategies,,,,,http://arxiv.org/abs/1407.5380v1,"As a contribution to the challenge of building game-playing AI systems, we
develop and analyse a formal language for representing and reasoning about
strategies. Our logical language builds on the existing general Game
Description Language (GDL) and extends it by a standard modality for linear
time along with two dual connectives to express preferences when combining
strategies. The semantics of the language is provided by a standard
state-transition model. As such, problems that require reasoning about games
can be solved by the standard methods for reasoning about actions and change.
We also endow the language with a specific semantics by which strategy formulas
are understood as move recommendations for a player. To illustrate how our
formalism supports automated reasoning about strategies, we demonstrate two
example methods of implementation\/: first, we formalise the semantic
interpretation of our language in conjunction with game rules and strategy
rules in the Situation Calculus; second, we show how the reasoning problem can
be solved with Answer Set Programming.",2022-11-13 15:59:03
XXX,journalArticle,2014,"Joseph Y. Halpern, Riccardo Pucella",Evidence with Uncertain Likelihoods,,,,,http://arxiv.org/abs/1407.7189v1,"An agent often has a number of hypotheses, and must choose among them based
on observations, or outcomes of experiments. Each of these observations can be
viewed as providing evidence for or against various hypotheses. All the
attempts to formalize this intuition up to now have assumed that associated
with each hypothesis h there is a likelihood function {\mu}h, which is a
probability measure that intuitively describes how likely each observation is,
conditional on h being the correct hypothesis. We consider an extension of this
framework where there is uncertainty as to which of a number of likelihood
functions is appropriate, and discuss how one formal approach to defining
evidence, which views evidence as a function from priors to posteriors, can be
generalized to accommodate this uncertainty.",2022-11-13 15:59:03
XXX,journalArticle,2014,Brian Tomasik,Do Artificial Reinforcement-Learning Agents Matter Morally?,,,,,http://arxiv.org/abs/1410.8233v1,"Artificial reinforcement learning (RL) is a widely used technique in
artificial intelligence that provides a general method for training agents to
perform a wide variety of behaviours. RL as used in computer science has
striking parallels to reward and punishment learning in animal and human
brains. I argue that present-day artificial RL agents have a very small but
nonzero degree of ethical importance. This is particularly plausible for views
according to which sentience comes in degrees based on the abilities and
complexities of minds, but even binary views on consciousness should assign
nonzero probability to RL programs having morally relevant experiences. While
RL programs are not a top ethical priority today, they may become more
significant in the coming decades as RL is increasingly applied to industry,
robotics, video games, and other areas. I encourage scientists, philosophers,
and citizens to begin a conversation about our ethical duties to reduce the
harm that we inflict on powerless, voiceless RL agents.",2022-11-13 15:59:04
XXX,journalArticle,2015,"Louise A. Dennis, Michael Fisher, Alan F. T. Winfield",Towards Verifiably Ethical Robot Behaviour,,,,,http://arxiv.org/abs/1504.03592v1,"Ensuring that autonomous systems work ethically is both complex and
difficult. However, the idea of having an additional `governor' that assesses
options the system has, and prunes them to select the most ethical choices is
well understood. Recent work has produced such a governor consisting of a
`consequence engine' that assesses the likely future outcomes of actions then
applies a Safety/Ethical logic to select actions. Although this is appealing,
it is impossible to be certain that the most ethical options are actually
taken. In this paper we extend and apply a well-known agent verification
approach to our consequence engine, allowing us to verify the correctness of
its ethical decision-making.",2022-11-13 15:59:04
XXX,journalArticle,2015,"Christopher H. Lin, Andrey Kolobov, Ece Kamar, Eric Horvitz",Metareasoning for Planning Under Uncertainty,,,,,http://arxiv.org/abs/1505.00399v1,"The conventional model for online planning under uncertainty assumes that an
agent can stop and plan without incurring costs for the time spent planning.
However, planning time is not free in most real-world settings. For example, an
autonomous drone is subject to nature's forces, like gravity, even while it
thinks, and must either pay a price for counteracting these forces to stay in
place, or grapple with the state change caused by acquiescing to them. Policy
optimization in these settings requires metareasoning---a process that trades
off the cost of planning and the potential policy improvement that can be
achieved. We formalize and analyze the metareasoning problem for Markov
Decision Processes (MDPs). Our work subsumes previously studied special cases
of metareasoning and shows that in the general case, metareasoning is at most
polynomially harder than solving MDPs with any given algorithm that disregards
the cost of thinking. For reasons we discuss, optimal general metareasoning
turns out to be impractical, motivating approximations. We present approximate
metareasoning procedures which rely on special properties of the BRTDP planning
algorithm and explore the effectiveness of our methods on a variety of
problems.",2022-11-13 15:59:05
XXX,journalArticle,2015,"Daniel Raggi, Alan Bundy, Gudmund Grov, Alison Pease",Automating change of representation for proofs in discrete mathematics,,,,,http://arxiv.org/abs/1505.02449v1,"Representation determines how we can reason about a specific problem.
Sometimes one representation helps us find a proof more easily than others.
Most current automated reasoning tools focus on reasoning within one
representation. There is, therefore, a need for the development of better tools
to mechanise and automate formal and logically sound changes of representation.
  In this paper we look at examples of representational transformations in
discrete mathematics, and show how we have used Isabelle's Transfer tool to
automate the use of these transformations in proofs. We give a brief overview
of a general theory of transformations that we consider appropriate for
thinking about the matter, and we explain how it relates to the Transfer
package. We show our progress towards developing a general tactic that
incorporates the automatic search for representation within the proving
process.",2022-11-13 15:59:05
XXX,journalArticle,2015,"Andreas Falkner, Anna Ryabokon, Gottfried Schenner, Kostyantyn Shchekotykhin",OOASP: Connecting Object-oriented and Logic Programming,,,,,http://arxiv.org/abs/1508.03032v1,"Most of contemporary software systems are implemented using an
object-oriented approach. Modeling phases -- during which software engineers
analyze requirements to the future system using some modeling language -- are
an important part of the development process, since modeling errors are often
hard to recognize and correct.
  In this paper we present a framework which allows the integration of Answer
Set Programming into the object-oriented software development process. OOASP
supports reasoning about object-oriented software models and their
instantiations. Preliminary results of the OOASP application in CSL Studio,
which is a Siemens internal modeling environment for product configurators,
show that it can be used as a lightweight approach to verify, create and
transform instantiations of object models at runtime and to support the
software development process during design and testing.",2022-11-13 15:59:06
XXX,journalArticle,2015,Miles Brundage,Modeling Progress in AI,,,,,http://arxiv.org/abs/1512.05849v1,"Participants in recent discussions of AI-related issues ranging from
intelligence explosion to technological unemployment have made diverse claims
about the nature, pace, and drivers of progress in AI. However, these theories
are rarely specified in enough detail to enable systematic evaluation of their
assumptions or to extrapolate progress quantitatively, as is often done with
some success in other technological domains. After reviewing relevant
literatures and justifying the need for more rigorous modeling of AI progress,
this paper contributes to that research program by suggesting ways to account
for the relationship between hardware speed increases and algorithmic
improvements in AI, the role of human inputs in enabling AI capabilities, and
the relationships between different sub-fields of AI. It then outlines ways of
tailoring AI progress models to generate insights on the specific issue of
technological unemployment, and outlines future directions for research on AI
progress.",2022-11-13 15:59:07
XXX,journalArticle,2015,"Alexander Kott, Michael Ownby",Toward a Research Agenda in Adversarial Reasoning: Computational Approaches to Anticipating the Opponent's Intent and Actions,,,,,http://arxiv.org/abs/1512.07943v1,"This paper defines adversarial reasoning as computational approaches to
inferring and anticipating an enemy's perceptions, intents and actions. It
argues that adversarial reasoning transcends the boundaries of game theory and
must also leverage such disciplines as cognitive modeling, control theory, AI
planning and others. To illustrate the challenges of applying adversarial
reasoning to real-world problems, the paper explores the lessons learned in the
CADET - a battle planning system that focuses on brigade-level ground
operations and involves adversarial reasoning. From this example of current
capabilities, the paper proceeds to describe RAID - a DARPA program that aims
to build capabilities in adversarial reasoning, and how such capabilities would
address practical requirements in Defense and other application areas.",2022-11-13 15:59:07
XXX,journalArticle,2016,Toby Walsh,The Singularity May Never Be Near,,,,,http://arxiv.org/abs/1602.06462v1,"There is both much optimism and pessimism around artificial intelligence (AI)
today. The optimists are investing millions of dollars, and even in some cases
billions of dollars into AI. The pessimists, on the other hand, predict that AI
will end many things: jobs, warfare, and even the human race. Both the
optimists and the pessimists often appeal to the idea of a technological
singularity, a point in time where machine intelligence starts to run away, and
a new, more intelligent species starts to inhabit the earth. If the optimists
are right, this will be a moment that fundamentally changes our economy and our
society. If the pessimists are right, this will be a moment that also
fundamentally changes our economy and our society. It is therefore very
worthwhile spending some time deciding if either of them might be right.",2022-11-13 15:59:08
XXX,journalArticle,2016,David J. Jilk,Limits to Verification and Validation of Agentic Behavior,,,,,http://arxiv.org/abs/1604.06963v2,"Verification and validation of agentic behavior have been suggested as
important research priorities in efforts to reduce risks associated with the
creation of general artificial intelligence (Russell et al 2015). In this paper
we question the appropriateness of using language of certainty with respect to
efforts to manage that risk. We begin by establishing a very general formalism
to characterize agentic behavior and to describe standards of acceptable
behavior. We show that determination of whether an agent meets any particular
standard is not computable. We discuss the extent of the burden associated with
verification by manual proof and by automated behavioral governance. We show
that to ensure decidability of the behavioral standard itself, one must further
limit the capabilities of the agent. We then demonstrate that if our concerns
relate to outcomes in the physical world, attempts at validation are futile.
Finally, we show that layered architectures aimed at making these challenges
tractable mistakenly equate intentions with actions or outcomes, thereby
failing to provide any guarantees. We conclude with a discussion of why
language of certainty should be eradicated from the conversation about the
safety of general artificial intelligence.",2022-11-13 15:59:09
XXX,journalArticle,2016,"Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, Dan Mané",Concrete Problems in AI Safety,,,,,http://arxiv.org/abs/1606.06565v2,"Rapid progress in machine learning and artificial intelligence (AI) has
brought increasing attention to the potential impacts of AI technologies on
society. In this paper we discuss one such potential impact: the problem of
accidents in machine learning systems, defined as unintended and harmful
behavior that may emerge from poor design of real-world AI systems. We present
a list of five practical research problems related to accident risk,
categorized according to whether the problem originates from having the wrong
objective function (""avoiding side effects"" and ""avoiding reward hacking""), an
objective function that is too expensive to evaluate frequently (""scalable
supervision""), or undesirable behavior during the learning process (""safe
exploration"" and ""distributional shift""). We review previous work in these
areas as well as suggesting research directions with a focus on relevance to
cutting-edge AI systems. Finally, we consider the high-level question of how to
think most productively about the safety of forward-looking applications of AI.",2022-11-13 15:59:09
XXX,journalArticle,2016,"I. Dan Melamed, Nobal B. Niraula",Towards A Virtual Assistant That Can Be Taught New Tasks In Any Domain By Its End-Users,,,,,http://arxiv.org/abs/1607.00061v1,"The challenge stated in the title can be divided into two main problems. The
first problem is to reliably mimic the way that users interact with user
interfaces. The second problem is to build an instructible agent, i.e. one that
can be taught to execute tasks expressed as previously unseen natural language
commands. This paper proposes a solution to the second problem, a system we
call Helpa. End-users can teach Helpa arbitrary new tasks whose level of
complexity is similar to the tasks available from today's most popular virtual
assistants. Teaching Helpa does not involve any programming. Instead, users
teach Helpa by providing just one example of a command paired with a
demonstration of how to execute that command. Helpa does not rely on any
pre-existing domain-specific knowledge. It is therefore completely
domain-independent. Our usability study showed that end-users can teach Helpa
many new tasks in less than a minute each, often much less.",2022-11-13 15:59:10
XXX,journalArticle,2016,"Gavin Rens, Deshendran Moodley",A Hybrid POMDP-BDI Agent Architecture with Online Stochastic Planning and Plan Caching,,,,,http://arxiv.org/abs/1607.00656v1,"This article presents an agent architecture for controlling an autonomous
agent in stochastic environments. The architecture combines the partially
observable Markov decision process (POMDP) model with the
belief-desire-intention (BDI) framework. The Hybrid POMDP-BDI agent
architecture takes the best features from the two approaches, that is, the
online generation of reward-maximizing courses of action from POMDP theory, and
sophisticated multiple goal management from BDI theory. We introduce the
advances made since the introduction of the basic architecture, including (i)
the ability to pursue multiple goals simultaneously and (ii) a plan library for
storing pre-written plans and for storing recently generated plans for future
reuse. A version of the architecture without the plan library is implemented
and is evaluated using simulations. The results of the simulation experiments
indicate that the approach is feasible.",2022-11-13 15:59:10
XXX,journalArticle,2016,"Michael Ownby, Alexander Kott",Predicting Enemy's Actions Improves Commander Decision-Making,,,,,http://arxiv.org/abs/1607.06759v1,"The Defense Advanced Research Projects Agency (DARPA) Real-time Adversarial
Intelligence and Decision-making (RAID) program is investigating the
feasibility of ""reading the mind of the enemy"" - to estimate and anticipate, in
real-time, the enemy's likely goals, deceptions, actions, movements and
positions. This program focuses specifically on urban battles at echelons of
battalion and below. The RAID program leverages approximate game-theoretic and
deception-sensitive algorithms to provide real-time enemy estimates to a
tactical commander. A key hypothesis of the program is that these predictions
and recommendations will make the commander more effective, i.e. he should be
able to achieve his operational goals safer, faster, and more efficiently.
Realistic experimentation and evaluation drive the development process using
human-in-the-loop wargames to compare humans and the RAID system. Two
experiments were conducted in 2005 as part of Phase I to determine if the RAID
software could make predictions and recommendations as effectively and
accurately as a 4-person experienced staff. This report discusses the
intriguing and encouraging results of these first two experiments conducted by
the RAID program. It also provides details about the experiment environment and
methodology that were used to demonstrate and prove the research goals.",2022-11-13 15:59:11
XXX,journalArticle,2016,"Ulle Endriss, Umberto Grandi",Graph Aggregation,"Artificial Intelligence, Volume 245, pages 86-114, 2017",,,10.1016/j.artint.2017.01.001,http://arxiv.org/abs/1609.03765v1,"Graph aggregation is the process of computing a single output graph that
constitutes a good compromise between several input graphs, each provided by a
different source. One needs to perform graph aggregation in a wide variety of
situations, e.g., when applying a voting rule (graphs as preference orders),
when consolidating conflicting views regarding the relationships between
arguments in a debate (graphs as abstract argumentation frameworks), or when
computing a consensus between several alternative clusterings of a given
dataset (graphs as equivalence relations). In this paper, we introduce a formal
framework for graph aggregation grounded in social choice theory. Our focus is
on understanding which properties shared by the individual input graphs will
transfer to the output graph returned by a given aggregation rule. We consider
both common properties of graphs, such as transitivity and reflexivity, and
arbitrary properties expressible in certain fragments of modal logic. Our
results establish several connections between the types of properties preserved
under aggregation and the choice-theoretic axioms satisfied by the rules used.
The most important of these results is a powerful impossibility theorem that
generalises Arrow's seminal result for the aggregation of preference orders to
a large collection of different types of graphs.",2022-11-13 15:59:12
XXX,journalArticle,2016,"Tathagata Chakraborti, Kartik Talamadupula, Kshitij P. Fadnis, Murray Campbell, Subbarao Kambhampati",UbuntuWorld 1.0 LTS - A Platform for Automated Problem Solving & Troubleshooting in the Ubuntu OS,,,,,http://arxiv.org/abs/1609.08524v2,"In this paper, we present UbuntuWorld 1.0 LTS - a platform for developing
automated technical support agents in the Ubuntu operating system.
Specifically, we propose to use the Bash terminal as a simulator of the Ubuntu
environment for a learning-based agent and demonstrate the usefulness of
adopting reinforcement learning (RL) techniques for basic problem solving and
troubleshooting in this environment. We provide a plug-and-play interface to
the simulator as a python package where different types of agents can be
plugged in and evaluated, and provide pathways for integrating data from online
support forums like AskUbuntu into an automated agent's learning process.
Finally, we show that the use of this data significantly improves the agent's
learning efficiency. We believe that this platform can be adopted as a
real-world test bed for research on automated technical support.",2022-11-13 15:59:12
XXX,journalArticle,2017,Mark Muraven,Designing a Safe Autonomous Artificial Intelligence Agent based on Human Self-Regulation,,,,,http://arxiv.org/abs/1701.01487v1,"There is a growing focus on how to design safe artificial intelligent (AI)
agents. As systems become more complex, poorly specified goals or control
mechanisms may cause AI agents to engage in unwanted and harmful outcomes. Thus
it is necessary to design AI agents that follow initial programming intentions
as the program grows in complexity. How to specify these initial intentions has
also been an obstacle to designing safe AI agents. Finally, there is a need for
the AI agent to have redundant safety mechanisms to ensure that any programming
errors do not cascade into major problems. Humans are autonomous intelligent
agents that have avoided these problems and the present manuscript argues that
by understanding human self-regulation and goal setting, we may be better able
to design safe AI agents. Some general principles of human self-regulation are
outlined and specific guidance for AI design is given.",2022-11-13 15:59:13
XXX,journalArticle,2017,"Zohreh Shams, Marina De Vos, Julian Padget, Wamberto W. Vasconcelos",Practical Reasoning with Norms for Autonomous Software Agents (Full Edition),,,,,http://arxiv.org/abs/1701.08306v1,"Autonomous software agents operating in dynamic environments need to
constantly reason about actions in pursuit of their goals, while taking into
consideration norms which might be imposed on those actions. Normative
practical reasoning supports agents making decisions about what is best for
them to (not) do in a given situation. What makes practical reasoning
challenging is the interplay between goals that agents are pursuing and the
norms that the agents are trying to uphold. We offer a formalisation to allow
agents to plan for multiple goals and norms in the presence of durative actions
that can be executed concurrently. We compare plans based on decision-theoretic
notions (i.e. utility) such that the utility gain of goals and utility loss of
norm violations are the basis for this comparison. The set of optimal plans
consists of plans that maximise the overall utility, each of which can be
chosen by the agent to execute. We provide an implementation of our proposal in
Answer Set Programming, thus allowing us to state the original problem in terms
of a logic program that can be queried for solutions with specific properties.
The implementation is proven to be sound and complete.",2022-11-13 15:59:13
XXX,journalArticle,2017,"Tathagata Chakraborti, Sarath Sreedharan, Yu Zhang, Subbarao Kambhampati",Plan Explanations as Model Reconciliation: Moving Beyond Explanation as Soliloquy,,,,,http://arxiv.org/abs/1701.08317v5,"When AI systems interact with humans in the loop, they are often called on to
provide explanations for their plans and behavior. Past work on plan
explanations primarily involved the AI system explaining the correctness of its
plan and the rationale for its decision in terms of its own model. Such
soliloquy is wholly inadequate in most realistic scenarios where the humans
have domain and task models that differ significantly from that used by the AI
system. We posit that the explanations are best studied in light of these
differing models. In particular, we show how explanation can be seen as a
""model reconciliation problem"" (MRP), where the AI system in effect suggests
changes to the human's model, so as to make its plan be optimal with respect to
that changed human model. We will study the properties of such explanations,
present algorithms for automatically computing them, and evaluate the
performance of the algorithms.",2022-11-13 15:59:14
XXX,journalArticle,2017,"Ewa Andrejczuk, Juan A. Rodriguez-Aguilar, Carme Roig, Carles Sierra",Synergistic Team Composition,,,,,http://arxiv.org/abs/1702.08222v1,"Effective teams are crucial for organisations, especially in environments
that require teams to be constantly created and dismantled, such as software
development, scientific experiments, crowd-sourcing, or the classroom. Key
factors influencing team performance are competences and personality of team
members. Hence, we present a computational model to compose proficient and
congenial teams based on individuals' personalities and their competences to
perform tasks of different nature. With this purpose, we extend Wilde's
post-Jungian method for team composition, which solely employs individuals'
personalities. The aim of this study is to create a model to partition agents
into teams that are balanced in competences, personality and gender. Finally,
we present some preliminary empirical results that we obtained when analysing
student performance. Results show the benefits of a more informed team
composition that exploits individuals' competences besides information about
their personalities.",2022-11-13 15:59:14
XXX,journalArticle,2017,"Raul Fervari, Andreas Herzig, Yanjun Li, Yanjing Wang",Strategically knowing how,,,,,http://arxiv.org/abs/1705.05254v1,"In this paper, we propose a single-agent logic of goal-directed knowing how
extending the standard epistemic logic of knowing that with a new knowing how
operator. The semantics of the new operator is based on the idea that knowing
how to achieve $\phi$ means that there exists a (uniform) strategy such that
the agent knows that it can make sure $\phi$. We give an intuitive
axiomatization of our logic and prove the soundness, completeness, and
decidability of the logic. The crucial axioms relating knowing that and knowing
how illustrate our understanding of knowing how in this setting. This logic can
be used in representing both knowledge-that and knowledge-how.",2022-11-13 15:59:15
XXX,journalArticle,2017,"Pavel Naumov, Jia Tao",Together We Know How to Achieve: An Epistemic Logic of Know-How,,,,,http://arxiv.org/abs/1705.09349v2,"The existence of a coalition strategy to achieve a goal does not necessarily
mean that the coalition has enough information to know how to follow the
strategy. Neither does it mean that the coalition knows that such a strategy
exists. The article studies an interplay between the distributed knowledge,
coalition strategies, and coalition ""know-how"" strategies. The main technical
result is a sound and complete trimodal logical system that describes the
properties of this interplay.",2022-11-13 15:59:16
XXX,journalArticle,2017,"Smitha Milli, Dylan Hadfield-Menell, Anca Dragan, Stuart Russell",Should Robots be Obedient?,,,,,http://arxiv.org/abs/1705.09990v1,"Intuitively, obedience -- following the order that a human gives -- seems
like a good property for a robot to have. But, we humans are not perfect and we
may give orders that are not best aligned to our preferences. We show that when
a human is not perfectly rational then a robot that tries to infer and act
according to the human's underlying preferences can always perform better than
a robot that simply follows the human's literal order. Thus, there is a
tradeoff between the obedience of a robot and the value it can attain for its
owner. We investigate how this tradeoff is impacted by the way the robot infers
the human's preferences, showing that some methods err more on the side of
obedience than others. We then analyze how performance degrades when the robot
has a misspecified model of the features that the human cares about or the
level of rationality of the human. Finally, we study how robots can start
detecting such model misspecification. Overall, our work suggests that there
might be a middle ground in which robots intelligently decide when to obey
human orders, but err on the side of obedience.",2022-11-13 15:59:16
XXX,journalArticle,2017,Toby Walsh,Expert and Non-Expert Opinion about Technological Unemployment,,,,,http://arxiv.org/abs/1706.06906v1,"There is significant concern that technological advances, especially in
Robotics and Artificial Intelligence (AI), could lead to high levels of
unemployment in the coming decades. Studies have estimated that around half of
all current jobs are at risk of automation. To look into this issue in more
depth, we surveyed experts in Robotics and AI about the risk, and compared
their views with those of non-experts. Whilst the experts predicted a
significant number of occupations were at risk of automation in the next two
decades, they were more cautious than people outside the field in predicting
occupations at risk. Their predictions were consistent with their estimates for
when computers might be expected to reach human level performance across a wide
range of skills. These estimates were typically decades later than those of the
non-experts. Technological barriers may therefore provide society with more
time to prepare for an automated future than the public fear. In addition,
public expectations may need to be dampened about the speed of progress to be
expected in Robotics and AI.",2022-11-13 15:59:17
XXX,journalArticle,2017,"Svetlin Penkov, Subramanian Ramamoorthy",Using Program Induction to Interpret Transition System Dynamics,,,,,http://arxiv.org/abs/1708.00376v1,"Explaining and reasoning about processes which underlie observed black-box
phenomena enables the discovery of causal mechanisms, derivation of suitable
abstract representations and the formulation of more robust predictions. We
propose to learn high level functional programs in order to represent abstract
models which capture the invariant structure in the observed data. We introduce
the $\pi$-machine (program-induction machine) -- an architecture able to induce
interpretable LISP-like programs from observed data traces. We propose an
optimisation procedure for program learning based on backpropagation, gradient
descent and A* search. We apply the proposed method to two problems: system
identification of dynamical systems and explaining the behaviour of a DQN
agent. Our results show that the $\pi$-machine can efficiently induce
interpretable programs from individual data traces.",2022-11-13 15:59:17
XXX,journalArticle,2017,"Philip S. Thomas, Bruno Castro da Silva, Andrew G. Barto, Emma Brunskill",On Ensuring that Intelligent Machines Are Well-Behaved,,,,,http://arxiv.org/abs/1708.05448v1,"Machine learning algorithms are everywhere, ranging from simple data analysis
and pattern recognition tools used across the sciences to complex systems that
achieve super-human performance on various tasks. Ensuring that they are
well-behaved---that they do not, for example, cause harm to humans or act in a
racist or sexist way---is therefore not a hypothetical problem to be dealt with
in the future, but a pressing one that we address here. We propose a new
framework for designing machine learning algorithms that simplifies the problem
of specifying and regulating undesirable behaviors. To show the viability of
this new framework, we use it to create new machine learning algorithms that
preclude the sexist and harmful behaviors exhibited by standard machine
learning algorithms in our experiments. Our framework for designing machine
learning algorithms simplifies the safe and responsible application of machine
learning.",2022-11-13 15:59:18
XXX,journalArticle,2017,"Ivan Y. Tyukin, Alexander N. Gorban, Konstantin Sofeikov, Ilya Romanenko",Knowledge Transfer Between Artificial Intelligence Systems,Front Neurorobot. 2018; 12: 49,,,10.3389/fnbot.2018.00049,http://arxiv.org/abs/1709.01547v2,"We consider the fundamental question: how a legacy ""student"" Artificial
Intelligent (AI) system could learn from a legacy ""teacher"" AI system or a
human expert without complete re-training and, most importantly, without
requiring significant computational resources. Here ""learning"" is understood as
an ability of one system to mimic responses of the other and vice-versa. We
call such learning an Artificial Intelligence knowledge transfer. We show that
if internal variables of the ""student"" Artificial Intelligent system have the
structure of an $n$-dimensional topological vector space and $n$ is
sufficiently high then, with probability close to one, the required knowledge
transfer can be implemented by simple cascades of linear functionals. In
particular, for $n$ sufficiently large, with probability close to one, the
""student"" system can successfully and non-iteratively learn $k\ll n$ new
examples from the ""teacher"" (or correct the same number of mistakes) at the
cost of two additional inner products. The concept is illustrated with an
example of knowledge transfer from a pre-trained convolutional neural network
to a simple linear classifier with HOG features.",2022-11-13 15:59:18
XXX,journalArticle,2017,"Kunal Menda, Katherine Driggs-Campbell, Mykel J. Kochenderfer",DropoutDAgger: A Bayesian Approach to Safe Imitation Learning,,,,,http://arxiv.org/abs/1709.06166v1,"While imitation learning is becoming common practice in robotics, this
approach often suffers from data mismatch and compounding errors. DAgger is an
iterative algorithm that addresses these issues by continually aggregating
training data from both the expert and novice policies, but does not consider
the impact of safety. We present a probabilistic extension to DAgger, which
uses the distribution over actions provided by the novice policy, for a given
observation. Our method, which we call DropoutDAgger, uses dropout to train the
novice as a Bayesian neural network that provides insight to its confidence.
Using the distribution over the novice's actions, we estimate a probabilistic
measure of safety with respect to the expert action, tuned to balance
exploration and exploitation. The utility of this approach is evaluated on the
MuJoCo HalfCheetah and in a simple driving experiment, demonstrating improved
performance and safety compared to other DAgger variants and classic imitation
learning.",2022-11-13 15:59:19
XXX,journalArticle,2017,Ryan Carey,Incorrigibility in the CIRL Framework,,,,,http://arxiv.org/abs/1709.06275v2,"A value learning system has incentives to follow shutdown instructions,
assuming the shutdown instruction provides information (in the technical sense)
about which actions lead to valuable outcomes. However, this assumption is not
robust to model mis-specification (e.g., in the case of programmer errors). We
demonstrate this by presenting some Supervised POMDP scenarios in which errors
in the parameterized reward function remove the incentive to follow shutdown
commands. These difficulties parallel those discussed by Soares et al. (2015)
in their paper on corrigibility. We argue that it is important to consider
systems that follow shutdown commands under some weaker set of assumptions
(e.g., that one small verified module is correctly implemented; as opposed to
an entire prior probability distribution and/or parameterized reward function).
We discuss some difficulties with simple ways to attempt to attain these sorts
of guarantees in a value learning framework.",2022-11-13 15:59:20
XXX,journalArticle,2017,"Ritesh Noothigattu, Snehalkumar 'Neil' S. Gaikwad, Edmond Awad, Sohan Dsouza, Iyad Rahwan, Pradeep Ravikumar, Ariel D. Procaccia",A Voting-Based System for Ethical Decision Making,,,,,http://arxiv.org/abs/1709.06692v2,"We present a general approach to automating ethical decisions, drawing on
machine learning and computational social choice. In a nutshell, we propose to
learn a model of societal preferences, and, when faced with a specific ethical
dilemma at runtime, efficiently aggregate those preferences to identify a
desirable choice. We provide a concrete algorithm that instantiates our
approach; some of its crucial steps are informed by a new theory of
swap-dominance efficient voting rules. Finally, we implement and evaluate a
system for ethical decision making in the autonomous vehicle domain, using
preference data collected from 1.3 million people through the Moral Machine
website.",2022-11-13 15:59:20
XXX,journalArticle,2017,"Stefano V. Albrecht, Peter Stone",Autonomous Agents Modelling Other Agents: A Comprehensive Survey and Open Problems,,,,10.1016/j.artint.2018.01.002,http://arxiv.org/abs/1709.08071v2,"Much research in artificial intelligence is concerned with the development of
autonomous agents that can interact effectively with other agents. An important
aspect of such agents is the ability to reason about the behaviours of other
agents, by constructing models which make predictions about various properties
of interest (such as actions, goals, beliefs) of the modelled agents. A variety
of modelling approaches now exist which vary widely in their methodology and
underlying assumptions, catering to the needs of the different sub-communities
within which they were developed and reflecting the different practical uses
for which they are intended. The purpose of the present article is to provide a
comprehensive survey of the salient modelling methods which can be found in the
literature. The article concludes with a discussion of open problems which may
form the basis for fruitful future research.",2022-11-13 15:59:21
XXX,journalArticle,2017,"Spyros Gkezerlis, Dimitris Kalles",Decision Trees for Helpdesk Advisor Graphs,"Bulletin of the Technical Committee on Learning Technology, Volume
  18, Issue 2-3, April 2016",,,,http://arxiv.org/abs/1710.07075v1,"We use decision trees to build a helpdesk agent reference network to
facilitate the on-the-job advising of junior or less experienced staff on how
to better address telecommunication customer fault reports. Such reports
generate field measurements and remote measurements which, when coupled with
location data and client attributes, and fused with organization-level
statistics, can produce models of how support should be provided. Beyond
decision support, these models can help identify staff who can act as advisors,
based on the quality, consistency and predictability of dealing with complex
troubleshooting reports. Advisor staff models are then used to guide less
experienced staff in their decision making; thus, we advocate the deployment of
a simple mechanism which exploits the availability of staff with a sound track
record at the helpdesk to act as dormant tutors.",2022-11-13 15:59:22
XXX,journalArticle,2017,Fabio Massimo Zanzotto,Human-in-the-loop Artificial Intelligence,"Journal of Artificial Intelligence Research, 2019",,,10.1613/jair.1.11345,http://arxiv.org/abs/1710.08191v1,"Little by little, newspapers are revealing the bright future that Artificial
Intelligence (AI) is building. Intelligent machines will help everywhere.
However, this bright future has a dark side: a dramatic job market contraction
before its unpredictable transformation. Hence, in a near future, large numbers
of job seekers will need financial support while catching up with these novel
unpredictable jobs. This possible job market crisis has an antidote inside. In
fact, the rise of AI is sustained by the biggest knowledge theft of the recent
years. Learning AI machines are extracting knowledge from unaware skilled or
unskilled workers by analyzing their interactions. By passionately doing their
jobs, these workers are digging their own graves.
  In this paper, we propose Human-in-the-loop Artificial Intelligence (HIT-AI)
as a fairer paradigm for Artificial Intelligence systems. HIT-AI will reward
aware and unaware knowledge producers with a different scheme: decisions of AI
systems generating revenues will repay the legitimate owners of the knowledge
used for taking those decisions. As modern Robin Hoods, HIT-AI researchers
should fight for a fairer Artificial Intelligence that gives back what it
steals.",2022-11-13 15:59:23
XXX,journalArticle,2017,"Andrew Critch, Stuart Russell",Servant of Many Masters: Shifting priorities in Pareto-optimal sequential decision-making,,,,,http://arxiv.org/abs/1711.00363v1,"It is often argued that an agent making decisions on behalf of two or more
principals who have different utility functions should adopt a {\em
Pareto-optimal} policy, i.e., a policy that cannot be improved upon for one
agent without making sacrifices for another. A famous theorem of Harsanyi shows
that, when the principals have a common prior on the outcome distributions of
all policies, a Pareto-optimal policy for the agent is one that maximizes a
fixed, weighted linear combination of the principals' utilities.
  In this paper, we show that Harsanyi's theorem does not hold for principals
with different priors, and derive a more precise generalization which does
hold, which constitutes our main result. In this more general case, the
relative weight given to each principal's utility should evolve over time
according to how well the agent's observations conform with that principal's
prior. The result has implications for the design of contracts, treaties, joint
ventures, and robots.",2022-11-13 15:59:24
XXX,journalArticle,2017,"Dylan Hadfield-Menell, Smitha Milli, Pieter Abbeel, Stuart Russell, Anca Dragan",Inverse Reward Design,,,,,http://arxiv.org/abs/1711.02827v2,"Autonomous agents optimize the reward function we give them. What they don't
know is how hard it is for us to design a reward function that actually
captures what we want. When designing the reward, we might think of some
specific training scenarios, and make sure that the reward will lead to the
right behavior in those scenarios. Inevitably, agents encounter new scenarios
(e.g., new types of terrain) where optimizing that same reward may lead to
undesired behavior. Our insight is that reward functions are merely
observations about what the designer actually wants, and that they should be
interpreted in the context in which they were designed. We introduce inverse
reward design (IRD) as the problem of inferring the true objective based on the
designed reward and the training MDP. We introduce approximate methods for
solving IRD problems, and use their solution to plan risk-averse behavior in
test MDPs. Empirical results suggest that this approach can help alleviate
negative side effects of misspecified reward functions and mitigate reward
hacking.",2022-11-13 15:59:24
XXX,journalArticle,2017,"Stuart Armstrong, Xavier O'Rorke",Good and safe uses of AI Oracles,,,,,http://arxiv.org/abs/1711.05541v5,"It is possible that powerful and potentially dangerous artificial
intelligence (AI) might be developed in the future. An Oracle is a design which
aims to restrain the impact of a potentially dangerous AI by restricting the
agent to no actions besides answering questions. Unfortunately, most Oracles
will be motivated to gain more control over the world by manipulating users
through the content of their answers, and Oracles of potentially high
intelligence might be very successful at this
\citep{DBLP:journals/corr/AlfonsecaCACAR16}. In this paper we present two
designs for Oracles which, even under pessimistic assumptions, will not
manipulate their users into releasing them and yet will still be incentivised
to provide their users with helpful answers. The first design is the
counterfactual Oracle -- which choses its answer as if it expected nobody to
ever read it. The second design is the low-bandwidth Oracle -- which is limited
by the quantity of information it can transmit.",2022-11-13 15:59:25
XXX,journalArticle,2017,"Housam Khalifa Bashier Babiker, Randy Goebel",Using KL-divergence to focus Deep Visual Explanation,,,,,http://arxiv.org/abs/1711.06431v2,"We present a method for explaining the image classification predictions of
deep convolution neural networks, by highlighting the pixels in the image which
influence the final class prediction. Our method requires the identification of
a heuristic method to select parameters hypothesized to be most relevant in
this prediction, and here we use Kullback-Leibler divergence to provide this
focus. Overall, our approach helps in understanding and interpreting deep
network predictions and we hope contributes to a foundation for such
understanding of deep learning networks. In this brief paper, our experiments
evaluate the performance of two popular networks in this context of
interpretability.",2022-11-13 15:59:25
XXX,journalArticle,2017,"Daniel Levy, Stefano Ermon",Deterministic Policy Optimization by Combining Pathwise and Score Function Estimators for Discrete Action Spaces,,,,,http://arxiv.org/abs/1711.08068v1,"Policy optimization methods have shown great promise in solving complex
reinforcement and imitation learning tasks. While model-free methods are
broadly applicable, they often require many samples to optimize complex
policies. Model-based methods greatly improve sample-efficiency but at the cost
of poor generalization, requiring a carefully handcrafted model of the system
dynamics for each task. Recently, hybrid methods have been successful in
trading off applicability for improved sample-complexity. However, these have
been limited to continuous action spaces. In this work, we present a new hybrid
method based on an approximation of the dynamics as an expectation over the
next state under the current policy. This relaxation allows us to derive a
novel hybrid policy gradient estimator, combining score function and pathwise
derivative estimators, that is applicable to discrete action spaces. We show
significant gains in sample complexity, ranging between $1.7$ and $25\times$,
when learning parameterized policies on Cart Pole, Acrobot, Mountain Car and
Hand Mass. Our method is applicable to both discrete and continuous action
spaces, when competing pathwise methods are limited to the latter.",2022-11-13 15:59:26
XXX,journalArticle,2017,"M. Botvinick, D. G. T. Barrett, P. Battaglia, N. de Freitas, D. Kumaran, J. Z Leibo, T. Lillicrap, J. Modayil, S. Mohamed, N. C. Rabinowitz, D. J. Rezende, A. Santoro, T. Schaul, C. Summerfield, G. Wayne, T. Weber, D. Wierstra, S. Legg, D. Hassabis","Building Machines that Learn and Think for Themselves: Commentary on Lake et al., Behavioral and Brain Sciences, 2017",,,,,http://arxiv.org/abs/1711.08378v1,"We agree with Lake and colleagues on their list of key ingredients for
building humanlike intelligence, including the idea that model-based reasoning
is essential. However, we favor an approach that centers on one additional
ingredient: autonomy. In particular, we aim toward agents that can both build
and exploit their own internal models, with minimal human hand-engineering. We
believe an approach centered on autonomous learning has the greatest chance of
success as we scale toward real-world complexity, tackling domains for which
ready-made formal models are not available. Here we survey several important
examples of the progress that has been made toward building autonomous agents
with humanlike abilities, and highlight some outstanding challenges.",2022-11-13 15:59:26
XXX,journalArticle,2017,"Stuart Armstrong, Sören Mindermann",Occam's razor is insufficient to infer the preferences of irrational agents,,,,,http://arxiv.org/abs/1712.05812v6,"Inverse reinforcement learning (IRL) attempts to infer human rewards or
preferences from observed behavior. Since human planning systematically
deviates from rationality, several approaches have been tried to account for
specific human shortcomings. However, the general problem of inferring the
reward function of an agent of unknown rationality has received little
attention. Unlike the well-known ambiguity problems in IRL, this one is
practically relevant but cannot be resolved by observing the agent's policy in
enough environments. This paper shows (1) that a No Free Lunch result implies
it is impossible to uniquely decompose a policy into a planning algorithm and
reward function, and (2) that even with a reasonable simplicity prior/Occam's
razor on the set of decompositions, we cannot distinguish between the true
decomposition and others that lead to high regret. To address this, we need
simple `normative' assumptions, which cannot be deduced exclusively from
observations.",2022-11-13 15:59:28
XXX,journalArticle,2017,"Stuart Armstrong, Xavier O'Rourke",'Indifference' methods for managing agent rewards,,,,,http://arxiv.org/abs/1712.06365v4,"`Indifference' refers to a class of methods used to control reward based
agents. Indifference techniques aim to achieve one or more of three distinct
goals: rewards dependent on certain events (without the agent being motivated
to manipulate the probability of those events), effective disbelief (where
agents behave as if particular events could never happen), and seamless
transition from one reward function to another (with the agent acting as if
this change is unanticipated). This paper presents several methods for
achieving these goals in the POMDP setting, establishing their uses, strengths,
and requirements. These methods of control work even when the implications of
the agent's reward are otherwise not fully understood.",2022-11-13 15:59:29
XXX,journalArticle,2017,"Feng Liu, Yong Shi, Ying Liu",Three IQs of AI Systems and their Testing Methods,,,,,http://arxiv.org/abs/1712.06440v1,"The rapid development of artificial intelligence has brought the artificial
intelligence threat theory as well as the problem about how to evaluate the
intelligence level of intelligent products. Both need to find a quantitative
method to evaluate the intelligence level of intelligence systems, including
human intelligence. Based on the standard intelligence system and the extended
Von Neumann architecture, this paper proposes General IQ, Service IQ and Value
IQ evaluation methods for intelligence systems, depending on different
evaluation purposes. Among them, the General IQ of intelligence systems is to
answer the question of whether the artificial intelligence can surpass the
human intelligence, which is reflected in putting the intelligence systems on
an equal status and conducting the unified evaluation. The Service IQ and Value
IQ of intelligence systems are used to answer the question of how the
intelligent products can better serve the human, reflecting the intelligence
and required cost of each intelligence system as a product in the process of
serving human.",2022-11-13 15:59:29
XXX,journalArticle,2018,"Ziming Li, Julia Kiseleva, Alekh Agarwal, Maarten de Rijke",Learning Data-Driven Objectives to Optimize Interactive Systems,,,,,http://arxiv.org/abs/1802.06306v8,"Effective optimization is essential for interactive systems to provide a
satisfactory user experience. However, it is often challenging to find an
objective to optimize for. Generally, such objectives are manually crafted and
rarely capture complex user needs in an accurate manner. We propose an approach
that infers the objective directly from observed user interactions. These
inferences can be made regardless of prior knowledge and across different types
of user behavior. We introduce interactive system optimization, a novel
algorithm that uses these inferred objectives for optimization. Our main
contribution is a new general principled approach to optimizing interactive
systems using data-driven objectives. We demonstrate the high effectiveness of
interactive system optimization over several simulations.",2022-11-13 15:59:30
XXX,journalArticle,2018,"Miles Brundage, Shahar Avin, Jack Clark, Helen Toner, Peter Eckersley, Ben Garfinkel, Allan Dafoe, Paul Scharre, Thomas Zeitzoff, Bobby Filar, Hyrum Anderson, Heather Roff, Gregory C. Allen, Jacob Steinhardt, Carrick Flynn, Seán Ó hÉigeartaigh, Simon Beard, Haydn Belfield, Sebastian Farquhar, Clare Lyle, Rebecca Crootof, Owain Evans, Michael Page, Joanna Bryson, Roman Yampolskiy, Dario Amodei","The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation",,,,,http://arxiv.org/abs/1802.07228v1,"This report surveys the landscape of potential security threats from
malicious uses of AI, and proposes ways to better forecast, prevent, and
mitigate these threats. After analyzing the ways in which AI may influence the
threat landscape in the digital, physical, and political domains, we make four
high-level recommendations for AI researchers and other stakeholders. We also
suggest several promising areas for further research that could expand the
portfolio of defenses, or make attacks less effective or harder to execute.
Finally, we discuss, but do not conclusively resolve, the long-term equilibrium
of attackers and defenders.",2022-11-13 15:59:30
XXX,journalArticle,2018,"Anusha Mujumdar, Swarup Kumar Mohalik, Ramamurthy Badrinath",Antifragility for Intelligent Autonomous Systems,,,,,http://arxiv.org/abs/1802.09159v1,"Antifragile systems grow measurably better in the presence of hazards. This
is in contrast to fragile systems which break down in the presence of hazards,
robust systems that tolerate hazards up to a certain degree, and resilient
systems that -- like self-healing systems -- revert to their earlier expected
behavior after a period of convalescence. The notion of antifragility was
introduced by Taleb for economics systems, but its applicability has been
illustrated in biological and engineering domains as well. In this paper, we
propose an architecture that imparts antifragility to intelligent autonomous
systems, specifically those that are goal-driven and based on AI-planning. We
argue that this architecture allows the system to self-improve by uncovering
new capabilities obtained either through the hazards themselves (opportunistic)
or through deliberation (strategic). An AI planning-based case study of an
autonomous wheeled robot is presented. We show that with the proposed
architecture, the robot develops antifragile behaviour with respect to an oil
spill hazard.",2022-11-13 15:59:31
XXX,journalArticle,2018,"Alexander Boer, Giovanni Sileno",Institutional Metaphors for Designing Large-Scale Distributed AI versus AI Techniques for Running Institutions,,,,,http://arxiv.org/abs/1803.03407v2,"Artificial Intelligence (AI) started out with an ambition to reproduce the
human mind, but, as the sheer scale of that ambition became manifest, it
quickly retreated into either studying specialized intelligent behaviours, or
proposing over-arching architectural concepts for interfacing specialized
intelligent behaviour components, conceived of as agents in a kind of
organization. This agent-based modeling paradigm, in turn, proves to have
interesting applications in understanding, simulating, and predicting the
behaviour of social and legal structures on an aggregate level. For these
reasons, this chapter examines a number of relevant cross-cutting concerns,
conceptualizations, modeling problems and design challenges in large-scale
distributed Artificial Intelligence, as well as in institutional systems, and
identifies potential grounds for novel advances.",2022-11-13 15:59:31
XXX,journalArticle,2018,"Daniel S. Weld, Gagan Bansal",The Challenge of Crafting Intelligible Intelligence,,,,,http://arxiv.org/abs/1803.04263v3,"Since Artificial Intelligence (AI) software uses techniques like deep
lookahead search and stochastic optimization of huge neural networks to fit
mammoth datasets, it often results in complex behavior that is difficult for
people to understand. Yet organizations are deploying AI algorithms in many
mission-critical settings. To trust their behavior, we must make AI
intelligible, either by using inherently interpretable models or by developing
new methods for explaining and controlling otherwise overwhelmingly complex
decisions using local approximation, vocabulary alignment, and interactive
explanation. This paper argues that intelligibility is essential, surveys
recent work on building such systems, and highlights key directions for
research.",2022-11-13 15:59:32
XXX,journalArticle,2018,"Virginia Dignum, Frank Dignum",A Logic of Agent Organizations,"Logic Journal of the IGPL, vol. 20, no. 1, pp. 283-316, Feb. 2012",,,10.1093/jigpal/jzr041,http://arxiv.org/abs/1804.10817v1,"Organization concepts and models are increasingly being adopted for the
design and specification of multi-agent systems. Agent organizations can be
seen as mechanisms of social order, created to achieve global (or
organizational) objectives by more or less autonomous agents. In order to
develop a theory on the relation between organizational structures,
organizational objectives and the actions of agents fulfilling roles in the
organization a theoretical framework is needed to describe organizational
structures and actions of (groups of) agents. Current logical formalisms focus
on specific aspects of organizations (e.g. power, delegation, agent actions, or
normative issues) but a framework that integrates and relates different aspects
is missing. Given the amount of aspects involved and the subsequent complexity
of a formalism encompassing them all, it is difficult to realize. In this
paper, a first step is taken to solve this problem. We present a generic formal
model that enables to specify and relate the main concepts of an organization
(including, activity, structure, environment and others) so that organizations
can be analyzed at a high level of abstraction. However, for some aspects we
use a simplified model in order to avoid the complexity of combining many
different types of (modal) operators.",2022-11-13 15:59:33
XXX,journalArticle,2018,"Richard Tomsett, Dave Braines, Dan Harborne, Alun Preece, Supriyo Chakraborty",Interpretable to Whom? A Role-based Model for Analyzing Interpretable Machine Learning Systems,,,,,http://arxiv.org/abs/1806.07552v1,"Several researchers have argued that a machine learning system's
interpretability should be defined in relation to a specific agent or task: we
should not ask if the system is interpretable, but to whom is it interpretable.
We describe a model intended to help answer this question, by identifying
different roles that agents can fulfill in relation to the machine learning
system. We illustrate the use of our model in a variety of scenarios, exploring
how an agent's role influences its goals, and the implications for defining
interpretability. Finally, we make suggestions for how our model could be
useful to interpretability researchers, system developers, and regulatory
bodies auditing machine learning systems.",2022-11-13 15:59:33
XXX,journalArticle,2018,Eray Özkural,The Foundations of Deep Learning with a Path Towards General Intelligence,,,,,http://arxiv.org/abs/1806.08874v1,"Like any field of empirical science, AI may be approached axiomatically. We
formulate requirements for a general-purpose, human-level AI system in terms of
postulates. We review the methodology of deep learning, examining the explicit
and tacit assumptions in deep learning research. Deep Learning methodology
seeks to overcome limitations in traditional machine learning research as it
combines facets of model richness, generality, and practical applicability. The
methodology so far has produced outstanding results due to a productive synergy
of function approximation, under plausible assumptions of irreducibility and
the efficiency of back-propagation family of algorithms. We examine these
winning traits of deep learning, and also observe the various known failure
modes of deep learning. We conclude by giving recommendations on how to extend
deep learning methodology to cover the postulates of general-purpose AI
including modularity, and cognitive architecture. We also relate deep learning
to advances in theoretical neuroscience research.",2022-11-13 15:59:34
XXX,journalArticle,2018,"James Max Kanter, Benjamin Schreck, Kalyan Veeramachaneni",Machine learning 2.0 : Engineering Data Driven AI Products,,,,,http://arxiv.org/abs/1807.00401v1,"ML 2.0: In this paper, we propose a paradigm shift from the current practice
of creating machine learning models - which requires months-long discovery,
exploration and ""feasibility report"" generation, followed by re-engineering for
deployment - in favor of a rapid, 8-week process of development, understanding,
validation and deployment that can executed by developers or subject matter
experts (non-ML experts) using reusable APIs. This accomplishes what we call a
""minimum viable data-driven model,"" delivering a ready-to-use machine learning
model for problems that haven't been solved before using machine learning. We
provide provisions for the refinement and adaptation of the ""model,"" with
strict enforcement and adherence to both the scaffolding/abstractions and the
process. We imagine that this will bring forth the second phase in machine
learning, in which discovery is subsumed by more targeted goals of delivery and
impact.",2022-11-13 15:59:35
XXX,journalArticle,2018,"Catarina Moreira, Andreas Wichert",Introducing Quantum-Like Influence Diagrams for Violations of the Sure Thing Principle,"Quantum Interactions, 2018",,,,http://arxiv.org/abs/1807.06142v2,"It is the focus of this work to extend and study the previously proposed
quantum-like Bayesian networks to deal with decision-making scenarios by
incorporating the notion of maximum expected utility in influence diagrams. The
general idea is to take advantage of the quantum interference terms produced in
the quantum-like Bayesian Network to influence the probabilities used to
compute the expected utility of some action. This way, we are not proposing a
new type of expected utility hypothesis. On the contrary, we are keeping it
under its classical definition. We are only incorporating it as an extension of
a probabilistic graphical model in a compact graphical representation called an
influence diagram in which the utility function depends on the probabilistic
influences of the quantum-like Bayesian network.
  Our findings suggest that the proposed quantum-like influence digram can
indeed take advantage of the quantum interference effects of quantum-like
Bayesian Networks to maximise the utility of a cooperative behaviour in
detriment of a fully rational defect behaviour under the prisoner's dilemma
game.",2022-11-13 15:59:35
XXX,journalArticle,2018,Jean-Marie Chauvet,The 30-Year Cycle In The AI Debate,,,,,http://arxiv.org/abs/1810.04053v1,"In the last couple of years, the rise of Artificial Intelligence and the
successes of academic breakthroughs in the field have been inescapable. Vast
sums of money have been thrown at AI start-ups. Many existing tech companies --
including the giants like Google, Amazon, Facebook, and Microsoft -- have
opened new research labs. The rapid changes in these everyday work and
entertainment tools have fueled a rising interest in the underlying technology
itself; journalists write about AI tirelessly, and companies -- of tech nature
or not -- brand themselves with AI, Machine Learning or Deep Learning whenever
they get a chance. Confronting squarely this media coverage, several analysts
are starting to voice concerns about over-interpretation of AI's blazing
successes and the sometimes poor public reporting on the topic. This paper
reviews briefly the track-record in AI and Machine Learning and finds this
pattern of early dramatic successes, followed by philosophical critique and
unexpected difficulties, if not downright stagnation, returning almost to the
clock in 30-year cycles since 1958.",2022-11-13 15:59:36
XXX,journalArticle,2018,"Yuu Jinnai, David Abel, D Ellis Hershkowitz, Michael Littman, George Konidaris",Finding Options that Minimize Planning Time,,,,,http://arxiv.org/abs/1810.07311v3,"We formalize the problem of selecting the optimal set of options for planning
as that of computing the smallest set of options so that planning converges in
less than a given maximum of value-iteration passes. We first show that the
problem is NP-hard, even if the task is constrained to be deterministic---the
first such complexity result for option discovery. We then present the first
polynomial-time boundedly suboptimal approximation algorithm for this setting,
and empirically evaluate it against both the optimal options and a
representative collection of heuristic approaches in simple grid-based domains
including the classic four-rooms problem.",2022-11-13 15:59:37
XXX,journalArticle,2018,"Tae Wan Kim, Thomas Donaldson, John Hooker",Mimetic vs Anchored Value Alignment in Artificial Intelligence,,,,,http://arxiv.org/abs/1810.11116v1,"""Value alignment"" (VA) is considered as one of the top priorities in AI
research. Much of the existing research focuses on the ""A"" part and not the ""V""
part of ""value alignment."" This paper corrects that neglect by emphasizing the
""value"" side of VA and analyzes VA from the vantage point of requirements in
value theory, in particular, of avoiding the ""naturalistic fallacy""--a major
epistemic caveat. The paper begins by isolating two distinct forms of VA:
""mimetic"" and ""anchored."" Then it discusses which VA approach better avoids the
naturalistic fallacy. The discussion reveals stumbling blocks for VA approaches
that neglect implications of the naturalistic fallacy. Such problems are more
serious in mimetic VA since the mimetic process imitates human behavior that
may or may not rise to the level of correct ethical behavior. Anchored VA,
including hybrid VA, in contrast, holds more promise for future VA since it
anchors alignment by normative concepts of intrinsic value.",2022-11-13 15:59:38
XXX,journalArticle,2018,Dmitry Maximov,An Optimal Itinerary Generation in a Configuration Space of Large Intellectual Agent Groups with Linear Logic,"Advances in Systems Science and Applications. 2019. Vol. 19, No 4.
  P. 79-86 https://ijassa.ipu.ru/index.php/ijassa/article/view/829/513",,,,http://arxiv.org/abs/1811.02216v1,"A group of intelligent agents which fulfill a set of tasks in parallel is
represented first by the tensor multiplication of corresponding processes in a
linear logic game category. An optimal itinerary in the configuration space of
the group states is defined as a play with maximal total reward in the
category. New moments also are: the reward is represented as a degree of
certainty (visibility) of an agent goal, and the system goals are chosen by the
greatest value corresponding to these processes in the system goal lattice.",2022-11-13 15:59:39
XXX,journalArticle,2018,Daniel Muller,Economics of Human-AI Ecosystem: Value Bias and Lost Utility in Multi-Dimensional Gaps,,,,,http://arxiv.org/abs/1811.06606v2,"In recent years, artificial intelligence (AI) decision-making and autonomous
systems became an integrated part of the economy, industry, and society. The
evolving economy of the human-AI ecosystem raising concerns regarding the risks
and values inherited in AI systems. This paper investigates the dynamics of
creation and exchange of values and points out gaps in perception of
cost-value, knowledge, space and time dimensions. It shows aspects of value
bias in human perception of achievements and costs that encoded in AI systems.
It also proposes rethinking hard goals definitions and cost-optimal
problem-solving principles in the lens of effectiveness and efficiency in the
development of trusted machines. The paper suggests a value-driven with cost
awareness strategy and principles for problem-solving and planning of effective
research progress to address real-world problems that involve diverse forms of
achievements, investments, and survival scenarios.",2022-11-13 15:59:39
XXX,journalArticle,2018,"Naveen Sundar Govindarajulu, Selmer Bringsjord, Rikhiya Ghosh",Toward the Engineering of Virtuous Machines,,,,,http://arxiv.org/abs/1812.03868v2,"While various traditions under the 'virtue ethics' umbrella have been studied
extensively and advocated by ethicists, it has not been clear that there exists
a version of virtue ethics rigorous enough to be a target for machine ethics
(which we take to include the engineering of an ethical sensibility in a
machine or robot itself, not only the study of ethics in the humans who might
create artificial agents). We begin to address this by presenting an embryonic
formalization of a key part of any virtue-ethics theory: namely, the learning
of virtue by a focus on exemplars of moral virtue. Our work is based in part on
a computational formal logic previously used to formally model other ethical
theories and principles therein, and to implement these models in artificial
agents.",2022-11-13 15:59:40
XXX,journalArticle,2018,"Yi Zeng, Enmeng Lu, Cunqing Huangfu",Linking Artificial Intelligence Principles,,,,,http://arxiv.org/abs/1812.04814v1,"Artificial Intelligence principles define social and ethical considerations
to develop future AI. They come from research institutes, government
organizations and industries. All versions of AI principles are with different
considerations covering different perspectives and making different emphasis.
None of them can be considered as complete and can cover the rest AI principle
proposals. Here we introduce LAIP, an effort and platform for linking and
analyzing different Artificial Intelligence Principles. We want to explicitly
establish the common topics and links among AI Principles proposed by different
organizations and investigate on their uniqueness. Based on these efforts, for
the long-term future of AI, instead of directly adopting any of the AI
principles, we argue for the necessity of incorporating various AI Principles
into a comprehensive framework and focusing on how they can interact and
complete each other.",2022-11-13 15:59:40
XXX,journalArticle,2018,Tshilidzi Marwala,Can rationality be measured?,,,,,http://arxiv.org/abs/1812.10144v1,"This paper studies whether rationality can be computed. Rationality is
defined as the use of complete information, which is processed with a perfect
biological or physical brain, in an optimized fashion. To compute rationality
one needs to quantify how complete is the information, how perfect is the
physical or biological brain and how optimized is the entire decision making
system. The rationality of a model (i.e. physical or biological brain) is
measured by the expected accuracy of the model. The rationality of the
optimization procedure is measured as the ratio of the achieved objective (i.e.
utility) to the global objective. The overall rationality of a decision is
measured as the product of the rationality of the model and the rationality of
the optimization procedure. The conclusion reached is that rationality can be
computed for convex optimization problems.",2022-11-13 15:59:41
XXX,journalArticle,2019,"Alexander Matt Turner, Dylan Hadfield-Menell, Prasad Tadepalli",Conservative Agency via Attainable Utility Preservation,,,,10.1145/3375627.3375851,http://arxiv.org/abs/1902.09725v3,"Reward functions are easy to misspecify; although designers can make
corrections after observing mistakes, an agent pursuing a misspecified reward
function can irreversibly change the state of its environment. If that change
precludes optimization of the correctly specified reward function, then
correction is futile. For example, a robotic factory assistant could break
expensive equipment due to a reward misspecification; even if the designers
immediately correct the reward function, the damage is done. To mitigate this
risk, we introduce an approach that balances optimization of the primary reward
function with preservation of the ability to optimize auxiliary reward
functions. Surprisingly, even when the auxiliary reward functions are randomly
generated and therefore uninformative about the correctly specified reward
function, this approach induces conservative, effective behavior.",2022-11-13 15:59:41
XXX,journalArticle,2019,"Tom Everitt, Pedro A. Ortega, Elizabeth Barnes, Shane Legg",Understanding Agent Incentives using Causal Influence Diagrams. Part I: Single Action Settings,,,,,http://arxiv.org/abs/1902.09980v7,"Agents are systems that optimize an objective function in an environment.
Together, the goal and the environment induce secondary objectives, incentives.
Modeling the agent-environment interaction using causal influence diagrams, we
can answer two fundamental questions about an agent's incentives directly from
the graph: (1) which nodes can the agent have an incentivize to observe, and
(2) which nodes can the agent have an incentivize to control? The answers tell
us which information and influence points need extra protection. For example,
we may want a classifier for job applications to not use the ethnicity of the
candidate, and a reinforcement learning agent not to take direct control of its
reward mechanism. Different algorithms and training paradigms can lead to
different causal influence diagrams, so our method can be used to identify
algorithms with problematic incentives and help in designing algorithms with
better incentives.",2022-11-13 15:59:42
XXX,journalArticle,2019,Thilo Hagendorff,The Ethics of AI Ethics -- An Evaluation of Guidelines,"Minds & Machines, 2020",,,10.1007/s11023-020-09517-8,http://arxiv.org/abs/1903.03425v2,"Current advances in research, development and application of artificial
intelligence (AI) systems have yielded a far-reaching discourse on AI ethics.
In consequence, a number of ethics guidelines have been released in recent
years. These guidelines comprise normative principles and recommendations aimed
to harness the ""disruptive"" potentials of new AI technologies. Designed as a
comprehensive evaluation, this paper analyzes and compares these guidelines
highlighting overlaps but also omissions. As a result, I give a detailed
overview of the field of AI ethics. Finally, I also examine to what extent the
respective ethical principles and values are implemented in the practice of
research, development and application of AI systems - and how the effectiveness
in the demands of AI ethics can be improved.",2022-11-13 15:59:42
XXX,journalArticle,2019,"Smitha Milli, Anca D. Dragan",Literal or Pedagogic Human? Analyzing Human Model Misspecification in Objective Learning,,,,,http://arxiv.org/abs/1903.03877v2,"It is incredibly easy for a system designer to misspecify the objective for
an autonomous system (""robot''), thus motivating the desire to have the robot
learn the objective from human behavior instead. Recent work has suggested that
people have an interest in the robot performing well, and will thus behave
pedagogically, choosing actions that are informative to the robot. In turn,
robots benefit from interpreting the behavior by accounting for this pedagogy.
In this work, we focus on misspecification: we argue that robots might not know
whether people are being pedagogic or literal and that it is important to ask
which assumption is safer to make. We cast objective learning into the more
general form of a common-payoff game between the robot and human, and prove
that in any such game literal interpretation is more robust to
misspecification. Experiments with human data support our theoretical results
and point to the sensitivity of the pedagogic assumption.",2022-11-13 15:59:43
XXX,journalArticle,2019,"Bharat Prakash, Mohit Khatwani, Nicholas Waytowich, Tinoosh Mohsenin",Improving Safety in Reinforcement Learning Using Model-Based Architectures and Human Intervention,,,,,http://arxiv.org/abs/1903.09328v1,"Recent progress in AI and Reinforcement learning has shown great success in
solving complex problems with high dimensional state spaces. However, most of
these successes have been primarily in simulated environments where failure is
of little or no consequence. Most real-world applications, however, require
training solutions that are safe to operate as catastrophic failures are
inadmissible especially when there is human interaction involved. Currently,
Safe RL systems use human oversight during training and exploration in order to
make sure the RL agent does not go into a catastrophic state. These methods
require a large amount of human labor and it is very difficult to scale up. We
present a hybrid method for reducing the human intervention time by combining
model-based approaches and training a supervised learner to improve sample
efficiency while also ensuring safety. We evaluate these methods on various
grid-world environments using both standard and visual representations and show
that our approach achieves better performance in terms of sample efficiency,
number of catastrophic states reached as well as overall task performance
compared to traditional model-free approaches",2022-11-13 15:59:43
XXX,journalArticle,2019,"Mohannad Babli, Eva Onaindia, Eliseo Marzal",Extending planning knowledge using ontologies for goal opportunities,"31st IBIMA Conference (2018), INNOVATION MANAGEMENT AND EDUCATION
  EXCELLENCE THROUGH VISION 2020, VOLS IV-VI (3199-3208)",,,,http://arxiv.org/abs/1904.03606v1,"Approaches to goal-directed behaviour including online planning and
opportunistic planning tackle a change in the environment by generating
alternative goals to avoid failures or seize opportunities. However, current
approaches only address unanticipated changes related to objects or object
types already defined in the planning task that is being solved. This article
describes a domain-independent approach that advances the state of the art by
extending the knowledge of a planning task with relevant objects of new types.
The approach draws upon the use of ontologies, semantic measures, and ontology
alignment to accommodate newly acquired data that trigger the formulation of
goal opportunities inducing a better-valued plan.",2022-11-13 15:59:44
XXX,journalArticle,2019,George Cevora,The relationship between Biological and Artificial Intelligence,,,,,http://arxiv.org/abs/1905.00547v1,"Intelligence can be defined as a predominantly human ability to accomplish
tasks that are generally hard for computers and animals. Artificial
Intelligence [AI] is a field attempting to accomplish such tasks with
computers. AI is becoming increasingly widespread, as are claims of its
relationship with Biological Intelligence. Often these claims are made to imply
higher chances of a given technology succeeding, working on the assumption that
AI systems which mimic the mechanisms of Biological Intelligence should be more
successful.
  In this article I will discuss the similarities and differences between AI
and the extent of our knowledge about the mechanisms of intelligence in
biology, especially within humans. I will also explore the validity of the
assumption that biomimicry in AI systems aids their advancement, and I will
argue that existing similarity to biological systems in the way Artificial
Neural Networks [ANNs] tackle tasks is due to design decisions, rather than
inherent similarity of underlying mechanisms. This article is aimed at people
who understand the basics of AI (especially ANNs), and would like to be better
able to evaluate the often wild claims about the value of biomimicry in AI.",2022-11-13 15:59:44
XXX,journalArticle,2019,"Philip Feldman, Aaron Dant, Aaron Massey",Integrating Artificial Intelligence into Weapon Systems,,,,,http://arxiv.org/abs/1905.03899v1,"The integration of Artificial Intelligence (AI) into weapon systems is one of
the most consequential tactical and strategic decisions in the history of
warfare. Current AI development is a remarkable combination of accelerating
capability, hidden decision mechanisms, and decreasing costs. Implementation of
these systems is in its infancy and exists on a spectrum from resilient and
flexible to simplistic and brittle. Resilient systems should be able to
effectively handle the complexities of a high-dimensional battlespace.
Simplistic AI implementations could be manipulated by an adversarial AI that
identifies and exploits their weaknesses.
  In this paper, we present a framework for understanding the development of
dynamic AI/ML systems that interactively and continuously adapt to their user's
needs. We explore the implications of increasingly capable AI in the kill chain
and how this will lead inevitably to a fully automated, always on system,
barring regulation by treaty. We examine the potential of total integration of
cyber and physical security and how this likelihood must inform the development
of AI-enabled systems with respect to the ""fog of war"", human morals, and
ethics.",2022-11-13 15:59:45
XXX,journalArticle,2019,"Stefano Giovanni Rizzo, Ji Lucas, Zoi Kaoudi, Jorge-Arnulfo Quiane-Ruiz, Sanjay Chawla",AI-CARGO: A Data-Driven Air-Cargo Revenue Management System,,,,,http://arxiv.org/abs/1905.09130v1,"We propose AI-CARGO, a revenue management system for air-cargo that combines
machine learning prediction with decision-making using mathematical
optimization methods. AI-CARGO addresses a problem that is unique to the
air-cargo business, namely the wide discrepancy between the quantity (weight or
volume) that a shipper will book and the actual received amount at departure
time by the airline. The discrepancy results in sub-optimal and inefficient
behavior by both the shipper and the airline resulting in the overall loss of
potential revenue for the airline. AI-CARGO also includes a data cleaning
component to deal with the heterogeneous forms in which booking data is
transmitted to the airline cargo system. AI-CARGO is deployed in the production
environment of a large commercial airline company. We have validated the
benefits of AI-CARGO using real and synthetic datasets. Especially, we have
carried out simulations using dynamic programming techniques to elicit the
impact on offloading costs and revenue generation of our proposed system. Our
results suggest that combining prediction within a decision-making framework
can help dramatically to reduce offloading costs and optimize revenue
generation.",2022-11-13 15:59:45
XXX,journalArticle,2019,"Cristian Ivan, Bipin Indurkhya",On modelling the emergence of logical thinking,,,,,http://arxiv.org/abs/1905.09730v1,"Recent progress in machine learning techniques have revived interest in
building artificial general intelligence using these particular tools. There
has been a tremendous success in applying them for narrow intellectual tasks
such as pattern recognition, natural language processing and playing Go. The
latter application vastly outperforms the strongest human player in recent
years. However, these tasks are formalized by people in such ways that it has
become ""easy"" for automated recipes to find better solutions than humans do. In
the sense of John Searle's Chinese Room Argument, the computer playing Go does
not actually understand anything from the game. Thinking like a human mind
requires to go beyond the curve fitting paradigm of current systems. There is a
fundamental limit to what they can achieve currently as only very specific
problem formalization can increase their performances in particular tasks. In
this paper, we argue than one of the most important aspects of the human mind
is its capacity for logical thinking, which gives rise to many intellectual
expressions that differentiate us from animal brains. We propose to model the
emergence of logical thinking based on Piaget's theory of cognitive
development.",2022-11-13 15:59:46
XXX,journalArticle,2019,"Tae Wan Kim, Thomas Donaldson, John Hooker",Grounding Value Alignment with Ethical Principles,,,,,http://arxiv.org/abs/1907.05447v1,"An important step in the development of value alignment (VA) systems in AI is
understanding how values can interrelate with facts. Designers of future VA
systems will need to utilize a hybrid approach in which ethical reasoning and
empirical observation interrelate successfully in machine behavior. In this
article we identify two problems about this interrelation that have been
overlooked by AI discussants and designers. The first problem is that many AI
designers commit inadvertently a version of what has been called by moral
philosophers the ""naturalistic fallacy,"" that is, they attempt to derive an
""ought"" from an ""is."" We illustrate when and why this occurs. The second
problem is that AI designers adopt training routines that fail fully to
simulate human ethical reasoning in the integration of ethical principles and
facts. Using concepts of quantified modal logic, we proceed to offer an
approach that promises to simulate ethical reasoning in humans by connecting
ethical principles on the one hand and propositions about states of affairs on
the other.",2022-11-13 15:59:47
XXX,journalArticle,2019,"Charu Aggarwal, Djallel Bouneffouf, Horst Samulowitz, Beat Buesser, Thanh Hoang, Udayan Khurana, Sijia Liu, Tejaswini Pedapati, Parikshit Ram, Ambrish Rawat, Martin Wistuba, Alexander Gray",How can AI Automate End-to-End Data Science?,,,,,http://arxiv.org/abs/1910.14436v1,"Data science is labor-intensive and human experts are scarce but heavily
involved in every aspect of it. This makes data science time consuming and
restricted to experts with the resulting quality heavily dependent on their
experience and skills. To make data science more accessible and scalable, we
need its democratization. Automated Data Science (AutoDS) is aimed towards that
goal and is emerging as an important research and business topic. We introduce
and define the AutoDS challenge, followed by a proposal of a general AutoDS
framework that covers existing approaches but also provides guidance for the
development of new methods. We categorize and review the existing literature
from multiple aspects of the problem setup and employed techniques. Then we
provide several views on how AI could succeed in automating end-to-end AutoDS.
We hope this survey can serve as insightful guideline for the AutoDS field and
provide inspiration for future research.",2022-11-13 15:59:47
XXX,journalArticle,2019,"Elsa Rizk, Roula Nassif, Ali H. Sayed",Network Classifiers With Output Smoothing,,,,,http://arxiv.org/abs/1911.04870v1,"This work introduces two strategies for training network classifiers with
heterogeneous agents. One strategy promotes global smoothing over the graph and
a second strategy promotes local smoothing over neighbourhoods. It is assumed
that the feature sizes can vary from one agent to another, with some agents
observing insufficient attributes to be able to make reliable decisions on
their own. As a result, cooperation with neighbours is necessary. However, due
to the fact that the feature dimensions are different across the agents, their
classifier dimensions will also be different. This means that cooperation
cannot rely on combining the classifier parameters. We instead propose
smoothing the outputs of the classifiers, which are the predicted labels. By
doing so, the dynamics that describes the evolution of the network classifier
becomes more challenging than usual because the classifier parameters end up
appearing as part of the regularization term as well. We illustrate performance
by means of computer simulations.",2022-11-13 15:59:48
XXX,journalArticle,2019,"Roel Dobbe, Thomas Krendl Gilbert, Yonatan Mintz",Hard Choices in Artificial Intelligence: Addressing Normative Uncertainty through Sociotechnical Commitments,,,,,http://arxiv.org/abs/1911.09005v1,"As AI systems become prevalent in high stakes domains such as surveillance
and healthcare, researchers now examine how to design and implement them in a
safe manner. However, the potential harms caused by systems to stakeholders in
complex social contexts and how to address these remains unclear. In this
paper, we explain the inherent normative uncertainty in debates about the
safety of AI systems. We then address this as a problem of vagueness by
examining its place in the design, training, and deployment stages of AI system
development. We adopt Ruth Chang's theory of intuitive comparability to
illustrate the dilemmas that manifest at each stage. We then discuss how
stakeholders can navigate these dilemmas by incorporating distinct forms of
dissent into the development pipeline, drawing on Elizabeth Anderson's work on
the epistemic powers of democratic institutions. We outline a framework of
sociotechnical commitments to formal, substantive and discursive challenges
that address normative uncertainty across stakeholders, and propose the
cultivation of related virtues by those responsible for development.",2022-11-13 15:59:48
XXX,journalArticle,2019,Samuel Allen Alexander,Measuring the intelligence of an idealized mechanical knowing agent,,,,,http://arxiv.org/abs/1912.09571v1,"We define a notion of the intelligence level of an idealized mechanical
knowing agent. This is motivated by efforts within artificial intelligence
research to define real-number intelligence levels of complicated intelligent
systems. Our agents are more idealized, which allows us to define a much
simpler measure of intelligence level for them. In short, we define the
intelligence level of a mechanical knowing agent to be the supremum of the
computable ordinals that have codes the agent knows to be codes of computable
ordinals. We prove that if one agent knows certain things about another agent,
then the former necessarily has a higher intelligence level than the latter.
This allows our intelligence notion to serve as a stepping stone to obtain
results which, by themselves, are not stated in terms of our intelligence
notion (results of potential interest even to readers totally skeptical that
our notion correctly captures intelligence). As an application, we argue that
these results comprise evidence against the possibility of intelligence
explosion (that is, the notion that sufficiently intelligent machines will
eventually be capable of designing even more intelligent machines, which can
then design even more intelligent machines, and so on).",2022-11-13 15:59:49
XXX,journalArticle,2019,Jordan Ott,Questions to Guide the Future of Artificial Intelligence Research,,,,,http://arxiv.org/abs/1912.10305v2,"The field of machine learning has focused, primarily, on discretized
sub-problems (i.e. vision, speech, natural language) of intelligence. While
neuroscience tends to be observation heavy, providing few guiding theories. It
is unlikely that artificial intelligence will emerge through only one of these
disciplines. Instead, it is likely to be some amalgamation of their algorithmic
and observational findings. As a result, there are a number of problems that
should be addressed in order to select the beneficial aspects of both fields.
In this article, we propose leading questions to guide the future of artificial
intelligence research. There are clear computational principles on which the
brain operates. The problem is finding these computational needles in a
haystack of biological complexity. Biology has clear constraints but by not
using it as a guide we are constraining ourselves.",2022-11-13 15:59:49
XXX,journalArticle,2020,"Alex Kearney, Anna Koop, Patrick M. Pilarski",What's a Good Prediction? Challenges in evaluating an agent's knowledge,,,,,http://arxiv.org/abs/2001.08823v2,"Constructing general knowledge by learning task-independent models of the
world can help agents solve challenging problems. However, both constructing
and evaluating such models remains an open challenge. The most common
approaches to evaluating models is to assess their accuracy with respect to
observable values. However, the prevailing reliance on estimator accuracy as a
proxy for the usefulness of the knowledge has the potential to lead us astray.
We demonstrate the conflict between accuracy and usefulness through a series of
illustrative examples including both a thought experiment and empirical example
in MineCraft, using the General Value Function framework (GVF). Having
identified challenges in assessing an agent's knowledge, we propose an
alternate evaluation approach that arises continually in the online continual
learning setting we recommend evaluation by examining internal learning
processes, specifically the relevance of a GVF's features to the prediction
task at hand. This paper contributes a first look into evaluation of
predictions through their use, an integral component of predictive knowledge
which is as of yet unexplored.",2022-11-13 15:59:50
XXX,journalArticle,2020,"Sarath Sreedharan, Utkarsh Soni, Mudit Verma, Siddharth Srivastava, Subbarao Kambhampati",Bridging the Gap: Providing Post-Hoc Symbolic Explanations for Sequential Decision-Making Problems with Inscrutable Representations,,,,,http://arxiv.org/abs/2002.01080v4,"As increasingly complex AI systems are introduced into our daily lives, it
becomes important for such systems to be capable of explaining the rationale
for their decisions and allowing users to contest these decisions. A
significant hurdle to allowing for such explanatory dialogue could be the
vocabulary mismatch between the user and the AI system. This paper introduces
methods for providing contrastive explanations in terms of user-specified
concepts for sequential decision-making settings where the system's model of
the task may be best represented as an inscrutable model. We do this by
building partial symbolic models of a local approximation of the task that can
be leveraged to answer the user queries. We test these methods on a popular
Atari game (Montezuma's Revenge) and variants of Sokoban (a well-known planning
benchmark) and report the results of user studies to evaluate whether people
find explanations generated in this form useful.",2022-11-13 15:59:50
XXX,journalArticle,2020,Cameron Reid,Student/Teacher Advising through Reward Augmentation,,,,,http://arxiv.org/abs/2002.02938v1,"Transfer learning is an important new subfield of multiagent reinforcement
learning that aims to help an agent learn about a problem by using knowledge
that it has gained solving another problem, or by using knowledge that is
communicated to it by an agent who already knows the problem. This is useful
when one wishes to change the architecture or learning algorithm of an agent
(so that the new knowledge need not be built ""from scratch""), when new agents
are frequently introduced to the environment with no knowledge, or when an
agent must adapt to similar but different problems. Great progress has been
made in the agent-to-agent case using the Teacher/Student framework proposed by
(Torrey and Taylor 2013). However, that approach requires that learning from a
teacher be treated differently from learning in every other reinforcement
learning context. In this paper, I propose a method which allows the
teacher/student framework to be applied in a way that fits directly and
naturally into the more general reinforcement learning framework by integrating
the teacher feedback into the reward signal received by the learning agent. I
show that this approach can significantly improve the rate of learning for an
agent playing a one-player stochastic game; I give examples of potential
pitfalls of the approach; and I propose further areas of research building on
this framework.",2022-11-13 15:59:51
XXX,journalArticle,2020,"Jens Braband, Hendrik Schäbe",On Safety Assessment of Artificial Intelligence,"Dependability, vol. 20 no. 4, 2020",,,10.21683/1729-2646-2020-20-4-25-34,http://arxiv.org/abs/2003.00260v1,"In this paper we discuss how systems with Artificial Intelligence (AI) can
undergo safety assessment. This is relevant, if AI is used in safety related
applications. Taking a deeper look into AI models, we show, that many models of
artificial intelligence, in particular machine learning, are statistical
models. Safety assessment would then have t o concentrate on the model that is
used in AI, besides the normal assessment procedure. Part of the budget of
dangerous random failures for the relevant safety integrity level needs to be
used for the probabilistic faulty behavior of the AI system. We demonstrate our
thoughts with a simple example and propose a research challenge that may be
decisive for the use of AI in safety related systems.",2022-11-13 15:59:51
XXX,journalArticle,2020,"Ernesto Jiménez-Ruiz, Asan Agibetov, Jiaoyan Chen, Matthias Samwald, Valerie Cross",Dividing the Ontology Alignment Task with Semantic Embeddings and Logic-based Modules,,,,,http://arxiv.org/abs/2003.05370v1,"Large ontologies still pose serious challenges to state-of-the-art ontology
alignment systems. In this paper we present an approach that combines a neural
embedding model and logic-based modules to accurately divide an input ontology
matching task into smaller and more tractable matching (sub)tasks. We have
conducted a comprehensive evaluation using the datasets of the Ontology
Alignment Evaluation Initiative. The results are encouraging and suggest that
the proposed method is adequate in practice and can be integrated within the
workflow of systems unable to cope with very large ontologies.",2022-11-13 15:59:52
XXX,journalArticle,2020,"S. Atakishiyev, H. Babiker, N. Farruque, R. Goebel1, M-Y. Kima, M. H. Motallebi, J. Rabelo, T. Syed, O. R. Zaïane",A multi-component framework for the analysis and design of explainable artificial intelligence,,,,,http://arxiv.org/abs/2005.01908v1,"The rapid growth of research in explainable artificial intelligence (XAI)
follows on two substantial developments. First, the enormous application
success of modern machine learning methods, especially deep and reinforcement
learning, which have created high expectations for industrial, commercial and
social value. Second, the emergence of concern for creating trusted AI systems,
including the creation of regulatory principles to ensure transparency and
trust of AI systems.These two threads have created a kind of ""perfect storm"" of
research activity, all eager to create and deliver it any set of tools and
techniques to address the XAI demand. As some surveys of current XAI suggest,
there is yet to appear a principled framework that respects the literature of
explainability in the history of science, and which provides a basis for the
development of a framework for transparent XAI. Here we intend to provide a
strategic inventory of XAI requirements, demonstrate their connection to a
history of XAI ideas, and synthesize those ideas into a simple framework to
calibrate five successive levels of XAI.",2022-11-13 15:59:53
XXX,journalArticle,2020,"Johannes Schneider, Frank Breitinger",AI Forensics: Did the Artificial Intelligence System Do It? Why?,,,,,http://arxiv.org/abs/2005.13635v2,"In an increasingly autonomous manner AI systems make decisions impacting our
daily life. Their actions might cause accidents, harm or, more generally,
violate regulations -- either intentionally or not. Thus, AI systems might be
considered suspects for various events. Therefore, it is essential to relate
particular events to an AI, its owner and its creator. Given a multitude of AI
systems from multiple manufactures, potentially, altered by their owner or
changing through self-learning, this seems non-trivial. This paper discusses
how to identify AI systems responsible for incidents as well as their motives
that might be ""malicious by design"". In addition to a conceptualization, we
conduct two case studies based on reinforcement learning and convolutional
neural networks to illustrate our proposed methods and challenges. Our cases
illustrate that ""catching AI systems"" seems often far from trivial and requires
extensive expertise in machine learning. Legislative measures that enforce
mandatory information to be collected during operation of AI systems as well as
means to uniquely identify systems might facilitate the problem.",2022-11-13 15:59:53
XXX,journalArticle,2020,"Andrea Aler Tubella, Andreas Theodorou, Virginia Dignum, Loizos Michael",Contestable Black Boxes,,,,10.1007/978-3-030-57977-7_12,http://arxiv.org/abs/2006.05133v2,"The right to contest a decision with consequences on individuals or the
society is a well-established democratic right. Despite this right also being
explicitly included in GDPR in reference to automated decision-making, its
study seems to have received much less attention in the AI literature compared,
for example, to the right for explanation. This paper investigates the type of
assurances that are needed in the contesting process when algorithmic
black-boxes are involved, opening new questions about the interplay of
contestability and explainability. We argue that specialised complementary
methodologies to evaluate automated decision-making in the case of a particular
decision being contested need to be developed. Further, we propose a
combination of well-established software engineering and rule-based approaches
as a possible socio-technical solution to the issue of contestability, one of
the new democratic challenges posed by the automation of decision making.",2022-11-13 15:59:54
XXX,journalArticle,2020,"Alexander Matt Turner, Neale Ratzlaff, Prasad Tadepalli",Avoiding Side Effects in Complex Environments,,,,,http://arxiv.org/abs/2006.06547v2,"Reward function specification can be difficult. Rewarding the agent for
making a widget may be easy, but penalizing the multitude of possible negative
side effects is hard. In toy environments, Attainable Utility Preservation
(AUP) avoided side effects by penalizing shifts in the ability to achieve
randomly generated goals. We scale this approach to large, randomly generated
environments based on Conway's Game of Life. By preserving optimal value for a
single randomly generated reward function, AUP incurs modest overhead while
leading the agent to complete the specified task and avoid many side effects.
Videos and code are available at https://avoiding-side-effects.github.io/.",2022-11-13 15:59:54
XXX,journalArticle,2020,"Gagan Bansal, Tongshuang Wu, Joyce Zhou, Raymond Fok, Besmira Nushi, Ece Kamar, Marco Tulio Ribeiro, Daniel S. Weld",Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance,,,,,http://arxiv.org/abs/2006.14779v3,"Many researchers motivate explainable AI with studies showing that human-AI
team performance on decision-making tasks improves when the AI explains its
recommendations. However, prior studies observed improvements from explanations
only when the AI, alone, outperformed both the human and the best team. Can
explanations help lead to complementary performance, where team accuracy is
higher than either the human or the AI working solo? We conduct mixed-method
user studies on three datasets, where an AI with accuracy comparable to humans
helps participants solve a task (explaining itself in some conditions). While
we observed complementary improvements from AI augmentation, they were not
increased by explanations. Rather, explanations increased the chance that
humans will accept the AI's recommendation, regardless of its correctness. Our
result poses new challenges for human-centered AI: Can we develop explanatory
approaches that encourage appropriate trust in AI, and therefore help generate
(or improve) complementary performance?",2022-11-13 15:59:55
XXX,journalArticle,2020,"Amir Hosein Afshar Sedigh, Martin K. Purvis, Bastin Tony Roy Savarimuthu, Maryam A. Purvis, Christopher K. Frantz",Impact of meta-roles on the evolution of organisational institutions,,,,,http://arxiv.org/abs/2008.04096v1,"This paper investigates the impact of changes in agents' beliefs coupled with
dynamics in agents' meta-roles on the evolution of institutions. The study
embeds agents' meta-roles in the BDI architecture. In this context, the study
scrutinises the impact of cognitive dissonance in agents due to unfairness of
institutions. To showcase our model, two historical long-distance trading
societies, namely Armenian merchants of New-Julfa and the English East India
Company are simulated. Results show how change in roles of agents coupled with
specific institutional characteristics leads to changes of the rules in the
system.",2022-11-13 15:59:55
XXX,journalArticle,2020,"Weichao Zhou, Ruihan Gao, BaekGyu Kim, Eunsuk Kang, Wenchao Li",Runtime-Safety-Guided Policy Repair,,,,,http://arxiv.org/abs/2008.07667v1,"We study the problem of policy repair for learning-based control policies in
safety-critical settings. We consider an architecture where a high-performance
learning-based control policy (e.g. one trained as a neural network) is paired
with a model-based safety controller. The safety controller is endowed with the
abilities to predict whether the trained policy will lead the system to an
unsafe state, and take over control when necessary. While this architecture can
provide added safety assurances, intermittent and frequent switching between
the trained policy and the safety controller can result in undesirable
behaviors and reduced performance. We propose to reduce or even eliminate
control switching by `repairing' the trained policy based on runtime data
produced by the safety controller in a way that deviates minimally from the
original policy. The key idea behind our approach is the formulation of a
trajectory optimization problem that allows the joint reasoning of policy
update and safety constraints. Experimental results demonstrate that our
approach is effective even when the system model in the safety controller is
unknown and only approximated.",2022-11-13 15:59:56
XXX,journalArticle,2020,"Zihan Ding, Tianyang Yu, Yanhua Huang, Hongming Zhang, Guo Li, Quancheng Guo, Luo Mai, Hao Dong",Efficient Reinforcement Learning Development with RLzoo,,,,,http://arxiv.org/abs/2009.08644v2,"Many researchers and developers are exploring for adopting Deep Reinforcement
Learning (DRL) techniques in their applications. They however often find such
an adoption challenging. Existing DRL libraries provide poor support for
prototyping DRL agents (i.e., models), customising the agents, and comparing
the performance of DRL agents. As a result, the developers often report low
efficiency in developing DRL agents. In this paper, we introduce RLzoo, a new
DRL library that aims to make the development of DRL agents efficient. RLzoo
provides developers with (i) high-level yet flexible APIs for prototyping DRL
agents, and further customising the agents for best performance, (ii) a model
zoo where users can import a wide range of DRL agents and easily compare their
performance, and (iii) an algorithm that can automatically construct DRL agents
with custom components (which are critical to improve agent's performance in
custom applications). Evaluation results show that RLzoo can effectively reduce
the development cost of DRL agents, while achieving comparable performance with
existing DRL libraries.",2022-11-13 15:59:57
XXX,journalArticle,2020,"Herman Yau, Chris Russell, Simon Hadfield",What Did You Think Would Happen? Explaining Agent Behaviour Through Intended Outcomes,,,,,http://arxiv.org/abs/2011.05064v1,"We present a novel form of explanation for Reinforcement Learning, based
around the notion of intended outcome. These explanations describe the outcome
an agent is trying to achieve by its actions. We provide a simple proof that
general methods for post-hoc explanations of this nature are impossible in
traditional reinforcement learning. Rather, the information needed for the
explanations must be collected in conjunction with training the agent. We
derive approaches designed to extract local explanations based on intention for
several variants of Q-function approximation and prove consistency between the
explanations and the Q-values learned. We demonstrate our method on multiple
reinforcement learning problems, and provide code to help researchers
introspecting their RL environments and algorithms.",2022-11-13 15:59:57
XXX,journalArticle,2020,"Spyros Angelopoulos, Shahin Kamali",Contract Scheduling With Predictions,,,,,http://arxiv.org/abs/2011.12439v1,"Contract scheduling is a general technique that allows to design a system
with interruptible capabilities, given an algorithm that is not necessarily
interruptible. Previous work on this topic has largely assumed that the
interruption is a worst-case deadline that is unknown to the scheduler. In this
work, we study the setting in which there is a potentially erroneous prediction
concerning the interruption. Specifically, we consider the setting in which the
prediction describes the time that the interruption occurs, as well as the
setting in which the prediction is obtained as a response to a single or
multiple binary queries. For both settings, we investigate tradeoffs between
the robustness (i.e., the worst-case performance assuming adversarial
prediction) and the consistency (i.e, the performance assuming that the
prediction is error-free), both from the side of positive and negative results.",2022-11-13 15:59:58
XXX,journalArticle,2020,"Francesca Foffano, Teresa Scantamburlo, Atia Cortés, Chiara Bissolo",European Strategy on AI: Are we truly fostering social good?,,,,,http://arxiv.org/abs/2011.12863v1,"Artificial intelligence (AI) is already part of our daily lives and is
playing a key role in defining the economic and social shape of the future. In
2018, the European Commission introduced its AI strategy able to compete in the
next years with world powers such as China and US, but relying on the respect
of European values and fundamental rights. As a result, most of the Member
States have published their own National Strategy with the aim to work on a
coordinated plan for Europe. In this paper, we present an ongoing study on how
European countries are approaching the field of Artificial Intelligence, with
its promises and risks, through the lens of their national AI strategies. In
particular, we aim to investigate how European countries are investing in AI
and to what extent the stated plans can contribute to the benefit of the whole
society. This paper reports the main findings of a qualitative analysis of the
investment plans reported in 15 European National Strategies",2022-11-13 15:59:59
XXX,journalArticle,2020,"Taoan Huang, Bistra Dilkina, Sven Koenig",Learning to Resolve Conflicts for Multi-Agent Path Finding with Conflict-Based Search,,,,,http://arxiv.org/abs/2012.06005v1,"Conflict-Based Search (CBS) is a state-of-the-art algorithm for multi-agent
path finding. At the high level, CBS repeatedly detects conflicts and resolves
one of them by splitting the current problem into two subproblems. Previous
work chooses the conflict to resolve by categorizing the conflict into three
classes and always picking a conflict from the highest-priority class. In this
work, we propose an oracle for conflict selection that results in smaller
search tree sizes than the one used in previous work. However, the computation
of the oracle is slow. Thus, we propose a machine-learning framework for
conflict selection that observes the decisions made by the oracle and learns a
conflict-selection strategy represented by a linear ranking function that
imitates the oracle's decisions accurately and quickly. Experiments on
benchmark maps indicate that our method significantly improves the success
rates, the search tree sizes and runtimes over the current state-of-the-art CBS
solver.",2022-11-13 15:59:59
XXX,journalArticle,2020,"Qi Zhang, Edmund H. Durfee, Satinder Singh",Efficient Querying for Cooperative Probabilistic Commitments,,,,,http://arxiv.org/abs/2012.07195v1,"Multiagent systems can use commitments as the core of a general coordination
infrastructure, supporting both cooperative and non-cooperative interactions.
Agents whose objectives are aligned, and where one agent can help another
achieve greater reward by sacrificing some of its own reward, should choose a
cooperative commitment to maximize their joint reward. We present a solution to
the problem of how cooperative agents can efficiently find an (approximately)
optimal commitment by querying about carefully-selected commitment choices. We
prove structural properties of the agents' values as functions of the
parameters of the commitment specification, and develop a greedy method for
composing a query with provable approximation bounds, which we empirically show
can find nearly optimal commitments in a fraction of the time methods that lack
our insights require.",2022-11-13 16:00:00
XXX,journalArticle,2020,"Tae Wan Kim, John Hooker, Thomas Donaldson",Taking Principles Seriously: A Hybrid Approach to Value Alignment,,,,,http://arxiv.org/abs/2012.11705v1,"An important step in the development of value alignment (VA) systems in AI is
understanding how VA can reflect valid ethical principles. We propose that
designers of VA systems incorporate ethics by utilizing a hybrid approach in
which both ethical reasoning and empirical observation play a role. This, we
argue, avoids committing the ""naturalistic fallacy,"" which is an attempt to
derive ""ought"" from ""is,"" and it provides a more adequate form of ethical
reasoning when the fallacy is not committed. Using quantified model logic, we
precisely formulate principles derived from deontological ethics and show how
they imply particular ""test propositions"" for any given action plan in an AI
rule base. The action plan is ethical only if the test proposition is
empirically true, a judgment that is made on the basis of empirical VA. This
permits empirical VA to integrate seamlessly with independently justified
ethical principles.",2022-11-13 16:00:00
XXX,journalArticle,2021,"Nodens Koren, Qiuhong Ke, Yisen Wang, James Bailey, Xingjun Ma",Adversarial Interaction Attack: Fooling AI to Misinterpret Human Intentions,,,,,http://arxiv.org/abs/2101.06704v1,"Understanding the actions of both humans and artificial intelligence (AI)
agents is important before modern AI systems can be fully integrated into our
daily life. In this paper, we show that, despite their current huge success,
deep learning based AI systems can be easily fooled by subtle adversarial noise
to misinterpret the intention of an action in interaction scenarios. Based on a
case study of skeleton-based human interactions, we propose a novel adversarial
attack on interactions, and demonstrate how DNN-based interaction models can be
tricked to predict the participants' reactions in unexpected ways. From a
broader perspective, the scope of our proposed attack method is not confined to
problems related to skeleton data but can also be extended to any type of
problems involving sequential regressions. Our study highlights potential risks
in the interaction loop with AI and humans, which need to be carefully
addressed when deploying AI systems in safety-critical applications.",2022-11-13 16:00:01
XXX,journalArticle,2021,"Samuel Alexander, Bill Hibbard",Measuring Intelligence and Growth Rate: Variations on Hibbard's Intelligence Measure,"Journal of Artificial General Intelligence 12(1), 2021",,,10.2478/jagi-2021-0001,http://arxiv.org/abs/2101.12047v1,"In 2011, Hibbard suggested an intelligence measure for agents who compete in
an adversarial sequence prediction game. We argue that Hibbard's idea should
actually be considered as two separate ideas: first, that the intelligence of
such agents can be measured based on the growth rates of the runtimes of the
competitors that they defeat; and second, one specific (somewhat arbitrary)
method for measuring said growth rates. Whereas Hibbard's intelligence measure
is based on the latter growth-rate-measuring method, we survey other methods
for measuring function growth rates, and exhibit the resulting Hibbard-like
intelligence measures and taxonomies. Of particular interest, we obtain
intelligence taxonomies based on Big-O and Big-Theta notation systems, which
taxonomies are novel in that they challenge conventional notions of what an
intelligence measure should look like. We discuss how intelligence measurement
of sequence predictors can indirectly serve as intelligence measurement for
agents with Artificial General Intelligence (AGIs).",2022-11-13 16:00:01
XXX,journalArticle,2021,"Violet Xinying Chen, J. N. Hooker",Fairness through Social Welfare Optimization,,,,,http://arxiv.org/abs/2102.00311v4,"We propose social welfare optimization as a general paradigm for formalizing
fairness in AI systems. We argue that optimization models allow formulation of
a wide range of fairness criteria as social welfare functions, while enabling
AI to take advantage of highly advanced solution technology. Rather than
attempting to reduce bias between selected groups, one can achieve equity
across all groups by incorporating fairness into the social welfare function.
This also allows a fuller accounting of the welfare of the individuals
involved. We show how to integrate social welfare optimization with both
rule-based AI and machine learning, using either an in-processing or a
post-processing approach. We present empirical results from a case study as a
preliminary examination of the validity and potential of these integration
strategies.",2022-11-13 16:00:02
XXX,journalArticle,2021,Koen Holtman,Counterfactual Planning in AGI Systems,,,,,http://arxiv.org/abs/2102.00834v1,"We present counterfactual planning as a design approach for creating a range
of safety mechanisms that can be applied in hypothetical future AI systems
which have Artificial General Intelligence.
  The key step in counterfactual planning is to use an AGI machine learning
system to construct a counterfactual world model, designed to be different from
the real world the system is in. A counterfactual planning agent determines the
action that best maximizes expected utility in this counterfactual planning
world, and then performs the same action in the real world.
  We use counterfactual planning to construct an AGI agent emergency stop
button, and a safety interlock that will automatically stop the agent before it
undergoes an intelligence explosion. We also construct an agent with an input
terminal that can be used by humans to iteratively improve the agent's reward
function, where the incentive for the agent to manipulate this improvement
process is suppressed. As an example of counterfactual planning in a non-agent
AGI system, we construct a counterfactual oracle.
  As a design approach, counterfactual planning is built around the use of a
graphical notation for defining mathematical counterfactuals. This two-diagram
notation also provides a compact and readable language for reasoning about the
complex types of self-referencing and indirect representation which are
typically present inside machine learning agents.",2022-11-13 16:00:02
XXX,journalArticle,2021,"Tom Everitt, Ryan Carey, Eric Langlois, Pedro A Ortega, Shane Legg",Agent Incentives: A Causal Perspective,,,,,http://arxiv.org/abs/2102.01685v2,"We present a framework for analysing agent incentives using causal influence
diagrams. We establish that a well-known criterion for value of information is
complete. We propose a new graphical criterion for value of control,
establishing its soundness and completeness. We also introduce two new concepts
for incentive analysis: response incentives indicate which changes in the
environment affect an optimal decision, while instrumental control incentives
establish whether an agent can influence its utility via a variable X. For both
new concepts, we provide sound and complete graphical criteria. We show by
example how these results can help with evaluating the safety and fairness of
an AI system.",2022-11-13 16:00:03
XXX,journalArticle,2021,"Sandhya Saisubramanian, Shlomo Zilberstein",Mitigating Negative Side Effects via Environment Shaping,,,,,http://arxiv.org/abs/2102.07017v1,"Agents operating in unstructured environments often produce negative side
effects (NSE), which are difficult to identify at design time. While the agent
can learn to mitigate the side effects from human feedback, such feedback is
often expensive and the rate of learning is sensitive to the agent's state
representation. We examine how humans can assist an agent, beyond providing
feedback, and exploit their broader scope of knowledge to mitigate the impacts
of NSE. We formulate this problem as a human-agent team with decoupled
objectives. The agent optimizes its assigned task, during which its actions may
produce NSE. The human shapes the environment through minor reconfiguration
actions so as to mitigate the impacts of the agent's side effects, without
affecting the agent's ability to complete its assigned task. We present an
algorithm to solve this problem and analyze its theoretical properties. Through
experiments with human subjects, we assess the willingness of users to perform
minor environment modifications to mitigate the impacts of NSE. Empirical
evaluation of our approach shows that the proposed framework can successfully
mitigate NSE, without affecting the agent's ability to complete its assigned
task.",2022-11-13 16:00:03
XXX,journalArticle,2021,"Eric D. Langlois, Tom Everitt",How RL Agents Behave When Their Actions Are Modified,"Proceedings of the AAAI Conference on Artificial Intelligence,
  35(13), 11586-11594 (2021)",,,,http://arxiv.org/abs/2102.07716v2,"Reinforcement learning in complex environments may require supervision to
prevent the agent from attempting dangerous actions. As a result of supervisor
intervention, the executed action may differ from the action specified by the
policy. How does this affect learning? We present the Modified-Action Markov
Decision Process, an extension of the MDP model that allows actions to differ
from the policy. We analyze the asymptotic behaviours of common reinforcement
learning algorithms in this setting and show that they adapt in different ways:
some completely ignore modifications while others go to various lengths in
trying to avoid action modifications that decrease reward. By choosing the
right algorithm, developers can prevent their agents from learning to
circumvent interruptions or constraints, and better control agent responses to
other kinds of action modification, like self-damage.",2022-11-13 16:00:04
XXX,journalArticle,2021,"Rukshan Wijesinghe, Kasun Vithanage, Dumindu Tissera, Alex Xavier, Subha Fernando, Jayathu Samarawickrama",Transferring Domain Knowledge with an Adviser in Continuous Tasks,,,,,http://arxiv.org/abs/2102.08029v1,"Recent advances in Reinforcement Learning (RL) have surpassed human-level
performance in many simulated environments. However, existing reinforcement
learning techniques are incapable of explicitly incorporating already known
domain-specific knowledge into the learning process. Therefore, the agents have
to explore and learn the domain knowledge independently through a trial and
error approach, which consumes both time and resources to make valid responses.
Hence, we adapt the Deep Deterministic Policy Gradient (DDPG) algorithm to
incorporate an adviser, which allows integrating domain knowledge in the form
of pre-learned policies or pre-defined relationships to enhance the agent's
learning process. Our experiments on OpenAi Gym benchmark tasks show that
integrating domain knowledge through advisers expedites the learning and
improves the policy towards better optima.",2022-11-13 16:00:04
XXX,journalArticle,2021,"Sebastian Graef, Ilche Georgievski",Software Architecture for Next-Generation AI Planning Systems,,,,,http://arxiv.org/abs/2102.10985v1,"Artificial Intelligence (AI) planning is a flourishing research and
development discipline that provides powerful tools for searching a course of
action that achieves some user goal. While these planning tools show excellent
performance on benchmark planning problems, they represent challenging software
systems when it comes to their use and integration in real-world applications.
In fact, even in-depth understanding of their internal mechanisms does not
guarantee that one can successfully set up, use and manipulate existing
planning tools. We contribute toward alleviating this situation by proposing a
service-oriented planning architecture to be at the core of the ability to
design, develop and use next-generation AI planning systems. We collect and
classify common planning capabilities to form the building blocks of the
planning architecture. We incorporate software design principles and patterns
into the architecture to allow for usability, interoperability and reusability
of the planning capabilities. Our prototype planning system demonstrates the
potential of our approach for rapid prototyping and flexibility of system
composition. Finally, we provide insight into the qualitative advantages of our
approach when compared to a typical planning tool.",2022-11-13 16:00:05
XXX,journalArticle,2021,"Grégoire Déletang, Jordi Grau-Moya, Miljan Martic, Tim Genewein, Tom McGrath, Vladimir Mikulik, Markus Kunesch, Shane Legg, Pedro A. Ortega",Causal Analysis of Agent Behavior for AI Safety,,,,,http://arxiv.org/abs/2103.03938v1,"As machine learning systems become more powerful they also become
increasingly unpredictable and opaque. Yet, finding human-understandable
explanations of how they work is essential for their safe deployment. This
technical report illustrates a methodology for investigating the causal
mechanisms that drive the behaviour of artificial agents. Six use cases are
covered, each addressing a typical question an analyst might ask about an
agent. In particular, we show that each question cannot be addressed by pure
observation alone, but instead requires conducting experiments with
systematically chosen manipulations so as to generate the correct causal
evidence.",2022-11-13 16:00:05
XXX,journalArticle,2021,"Daniel Zhang, Saurabh Mishra, Erik Brynjolfsson, John Etchemendy, Deep Ganguli, Barbara Grosz, Terah Lyons, James Manyika, Juan Carlos Niebles, Michael Sellitto, Yoav Shoham, Jack Clark, Raymond Perrault",The AI Index 2021 Annual Report,,,,,http://arxiv.org/abs/2103.06312v1,"Welcome to the fourth edition of the AI Index Report. This year we
significantly expanded the amount of data available in the report, worked with
a broader set of external organizations to calibrate our data, and deepened our
connections with the Stanford Institute for Human-Centered Artificial
Intelligence (HAI). The AI Index Report tracks, collates, distills, and
visualizes data related to artificial intelligence. Its mission is to provide
unbiased, rigorously vetted, and globally sourced data for policymakers,
researchers, executives, journalists, and the general public to develop
intuitions about the complex field of AI. The report aims to be the most
credible and authoritative source for data and insights about AI in the world.",2022-11-13 16:00:06
XXX,journalArticle,2021,"Alexandros Nikou, Anusha Mujumdar, Marin Orlic, Aneta Vulgarakis Feljan",Symbolic Reinforcement Learning for Safe RAN Control,,,,,http://arxiv.org/abs/2103.06602v1,"In this paper, we demonstrate a Symbolic Reinforcement Learning (SRL)
architecture for safe control in Radio Access Network (RAN) applications. In
our automated tool, a user can select a high-level safety specifications
expressed in Linear Temporal Logic (LTL) to shield an RL agent running in a
given cellular network with aim of optimizing network performance, as measured
through certain Key Performance Indicators (KPIs). In the proposed
architecture, network safety shielding is ensured through model-checking
techniques over combined discrete system models (automata) that are abstracted
through reinforcement learning. We demonstrate the user interface (UI) helping
the user set intent specifications to the architecture and inspect the
difference in allowed and blocked actions.",2022-11-13 16:00:07
XXX,journalArticle,2021,"Ramya Ramakrishnan, Vaibhav Unhelkar, Ece Kamar, Julie Shah",A Bayesian Approach to Identifying Representational Errors,,,,,http://arxiv.org/abs/2103.15171v1,"Trained AI systems and expert decision makers can make errors that are often
difficult to identify and understand. Determining the root cause for these
errors can improve future decisions. This work presents Generative Error Model
(GEM), a generative model for inferring representational errors based on
observations of an actor's behavior (either simulated agent, robot, or human).
The model considers two sources of error: those that occur due to
representational limitations -- ""blind spots"" -- and non-representational
errors, such as those caused by noise in execution or systematic errors present
in the actor's policy. Disambiguating these two error types allows for targeted
refinement of the actor's policy (i.e., representational errors require
perceptual augmentation, while other errors can be reduced through methods such
as improved training or attention support). We present a Bayesian inference
algorithm for GEM and evaluate its utility in recovering representational
errors on multiple domains. Results show that our approach can recover blind
spots of both reinforcement learning agents as well as human users.",2022-11-13 16:00:07
XXX,journalArticle,2021,"Siani Pearson, Martin Lloyd, Vivek Nallur",Towards An Ethics-Audit Bot,,,,,http://arxiv.org/abs/2103.15746v1,"In this paper we focus on artificial intelligence (AI) for governance, not
governance for AI, and on just one aspect of governance, namely ethics audit.
Different kinds of ethical audit bots are possible, but who makes the choices
and what are the implications? In this paper, we do not provide
ethical/philosophical solutions, but rather focus on the technical aspects of
what an AI-based solution for validating the ethical soundness of a target
system would be like. We propose a system that is able to conduct an ethical
audit of a target system, given certain socio-technical conditions. To be more
specific, we propose the creation of a bot that is able to support
organisations in ensuring that their software development lifecycles contain
processes that meet certain ethical standards.",2022-11-13 16:00:08
XXX,journalArticle,2021,Melanie Mitchell,Why AI is Harder Than We Think,,,,,http://arxiv.org/abs/2104.12871v2,"Since its beginning in the 1950s, the field of artificial intelligence has
cycled several times between periods of optimistic predictions and massive
investment (""AI spring"") and periods of disappointment, loss of confidence, and
reduced funding (""AI winter""). Even with today's seemingly fast pace of AI
breakthroughs, the development of long-promised technologies such as
self-driving cars, housekeeping robots, and conversational companions has
turned out to be much harder than many people expected. One reason for these
repeating cycles is our limited understanding of the nature and complexity of
intelligence itself. In this paper I describe four fallacies in common
assumptions made by AI researchers, which can lead to overconfident predictions
about the field. I conclude by discussing the open questions spurred by these
fallacies, including the age-old challenge of imbuing machines with humanlike
common sense.",2022-11-13 16:00:09
XXX,journalArticle,2021,"Anagha Kulkarni, Siddharth Srivastava, Subbarao Kambhampati",Planning for Proactive Assistance in Environments with Partial Observability,,,,,http://arxiv.org/abs/2105.00525v2,"This paper addresses the problem of synthesizing the behavior of an AI agent
that provides proactive task assistance to a human in settings like factory
floors where they may coexist in a common environment. Unlike in the case of
requested assistance, the human may not be expecting proactive assistance and
hence it is crucial for the agent to ensure that the human is aware of how the
assistance affects her task. This becomes harder when there is a possibility
that the human may neither have full knowledge of the AI agent's capabilities
nor have full observability of its activities. Therefore, our \textit{proactive
assistant} is guided by the following three principles: \textbf{(1)} its
activity decreases the human's cost towards her goal; \textbf{(2)} the human is
able to recognize the potential reduction in her cost; \textbf{(3)} its
activity optimizes the human's overall cost (time/resources) of achieving her
goal. Through empirical evaluation and user studies, we demonstrate the
usefulness of our approach.",2022-11-13 16:00:09
XXX,journalArticle,2021,"Dominik Dellermann, Nikolaus Lipusch, Philipp Ebel, Karl Michael Popp, Jan Marco Leimeister",Finding the unicorn: Predicting early stage startup success through a hybrid intelligence method,,,,,http://arxiv.org/abs/2105.03360v1,"Artificial intelligence is an emerging topic and will soon be able to perform
decisions better than humans. In more complex and creative contexts such as
innovation, however, the question remains whether machines are superior to
humans. Machines fail in two kinds of situations: processing and interpreting
soft information (information that cannot be quantified) and making predictions
in unknowable risk situations of extreme uncertainty. In such situations, the
machine does not have representative information for a certain outcome.
Thereby, humans are still the gold standard for assessing soft signals and make
use of intuition. To predict the success of startups, we, thus, combine the
complementary capabilities of humans and machines in a Hybrid Intelligence
method. To reach our aim, we follow a design science research approach to
develop a Hybrid Intelligence method that combines the strength of both machine
and collective intelligence to demonstrate its utility for predictions under
extreme uncertainty.",2022-11-13 16:00:10
XXX,journalArticle,2021,Hal Ashton,Definitions of intent suitable for algorithms,,,,,http://arxiv.org/abs/2106.04235v1,"Intent modifies an actor's culpability of many types wrongdoing. Autonomous
Algorithmic Agents have the capability of causing harm, and whilst their
current lack of legal personhood precludes them from committing crimes, it is
useful for a number of parties to understand under what type of intentional
mode an algorithm might transgress. From the perspective of the creator or
owner they would like ensure that their algorithms never intend to cause harm
by doing things that would otherwise be labelled criminal if committed by a
legal person. Prosecutors might have an interest in understanding whether the
actions of an algorithm were internally intended according to a transparent
definition of the concept. The presence or absence of intention in the
algorithmic agent might inform the court as to the complicity of its owner.
This article introduces definitions for direct, oblique (or indirect) and
ulterior intent which can be used to test for intent in an algorithmic actor.",2022-11-13 16:00:10
XXX,journalArticle,2021,"Utkarsh Soni, Sarath Sreedharan, Subbarao Kambhampati",Not all users are the same: Providing personalized explanations for sequential decision making problems,,,,,http://arxiv.org/abs/2106.12207v1,"There is a growing interest in designing autonomous agents that can work
alongside humans. Such agents will undoubtedly be expected to explain their
behavior and decisions. While generating explanations is an actively researched
topic, most works tend to focus on methods that generate explanations that are
one size fits all. As in the specifics of the user-model are completely
ignored. The handful of works that look at tailoring their explanation to the
user's background rely on having specific models of the users (either analytic
models or learned labeling models). The goal of this work is thus to propose an
end-to-end adaptive explanation generation system that begins by learning the
different types of users that the agent could interact with. Then during the
interaction with the target user, it is tasked with identifying the type on the
fly and adjust its explanations accordingly. The former is achieved by a
data-driven clustering approach while for the latter, we compile our
explanation generation problem into a POMDP. We demonstrate the usefulness of
our system on two domains using state-of-the-art POMDP solvers. We also report
the results of a user study that investigates the benefits of providing
personalized explanations in a human-robot interaction setting.",2022-11-13 16:00:11
XXX,journalArticle,2021,"Sriram Gopalakrishnan, Utkarsh Soni, Tung Thai, Panagiotis Lymperopoulos, Matthias Scheutz, Subbarao Kambhampati","Integrating Planning, Execution and Monitoring in the presence of Open World Novelties: Case Study of an Open World Monopoly Solver",,,,,http://arxiv.org/abs/2107.04303v2,"The game of monopoly is an adversarial multi-agent domain where there is no
fixed goal other than to be the last player solvent, There are useful subgoals
like monopolizing sets of properties, and developing them. There is also a lot
of randomness from dice rolls, card-draws, and adversaries' strategies. This
unpredictability is made worse when unknown novelties are added during
gameplay. Given these challenges, Monopoly was one of the test beds chosen for
the DARPA-SAILON program which aims to create agents that can detect and
accommodate novelties. To handle the game complexities, we developed an agent
that eschews complete plans, and adapts it's policy online as the game evolves.
In the most recent independent evaluation in the SAILON program, our agent was
the best performing agent on most measures. We herein present our approach and
results.",2022-11-13 16:00:11
XXX,journalArticle,2021,"Pulkit Verma, Shashank Rao Marpally, Siddharth Srivastava",Discovering User-Interpretable Capabilities of Black-Box Planning Agents,,,,,http://arxiv.org/abs/2107.13668v3,"Several approaches have been developed for answering users' specific
questions about AI behavior and for assessing their core functionality in terms
of primitive executable actions. However, the problem of summarizing an AI
agent's broad capabilities for a user is comparatively new. This paper presents
an algorithm for discovering from scratch the suite of high-level
""capabilities"" that an AI system with arbitrary internal planning
algorithms/policies can perform. It computes conditions describing the
applicability and effects of these capabilities in user-interpretable terms.
Starting from a set of user-interpretable state properties, an AI agent, and a
simulator that the agent can interact with, our algorithm returns a set of
high-level capabilities with their parameterized descriptions. Empirical
evaluation on several game-based scenarios shows that this approach efficiently
learns descriptions of various types of AI agents in deterministic, fully
observable settings. User studies show that such descriptions are easier to
understand and reason with than the agent's primitive actions.",2022-11-13 16:00:12
XXX,journalArticle,2021,"Mingyi Liu, Zhiying Tu, Xiaofei Xu, Zhongjie Wang",DySR: A Dynamic Representation Learning and Aligning based Model for Service Bundle Recommendation,,,,,http://arxiv.org/abs/2108.03360v1,"An increasing number and diversity of services are available, which result in
significant challenges to effective reuse service during requirement
satisfaction. There have been many service bundle recommendation studies and
achieved remarkable results. However, there is still plenty of room for
improvement in the performance of these methods. The fundamental problem with
these studies is that they ignore the evolution of services over time and the
representation gap between services and requirements. In this paper, we propose
a dynamic representation learning and aligning based model called DySR to
tackle these issues. DySR eliminates the representation gap between services
and requirements by learning a transformation function and obtains service
representations in an evolving social environment through dynamic graph
representation learning. Extensive experiments conducted on a real-world
dataset from ProgrammableWeb show that DySR outperforms existing
state-of-the-art methods in commonly used evaluation metrics, improving $F1@5$
from $36.1\%$ to $69.3\%$.",2022-11-13 16:00:12
XXX,journalArticle,2021,"Pulkit Verma, Siddharth Srivastava",Learning Causal Models of Autonomous Agents using Interventions,,,,,http://arxiv.org/abs/2108.09586v1,"One of the several obstacles in the widespread use of AI systems is the lack
of requirements of interpretability that can enable a layperson to ensure the
safe and reliable behavior of such systems. We extend the analysis of an agent
assessment module that lets an AI system execute high-level instruction
sequences in simulators and answer the user queries about its execution of
sequences of actions. We show that such a primitive query-response capability
is sufficient to efficiently derive a user-interpretable causal model of the
system in stationary, fully observable, and deterministic settings. We also
introduce dynamic causal decision networks (DCDNs) that capture the causal
structure of STRIPS-like domains. A comparative analysis of different classes
of queries is also presented in terms of the computational requirements needed
to answer them and the efforts required to evaluate their responses to learn
the correct model.",2022-11-13 16:00:13
XXX,journalArticle,2021,"Ido Shapira, Amos Azaria",A Socially Aware Reinforcement Learning Agent for The Single Track Road Problem,,,,,http://arxiv.org/abs/2109.05486v3,"We present the single track road problem. In this problem two agents face
each-other at opposite positions of a road that can only have one agent pass at
a time. We focus on the scenario in which one agent is human, while the other
is an autonomous agent. We run experiments with human subjects in a simple grid
domain, which simulates the single track road problem. We show that when data
is limited, building an accurate human model is very challenging, and that a
reinforcement learning agent, which is based on this data, does not perform
well in practice. However, we show that an agent that tries to maximize a
linear combination of the human's utility and its own utility, achieves a high
score, and significantly outperforms other baselines, including an agent that
tries to maximize only its own utility.",2022-11-13 16:00:14
XXX,journalArticle,2021,"Helen Bubinger, Jesse David Dinneen",Actionable Approaches to Promote Ethical AI in Libraries,,,,,http://arxiv.org/abs/2109.09672v1,"The widespread use of artificial intelligence (AI) in many domains has
revealed numerous ethical issues from data and design to deployment. In
response, countless broad principles and guidelines for ethical AI have been
published, and following those, specific approaches have been proposed for how
to encourage ethical outcomes of AI. Meanwhile, library and information
services too are seeing an increase in the use of AI-powered and machine
learning-powered information systems, but no practical guidance currently
exists for libraries to plan for, evaluate, or audit the ethics of intended or
deployed AI. We therefore report on several promising approaches for promoting
ethical AI that can be adapted from other contexts to AI-powered information
services and in different stages of the software lifecycle.",2022-11-13 16:00:14
XXX,journalArticle,2021,"Antti Keurulainen, Isak Westerlund, Samuel Kaski, Alexander Ilin",Learning to Assist Agents by Observing Them,,,,,http://arxiv.org/abs/2110.01311v1,"The ability of an AI agent to assist other agents, such as humans, is an
important and challenging goal, which requires the assisting agent to reason
about the behavior and infer the goals of the assisted agent. Training such an
ability by using reinforcement learning usually requires large amounts of
online training, which is difficult and costly. On the other hand, offline data
about the behavior of the assisted agent might be available, but is non-trivial
to take advantage of by methods such as offline reinforcement learning. We
introduce methods where the capability to create a representation of the
behavior is first pre-trained with offline data, after which only a small
amount of interaction data is needed to learn an assisting policy. We test the
setting in a gridworld where the helper agent has the capability to manipulate
the environment of the assisted artificial agents, and introduce three
different scenarios where the assistance considerably improves the performance
of the assisted agents.",2022-11-13 16:00:15
XXX,journalArticle,2021,"Carles Sierra, Nardine Osman, Pablo Noriega, Jordi Sabater-Mir, Antoni Perelló",Value alignment: a formal approach,,,,,http://arxiv.org/abs/2110.09240v1,"principles that should govern autonomous AI systems. It essentially states
that a system's goals and behaviour should be aligned with human values. But
how to ensure value alignment? In this paper we first provide a formal model to
represent values through preferences and ways to compute value aggregations;
i.e. preferences with respect to a group of agents and/or preferences with
respect to sets of values. Value alignment is then defined, and computed, for a
given norm with respect to a given value through the increase/decrease that it
results in the preferences of future states of the world. We focus on norms as
it is norms that govern behaviour, and as such, the alignment of a given system
with a given value will be dictated by the norms the system follows.",2022-11-13 16:00:16
XXX,journalArticle,2021,"Montaser Mohammedalamen, Dustin Morrill, Alexander Sieusahai, Yash Satsangi, Michael Bowling",Learning to Be Cautious,,,,,http://arxiv.org/abs/2110.15907v1,"A key challenge in the field of reinforcement learning is to develop agents
that behave cautiously in novel situations. It is generally impossible to
anticipate all situations that an autonomous system may face or what behavior
would best avoid bad outcomes. An agent that could learn to be cautious would
overcome this challenge by discovering for itself when and how to behave
cautiously. In contrast, current approaches typically embed task-specific
safety information or explicit cautious behaviors into the system, which is
error-prone and imposes extra burdens on practitioners. In this paper, we
present both a sequence of tasks where cautious behavior becomes increasingly
non-obvious, as well as an algorithm to demonstrate that it is possible for a
system to \emph{learn} to be cautious. The essential features of our algorithm
are that it characterizes reward function uncertainty without task-specific
safety information and uses this uncertainty to construct a robust policy.
Specifically, we construct robust policies with a $k$-of-$N$ counterfactual
regret minimization (CFR) subroutine given a learned reward function
uncertainty represented by a neural network ensemble belief. These policies
exhibit caution in each of our tasks without any task-specific safety tuning.",2022-11-13 16:00:16
XXX,journalArticle,2021,"Haofeng Liu, Yiwen Chen, Jiayi Tan, Marcelo H Ang Jr",Improving Learning from Demonstrations by Learning from Experience,,,,,http://arxiv.org/abs/2111.08156v1,"How to make imitation learning more general when demonstrations are
relatively limited has been a persistent problem in reinforcement learning
(RL). Poor demonstrations lead to narrow and biased date distribution,
non-Markovian human expert demonstration makes it difficult for the agent to
learn, and over-reliance on sub-optimal trajectories can make it hard for the
agent to improve its performance. To solve these problems we propose a new
algorithm named TD3fG that can smoothly transition from learning from experts
to learning from experience. Our algorithm achieves good performance in the
MUJOCO environment with limited and sub-optimal demonstrations. We use behavior
cloning to train the network as a reference action generator and utilize it in
terms of both loss function and exploration noise. This innovation can help
agents extract a priori knowledge from demonstrations while reducing the
detrimental effects of the poor Markovian properties of the demonstrations. It
has a better performance compared to the BC+ fine-tuning and DDPGfD approach,
especially when the demonstrations are relatively limited. We call our method
TD3fG meaning TD3 from a generator.",2022-11-13 16:00:17
XXX,journalArticle,2021,"Qinghua Lu, Liming Zhu, Xiwei Xu, Jon Whittle, David Douglas, Conrad Sanderson",Software Engineering for Responsible AI: An Empirical Study and Operationalised Patterns,,,,10.1109/ICSE-SEIP55303.2022.9793864,http://arxiv.org/abs/2111.09478v1,"Although artificial intelligence (AI) is solving real-world challenges and
transforming industries, there are serious concerns about its ability to behave
and make decisions in a responsible way. Many AI ethics principles and
guidelines for responsible AI have been recently issued by governments,
organisations, and enterprises. However, these AI ethics principles and
guidelines are typically high-level and do not provide concrete guidance on how
to design and develop responsible AI systems. To address this shortcoming, we
first present an empirical study where we interviewed 21 scientists and
engineers to understand the practitioners' perceptions on AI ethics principles
and their implementation. We then propose a template that enables AI ethics
principles to be operationalised in the form of concrete patterns and suggest a
list of patterns using the newly created template. These patterns provide
concrete, operationalised guidance that facilitate the development of
responsible AI systems.",2022-11-13 16:00:17
XXX,journalArticle,2021,Koen Holtman,Demanding and Designing Aligned Cognitive Architectures,,,,,http://arxiv.org/abs/2112.10190v1,"With AI systems becoming more powerful and pervasive, there is increasing
debate about keeping their actions aligned with the broader goals and needs of
humanity. This multi-disciplinary and multi-stakeholder debate must resolve
many issues, here we examine three of them. The first issue is to clarify what
demands stakeholders might usefully make on the designers of AI systems, useful
because the technology exists to implement them. We make this technical topic
more accessible by using the framing of cognitive architectures. The second
issue is to move beyond an analytical framing that treats useful intelligence
as being reward maximization only. To support this move, we define several AI
cognitive architectures that combine reward maximization with other technical
elements designed to improve alignment. The third issue is how stakeholders
should calibrate their interactions with modern machine learning researchers.
We consider how current fashions in machine learning create a narrative pull
that participants in technical and policy discussions should be aware of, so
that they can compensate for it. We identify several technically tractable but
currently unfashionable options for improving AI alignment.",2022-11-13 16:00:18
XXX,journalArticle,2022,"Ruiqi He, Yash Raj Jain, Falk Lieder",Have I done enough planning or should I plan more?,,,,,http://arxiv.org/abs/2201.00764v1,"People's decisions about how to allocate their limited computational
resources are essential to human intelligence. An important component of this
metacognitive ability is deciding whether to continue thinking about what to do
and move on to the next decision. Here, we show that people acquire this
ability through learning and reverse-engineer the underlying learning
mechanisms. Using a process-tracing paradigm that externalises human planning,
we find that people quickly adapt how much planning they perform to the cost
and benefit of planning. To discover the underlying metacognitive learning
mechanisms we augmented a set of reinforcement learning models with
metacognitive features and performed Bayesian model selection. Our results
suggest that the metacognitive ability to adjust the amount of planning might
be learned through a policy-gradient mechanism that is guided by metacognitive
pseudo-rewards that communicate the value of planning.",2022-11-13 16:00:18
XXX,journalArticle,2022,"Federico Malato, Joona Jehkonen, Ville Hautamäki",Improving Behavioural Cloning with Human-Driven Dynamic Dataset Augmentation,,,,,http://arxiv.org/abs/2201.07719v1,"Behavioural cloning has been extensively used to train agents and is
recognized as a fast and solid approach to teach general behaviours based on
expert trajectories. Such method follows the supervised learning paradigm and
it strongly depends on the distribution of the data. In our paper, we show how
combining behavioural cloning with human-in-the-loop training solves some of
its flaws and provides an agent task-specific corrections to overcome tricky
situations while speeding up the training time and lowering the required
resources. To do this, we introduce a novel approach that allows an expert to
take control of the agent at any moment during a simulation and provide optimal
solutions to its problematic situations. Our experiments show that this
approach leads to better policies both in terms of quantitative evaluation and
in human-likeliness.",2022-11-13 16:00:19
XXX,journalArticle,2022,Junchen Zhao,Safety-Aware Multi-Agent Apprenticeship Learning,,,,,http://arxiv.org/abs/2201.08111v2,"Our objective of this project is to make the extension based on the technique
mentioned in the paper ""Safety-Aware Apprenticeship Learning"" to improve the
utility and the efficiency of the existing Reinforcement Learning model from a
Single-Agent Learning framework to a Multi-Agent Learning framework. Our
contributions to the project are presented in the following bullet points: 1.
Regarding the fact that we will add an extension to the Inverse Reinforcement
Learning model from a Single-Agent scenario to a Multi-Agentscenario. Our first
contribution to this project is considering the case of extracting safe reward
functions from expert behaviors in a Multi-Agent scenario instead of being from
the Single-Agent scenario. 2. Our second contribution is extending the
Single-Agent Learning Framework to a Multi-Agent Learning framework and
designing a novel Learning Framework based on the extension in the end. 3. Our
final contribution to this project is evaluating empirically the performance of
my extension to the Single-Agent Inverse Reinforcement Learning framework.",2022-11-13 16:00:19
XXX,journalArticle,2022,"Sasha Salter, Kristian Hartikainen, Walter Goodwin, Ingmar Posner","Priors, Hierarchy, and Information Asymmetry for Skill Transfer in Reinforcement Learning",,,,,http://arxiv.org/abs/2201.08115v1,"The ability to discover behaviours from past experience and transfer them to
new tasks is a hallmark of intelligent agents acting sample-efficiently in the
real world. Equipping embodied reinforcement learners with the same ability may
be crucial for their successful deployment in robotics. While hierarchical and
KL-regularized RL individually hold promise here, arguably a hybrid approach
could combine their respective benefits. Key to these fields is the use of
information asymmetry to bias which skills are learnt. While asymmetric choice
has a large influence on transferability, prior works have explored a narrow
range of asymmetries, primarily motivated by intuition. In this paper, we
theoretically and empirically show the crucial trade-off, controlled by
information asymmetry, between the expressivity and transferability of skills
across sequential tasks. Given this insight, we provide a principled approach
towards choosing asymmetry and apply our approach to a complex, robotic block
stacking domain, unsolvable by baselines, demonstrating the effectiveness of
hierarchical KL-regularized RL, coupled with correct asymmetric choice, for
sample-efficient transfer learning.",2022-11-13 16:00:20
XXX,journalArticle,2022,"Stephanie Galaitsi, Benjamin D. Trump, Jeffrey M. Keisler, Igor Linkov, Alexander Kott",Cybertrust: From Explainable to Actionable and Interpretable AI (AI2),,,,,http://arxiv.org/abs/2201.11117v1,"To benefit from AI advances, users and operators of AI systems must have
reason to trust it. Trust arises from multiple interactions, where predictable
and desirable behavior is reinforced over time. Providing the system's users
with some understanding of AI operations can support predictability, but
forcing AI to explain itself risks constraining AI capabilities to only those
reconcilable with human cognition. We argue that AI systems should be designed
with features that build trust by bringing decision-analytic perspectives and
formal tools into AI. Instead of trying to achieve explainable AI, we should
develop interpretable and actionable AI. Actionable and Interpretable AI (AI2)
will incorporate explicit quantifications and visualizations of user confidence
in AI recommendations. In doing so, it will allow examining and testing of AI
system predictions to establish a basis for trust in the systems' decision
making and ensure broad benefits from deploying and advancing its computational
capabilities.",2022-11-13 16:00:21
XXX,journalArticle,2022,"Raphael Koster, Jan Balaguer, Andrea Tacchetti, Ari Weinstein, Tina Zhu, Oliver Hauser, Duncan Williams, Lucy Campbell-Gillingham, Phoebe Thacker, Matthew Botvinick, Christopher Summerfield",Human-centered mechanism design with Democratic AI,,,,,http://arxiv.org/abs/2201.11441v1,"Building artificial intelligence (AI) that aligns with human values is an
unsolved problem. Here, we developed a human-in-the-loop research pipeline
called Democratic AI, in which reinforcement learning is used to design a
social mechanism that humans prefer by majority. A large group of humans played
an online investment game that involved deciding whether to keep a monetary
endowment or to share it with others for collective benefit. Shared revenue was
returned to players under two different redistribution mechanisms, one designed
by the AI and the other by humans. The AI discovered a mechanism that redressed
initial wealth imbalance, sanctioned free riders, and successfully won the
majority vote. By optimizing for human preferences, Democratic AI may be a
promising method for value-aligned policy innovation.",2022-11-13 16:00:21
XXX,journalArticle,2022,Vacslav Glukhov,Reward is not enough: can we liberate AI from the reinforcement learning paradigm?,,,,,http://arxiv.org/abs/2202.03192v2,"I present arguments against the hypothesis put forward by Silver, Singh,
Precup, and Sutton (
https://www.sciencedirect.com/science/article/pii/S0004370221000862 ) : reward
maximization is not enough to explain many activities associated with natural
and artificial intelligence including knowledge, learning, perception, social
intelligence, evolution, language, generalisation and imitation. I show such
reductio ad lucrum has its intellectual origins in the political economy of
Homo economicus and substantially overlaps with the radical version of
behaviourism. I show why the reinforcement learning paradigm, despite its
demonstrable usefulness in some practical application, is an incomplete
framework for intelligence -- natural and artificial. Complexities of
intelligent behaviour are not simply second-order complications on top of
reward maximisation. This fact has profound implications for the development of
practically usable, smart, safe and robust artificially intelligent agents.",2022-11-13 16:00:22
XXX,journalArticle,2022,"Cameron Haigh, Zichen Zhang, Negar Hassanpour, Khurram Javed, Yingying Fu, Shayan Shahramian, Shawn Zhang, Jun Luo",Drawing Inductor Layout with a Reinforcement Learning Agent: Method and Application for VCO Inductors,,,,,http://arxiv.org/abs/2202.11798v2,"Design of Voltage-Controlled Oscillator (VCO) inductors is a laborious and
time-consuming task that is conventionally done manually by human experts. In
this paper, we propose a framework for automating the design of VCO inductors,
using Reinforcement Learning (RL). We formulate the problem as a sequential
procedure, where wire segments are drawn one after another, until a complete
inductor is created. We then employ an RL agent to learn to draw inductors that
meet certain target specifications. In light of the need to tweak the target
specifications throughout the circuit design cycle, we also develop a variant
in which the agent can learn to quickly adapt to draw new inductors for
moderately different target specifications. Our empirical results show that the
proposed framework is successful at automatically generating VCO inductors that
meet or exceed the target specification.",2022-11-13 16:00:22
XXX,journalArticle,2022,"Peter Schüller, João Paolo Costeira, James Crowley, Jasmin Grosinger, Félix Ingrand, Uwe Köckemann, Alessandro Saffiotti, Martin Welss",Composing Complex and Hybrid AI Solutions,,,,,http://arxiv.org/abs/2202.12566v1,"Progress in several areas of computer science has been enabled by comfortable
and efficient means of experimentation, clear interfaces, and interchangable
components, for example using OpenCV for computer vision or ROS for robotics.
We describe an extension of the Acumos system towards enabling the above
features for general AI applications. Originally, Acumos was created for
telecommunication purposes, mainly for creating linear pipelines of machine
learning components. Our extensions include support for more generic components
with gRPC/Protobuf interfaces, automatic orchestration of graphically assembled
solutions including control loops, sub-component topologies, and event-based
communication,and provisions for assembling solutions which contain user
interfaces and shared storage areas. We provide examples of deployable
solutions and their interfaces. The framework is deployed at
http://aiexp.ai4europe.eu/ and its source code is managed as an open source
Eclipse project.",2022-11-13 16:00:23
XXX,journalArticle,2022,"Rebecca Gorman, Stuart Armstrong",The dangers in algorithms learning humans' values and irrationalities,,,,,http://arxiv.org/abs/2202.13985v2,"For an artificial intelligence (AI) to be aligned with human values (or human
preferences), it must first learn those values. AI systems that are trained on
human behavior, risk miscategorising human irrationalities as human values --
and then optimising for these irrationalities. Simply learning human values
still carries risks: AI learning them will inevitably also gain information on
human irrationalities and human behaviour/policy. Both of these can be
dangerous: knowing human policy allows an AI to become generically more
powerful (whether it is partially aligned or not aligned at all), while
learning human irrationalities allows it to exploit humans without needing to
provide value in return. This paper analyses the danger in developing
artificial intelligence that learns about human irrationalities and human
policy, and constructs a model recommendation system with various levels of
information about human biases, human policy, and human values. It concludes
that, whatever the power and knowledge of the AI, it is more dangerous for it
to know human irrationalities than human values. Thus it is better for the AI
to learn human values directly, rather than learning human biases and then
deducing values from behaviour.",2022-11-13 16:00:23
XXX,journalArticle,2022,"Qinghua Lu, Liming Zhu, Xiwei Xu, Jon Whittle",Responsible-AI-by-Design: a Pattern Collection for Designing Responsible AI Systems,,,,,http://arxiv.org/abs/2203.00905v2,"Although AI has significant potential to transform society, there are serious
concerns about its ability to behave and make decisions responsibly. Many
ethical regulations, principles, and guidelines for responsible AI have been
issued recently. However, these principles are high-level and difficult to put
into practice. In the meantime much effort has been put into responsible AI
from the algorithm perspective, but they are limited to a small subset of
ethical principles amenable to mathematical analysis. Responsible AI issues go
beyond data and algorithms and are often at the system-level crosscutting many
system components and the entire software engineering lifecycle. Based on the
result of a systematic literature review, this paper identifies one missing
element as the system-level guidance - how to design the architecture of
responsible AI systems. We present a summary of design patterns that can be
embedded into the AI systems as product features to contribute to
responsible-AI-by-design.",2022-11-13 16:00:24
XXX,journalArticle,2022,"Jinghui Lu, Linyi Yang, Brian Mac Namee, Yue Zhang",A Rationale-Centric Framework for Human-in-the-loop Machine Learning,,,,,http://arxiv.org/abs/2203.12918v1,"We present a novel rationale-centric framework with human-in-the-loop --
Rationales-centric Double-robustness Learning (RDL) -- to boost model
out-of-distribution performance in few-shot learning scenarios. By using static
semi-factual generation and dynamic human-intervened correction, RDL exploits
rationales (i.e. phrases that cause the prediction), human interventions and
semi-factual augmentations to decouple spurious associations and bias models
towards generally applicable underlying distributions, which enables fast and
accurate generalisation. Experimental results show that RDL leads to
significant prediction benefits on both in-distribution and out-of-distribution
tests compared to many state-of-the-art benchmarks -- especially for few-shot
learning scenarios. We also perform extensive ablation studies to support
in-depth analyses of each component in our framework.",2022-11-13 16:00:25
XXX,journalArticle,2022,"Daniel Zhang, Nestor Maslej, Erik Brynjolfsson, John Etchemendy, Terah Lyons, James Manyika, Helen Ngo, Juan Carlos Niebles, Michael Sellitto, Ellie Sakhaee, Yoav Shoham, Jack Clark, Raymond Perrault",The AI Index 2022 Annual Report,,,,,http://arxiv.org/abs/2205.03468v1,"Welcome to the fifth edition of the AI Index Report! The latest edition
includes data from a broad set of academic, private, and nonprofit
organizations as well as more self-collected data and original analysis than
any previous editions, including an expanded technical performance chapter, a
new survey of robotics researchers around the world, data on global AI
legislation records in 25 countries, and a new chapter with an in-depth
analysis of technical AI ethics metrics.
  The AI Index Report tracks, collates, distills, and visualizes data related
to artificial intelligence. Its mission is to provide unbiased, rigorously
vetted, and globally sourced data for policymakers, researchers, executives,
journalists, and the general public to develop a more thorough and nuanced
understanding of the complex field of AI. The report aims to be the world's
most credible and authoritative source for data and insights about AI.",2022-11-13 16:00:25
XXX,journalArticle,2022,"Giuseppe De Giacomo, Dror Fried, Fabio Patrizi, Shufang Zhu",Mimicking Behaviors in Separated Domains,,,,,http://arxiv.org/abs/2205.09201v1,"Devising a strategy to make a system mimicking behaviors from another system
is a problem that naturally arises in many areas of Computer Science. In this
work, we interpret this problem in the context of intelligent agents, from the
perspective of LTLf, a formalism commonly used in AI for expressing
finite-trace properties. Our model consists of two separated dynamic domains,
D_A and D_B, and an LTLf specification that formalizes the notion of mimicking
by mapping properties on behaviors (traces) of D_A into properties on behaviors
of D_B. The goal is to synthesize a strategy that step-by-step maps every
behavior of D_A into a behavior of D_B so that the specification is met. We
consider several forms of mapping specifications, ranging from simple ones to
full LTLf, and for each we study synthesis algorithms and computational
properties.",2022-11-13 16:00:26
XXX,journalArticle,2022,"Julien Girard-Satabin, Michele Alberti, François Bobot, Zakaria Chihani, Augustin Lemesle",CAISAR: A platform for Characterizing Artificial Intelligence Safety and Robustness,"AISafety, Jul 2022, Vienne, Austria",,,,http://arxiv.org/abs/2206.03044v2,"We present CAISAR, an open-source platform under active development for the
characterization of AI systems' robustness and safety. CAISAR provides a
unified entry point for defining verification problems by using WhyML, the
mature and expressive language of the Why3 verification platform. Moreover,
CAISAR orchestrates and composes state-of-the-art machine learning verification
tools which, individually, are not able to efficiently handle all problems but,
collectively, can cover a growing number of properties. Our aim is to assist,
on the one hand, the V\&V process by reducing the burden of choosing the
methodology tailored to a given verification problem, and on the other hand the
tools developers by factorizing useful features-visualization, report
generation, property description-in one platform. CAISAR will soon be available
at https://git.frama-c.com/pub/caisar.",2022-11-13 16:00:27
XXX,journalArticle,2022,"Alexander Matt Turner, Aseem Saxena, Prasad Tadepalli",Formalizing the Problem of Side Effect Regularization,,,,,http://arxiv.org/abs/2206.11812v3,"AI objectives are often hard to specify properly. Some approaches tackle this
problem by regularizing the AI's side effects: Agents must weigh off ""how much
of a mess they make"" with an imperfectly specified proxy objective. We propose
a formal criterion for side effect regularization via the assistance game
framework. In these games, the agent solves a partially observable Markov
decision process (POMDP) representing its uncertainty about the objective
function it should optimize. We consider the setting where the true objective
is revealed to the agent at a later time step. We show that this POMDP is
solved by trading off the proxy reward with the agent's ability to achieve a
range of future tasks. We empirically demonstrate the reasonableness of our
problem formalization via ground-truth evaluation in two gridworld
environments.",2022-11-13 16:00:27
XXX,journalArticle,2022,Alexander Matt Turner,On Avoiding Power-Seeking by Artificial Intelligence,,,,,http://arxiv.org/abs/2206.11831v1,"We do not know how to align a very intelligent AI agent's behavior with human
interests. I investigate whether -- absent a full solution to this AI alignment
problem -- we can build smart AI agents which have limited impact on the world,
and which do not autonomously seek power. In this thesis, I introduce the
attainable utility preservation (AUP) method. I demonstrate that AUP produces
conservative, option-preserving behavior within toy gridworlds and within
complex environments based off of Conway's Game of Life. I formalize the
problem of side effect avoidance, which provides a way to quantify the side
effects an agent had on the world. I also give a formal definition of
power-seeking in the context of AI agents and show that optimal policies tend
to seek power. In particular, most reward functions have optimal policies which
avoid deactivation. This is a problem if we want to deactivate or correct an
intelligent agent after we have deployed it. My theorems suggest that since
most agent goals conflict with ours, the agent would very probably resist
correction. I extend these theorems to show that power-seeking incentives occur
not just for optimal decision-makers, but under a wide range of decision-making
procedures.",2022-11-13 16:00:28
XXX,journalArticle,2022,"Alexander Matt Turner, Prasad Tadepalli",Parametrically Retargetable Decision-Makers Tend To Seek Power,,,,,http://arxiv.org/abs/2206.13477v2,"If capable AI agents are generally incentivized to seek power in service of
the objectives we specify for them, then these systems will pose enormous
risks, in addition to enormous benefits. In fully observable environments, most
reward functions have an optimal policy which seeks power by keeping options
open and staying alive. However, the real world is neither fully observable,
nor must trained agents be even approximately reward-optimal. We consider a
range of models of AI decision-making, from optimal, to random, to choices
informed by learning and interacting with an environment. We discover that many
decision-making functions are retargetable, and that retargetability is
sufficient to cause power-seeking tendencies. Our functional criterion is
simple and broad. We show that a range of qualitatively dissimilar
decision-making procedures incentivize agents to seek power. We demonstrate the
flexibility of our results by reasoning about learned policy incentives in
Montezuma's Revenge. These results suggest a safety risk: Eventually,
retargetable training procedures may train real-world agents which seek power
over humans.",2022-11-13 16:00:28
XXX,journalArticle,2022,"Zachary Kenton, Ramana Kumar, Sebastian Farquhar, Jonathan Richens, Matt MacDermott, Tom Everitt",Discovering Agents,,,,,http://arxiv.org/abs/2208.08345v2,"Causal models of agents have been used to analyse the safety aspects of
machine learning systems. But identifying agents is non-trivial -- often the
causal model is just assumed by the modeler without much justification -- and
modelling failures can lead to mistakes in the safety analysis. This paper
proposes the first formal causal definition of agents -- roughly that agents
are systems that would adapt their policy if their actions influenced the world
in a different way. From this we derive the first causal discovery algorithm
for discovering agents from empirical data, and give algorithms for translating
between causal models and game-theoretic influence diagrams. We demonstrate our
approach by resolving some previous confusions caused by incorrect causal
modelling of agents.",2022-11-13 16:00:29
XXX,journalArticle,2022,"Bettina Könighofer, Roderick Bloem, Rüdiger Ehlers, Christian Pek",Correct-by-Construction Runtime Enforcement in AI -- A Survey,,,,,http://arxiv.org/abs/2208.14426v1,"Runtime enforcement refers to the theories, techniques, and tools for
enforcing correct behavior with respect to a formal specification of systems at
runtime. In this paper, we are interested in techniques for constructing
runtime enforcers for the concrete application domain of enforcing safety in
AI. We discuss how safety is traditionally handled in the field of AI and how
more formal guarantees on the safety of a self-learning agent can be given by
integrating a runtime enforcer. We survey a selection of work on such
enforcers, where we distinguish between approaches for discrete and continuous
action spaces. The purpose of this paper is to foster a better understanding of
advantages and limitations of different enforcement techniques, focusing on the
specific challenges that arise due to their application in AI. Finally, we
present some open challenges and avenues for future work.",2022-11-13 16:00:29
XXX,journalArticle,2022,"Peter Jamieson, Indrima Upadhyay",A Technique to Create Weaker Abstract Board Game Agents via Reinforcement Learning,,,,,http://arxiv.org/abs/2209.00711v1,"Board games, with the exception of solo games, need at least one other player
to play. Because of this, we created Artificial Intelligent (AI) agents to play
against us when an opponent is missing. These AI agents are created in a number
of ways, but one challenge with these agents is that an agent can have superior
ability compared to us. In this work, we describe how to create weaker AI
agents that play board games. We use Tic-Tac-Toe, Nine-Men's Morris, and
Mancala, and our technique uses a Reinforcement Learning model where an agent
uses the Q-learning algorithm to learn these games. We show how these agents
can learn to play the board game perfectly, and we then describe our approach
to making weaker versions of these agents. Finally, we provide a methodology to
compare AI agents.",2022-11-13 16:00:30
XXX,journalArticle,2022,"Yohai Trabelsi, Abhijin Adiga, Sarit Kraus, S. S. Ravi",Resource Allocation to Agents with Restrictions: Maximizing Likelihood with Minimum Compromise,,,,,http://arxiv.org/abs/2209.05170v1,"Many scenarios where agents with restrictions compete for resources can be
cast as maximum matching problems on bipartite graphs. Our focus is on resource
allocation problems where agents may have restrictions that make them
incompatible with some resources. We assume that a Principle chooses a maximum
matching randomly so that each agent is matched to a resource with some
probability. Agents would like to improve their chances of being matched by
modifying their restrictions within certain limits. The Principle's goal is to
advise an unsatisfied agent to relax its restrictions so that the total cost of
relaxation is within a budget (chosen by the agent) and the increase in the
probability of being assigned a resource is maximized. We establish hardness
results for some variants of this budget-constrained maximization problem and
present algorithmic results for other variants. We experimentally evaluate our
methods on synthetic datasets as well as on two novel real-world datasets: a
vacation activities dataset and a classrooms dataset.",2022-11-13 16:00:30
XXX,journalArticle,2022,"Dylan M. Asmar, Mykel J. Kochenderfer",Collaborative Decision Making Using Action Suggestions,,,,,http://arxiv.org/abs/2209.13160v1,"The level of autonomy is increasing in systems spanning multiple domains, but
these systems still experience failures. One way to mitigate the risk of
failures is to integrate human oversight of the autonomous systems and rely on
the human to take control when the autonomy fails. In this work, we formulate a
method of collaborative decision making through action suggestions that
improves action selection without taking control of the system. Our approach
uses each suggestion efficiently by incorporating the implicit information
shared through suggestions to modify the agent's belief and achieves better
performance with fewer suggestions than naively following the suggested
actions. We assume collaborative agents share the same objective and
communicate through valid actions. By assuming the suggested action is
dependent only on the state, we can incorporate the suggested action as an
independent observation of the environment. The assumption of a collaborative
environment enables us to use the agent's policy to estimate the distribution
over action suggestions. We propose two methods that use suggested actions and
demonstrate the approach through simulated experiments. The proposed
methodology results in increased performance while also being robust to
suboptimal suggestions.",2022-11-13 16:00:31
XXX,journalArticle,2022,"Sander Beckers, Hana Chockler, Joseph Y. Halpern",Quantifying Harm,,,,,http://arxiv.org/abs/2209.15111v2,"In a companion paper (Beckers et al. 2022), we defined a qualitative notion
of harm: either harm is caused, or it is not. For practical applications, we
often need to quantify harm; for example, we may want to choose the lest
harmful of a set of possible interventions. We first present a quantitative
definition of harm in a deterministic context involving a single individual,
then we consider the issues involved in dealing with uncertainty regarding the
context and going from a notion of harm for a single individual to a notion of
""societal harm"", which involves aggregating the harm to individuals. We show
that the ""obvious"" way of doing this (just taking the expected harm for an
individual and then summing the expected harm over all individuals can lead to
counterintuitive or inappropriate answers, and discuss alternatives, drawing on
work from the decision-theory literature.",2022-11-13 16:00:31
XXX,journalArticle,2022,"Cosmin Badea, Leilani Gilpin","Establishing Meta-Decision-Making for AI: An Ontology of Relevance, Representation and Reasoning",,,,,http://arxiv.org/abs/2210.00608v1,"We propose an ontology of building decision-making systems, with the aim of
establishing Meta-Decision-Making for Artificial Intelligence (AI), improving
autonomy, and creating a framework to build metrics and benchmarks upon. To
this end, we propose the three parts of Relevance, Representation, and
Reasoning, and discuss their value in ensuring safety and mitigating risk in
the context of third wave cognitive systems. Our nomenclature reflects the
literature on decision-making, and our ontology allows researchers that adopt
it to frame their work in relation to one or more of these parts.",2022-11-13 16:00:32
XXX,journalArticle,2022,"Hengyuan Hu, David J Wu, Adam Lerer, Jakob Foerster, Noam Brown",Human-AI Coordination via Human-Regularized Search and Learning,,,,,http://arxiv.org/abs/2210.05125v1,"We consider the problem of making AI agents that collaborate well with humans
in partially observable fully cooperative environments given datasets of human
behavior. Inspired by piKL, a human-data-regularized search method that
improves upon a behavioral cloning policy without diverging far away from it,
we develop a three-step algorithm that achieve strong performance in
coordinating with real humans in the Hanabi benchmark. We first use a
regularized search algorithm and behavioral cloning to produce a better human
model that captures diverse skill levels. Then, we integrate the policy
regularization idea into reinforcement learning to train a human-like best
response to the human model. Finally, we apply regularized search on top of the
best response policy at test time to handle out-of-distribution challenges when
playing with humans. We evaluate our method in two large scale experiments with
humans. First, we show that our method outperforms experts when playing with a
group of diverse human players in ad-hoc teams. Second, we show that our method
beats a vanilla best response to behavioral cloning baseline by having experts
play repeatedly with the two agents.",2022-11-13 16:00:32
XXX,journalArticle,2022,"Anthony Zador, Blake Richards, Bence Ölveczky, Sean Escola, Yoshua Bengio, Kwabena Boahen, Matthew Botvinick, Dmitri Chklovskii, Anne Churchland, Claudia Clopath, James DiCarlo, Surya Ganguli, Jeff Hawkins, Konrad Koerding, Alexei Koulakov, Yann LeCun, Timothy Lillicrap, Adam Marblestone, Bruno Olshausen, Alexandre Pouget, Cristina Savin, Terrence Sejnowski, Eero Simoncelli, Sara Solla, David Sussillo, Andreas S. Tolias, Doris Tsao",Toward Next-Generation Artificial Intelligence: Catalyzing the NeuroAI Revolution,,,,,http://arxiv.org/abs/2210.08340v2,"Neuroscience has long been an important driver of progress in artificial
intelligence (AI). We propose that to accelerate progress in AI, we must invest
in fundamental research in NeuroAI.",2022-11-13 16:00:33
XXX,journalArticle,2022,"Andrea Tocchetti, Lorenzo Corti, Agathe Balayn, Mireia Yurrita, Philip Lippmann, Marco Brambilla, Jie Yang",A.I. Robustness: a Human-Centered Perspective on Technological Challenges and Opportunities,,,,,http://arxiv.org/abs/2210.08906v2,"Despite the impressive performance of Artificial Intelligence (AI) systems,
their robustness remains elusive and constitutes a key issue that impedes
large-scale adoption. Robustness has been studied in many domains of AI, yet
with different interpretations across domains and contexts. In this work, we
systematically survey the recent progress to provide a reconciled terminology
of concepts around AI robustness. We introduce three taxonomies to organize and
describe the literature both from a fundamental and applied point of view: 1)
robustness by methods and approaches in different phases of the machine
learning pipeline; 2) robustness for specific model architectures, tasks, and
systems; and in addition, 3) robustness assessment methodologies and insights,
particularly the trade-offs with other trustworthiness properties. Finally, we
identify and discuss research gaps and opportunities and give an outlook on the
field. We highlight the central role of humans in evaluating and enhancing AI
robustness, considering the necessary knowledge humans can provide, and discuss
the need for better understanding practices and developing supportive tools in
the future.",2022-11-13 16:00:34
XXX,journalArticle,2006,"V. V. Kryssanov, V. A. Abramov, Y. Fukuda, K. Konishi",A Decision-Making Support System Based on Know-How,"CIRP Journal of Manufacturing Systems. 1998, Vol. 27, No.4,
  427-432",,,,http://arxiv.org/abs/cs/0606010v1,"The research results described are concerned with: - developing a domain
modeling method and tools to provide the design and implementation of
decision-making support systems for computer integrated manufacturing; -
building a decision-making support system based on know-how and its software
environment. The research is funded by NEDO, Japan.",2022-11-13 16:00:34
XXX,journalArticle,2012,"C. Calderón, L. Delaye, V. Mireles, P. Miramontes",Detecting lateral genetic material transfer,,,,,http://arxiv.org/abs/1204.2601v1,"The bioinformatical methods to detect lateral gene transfer events are mainly
based on functional coding DNA characteristics. In this paper, we propose the
use of DNA traits not depending on protein coding requirements. We introduce
several semilocal variables that depend on DNA primary sequence and that
reflect thermodynamic as well as physico-chemical magnitudes that are able to
tell apart the genome of different organisms. After combining these variables
in a neural classificator, we obtain results whose power of resolution go as
far as to detect the exchange of genomic material between bacteria that are
phylogenetically close.",2022-11-13 16:00:35
XXX,journalArticle,2013,Piyush Ahuja,"Man and Machine: Questions of Risk, Trust and Accountability in Today's AI Technology",,,,,http://arxiv.org/abs/1307.7127v1,"Artificial Intelligence began as a field probing some of the most fundamental
questions of science - the nature of intelligence and the design of intelligent
artifacts. But it has grown into a discipline that is deeply entwined with
commerce and society. Today's AI technology, such as expert systems and
intelligent assistants, pose some difficult questions of risk, trust and
accountability. In this paper, we present these concerns, examining them in the
context of historical developments that have shaped the nature and direction of
AI research. We also suggest the exploration and further development of two
paradigms, human intelligence-machine cooperation, and a sociological view of
intelligence, which might help address some of these concerns.",2022-11-13 16:00:36
XXX,journalArticle,2015,"Guido Governatori, Francesco Olivieri, Simone Scannapieco, Antonino Rotolo, Matteo Cristani",The Rationale behind the Concept of Goal,Theory and Practice of Logic Programming 16 (2016) 296-324,,,10.1017/S1471068416000053,http://arxiv.org/abs/1512.04021v1,"The paper proposes a fresh look at the concept of goal and advances that
motivational attitudes like desire, goal and intention are just facets of the
broader notion of (acceptable) outcome. We propose to encode the preferences of
an agent as sequences of ""alternative acceptable outcomes"". We then study how
the agent's beliefs and norms can be used to filter the mental attitudes out of
the sequences of alternative acceptable outcomes. Finally, we formalise such
intuitions in a novel Modal Defeasible Logic and we prove that the resulting
formalisation is computationally feasible.",2022-11-13 16:00:36
XXX,journalArticle,2017,Richard Pettigrew,Aggregating incoherent agents who disagree,,,,,http://arxiv.org/abs/1709.03981v1,"In this paper, we explore how we should aggregate the degrees of belief of of
a group of agents to give a single coherent set of degrees of belief, when at
least some of those agents might be probabilistically incoherent. There are a
number of way of aggregating degrees of belief, and there are a number of ways
of fixing incoherent degrees of belief. When we have picked one of each, should
we aggregate first and then fix, or fix first and then aggregate? Or should we
try to do both at once? And when do these different procedures agree with one
another? In this paper, we focus particularly on the final question.",2022-11-13 16:00:37
XXX,journalArticle,2019,"Saurabh Srivastava, Vinay P. Namboodiri, T. V. Prabhakar",PUTWorkbench: Analysing Privacy in AI-intensive Systems,,,,,http://arxiv.org/abs/1902.01580v1,"AI intensive systems that operate upon user data face the challenge of
balancing data utility with privacy concerns. We propose the idea and present
the prototype of an open-source tool called Privacy Utility Trade-off (PUT)
Workbench which seeks to aid software practitioners to take such crucial
decisions. We pick a simple privacy model that doesn't require any background
knowledge in Data Science and show how even that can achieve significant
results over standard and real-life datasets. The tool and the source code is
made freely available for extensions and usage.",2022-11-13 16:00:38
XXX,journalArticle,2019,"Kristian Kersting, Jan Peters, Constantin Rothkopf",Was ist eine Professur fuer Kuenstliche Intelligenz?,,,,,http://arxiv.org/abs/1903.09516v1,"The Federal Government of Germany aims to boost the research in the field of
Artificial Intelligence (AI). For instance, 100 new professorships are said to
be established. However, the white paper of the government does not answer what
an AI professorship is at all. In order to give colleagues, politicians, and
citizens an idea, we present a view that is often followed when appointing
professors for AI at German and international universities. We hope that it
will help to establish a guideline with internationally accepted measures and
thus make the public debate more informed.",2022-11-13 16:00:38
XXX,journalArticle,2019,"Amanda Askell, Miles Brundage, Gillian Hadfield",The Role of Cooperation in Responsible AI Development,,,,,http://arxiv.org/abs/1907.04534v1,"In this paper, we argue that competitive pressures could incentivize AI
companies to underinvest in ensuring their systems are safe, secure, and have a
positive social impact. Ensuring that AI systems are developed responsibly may
therefore require preventing and solving collective action problems between
companies. We note that there are several key factors that improve the
prospects for cooperation in collective action problems. We use this to
identify strategies to improve the prospects for industry cooperation on the
responsible development of AI.",2022-11-13 16:00:39
XXX,journalArticle,2019,"Nikos Arechiga, Jonathan DeCastro, Soonho Kong, Karen Leung",Better AI through Logical Scaffolding,,,,,http://arxiv.org/abs/1909.06965v1,"We describe the concept of logical scaffolds, which can be used to improve
the quality of software that relies on AI components. We explain how some of
the existing ideas on runtime monitors for perception systems can be seen as a
specific instance of logical scaffolds. Furthermore, we describe how logical
scaffolds may be useful for improving AI programs beyond perception systems, to
include general prediction systems and agent behavior models.",2022-11-13 16:00:39
XXX,journalArticle,2020,Samuel Allen Alexander,The Archimedean trap: Why traditional reinforcement learning will probably not yield AGI,Journal of Artificial General Intelligence 11(1): 70--85 (2020),,,10.2478/jagi-2020-0004,http://arxiv.org/abs/2002.10221v2,"After generalizing the Archimedean property of real numbers in such a way as
to make it adaptable to non-numeric structures, we demonstrate that the real
numbers cannot be used to accurately measure non-Archimedean structures. We
argue that, since an agent with Artificial General Intelligence (AGI) should
have no problem engaging in tasks that inherently involve non-Archimedean
rewards, and since traditional reinforcement learning rewards are real numbers,
therefore traditional reinforcement learning probably will not lead to AGI. We
indicate two possible ways traditional reinforcement learning could be altered
to remove this roadblock.",2022-11-13 16:00:40
XXX,journalArticle,2020,"Gabriel Lima, Meeyoung Cha",Responsible AI and Its Stakeholders,,,,,http://arxiv.org/abs/2004.11434v1,"Responsible Artificial Intelligence (AI) proposes a framework that holds all
stakeholders involved in the development of AI to be responsible for their
systems. It, however, fails to accommodate the possibility of holding AI
responsible per se, which could close some legal and moral gaps concerning the
deployment of autonomous and self-learning systems. We discuss three notions of
responsibility (i.e., blameworthiness, accountability, and liability) for all
stakeholders, including AI, and suggest the roles of jurisdiction and the
general public in this matter.",2022-11-13 16:00:40
XXX,journalArticle,2020,Markus Borg,The AIQ Meta-Testbed: Pragmatically Bridging Academic AI Testing and Industrial Q Needs,,,,,http://arxiv.org/abs/2009.05260v1,"AI solutions seem to appear in any and all application domains. As AI becomes
more pervasive, the importance of quality assurance increases. Unfortunately,
there is no consensus on what artificial intelligence means and interpretations
range from simple statistical analysis to sentient humanoid robots. On top of
that, quality is a notoriously hard concept to pinpoint. What does this mean
for AI quality? In this paper, we share our working definition and a pragmatic
approach to address the corresponding quality assurance with a focus on
testing. Finally, we present our ongoing work on establishing the AIQ
Meta-Testbed.",2022-11-13 16:00:41
XXX,journalArticle,2020,Evan Hubinger,An overview of 11 proposals for building safe advanced AI,,,,,http://arxiv.org/abs/2012.07532v1,"This paper analyzes and compares 11 different proposals for building safe
advanced AI under the current machine learning paradigm, including major
contenders such as iterated amplification, AI safety via debate, and recursive
reward modeling. Each proposal is evaluated on the four components of outer
alignment, inner alignment, training competitiveness, and performance
competitiveness, of which the distinction between the latter two is introduced
in this paper. While prior literature has primarily focused on analyzing
individual proposals, or primarily focused on outer alignment at the expense of
inner alignment, this analysis seeks to take a comparative look at a wide range
of proposals including a comparative analysis across all four previously
mentioned components.",2022-11-13 16:00:42
XXX,journalArticle,2021,"Jurriaan van Diggelen, Wiard Jorritsma, Bob van der Vecht",Teaming up with information agents,,,,,http://arxiv.org/abs/2101.06133v1,"Despite the intricacies involved in designing a computer as a teampartner, we
can observe patterns in team behavior which allow us to describe at a general
level how AI systems are to collaborate with humans. Whereas most work on
human-machine teaming has focused on physical agents (e.g. robotic systems),
our aim is to study how humans can collaborate with information agents. We
propose some appropriate team design patterns, and test them using our
Collaborative Intelligence Analysis (CIA) tool.",2022-11-13 16:00:42
XXX,journalArticle,2021,Bryce Goodman,Hard Choices and Hard Limits for Artificial Intelligence,,,,10.1145/3461702.3462539,http://arxiv.org/abs/2105.07852v1,"Artificial intelligence (AI) is supposed to help us make better choices. Some
of these choices are small, like what route to take to work, or what music to
listen to. Others are big, like what treatment to administer for a disease or
how long to sentence someone for a crime. If AI can assist with these big
decisions, we might think it can also help with hard choices, cases where
alternatives are neither better, worse nor equal but on a par. The aim of this
paper, however, is to show that this view is mistaken: the fact of parity shows
that there are hard limits on AI in decision making and choices that AI cannot,
and should not, resolve.",2022-11-13 16:00:43
XXX,journalArticle,2021,"Abdullah Khan, Alexei Vernitski, Alexei Lisitsa",Untangling Braids with Multi-agent Q-Learning,,,,,http://arxiv.org/abs/2109.14502v1,"We use reinforcement learning to tackle the problem of untangling braids. We
experiment with braids with 2 and 3 strands. Two competing players learn to
tangle and untangle a braid. We interface the braid untangling problem with the
OpenAI Gym environment, a widely used way of connecting agents to reinforcement
learning problems. The results provide evidence that the more we train the
system, the better the untangling player gets at untangling braids. At the same
time, our tangling player produces good examples of tangled braids.",2022-11-13 16:00:43
XXX,journalArticle,2022,John Piorkowski,The 6-Ds of Creating AI-Enabled Systems,,,,,http://arxiv.org/abs/2202.03172v1,"We are entering our tenth year of the current Artificial Intelligence (AI)
spring, and, as with previous AI hype cycles, the threat of an AI winter looms.
AI winters occurred because of ineffective approaches towards navigating the
technology valley of death. The 6-D framework provides an end-to-end framework
to successfully navigate this challenge. The 6-D framework starts with problem
decomposition to identify potential AI solutions, and ends with considerations
for deployment of AI-enabled systems. Each component of the 6-D framework and a
precision medicine use case is described in this paper.",2022-11-13 16:00:44
XXX,journalArticle,2022,"Thao Le, Tim Miller, Ronal Singh, Liz Sonenberg",Improving Model Understanding and Trust with Counterfactual Explanations of Model Confidence,,,,,http://arxiv.org/abs/2206.02790v1,"In this paper, we show that counterfactual explanations of confidence scores
help users better understand and better trust an AI model's prediction in
human-subject studies. Showing confidence scores in human-agent interaction
systems can help build trust between humans and AI systems. However, most
existing research only used the confidence score as a form of communication,
and we still lack ways to explain why the algorithm is confident. This paper
also presents two methods for understanding model confidence using
counterfactual explanation: (1) based on counterfactual examples; and (2) based
on visualisation of the counterfactual space.",2022-11-13 16:00:44
XXX,journalArticle,2022,"Aastha Acharya, Rebecca Russell, Nisar R. Ahmed",Uncertainty Quantification for Competency Assessment of Autonomous Agents,,,,,http://arxiv.org/abs/2206.10553v1,"For safe and reliable deployment in the real world, autonomous agents must
elicit appropriate levels of trust from human users. One method to build trust
is to have agents assess and communicate their own competencies for performing
given tasks. Competency depends on the uncertainties affecting the agent,
making accurate uncertainty quantification vital for competency assessment. In
this work, we show how ensembles of deep generative models can be used to
quantify the agent's aleatoric and epistemic uncertainties when forecasting
task outcomes as part of competency assessment.",2022-11-13 16:00:45
XXX,journalArticle,2022,Shuchan Wang,Information-Theoretic Equivalence of Entropic Multi-Marginal Optimal Transport: A Theory for Multi-Agent Communication,,,,,http://arxiv.org/abs/2208.10256v2,"In this paper, we propose our information-theoretic equivalence of entropic
multi-marginal optimal transport (MOT). This equivalence can be easily reduced
to the case of entropic optimal transport (OT). Because OT is widely used to
compare differences between knowledge or beliefs, we apply this result to the
communication between agents with different beliefs. Our results formally prove
the statement that entropic OT is information-theoretically optimal given by
Wang et al. [2020] and generalize it to the multi-agent case. We believe that
our work can shed light on OT theory in future multi-agent teaming systems.",2022-11-13 16:00:46
XXX,journalArticle,2022,"Vasudev Gohil, Satwik Patnaik, Hao Guo, Dileep Kalathil, Jeyavijayan, Rajendran",DETERRENT: Detecting Trojans using Reinforcement Learning,,,,10.1145/3489517.3530518,http://arxiv.org/abs/2208.12878v1,"Insertion of hardware Trojans (HTs) in integrated circuits is a pernicious
threat. Since HTs are activated under rare trigger conditions, detecting them
using random logic simulations is infeasible. In this work, we design a
reinforcement learning (RL) agent that circumvents the exponential search space
and returns a minimal set of patterns that is most likely to detect HTs.
Experimental results on a variety of benchmarks demonstrate the efficacy and
scalability of our RL agent, which obtains a significant reduction
($169\times$) in the number of test patterns required while maintaining or
improving coverage ($95.75\%$) compared to the state-of-the-art techniques.",2022-11-13 16:00:46
XXX,journalArticle,2022,"Andrés García-Silva, Cristian Berrío, José Manuel Gómez-Pérez",Generating Quizzes to Support Training on Quality Management and Assurance in Space Science and Engineering,,,,,http://arxiv.org/abs/2210.03427v2,"Quality management and assurance is key for space agencies to guarantee the
success of space missions, which are high-risk and extremely costly. In this
paper, we present a system to generate quizzes, a common resource to evaluate
the effectiveness of training sessions, from documents about quality assurance
procedures in the Space domain. Our system leverages state of the art
auto-regressive models like T5 and BART to generate questions, and a RoBERTa
model to extract answers for such questions, thus verifying their suitability.",2022-11-13 16:00:47
XXX,journalArticle,1995,"S. J. Russell, D. Subramanian",Provably Bounded-Optimal Agents,"Journal of Artificial Intelligence Research, Vol 2, (1995),
  575-609",,,,http://arxiv.org/abs/cs/9505103v1,"Since its inception, artificial intelligence has relied upon a theoretical
foundation centered around perfect rationality as the desired property of
intelligent systems. We argue, as others have done, that this foundation is
inadequate because it imposes fundamentally unsatisfiable requirements. As a
result, there has arisen a wide gap between theory and practice in AI,
hindering progress in the field. We propose instead a property called bounded
optimality. Roughly speaking, an agent is bounded-optimal if its program is a
solution to the constrained optimization problem presented by its architecture
and the task environment. We show how to construct agents with this property
for a simple class of machine architectures in a broad class of real-time
environments. We illustrate these results using a simple model of an automated
mail sorting facility. We also define a weaker property, asymptotic bounded
optimality (ABO), that generalizes the notion of optimality in classical
complexity theory. We then construct universal ABO programs, i.e., programs
that are ABO no matter what real-time constraints are applied. Universal ABO
programs can be used as building blocks for more complex systems. We conclude
with a discussion of the prospects for bounded optimality as a theoretical
basis for AI, and relate it to similar trends in philosophy, economics, and
game theory.",2022-11-13 16:00:47
XXX,journalArticle,1997,M. Tambe,Towards Flexible Teamwork,"Journal of Artificial Intelligence Research, Vol 7, (1997), 83-124",,,,http://arxiv.org/abs/cs/9709101v1,"Many AI researchers are today striving to build agent teams for complex,
dynamic multi-agent domains, with intended applications in arenas such as
education, training, entertainment, information integration, and collective
robotics. Unfortunately, uncertainties in these complex, dynamic domains
obstruct coherent teamwork. In particular, team members often encounter
differing, incomplete, and possibly inconsistent views of their environment.
Furthermore, team members can unexpectedly fail in fulfilling responsibilities
or discover unexpected opportunities. Highly flexible coordination and
communication is key in addressing such uncertainties. Simply fitting
individual agents with precomputed coordination plans will not do, for their
inflexibility can cause severe failures in teamwork, and their
domain-specificity hinders reusability. Our central hypothesis is that the key
to such flexibility and reusability is providing agents with general models of
teamwork. Agents exploit such models to autonomously reason about coordination
and communication, providing requisite flexibility. Furthermore, the models
enable reuse across domains, both saving implementation effort and enforcing
consistency. This article presents one general, implemented model of teamwork,
called STEAM. The basic building block of teamwork in STEAM is joint intentions
(Cohen & Levesque, 1991b); teamwork in STEAM is based on agents' building up a
(partial) hierarchy of joint intentions (this hierarchy is seen to parallel
Grosz & Kraus's partial SharedPlans, 1996). Furthermore, in STEAM, team members
monitor the team's and individual members' performance, reorganizing the team
as necessary. Finally, decision-theoretic communication selectivity in STEAM
ensures reduction in communication overheads of teamwork, with appropriate
sensitivity to the environmental conditions. This article describes STEAM's
application in three different complex domains, and presents detailed empirical
results.",2022-11-13 16:00:48
XXX,journalArticle,2012,"Tobias Jung, Daniel Polani, Peter Stone",Empowerment for Continuous Agent-Environment Systems,"Adaptive Behavior 19(1),2011",,,,http://arxiv.org/abs/1201.6583v1,"This paper develops generalizations of empowerment to continuous states.
Empowerment is a recently introduced information-theoretic quantity motivated
by hypotheses about the efficiency of the sensorimotor loop in biological
organisms, but also from considerations stemming from curiosity-driven
learning. Empowemerment measures, for agent-environment systems with stochastic
transitions, how much influence an agent has on its environment, but only that
influence that can be sensed by the agent sensors. It is an
information-theoretic generalization of joint controllability (influence on
environment) and observability (measurement by sensors) of the environment by
the agent, both controllability and observability being usually defined in
control theory as the dimensionality of the control/observation spaces. Earlier
work has shown that empowerment has various interesting and relevant
properties, e.g., it allows us to identify salient states using only the
dynamics, and it can act as intrinsic reward without requiring an external
reward. However, in this previous work empowerment was limited to the case of
small-scale and discrete domains and furthermore state transition probabilities
were assumed to be known. The goal of this paper is to extend empowerment to
the significantly more important and relevant case of continuous vector-valued
state spaces and initially unknown state transition probabilities. The
continuous state space is addressed by Monte-Carlo approximation; the unknown
transitions are addressed by model learning and prediction for which we apply
Gaussian processes regression with iterated forecasting. In a number of
well-known continuous control tasks we examine the dynamics induced by
empowerment and include an application to exploration and online model
learning.",2022-11-13 16:00:49
XXX,journalArticle,2012,"Patrick Rodler, Kostyantyn Shchekotykhin, Philipp Fleiss, Gerhard Friedrich",RIO: Minimizing User Interaction in Ontology Debugging,,,,,http://arxiv.org/abs/1209.3734v1,"Efficient ontology debugging is a cornerstone for many activities in the
context of the Semantic Web, especially when automatic tools produce (parts of)
ontologies such as in the field of ontology matching. The best currently known
interactive debugging systems rely upon some meta information in terms of fault
probabilities, which can speed up the debugging procedure in the good case, but
can also have negative impact on the performance in the bad case. The problem
is that assessment of the meta information is only possible a-posteriori.
Consequently, as long as the actual fault is unknown, there is always some risk
of suboptimal interactive diagnoses discrimination. As an alternative, one
might prefer to rely on a tool which pursues a no-risk strategy. In this case,
however, possibly well-chosen meta information cannot be exploited, resulting
again in inefficient debugging actions. In this work we present a reinforcement
learning strategy that continuously adapts its behavior depending on the
performance achieved and minimizes the risk of using low-quality meta
information. Therefore, this method is suitable for application scenarios where
reliable a-priori fault estimates are difficult to obtain. Using problematic
ontologies in the field of ontology matching, we show that the proposed
risk-aware query strategy outperforms both active learning approaches and
no-risk strategies on average in terms of required amount of user interaction.",2022-11-13 16:00:49
XXX,journalArticle,2014,"Yifeng Zeng, Prashant Doshi",Exploiting Model Equivalences for Solving Interactive Dynamic Influence Diagrams,"Journal Of Artificial Intelligence Research, Volume 43, pages
  211-255, 2012",,,10.1613/jair.3461,http://arxiv.org/abs/1401.4600v1,"We focus on the problem of sequential decision making in partially observable
environments shared with other agents of uncertain types having similar or
conflicting objectives. This problem has been previously formalized by multiple
frameworks one of which is the interactive dynamic influence diagram (I-DID),
which generalizes the well-known influence diagram to the multiagent setting.
I-DIDs are graphical models and may be used to compute the policy of an agent
given its belief over the physical state and others models, which changes as
the agent acts and observes in the multiagent setting.
  As we may expect, solving I-DIDs is computationally hard. This is
predominantly due to the large space of candidate models ascribed to the other
agents and its exponential growth over time. We present two methods for
reducing the size of the model space and stemming its exponential growth. Both
these methods involve aggregating individual models into equivalence classes.
Our first method groups together behaviorally equivalent models and selects
only those models for updating which will result in predictive behaviors that
are distinct from others in the updated model space. The second method further
compacts the model space by focusing on portions of the behavioral predictions.
Specifically, we cluster actionally equivalent models that prescribe identical
actions at a single time step. Exactly identifying the equivalences would
require us to solve all models in the initial set. We avoid this by selectively
solving some of the models, thereby introducing an approximation. We discuss
the error introduced by the approximation, and empirically demonstrate the
improved efficiency in solving I-DIDs due to the equivalences.",2022-11-13 16:00:50
XXX,journalArticle,2015,Hao Wu,What is Learning? A primary discussion about information and Representation,,,,,http://arxiv.org/abs/1505.04813v1,"Nowadays, represented by Deep Learning techniques, the field of machine
learning is experiencing unprecedented prosperity and its influence is
demonstrated in academia, industry and civil society. ""Intelligent"" has become
a label which could not be neglected for most applications; celebrities and
scientists also warned that the development of full artificial intelligence may
spell the end of the human race. It seems that the answer to building a
computer system that could automatically improve with experience is right on
the next corner. While for AI and machine learning researchers, it is a
consensus that we are not anywhere near the core technique which could bring
the Terminator, Number 5 or R2D2 into real life, and there is not even a formal
definition about what is intelligence, or one of its basic properties:
Learning. Therefore, even though researchers know these concerns are not
necessary currently, there is no generalized explanation about why these
concerns are not necessary, and what properties people should take into account
that would make these concerns to be necessary. In this paper, starts from
analysing the relation between information and its representation, a necessary
condition for a model to be a learning model is proposed. This condition and
related future works could be used to verify whether a system is able to learn
or not, and enrich our understanding of learning: one important property of
Intelligence.",2022-11-13 16:00:51
XXX,journalArticle,2015,"Song-Ju Kim, Tohru Tsuruoka, Tsuyoshi Hasegawa, Masakazu Aono",Decision Maker based on Atomic Switches,,,,,http://arxiv.org/abs/1507.05895v1,"We propose a simple model for an atomic switch-based decision maker (ASDM),
and show that, as long as its total volume of precipitated Ag atoms is
conserved when coupled with suitable operations, an atomic switch system
provides a sophisticated ""decision-making"" capability that is known to be one
of the most important intellectual abilities in human beings. We considered the
multi-armed bandit problem (MAB); the problem of finding, as accurately and
quickly as possible, the most profitable option from a set of options that
gives stochastic rewards. These decisions are made as dictated by each volume
of precipitated Ag atoms, which is moved in a manner similar to the
fluctuations of a rigid body in a tug-of-war game. The ""tug-of-war (TOW)
dynamics"" of the ASDM exhibits higher efficiency than conventional MAB solvers.
We show analytical calculations that validate the statistical reasons for the
ASDM dynamics to produce such high performance, despite its simplicity. These
results imply that various physical systems, in which some conservation law
holds, can be used to implement efficient ""decision-making objects."" Efficient
MAB solvers are useful for many practical applications, because MAB abstracts a
variety of decision-making problems in real- world situations where an
efficient trial-and-error is required. The proposed scheme will introduce a new
physics-based analog computing paradigm, which will include such things as
""intelligent nano devices"" and ""intelligent information networks"" based on
self-detection and self-judgment.",2022-11-13 16:00:51
XXX,journalArticle,2015,"Owain Evans, Andreas Stuhlmueller, Noah D. Goodman","Learning the Preferences of Ignorant, Inconsistent Agents",,,,,http://arxiv.org/abs/1512.05832v1,"An important use of machine learning is to learn what people value. What
posts or photos should a user be shown? Which jobs or activities would a person
find rewarding? In each case, observations of people's past choices can inform
our inferences about their likes and preferences. If we assume that choices are
approximately optimal according to some utility function, we can treat
preference inference as Bayesian inverse planning. That is, given a prior on
utility functions and some observed choices, we invert an optimal
decision-making process to infer a posterior distribution on utility functions.
However, people often deviate from approximate optimality. They have false
beliefs, their planning is sub-optimal, and their choices may be temporally
inconsistent due to hyperbolic discounting and other biases. We demonstrate how
to incorporate these deviations into algorithms for preference inference by
constructing generative models of planning for agents who are subject to false
beliefs and time inconsistency. We explore the inferences these models make
about preferences, beliefs, and biases. We present a behavioral experiment in
which human subjects perform preference inference given the same observations
of choices as our model. Results show that human subjects (like our model)
explain choices in terms of systematic deviations from optimal behavior and
suggest that they take such deviations into account when inferring preferences.",2022-11-13 16:00:52
XXX,journalArticle,2016,Andrew MacFie,Analysis of Algorithms and Partial Algorithms,"Artificial General Intelligence 2016, New York, USA, July 16-19,
  2016, Proceedings, 284-293",,,10.1007/978-3-319-41649-6_29,http://arxiv.org/abs/1601.03411v5,"We present an alternative methodology for the analysis of algorithms, based
on the concept of expected discounted reward. This methodology naturally
handles algorithms that do not always terminate, so it can (theoretically) be
used with partial algorithms for undecidable problems, such as those found in
artificial general intelligence (AGI) and automated theorem proving. We mention
an approach to self-improving AGI enabled by this methodology.
  Aug 2017 addendum: This article was originally written with multiple
audiences in mind. It is really best put in the following terms. Goertzel,
Hutter, Legg, and others have developed a definition of an intelligence score
for a general abstract agent: expected lifetime reward in a random environment.
AIXI is generally the optimal agent according to this score, but there may be
reasons to analyze other agents and compare score values. If we want to use
this definition of intelligence in practice, perhaps we can start by analyzing
some simple agents. Common algorithms can be thought of as simple agents
(environment is input, reward is based on running time) so we take the goal of
applying the agent intelligence score to algorithms. That is, we want to find,
what are the IQ scores of algorithms? We can do some very simple analysis, but
the real answer is that even for simple algorithms, the intelligence score is
too difficult to work with in practice.",2022-11-13 16:00:52
XXX,journalArticle,2016,"Peter M. Krafft, Chris L. Baker, Alex Pentland, Joshua B. Tenenbaum",Modeling Human Ad Hoc Coordination,,,,,http://arxiv.org/abs/1602.03924v1,"Whether in groups of humans or groups of computer agents, collaboration is
most effective between individuals who have the ability to coordinate on a
joint strategy for collective action. However, in general a rational actor will
only intend to coordinate if that actor believes the other group members have
the same intention. This circular dependence makes rational coordination
difficult in uncertain environments if communication between actors is
unreliable and no prior agreements have been made. An important normative
question with regard to coordination in these ad hoc settings is therefore how
one can come to believe that other actors will coordinate, and with regard to
systems involving humans, an important empirical question is how humans arrive
at these expectations. We introduce an exact algorithm for computing the
infinitely recursive hierarchy of graded beliefs required for rational
coordination in uncertain environments, and we introduce a novel mechanism for
multiagent coordination that uses it. Our algorithm is valid in any environment
with a finite state space, and extensions to certain countably infinite state
spaces are likely possible. We test our mechanism for multiagent coordination
as a model for human decisions in a simple coordination game using existing
experimental data. We then explore via simulations whether modeling humans in
this way may improve human-agent collaboration.",2022-11-13 16:00:52
XXX,journalArticle,2016,"Kevin H. Knuth, Philip M. Erner, Scott Frasso",Designing Intelligent Instruments,"AIP Conference Proceedings 954, American Institute of Physics,
  Melville NY, 203-211, 2007",,,10.1063/1.2821263,http://arxiv.org/abs/1602.04290v1,"Remote science operations require automated systems that can both act and
react with minimal human intervention. One such vision is that of an
intelligent instrument that collects data in an automated fashion, and based on
what it learns, decides which new measurements to take. This innovation
implements experimental design and unites it with data analysis in such a way
that it completes the cycle of learning. This cycle is the basis of the
Scientific Method.
  The three basic steps of this cycle are hypothesis generation, inquiry, and
inference. Hypothesis generation is implemented by artificially supplying the
instrument with a parameterized set of possible hypotheses that might be used
to describe the physical system. The act of inquiry is handled by an inquiry
engine that relies on Bayesian adaptive exploration where the optimal
experiment is chosen as the one which maximizes the expected information gain.
The inference engine is implemented using the nested sampling algorithm, which
provides the inquiry engine with a set of posterior samples from which the
expected information gain can be estimated. With these computational structures
in place, the instrument will refine its hypotheses, and repeat the learning
cycle by taking measurements until the system under study is described within a
pre-specified tolerance. We will demonstrate our first attempts toward
achieving this goal with an intelligent instrument constructed using the LEGO
MINDSTORMS NXT robotics platform.",2022-11-13 16:00:53
XXX,journalArticle,2016,"Juan M. Alberola, Elena Del Val, Victor Sanchez-Anguix, Alberto Palomares, Maria Dolores Teruel",An artificial intelligence tool for heterogeneous team formation in the classroom,"Knowledge-Based Systems, 2016",,,10.1016/j.knosys.2016.02.010,http://arxiv.org/abs/1604.04721v1,"Nowadays, there is increasing interest in the development of teamwork skills
in the educational context. This growing interest is motivated by its
pedagogical effectiveness and the fact that, in labour contexts, enterprises
organize their employees in teams to carry out complex projects. Despite its
crucial importance in the classroom and industry, there is a lack of support
for the team formation process. Not only do many factors influence team
performance, but the problem becomes exponentially costly if teams are to be
optimized. In this article, we propose a tool whose aim it is to cover such a
gap. It combines artificial intelligence techniques such as coalition structure
generation, Bayesian learning, and Belbin's role theory to facilitate the
generation of working groups in an educational context. This tool improves
current state of the art proposals in three ways: i) it takes into account the
feedback of other teammates in order to establish the most predominant role of
a student instead of self-perception questionnaires; ii) it handles uncertainty
with regard to each student's predominant team role; iii) it is iterative since
it considers information from several interactions in order to improve the
estimation of role assignments. We tested the performance of the proposed tool
in an experiment involving students that took part in three different team
activities. The experiments suggest that the proposed tool is able to improve
different teamwork aspects such as team dynamics and student satisfaction.",2022-11-13 16:00:53
XXX,journalArticle,2016,"Dylan Hadfield-Menell, Anca Dragan, Pieter Abbeel, Stuart Russell",The Off-Switch Game,,,,,http://arxiv.org/abs/1611.08219v3,"It is clear that one of the primary tools we can use to mitigate the
potential risk from a misbehaving AI system is the ability to turn the system
off. As the capabilities of AI systems improve, it is important to ensure that
such systems do not adopt subgoals that prevent a human from switching them
off. This is a challenge because many formulations of rational agents create
strong incentives for self-preservation. This is not caused by a built-in
instinct, but because a rational agent will maximize expected utility and
cannot achieve whatever objective it has been given if it is dead. Our goal is
to study the incentives an agent has to allow itself to be switched off. We
analyze a simple game between a human H and a robot R, where H can press R's
off switch but R can disable the off switch. A traditional agent takes its
reward function for granted: we show that such agents have an incentive to
disable the off switch, except in the special case where H is perfectly
rational. Our key insight is that for R to want to preserve its off switch, it
needs to be uncertain about the utility associated with the outcome, and to
treat H's actions as important observations about that utility. (R also has no
incentive to switch itself off in this setting.) We conclude that giving
machines an appropriate level of uncertainty about their objectives leads to
safer designs, and we argue that this setting is a useful generalization of the
classical AI paradigm of rational agents.",2022-11-13 16:00:54
XXX,journalArticle,2017,"William Saunders, Girish Sastry, Andreas Stuhlmueller, Owain Evans",Trial without Error: Towards Safe Reinforcement Learning via Human Intervention,,,,,http://arxiv.org/abs/1707.05173v1,"AI systems are increasingly applied to complex tasks that involve interaction
with humans. During training, such systems are potentially dangerous, as they
haven't yet learned to avoid actions that could cause serious harm. How can an
AI system explore and learn without making a single mistake that harms humans
or otherwise causes serious damage? For model-free reinforcement learning,
having a human ""in the loop"" and ready to intervene is currently the only way
to prevent all catastrophes. We formalize human intervention for RL and show
how to reduce the human labor required by training a supervised learner to
imitate the human's intervention decisions. We evaluate this scheme on Atari
games, with a Deep RL agent being overseen by a human for four hours. When the
class of catastrophes is simple, we are able to prevent all catastrophes
without affecting the agent's learning (whereas an RL baseline fails due to
catastrophic forgetting). However, this scheme is less successful when
catastrophes are more complex: it reduces but does not eliminate catastrophes
and the supervised learner fails on adversarial examples found by the agent.
Extrapolating to more challenging environments, we show that our implementation
would not scale (due to the infeasible amount of human labor required). We
outline extensions of the scheme that are necessary if we are to train
model-free agents without a single catastrophe.",2022-11-13 16:00:55
XXX,journalArticle,2017,"Marco Gavanelli, Maddalena Nonato, Andrea Peano, Davide Bertozzi",Logic Programming approaches for routing fault-free and maximally-parallel Wavelength Routed Optical Networks on Chip (Application paper),,,,,http://arxiv.org/abs/1707.05858v1,"One promising trend in digital system integration consists of boosting
on-chip communication performance by means of silicon photonics, thus
materializing the so-called Optical Networks-on-Chip (ONoCs). Among them,
wavelength routing can be used to route a signal to destination by univocally
associating a routing path to the wavelength of the optical carrier. Such
wavelengths should be chosen so to minimize interferences among optical
channels and to avoid routing faults. As a result, physical parameter selection
of such networks requires the solution of complex constrained optimization
problems. In previous work, published in the proceedings of the International
Conference on Computer-Aided Design, we proposed and solved the problem of
computing the maximum parallelism obtainable in the communication between any
two endpoints while avoiding misrouting of optical signals. The underlying
technology, only quickly mentioned in that paper, is Answer Set Programming
(ASP). In this work, we detail the ASP approach we used to solve such problem.
  Another important design issue is to select the wavelengths of optical
carriers such that they are spread across the available spectrum, in order to
reduce the likelihood that, due to imperfections in the manufacturing process,
unintended routing faults arise. We show how to address such problem in
Constraint Logic Programming on Finite Domains (CLP(FD)).
  This paper is under consideration for possible publication on Theory and
Practice of Logic Programming.",2022-11-13 16:00:55
XXX,journalArticle,2017,"Jaime F. Fisac, Monica A. Gates, Jessica B. Hamrick, Chang Liu, Dylan Hadfield-Menell, Malayandi Palaniappan, Dhruv Malik, S. Shankar Sastry, Thomas L. Griffiths, Anca D. Dragan",Pragmatic-Pedagogic Value Alignment,"International Symposium on Robotics Research, 2017",,,,http://arxiv.org/abs/1707.06354v2,"As intelligent systems gain autonomy and capability, it becomes vital to
ensure that their objectives match those of their human users; this is known as
the value-alignment problem. In robotics, value alignment is key to the design
of collaborative robots that can integrate into human workflows, successfully
inferring and adapting to their users' objectives as they go. We argue that a
meaningful solution to value alignment must combine multi-agent decision theory
with rich mathematical models of human cognition, enabling robots to tap into
people's natural collaborative capabilities. We present a solution to the
cooperative inverse reinforcement learning (CIRL) dynamic game based on
well-established cognitive models of decision making and theory of mind. The
solution captures a key reciprocity relation: the human will not plan her
actions in isolation, but rather reason pedagogically about how the robot might
learn from them; the robot, in turn, can anticipate this and interpret the
human's actions pragmatically. To our knowledge, this work constitutes the
first formal analysis of value alignment grounded in empirically validated
cognitive models.",2022-11-13 16:00:56
XXX,journalArticle,2017,"Ion Stoica, Dawn Song, Raluca Ada Popa, David Patterson, Michael W. Mahoney, Randy Katz, Anthony D. Joseph, Michael Jordan, Joseph M. Hellerstein, Joseph E. Gonzalez, Ken Goldberg, Ali Ghodsi, David Culler, Pieter Abbeel",A Berkeley View of Systems Challenges for AI,,,,,http://arxiv.org/abs/1712.05855v1,"With the increasing commoditization of computer vision, speech recognition
and machine translation systems and the widespread deployment of learning-based
back-end technologies such as digital advertising and intelligent
infrastructures, AI (Artificial Intelligence) has moved from research labs to
production. These changes have been made possible by unprecedented levels of
data and computation, by methodological advances in machine learning, by
innovations in systems software and architectures, and by the broad
accessibility of these technologies.
  The next generation of AI systems promises to accelerate these developments
and increasingly impact our lives via frequent interactions and making (often
mission-critical) decisions on our behalf, often in highly personalized
contexts. Realizing this promise, however, raises daunting challenges. In
particular, we need AI systems that make timely and safe decisions in
unpredictable environments, that are robust against sophisticated adversaries,
and that can process ever increasing amounts of data across organizations and
individuals without compromising confidentiality. These challenges will be
exacerbated by the end of the Moore's Law, which will constrain the amount of
data these technologies can store and process. In this paper, we propose
several open research directions in systems, architectures, and security that
can address these challenges and help unlock AI's potential to improve lives
and society.",2022-11-13 16:00:56
XXX,journalArticle,2017,Rajesh Chidambaram,Towards an unanimous international regulatory body for responsible use of Artificial Intelligence [UIRB-AI],,,,,http://arxiv.org/abs/1712.07752v3,"Artificial Intelligence (AI), is once again in the phase of drastic
advancements. Unarguably, the technology itself can revolutionize the way we
live our everyday life. But the exponential growth of technology poses a
daunting task for policy researchers and law makers in making amendments to the
existing norms. In addition, not everyone in the society is studying the
potential socio-economic intricacies and cultural drifts that AI can bring
about. It is prudence to reflect from our historical past to propel the
development of technology in the right direction. To benefit the society of the
present and future, I scientifically explore the societal impact of AI. While
there are many public and private partnerships working on similar aspects, here
I describe the necessity for an Unanimous International Regulatory Body for all
applications of AI (UIRB-AI). I also discuss the benefits and drawbacks of such
an organization. To combat any drawbacks in the formation of an UIRB-AI, both
idealistic and pragmatic perspectives are discussed alternatively. The paper
further advances the discussion by proposing novel policies on how such
organization should be structured and how it can bring about a win-win
situation for everyone in the society.",2022-11-13 16:00:57
XXX,journalArticle,2018,"Jahanzaib Shabbir, Tarique Anwer",Artificial Intelligence and its Role in Near Future,,,,,http://arxiv.org/abs/1804.01396v1,"AI technology has a long history which is actively and constantly changing
and growing. It focuses on intelligent agents, which contain devices that
perceive the environment and based on which takes actions in order to maximize
goal success chances. In this paper, we will explain the modern AI basics and
various representative applications of AI. In the context of the modern
digitalized world, AI is the property of machines, computer programs, and
systems to perform the intellectual and creative functions of a person,
independently find ways to solve problems, be able to draw conclusions and make
decisions. Most artificial intelligence systems have the ability to learn,
which allows people to improve their performance over time. The recent research
on AI tools, including machine learning, deep learning and predictive analysis
intended toward increasing the planning, learning, reasoning, thinking and
action taking ability. Based on which, the proposed research intends towards
exploring on how the human intelligence differs from the artificial
intelligence. Moreover, we critically analyze what AI of today is capable of
doing, why it still cannot reach human intelligence and what are the open
challenges existing in front of AI to reach and outperform human level of
intelligence. Furthermore, it will explore the future predictions for
artificial intelligence and based on which potential solution will be
recommended to solve it within next decades.",2022-11-13 16:00:58
XXX,journalArticle,2018,"Weiran Shen, Pingzhong Tang, Song Zuo",Automated Mechanism Design via Neural Networks,,,,,http://arxiv.org/abs/1805.03382v2,"Using AI approaches to automatically design mechanisms has been a central
research mission at the interface of AI and economics [Conitzer and Sandholm,
2002]. Previous approaches that attempt to design revenue optimal auctions for
the multi-dimensional settings fall short in at least one of the three aspects:
1) representation -- search in a space that probably does not even contain the
optimal mechanism; 2) exactness -- finding a mechanism that is either not
truthful or far from optimal; 3) domain dependence -- need a different design
for different environment settings.
  To resolve the three difficulties, in this paper, we put forward -- MenuNet
-- a unified neural network based framework that automatically learns to design
revenue optimal mechanisms. Our framework consists of a mechanism network that
takes an input distribution for training and outputs a mechanism, as well as a
buyer network that takes a mechanism as input and output an action. Such a
separation in design mitigates the difficulty to impose incentive compatibility
constraints on the mechanism, by making it a rational choice of the buyer. As a
result, our framework easily overcomes the previously mentioned difficulty in
incorporating IC constraints and always returns exactly incentive compatible
mechanisms.
  We then apply our framework to a number of multi-item revenue optimal design
settings, for a few of which the theoretically optimal mechanisms are unknown.
We then go on to theoretically prove that the mechanisms found by our framework
are indeed optimal.
  To the best of our knowledge, we are the first to apply neural networks to
discover optimal auction mechanisms with provable optimality.",2022-11-13 16:00:58
XXX,journalArticle,2018,"Fernando Martínez-Plumed, Shahar Avin, Miles Brundage, Allan Dafoe, Sean Ó hÉigeartaigh, José Hernández-Orallo",Between Progress and Potential Impact of AI: the Neglected Dimensions,,,,,http://arxiv.org/abs/1806.00610v2,"We reframe the analysis of progress in AI by incorporating into an overall
framework both the task performance of a system, and the time and resource
costs incurred in the development and deployment of the system. These costs
include: data, expert knowledge, human oversight, software resources, computing
cycles, hardware and network facilities, and (what kind of) time. These costs
are distributed over the life cycle of the system, and may place differing
demands on different developers and users. The multidimensional performance and
cost space we present can be collapsed to a single utility metric that measures
the value of the system for different stakeholders. Even without a single
utility function, AI advances can be generically assessed by whether they
expand the Pareto surface. We label these types of costs as neglected
dimensions of AI progress, and explore them using four case studies: Alpha*
(Go, Chess, and other board games), ALE (Atari games), ImageNet (Image
classification) and Virtual Personal Assistants (Siri, Alexa, Cortana, and
Google Assistant). This broader model of progress in AI will lead to novel ways
of estimating the potential societal use and impact of an AI system, and the
establishment of milestones for future progress.",2022-11-13 16:00:59
XXX,journalArticle,2018,"Oshani Seneviratne, Sabbir M. Rashid, Shruthi Chari, James P. McCusker, Kristin P. Bennett, James A. Hendler, Deborah L. McGuinness",Knowledge Integration for Disease Characterization: A Breast Cancer Example,,,,,http://arxiv.org/abs/1807.07991v1,"With the rapid advancements in cancer research, the information that is
useful for characterizing disease, staging tumors, and creating treatment and
survivorship plans has been changing at a pace that creates challenges when
physicians try to remain current. One example involves increasing usage of
biomarkers when characterizing the pathologic prognostic stage of a breast
tumor. We present our semantic technology approach to support cancer
characterization and demonstrate it in our end-to-end prototype system that
collects the newest breast cancer staging criteria from authoritative oncology
manuals to construct an ontology for breast cancer. Using a tool we developed
that utilizes this ontology, physician-facing applications can be used to
quickly stage a new patient to support identifying risks, treatment options,
and monitoring plans based on authoritative and best practice guidelines.
Physicians can also re-stage existing patients or patient populations, allowing
them to find patients whose stage has changed in a given patient cohort. As new
guidelines emerge, using our proposed mechanism, which is grounded by semantic
technologies for ingesting new data from staging manuals, we have created an
enriched cancer staging ontology that integrates relevant data from several
sources with very little human intervention.",2022-11-13 16:01:00
XXX,journalArticle,2018,"Chia-Chun Hung, Timothy Lillicrap, Josh Abramson, Yan Wu, Mehdi Mirza, Federico Carnevale, Arun Ahuja, Greg Wayne",Optimizing Agent Behavior over Long Time Scales by Transporting Value,,,,,http://arxiv.org/abs/1810.06721v2,"Humans spend a remarkable fraction of waking life engaged in acts of ""mental
time travel"". We dwell on our actions in the past and experience satisfaction
or regret. More than merely autobiographical storytelling, we use these event
recollections to change how we will act in similar scenarios in the future.
This process endows us with a computationally important ability to link actions
and consequences across long spans of time, which figures prominently in
addressing the problem of long-term temporal credit assignment; in artificial
intelligence (AI) this is the question of how to evaluate the utility of the
actions within a long-duration behavioral sequence leading to success or
failure in a task. Existing approaches to shorter-term credit assignment in AI
cannot solve tasks with long delays between actions and consequences. Here, we
introduce a new paradigm for reinforcement learning where agents use recall of
specific memories to credit actions from the past, allowing them to solve
problems that are intractable for existing algorithms. This paradigm broadens
the scope of problems that can be investigated in AI and offers a mechanistic
account of behaviors that may inspire computational models in neuroscience,
psychology, and behavioral economics.",2022-11-13 16:01:00
XXX,journalArticle,2018,"Francesca Rossi, Nicholas Mattei",Building Ethically Bounded AI,,,,,http://arxiv.org/abs/1812.03980v1,"The more AI agents are deployed in scenarios with possibly unexpected
situations, the more they need to be flexible, adaptive, and creative in
achieving the goal we have given them. Thus, a certain level of freedom to
choose the best path to the goal is inherent in making AI robust and flexible
enough. At the same time, however, the pervasive deployment of AI in our life,
whether AI is autonomous or collaborating with humans, raises several ethical
challenges. AI agents should be aware and follow appropriate ethical principles
and should thus exhibit properties such as fairness or other virtues. These
ethical principles should define the boundaries of AI's freedom and creativity.
However, it is still a challenge to understand how to specify and reason with
ethical boundaries in AI agents and how to combine them appropriately with
subjective preferences and goal specifications. Some initial attempts employ
either a data-driven example-based approach for both, or a symbolic rule-based
approach for both. We envision a modular approach where any AI technique can be
used for any of these essential ingredients in decision making or decision
support systems, paired with a contextual approach to define their combination
and relative weight. In a world where neither humans nor AI systems work in
isolation, but are tightly interconnected, e.g., the Internet of Things, we
also envision a compositional approach to building ethically bounded AI, where
the ethical properties of each component can be fruitfully exploited to derive
those of the overall system. In this paper we define and motivate the notion of
ethically-bounded AI, we describe two concrete examples, and we outline some
outstanding challenges.",2022-11-13 16:01:01
XXX,journalArticle,2018,Peter Eckersley,Impossibility and Uncertainty Theorems in AI Value Alignment (or why your AGI should not have a utility function),,,,,http://arxiv.org/abs/1901.00064v3,"Utility functions or their equivalents (value functions, objective functions,
loss functions, reward functions, preference orderings) are a central tool in
most current machine learning systems. These mechanisms for defining goals and
guiding optimization run into practical and conceptual difficulty when there
are independent, multi-dimensional objectives that need to be pursued
simultaneously and cannot be reduced to each other. Ethicists have proved
several impossibility theorems that stem from this origin; those results appear
to show that there is no way of formally specifying what it means for an
outcome to be good for a population without violating strong human ethical
intuitions (in such cases, the objective function is a social welfare
function). We argue that this is a practical problem for any machine learning
system (such as medical decision support systems or autonomous weapons) or
rigidly rule-based bureaucracy that will make high stakes decisions about human
lives: such systems should not use objective functions in the strict
mathematical sense.
  We explore the alternative of using uncertain objectives, represented for
instance as partially ordered preferences, or as probability distributions over
total orders. We show that previously known impossibility theorems can be
transformed into uncertainty theorems in both of those settings, and prove
lower bounds on how much uncertainty is implied by the impossibility results.
We close by proposing two conjectures about the relationship between
uncertainty in objectives and severe unintended consequences from AI systems.",2022-11-13 16:01:01
XXX,journalArticle,2019,"Brian Lubars, Chenhao Tan","Ask Not What AI Can Do, But What AI Should Do: Towards a Framework of Task Delegability",,,,,http://arxiv.org/abs/1902.03245v2,"While artificial intelligence (AI) holds promise for addressing societal
challenges, issues of exactly which tasks to automate and to what extent to do
so remain understudied. We approach this problem of task delegability from a
human-centered perspective by developing a framework on human perception of
task delegation to AI. We consider four high-level factors that can contribute
to a delegation decision: motivation, difficulty, risk, and trust. To obtain an
empirical understanding of human preferences in different tasks, we build a
dataset of 100 tasks from academic papers, popular media portrayal of AI, and
everyday life, and administer a survey based on our proposed framework. We find
little preference for full AI control and a strong preference for
machine-in-the-loop designs, in which humans play the leading role. Among the
four factors, trust is the most correlated with human preferences of optimal
human-machine delegation. This framework represents a first step towards
characterizing human preferences of AI automation across tasks. We hope this
work encourages future efforts towards understanding such individual attitudes;
our goal is to inform the public and the AI research community rather than
dictating any direction in technology development.",2022-11-13 16:01:02
XXX,journalArticle,2019,"Christoph Benzmüller, Xavier Parent, Leendert van der Torre","Designing Normative Theories for Ethical and Legal Reasoning: LogiKEy Framework, Methodology, and Tool Support",,,,,http://arxiv.org/abs/1903.10187v6,"A framework and methodology---termed LogiKEy---for the design and engineering
of ethical reasoners, normative theories and deontic logics is presented. The
overall motivation is the development of suitable means for the control and
governance of intelligent autonomous systems. LogiKEy's unifying formal
framework is based on semantical embeddings of deontic logics, logic
combinations and ethico-legal domain theories in expressive classic
higher-order logic (HOL). This meta-logical approach enables the provision of
powerful tool support in LogiKEy: off-the-shelf theorem provers and model
finders for HOL are assisting the LogiKEy designer of ethical intelligent
agents to flexibly experiment with underlying logics and their combinations,
with ethico-legal domain theories, and with concrete examples---all at the same
time. Continuous improvements of these off-the-shelf provers, without further
ado, leverage the reasoning performance in LogiKEy. Case studies, in which the
LogiKEy framework and methodology has been applied and tested, give evidence
that HOL's undecidability often does not hinder efficient experimentation.",2022-11-13 16:01:03
XXX,journalArticle,2019,"Patrick Rodler, Dietmar Jannach, Konstantin Schekotihin, Philipp Fleiss",Are Query-Based Ontology Debuggers Really Helping Knowledge Engineers?,,,,,http://arxiv.org/abs/1904.01484v2,"Real-world semantic or knowledge-based systems, e.g., in the biomedical
domain, can become large and complex. Tool support for the localization and
repair of faults within knowledge bases of such systems can therefore be
essential for their practical success. Correspondingly, a number of knowledge
base debugging approaches, in particular for ontology-based systems, were
proposed throughout recent years. Query-based debugging is a comparably recent
interactive approach that localizes the true cause of an observed problem by
asking knowledge engineers a series of questions. Concrete implementations of
this approach exist, such as the OntoDebug plug-in for the ontology editor
Prot\'eg\'e.
  To validate that a newly proposed method is favorable over an existing one,
researchers often rely on simulation-based comparisons. Such an evaluation
approach however has certain limitations and often cannot fully inform us about
a method's true usefulness. We therefore conducted different user studies to
assess the practical value of query-based ontology debugging. One main insight
from the studies is that the considered interactive approach is indeed more
efficient than an alternative algorithmic debugging based on test cases. We
also observed that users frequently made errors in the process, which
highlights the importance of a careful design of the queries that users need to
answer.",2022-11-13 16:01:03
XXX,journalArticle,2019,"Julian Stier, Gabriele Gianini, Michael Granitzer, Konstantin Ziegler",Analysing Neural Network Topologies: a Game Theoretic Approach,Procedia Computer Science 126 (2018): 234-243,,,10.1016/j.procs.2018.07.257,http://arxiv.org/abs/1904.08166v1,"Artificial Neural Networks have shown impressive success in very different
application cases. Choosing a proper network architecture is a critical
decision for a network's success, usually done in a manual manner. As a
straightforward strategy, large, mostly fully connected architectures are
selected, thereby relying on a good optimization strategy to find proper
weights while at the same time avoiding overfitting. However, large parts of
the final network are redundant. In the best case, large parts of the network
become simply irrelevant for later inferencing. In the worst case, highly
parameterized architectures hinder proper optimization and allow the easy
creation of adverserial examples fooling the network. A first step in removing
irrelevant architectural parts lies in identifying those parts, which requires
measuring the contribution of individual components such as neurons. In
previous work, heuristics based on using the weight distribution of a neuron as
contribution measure have shown some success, but do not provide a proper
theoretical understanding. Therefore, in our work we investigate game theoretic
measures, namely the Shapley value (SV), in order to separate relevant from
irrelevant parts of an artificial neural network. We begin by designing a
coalitional game for an artificial neural network, where neurons form
coalitions and the average contributions of neurons to coalitions yield to the
Shapley value. In order to measure how well the Shapley value measures the
contribution of individual neurons, we remove low-contributing neurons and
measure its impact on the network performance. In our experiments we show that
the Shapley value outperforms other heuristics for measuring the contribution
of neurons.",2022-11-13 16:01:04
XXX,journalArticle,2019,"Ramon Fraga Pereira, Nir Oren, Felipe Meneguzzi",Using Sub-Optimal Plan Detection to Identify Commitment Abandonment in Discrete Environments,,,,,http://arxiv.org/abs/1904.11737v2,"Assessing whether an agent has abandoned a goal or is actively pursuing it is
important when multiple agents are trying to achieve joint goals, or when
agents commit to achieving goals for each other. Making such a determination
for a single goal by observing only plan traces is not trivial as agents often
deviate from optimal plans for various reasons, including the pursuit of
multiple goals or the inability to act optimally. In this article, we develop
an approach based on domain independent heuristics from automated planning,
landmarks, and fact partitions to identify sub-optimal action steps - with
respect to a plan - within a plan execution trace. Such capability is very
important in domains where multiple agents cooperate and delegate tasks among
themselves, e.g. through social commitments, and need to ensure that a
delegating agent can infer whether or not another agent is actually progressing
towards a delegated task. We demonstrate how an agent can use our technique to
determine - by observing a trace - whether an agent is honouring a commitment.
We empirically show, for a number of representative domains, that our approach
infers sub-optimal action steps with very high accuracy and detects commitment
abandonment in nearly all cases.",2022-11-13 16:01:05
XXX,journalArticle,2019,"Rocio Gomez, Mohan Sridharan, Heather Riley",Towards a Theory of Intentions for Human-Robot Collaboration,,,,,http://arxiv.org/abs/1907.13275v1,"The architecture described in this paper encodes a theory of intentions based
on the the key principles of non-procrastination, persistence, and
automatically limiting reasoning to relevant knowledge and observations. The
architecture reasons with transition diagrams of any given domain at two
different resolutions, with the fine-resolution description defined as a
refinement of, and hence tightly-coupled to, a coarse-resolution description.
Non-monotonic logical reasoning with the coarse-resolution description computes
an activity (i.e., plan) comprising abstract actions for any given goal. Each
abstract action is implemented as a sequence of concrete actions by
automatically zooming to and reasoning with the part of the fine-resolution
transition diagram relevant to the current coarse-resolution transition and the
goal. Each concrete action in this sequence is executed using probabilistic
models of the uncertainty in sensing and actuation, and the corresponding
fine-resolution outcomes are used to infer coarse-resolution observations that
are added to the coarse-resolution history. The architecture's capabilities are
evaluated in the context of a simulated robot assisting humans in an office
domain, on a physical robot (Baxter) manipulating tabletop objects, and on a
wheeled robot (Turtlebot) moving objects to particular places or people. The
experimental results indicate improvements in reliability and computational
efficiency compared with an architecture that does not include the theory of
intentions, and an architecture that does not include zooming for
fine-resolution reasoning.",2022-11-13 16:01:05
XXX,journalArticle,2019,"Dung T. Phan, Radu Grosu, Nils Jansen, Nicola Paoletti, Scott A. Smolka, Scott D. Stoller",Neural Simplex Architecture,,,,,http://arxiv.org/abs/1908.00528v2,"We present the Neural Simplex Architecture (NSA), a new approach to runtime
assurance that provides safety guarantees for neural controllers (obtained e.g.
using reinforcement learning) of autonomous and other complex systems without
unduly sacrificing performance. NSA is inspired by the Simplex control
architecture of Sha et al., but with some significant differences. In the
traditional approach, the advanced controller (AC) is treated as a black box;
when the decision module switches control to the baseline controller (BC), the
BC remains in control forever. There is relatively little work on switching
control back to the AC, and there are no techniques for correcting the AC's
behavior after it generates a potentially unsafe control input that causes a
failover to the BC. Our NSA addresses both of these limitations. NSA not only
provides safety assurances in the presence of a possibly unsafe neural
controller, but can also improve the safety of such a controller in an online
setting via retraining, without overly degrading its performance. To
demonstrate NSA's benefits, we have conducted several significant case studies
in the continuous control domain. These include a target-seeking ground rover
navigating an obstacle field, and a neural controller for an artificial
pancreas system.",2022-11-13 16:01:06
XXX,journalArticle,2019,"Alexander Matt Turner, Logan Smith, Rohin Shah, Andrew Critch, Prasad Tadepalli",Optimal Policies Tend to Seek Power,,,,,http://arxiv.org/abs/1912.01683v9,"Some researchers speculate that intelligent reinforcement learning (RL)
agents would be incentivized to seek resources and power in pursuit of their
objectives. Other researchers point out that RL agents need not have human-like
power-seeking instincts. To clarify this discussion, we develop the first
formal theory of the statistical tendencies of optimal policies. In the context
of Markov decision processes, we prove that certain environmental symmetries
are sufficient for optimal policies to tend to seek power over the environment.
These symmetries exist in many environments in which the agent can be shut down
or destroyed. We prove that in these environments, most reward functions make
it optimal to seek power by keeping a range of options available and, when
maximizing average reward, by navigating towards larger sets of potential
terminal states.",2022-11-13 16:01:07
XXX,journalArticle,2019,"Thomas Chatain, Mathilde Boltenhagen, Josep Carmona",Anti-Alignments -- Measuring The Precision of Process Models and Event Logs,,,,,http://arxiv.org/abs/1912.05907v1,"Processes are a crucial artefact in organizations, since they coordinate the
execution of activities so that products and services are provided. The use of
models to analyse the underlying processes is a well-known practice. However,
due to the complexity and continuous evolution of their processes,
organizations need an effective way of analysing the relation between processes
and models. Conformance checking techniques asses the suitability of a process
model in representing an underlying process, observed through a collection of
real executions. One important metric in conformance checking is to asses the
precision of the model with respect to the observed executions, i.e.,
characterize the ability of the model to produce behavior unrelated to the one
observed. In this paper we present the notion of anti-alignment as a concept to
help unveiling runs in the model that may deviate significantly from the
observed behavior. Using anti-alignments, a new metric for precision is
proposed. In contrast to existing metrics, anti-alignment based precision
metrics satisfy most of the required axioms highlighted in a recent
publication. Moreover, a complexity analysis of the problem of computing
anti-alignments is provided, which sheds light into the practicability of using
anti-alignment to estimate precision. Experiments are provided that witness the
validity of the concepts introduced in this paper.",2022-11-13 16:01:08
XXX,journalArticle,2020,"Min Chen, Mateu Sbert, Alfie Abdul-Rahman, Deborah Silver",A Bounded Measure for Estimating the Benefit of Visualization,"Entropy, 24(2), 228, 2022",,,10.3390/e24020228,http://arxiv.org/abs/2002.05282v2,"Information theory can be used to analyze the cost-benefit of visualization
processes. However, the current measure of benefit contains an unbounded term
that is neither easy to estimate nor intuitive to interpret. In this work, we
propose to revise the existing cost-benefit measure by replacing the unbounded
term with a bounded one. We examine a number of bounded measures that include
the Jenson-Shannon divergence and a new divergence measure formulated as part
of this work. We use visual analysis to support the multi-criteria comparison,
narrowing the search down to those options with better mathematical properties.
We apply those remaining options to two visualization case studies to
instantiate their uses in practical scenarios, while the collected real world
data further informs the selection of a bounded measure, which can be used to
estimate the benefit of visualization.",2022-11-13 16:01:08
XXX,journalArticle,2020,"Emile van Krieken, Erman Acar, Frank van Harmelen",Analyzing Differentiable Fuzzy Logic Operators,,,,10.1016/j.artint.2021.103602,http://arxiv.org/abs/2002.06100v2,"The AI community is increasingly putting its attention towards combining
symbolic and neural approaches, as it is often argued that the strengths and
weaknesses of these approaches are complementary. One recent trend in the
literature are weakly supervised learning techniques that employ operators from
fuzzy logics. In particular, these use prior background knowledge described in
such logics to help the training of a neural network from unlabeled and noisy
data. By interpreting logical symbols using neural networks, this background
knowledge can be added to regular loss functions, hence making reasoning a part
of learning. We study, both formally and empirically, how a large collection of
logical operators from the fuzzy logic literature behave in a differentiable
learning setting. We find that many of these operators, including some of the
most well-known, are highly unsuitable in this setting. A further finding
concerns the treatment of implication in these fuzzy logics, and shows a strong
imbalance between gradients driven by the antecedent and the consequent of the
implication. Furthermore, we introduce a new family of fuzzy implications
(called sigmoidal implications) to tackle this phenomenon. Finally, we
empirically show that it is possible to use Differentiable Fuzzy Logics for
semi-supervised learning, and compare how different operators behave in
practice. We find that, to achieve the largest performance improvement over a
supervised baseline, we have to resort to non-standard combinations of logical
operators which perform well in learning, but no longer satisfy the usual
logical laws.",2022-11-13 16:01:09
XXX,journalArticle,2020,Philip Paquette,A Road Map to Strong Intelligence,,,,,http://arxiv.org/abs/2002.09044v1,"I wrote this paper because technology can really improve people's lives. With
it, we can live longer in a healthy body, save time through increased
efficiency and automation, and make better decisions. To get to the next level,
we need to start looking at intelligence from a much broader perspective, and
promote international interdisciplinary collaborations. Section 1 of this paper
delves into sociology and social psychology to explain that the mechanisms
underlying intelligence are inherently social. Section 2 proposes a method to
classify intelligence, and describes the differences between weak and strong
intelligence. Section 3 examines the Chinese Room argument from a different
perspective. It demonstrates that a Turing-complete machine cannot have strong
intelligence, and considers the modifications necessary for a computer to be
intelligent and have understanding. Section 4 argues that the existential risk
caused by the technological explosion of a single agent should not be of
serious concern. Section 5 looks at the AI control problem and argues that it
is impossible to build a super-intelligent machine that will do what it
creators want. By using insights from biology, it also proposes a solution to
the control problem. Section 6 discusses some of the implications of strong
intelligence. Section 7 lists the main challenges with deep learning, and
asserts that radical changes will be required to reach strong intelligence.
Section 8 examines a neuroscience framework that could help explain how a
cortical column works. Section 9 lays out the broad strokes of a road map
towards strong intelligence. Finally, section 10 analyzes the impacts and the
challenges of greater intelligence.",2022-11-13 16:01:10
XXX,journalArticle,2020,Andrés Páez,The Pragmatic Turn in Explainable Artificial Intelligence (XAI),"Minds and Machines, 29(3), 441-459, 2019",,,10.1007/s11023-019-09502-w,http://arxiv.org/abs/2002.09595v1,"In this paper I argue that the search for explainable models and
interpretable decisions in AI must be reformulated in terms of the broader
project of offering a pragmatic and naturalistic account of understanding in
AI. Intuitively, the purpose of providing an explanation of a model or a
decision is to make it understandable to its stakeholders. But without a
previous grasp of what it means to say that an agent understands a model or a
decision, the explanatory strategies will lack a well-defined goal. Aside from
providing a clearer objective for XAI, focusing on understanding also allows us
to relax the factivity condition on explanation, which is impossible to fulfill
in many machine learning models, and to focus instead on the pragmatic
conditions that determine the best fit between a model and the methods and
devices deployed to understand it. After an examination of the different types
of understanding discussed in the philosophical and psychological literature, I
conclude that interpretative or approximation models not only provide the best
way to achieve the objectual understanding of a machine learning model, but are
also a necessary condition to achieve post-hoc interpretability. This
conclusion is partly based on the shortcomings of the purely functionalist
approach to post-hoc interpretability that seems to be predominant in most
recent literature.",2022-11-13 16:01:10
XXX,journalArticle,2020,"Jianyu Su, Stephen Adams, Peter A. Beling",Counterfactual Multi-Agent Reinforcement Learning with Graph Convolution Communication,,,,,http://arxiv.org/abs/2004.00470v2,"We consider a fully cooperative multi-agent system where agents cooperate to
maximize a system's utility in a partial-observable environment. We propose
that multi-agent systems must have the ability to (1) communicate and
understand the inter-plays between agents and (2) correctly distribute rewards
based on an individual agent's contribution. In contrast, most work in this
setting considers only one of the above abilities. In this study, we develop an
architecture that allows for communication among agents and tailors the
system's reward for each individual agent. Our architecture represents agent
communication through graph convolution and applies an existing credit
assignment structure, counterfactual multi-agent policy gradient (COMA), to
assist agents to learn communication by back-propagation. The flexibility of
the graph structure enables our method to be applicable to a variety of
multi-agent systems, e.g. dynamic systems that consist of varying numbers of
agents and static systems with a fixed number of agents. We evaluate our method
on a range of tasks, demonstrating the advantage of marrying communication with
credit assignment. In the experiments, our proposed method yields better
performance than the state-of-art methods, including COMA. Moreover, we show
that the communication strategies offers us insights and interpretability of
the system's cooperative policies.",2022-11-13 16:01:11
XXX,journalArticle,2020,"Gagan Bansal, Besmira Nushi, Ece Kamar, Eric Horvitz, Daniel S. Weld",Is the Most Accurate AI the Best Teammate? Optimizing AI for Teamwork,,,,,http://arxiv.org/abs/2004.13102v3,"AI practitioners typically strive to develop the most accurate systems,
making an implicit assumption that the AI system will function autonomously.
However, in practice, AI systems often are used to provide advice to people in
domains ranging from criminal justice and finance to healthcare. In such
AI-advised decision making, humans and machines form a team, where the human is
responsible for making final decisions. But is the most accurate AI the best
teammate? We argue ""No"" -- predictable performance may be worth a slight
sacrifice in AI accuracy. Instead, we argue that AI systems should be trained
in a human-centered manner, directly optimized for team performance. We study
this proposal for a specific type of human-AI teaming, where the human overseer
chooses to either accept the AI recommendation or solve the task themselves. To
optimize the team performance for this setting we maximize the team's expected
utility, expressed in terms of the quality of the final decision, cost of
verifying, and individual accuracies of people and machines. Our experiments
with linear and non-linear models on real-world, high-stakes datasets show that
the most accuracy AI may not lead to highest team performance and show the
benefit of modeling teamwork during training through improvements in expected
team utility across datasets, considering parameters such as human skill and
the cost of mistakes. We discuss the shortcoming of current optimization
approaches beyond well-studied loss functions such as log-loss, and encourage
future work on AI optimization problems motivated by human-AI collaboration.",2022-11-13 16:01:11
XXX,journalArticle,2020,"Eric MSP Veith, Nils Wenninghoff, Emilie Frost","The Adversarial Resilience Learning Architecture for AI-based Modelling, Exploration, and Operation of Complex Cyber-Physical Systems",,,,,http://arxiv.org/abs/2005.13601v1,"Modern algorithms in the domain of Deep Reinforcement Learning (DRL)
demonstrated remarkable successes; most widely known are those in game-based
scenarios, from ATARI video games to Go and the StarCraft~\textsc{II} real-time
strategy game. However, applications in the domain of modern Cyber-Physical
Systems (CPS) that take advantage a vast variety of DRL algorithms are few. We
assume that the benefits would be considerable: Modern CPS have become
increasingly complex and evolved beyond traditional methods of modelling and
analysis. At the same time, these CPS are confronted with an increasing amount
of stochastic inputs, from volatile energy sources in power grids to broad user
participation stemming from markets. Approaches of system modelling that use
techniques from the domain of Artificial Intelligence (AI) do not focus on
analysis and operation. In this paper, we describe the concept of Adversarial
Resilience Learning (ARL) that formulates a new approach to complex environment
checking and resilient operation: It defines two agent classes, attacker and
defender agents. The quintessence of ARL lies in both agents exploring the
system and training each other without any domain knowledge. Here, we introduce
the ARL software architecture that allows to use a wide range of model-free as
well as model-based DRL-based algorithms, and document results of concrete
experiment runs on a complex power grid.",2022-11-13 16:01:12
XXX,journalArticle,2020,"Michael K. Cohen, Marcus Hutter",Pessimism About Unknown Unknowns Inspires Conservatism,,,,,http://arxiv.org/abs/2006.08753v1,"If we could define the set of all bad outcomes, we could hard-code an agent
which avoids them; however, in sufficiently complex environments, this is
infeasible. We do not know of any general-purpose approaches in the literature
to avoiding novel failure modes. Motivated by this, we define an idealized
Bayesian reinforcement learner which follows a policy that maximizes the
worst-case expected reward over a set of world-models. We call this agent
pessimistic, since it optimizes assuming the worst case. A scalar parameter
tunes the agent's pessimism by changing the size of the set of world-models
taken into account. Our first main contribution is: given an assumption about
the agent's model class, a sufficiently pessimistic agent does not cause
""unprecedented events"" with probability $1-\delta$, whether or not designers
know how to precisely specify those precedents they are concerned with. Since
pessimism discourages exploration, at each timestep, the agent may defer to a
mentor, who may be a human or some known-safe policy we would like to improve.
Our other main contribution is that the agent's policy's value approaches at
least that of the mentor, while the probability of deferring to the mentor goes
to 0. In high-stakes environments, we might like advanced artificial agents to
pursue goals cautiously, which is a non-trivial problem even if the agent were
allowed arbitrary computing power; we present a formal solution.",2022-11-13 16:01:12
XXX,journalArticle,2020,"Yuqing Du, Stas Tiomkin, Emre Kiciman, Daniel Polani, Pieter Abbeel, Anca Dragan",AvE: Assistance via Empowerment,,,,,http://arxiv.org/abs/2006.14796v5,"One difficulty in using artificial agents for human-assistive applications
lies in the challenge of accurately assisting with a person's goal(s). Existing
methods tend to rely on inferring the human's goal, which is challenging when
there are many potential goals or when the set of candidate goals is difficult
to identify. We propose a new paradigm for assistance by instead increasing the
human's ability to control their environment, and formalize this approach by
augmenting reinforcement learning with human empowerment. This task-agnostic
objective preserves the person's autonomy and ability to achieve any eventual
state. We test our approach against assistance based on goal inference,
highlighting scenarios where our method overcomes failure modes stemming from
goal ambiguity or misspecification. As existing methods for estimating
empowerment in continuous domains are computationally hard, precluding its use
in real time learned assistance, we also propose an efficient
empowerment-inspired proxy metric. Using this, we are able to successfully
demonstrate our method in a shared autonomy user study for a challenging
simulated teleoperation task with human-in-the-loop training.",2022-11-13 16:01:13
XXX,journalArticle,2020,"Vedant Nanda, Till Speicher, John P. Dickerson, Krishna P. Gummadi, Muhammad Bilal Zafar",Unifying Model Explainability and Robustness via Machine-Checkable Concepts,,,,,http://arxiv.org/abs/2007.00251v2,"As deep neural networks (DNNs) get adopted in an ever-increasing number of
applications, explainability has emerged as a crucial desideratum for these
models. In many real-world tasks, one of the principal reasons for requiring
explainability is to in turn assess prediction robustness, where predictions
(i.e., class labels) that do not conform to their respective explanations
(e.g., presence or absence of a concept in the input) are deemed to be
unreliable. However, most, if not all, prior methods for checking
explanation-conformity (e.g., LIME, TCAV, saliency maps) require significant
manual intervention, which hinders their large-scale deployability. In this
paper, we propose a robustness-assessment framework, at the core of which is
the idea of using machine-checkable concepts. Our framework defines a large
number of concepts that the DNN explanations could be based on and performs the
explanation-conformity check at test time to assess prediction robustness. Both
steps are executed in an automated manner without requiring any human
intervention and are easily scaled to datasets with a very large number of
classes. Experiments on real-world datasets and human surveys show that our
framework is able to enhance prediction robustness significantly: the
predictions marked to be robust by our framework have significantly higher
accuracy and are more robust to adversarial perturbations.",2022-11-13 16:01:13
XXX,journalArticle,2020,"Connor Basich, Justin Svegliato, Kyle Hollins Wray, Stefan J. Witwicki, Shlomo Zilberstein",Improving Competence for Reliable Autonomy,"EPTCS 319, 2020, pp. 37-53",,,10.4204/EPTCS.319.4,http://arxiv.org/abs/2007.11740v1,"Given the complexity of real-world, unstructured domains, it is often
impossible or impractical to design models that include every feature needed to
handle all possible scenarios that an autonomous system may encounter. For an
autonomous system to be reliable in such domains, it should have the ability to
improve its competence online. In this paper, we propose a method for improving
the competence of a system over the course of its deployment. We specifically
focus on a class of semi-autonomous systems known as competence-aware systems
that model their own competence -- the optimal extent of autonomy to use in any
given situation -- and learn this competence over time from feedback received
through interactions with a human authority. Our method exploits such feedback
to identify important state features missing from the system's initial model,
and incorporates them into its state representation. The result is an agent
that better predicts human involvement, leading to improvements in its
competence and reliability, and as a result, its overall performance.",2022-11-13 16:01:14
XXX,journalArticle,2020,"Mariela Morveli-Espinoza, Juan Carlos Nieves, Ayslan Possebom, Josep Puyol-Gruart, Cesar Augusto Tacla",An Argumentation-based Approach for Identifying and Dealing with Incompatibilities among Procedural Goals,"International Journal of Approximate Reasoning, year 2019, vol.
  105, pp. 1-26",,,10.4114/intartif.vol22iss64pp47-62,http://arxiv.org/abs/2009.05186v1,"During the first step of practical reasoning, i.e. deliberation, an
intelligent agent generates a set of pursuable goals and then selects which of
them he commits to achieve. An intelligent agent may in general generate
multiple pursuable goals, which may be incompatible among them. In this paper,
we focus on the definition, identification and resolution of these
incompatibilities. The suggested approach considers the three forms of
incompatibility introduced by Castelfranchi and Paglieri, namely the terminal
incompatibility, the instrumental or resources incompatibility and the
superfluity. We characterise computationally these forms of incompatibility by
means of arguments that represent the plans that allow an agent to achieve his
goals. Thus, the incompatibility among goals is defined based on the conflicts
among their plans, which are represented by means of attacks in an
argumentation framework. We also work on the problem of goals selection; we
propose to use abstract argumentation theory to deal with this problem, i.e. by
applying argumentation semantics. We use a modified version of the ""cleaner
world"" scenario in order to illustrate the performance of our proposal.",2022-11-13 16:01:14
XXX,journalArticle,2020,"Lun Ai, Stephen H. Muggleton, Céline Hocquette, Mark Gromowski, Ute Schmid",Beneficial and Harmful Explanatory Machine Learning,,,,,http://arxiv.org/abs/2009.06410v2,"Given the recent successes of Deep Learning in AI there has been increased
interest in the role and need for explanations in machine learned theories. A
distinct notion in this context is that of Michie's definition of Ultra-Strong
Machine Learning (USML). USML is demonstrated by a measurable increase in human
performance of a task following provision to the human of a symbolic machine
learned theory for task performance. A recent paper demonstrates the beneficial
effect of a machine learned logic theory for a classification task, yet no
existing work to our knowledge has examined the potential harmfulness of
machine's involvement for human comprehension during learning. This paper
investigates the explanatory effects of a machine learned theory in the context
of simple two person games and proposes a framework for identifying the
harmfulness of machine explanations based on the Cognitive Science literature.
The approach involves a cognitive window consisting of two quantifiable bounds
and it is supported by empirical evidence collected from human trials. Our
quantitative and qualitative results indicate that human learning aided by a
symbolic machine learned theory which satisfies a cognitive window has achieved
significantly higher performance than human self learning. Results also
demonstrate that human learning aided by a symbolic machine learned theory that
fails to satisfy this window leads to significantly worse performance than
unaided human learning.",2022-11-13 16:01:15
XXX,journalArticle,2020,"Theodore R. Sumers, Mark K. Ho, Robert D. Hawkins, Karthik Narasimhan, Thomas L. Griffiths",Learning Rewards from Linguistic Feedback,,,,,http://arxiv.org/abs/2009.14715v3,"We explore unconstrained natural language feedback as a learning signal for
artificial agents. Humans use rich and varied language to teach, yet most prior
work on interactive learning from language assumes a particular form of input
(e.g., commands). We propose a general framework which does not make this
assumption, using aspect-based sentiment analysis to decompose feedback into
sentiment about the features of a Markov decision process. We then perform an
analogue of inverse reinforcement learning, regressing the sentiment on the
features to infer the teacher's latent reward function. To evaluate our
approach, we first collect a corpus of teaching behavior in a cooperative task
where both teacher and learner are human. We implement three artificial
learners: sentiment-based ""literal"" and ""pragmatic"" models, and an inference
network trained end-to-end to predict latent rewards. We then repeat our
initial experiment and pair them with human teachers. All three successfully
learn from interactive human feedback. The sentiment models outperform the
inference network, with the ""pragmatic"" model approaching human performance.
Our work thus provides insight into the information structure of naturalistic
linguistic feedback as well as methods to leverage it for reinforcement
learning.",2022-11-13 16:01:16
XXX,journalArticle,2020,"The Anh Han, Luis Moniz Pereira, Tom Lenaerts, Francisco C. Santos",Mediating Artificial Intelligence Developments through Negative and Positive Incentives,,,,10.1371/journal.pone.0244592,http://arxiv.org/abs/2010.00403v1,"The field of Artificial Intelligence (AI) is going through a period of great
expectations, introducing a certain level of anxiety in research, business and
also policy. This anxiety is further energised by an AI race narrative that
makes people believe they might be missing out. Whether real or not, a belief
in this narrative may be detrimental as some stake-holders will feel obliged to
cut corners on safety precautions, or ignore societal consequences just to
""win"". Starting from a baseline model that describes a broad class of
technology races where winners draw a significant benefit compared to others
(such as AI advances, patent race, pharmaceutical technologies), we investigate
here how positive (rewards) and negative (punishments) incentives may
beneficially influence the outcomes. We uncover conditions in which punishment
is either capable of reducing the development speed of unsafe participants or
has the capacity to reduce innovation through over-regulation. Alternatively,
we show that, in several scenarios, rewarding those that follow safety measures
may increase the development speed while ensuring safe choices. Moreover, in
{the latter} regimes, rewards do not suffer from the issue of over-regulation
as is the case for punishment. Overall, our findings provide valuable insights
into the nature and kinds of regulatory actions most suitable to improve safety
compliance in the contexts of both smooth and sudden technological shifts.",2022-11-13 16:01:16
XXX,journalArticle,2020,"Jakub Tětek, Marek Sklenka, Tomáš Gavenčiak",Performance of Bounded-Rational Agents With the Ability to Self-Modify,,,,,http://arxiv.org/abs/2011.06275v2,"Self-modification of agents embedded in complex environments is hard to
avoid, whether it happens via direct means (e.g. own code modification) or
indirectly (e.g. influencing the operator, exploiting bugs or the environment).
It has been argued that intelligent agents have an incentive to avoid modifying
their utility function so that their future instances work towards the same
goals.
  Everitt et al. (2016) formally show that providing an option to self-modify
is harmless for perfectly rational agents. We show that this result is no
longer true for agents with bounded rationality. In such agents,
self-modification may cause exponential deterioration in performance and
gradual misalignment of a previously aligned agent. We investigate how the size
of this effect depends on the type and magnitude of imperfections in the
agent's rationality (1-4 below). We also discuss model assumptions and the
wider problem and framing space.
  We examine four ways in which an agent can be bounded-rational: it either (1)
doesn't always choose the optimal action, (2) is not perfectly aligned with
human values, (3) has an inaccurate model of the environment, or (4) uses the
wrong temporal discounting factor. We show that while in the cases (2)-(4) the
misalignment caused by the agent's imperfection does not increase over time,
with (1) the misalignment may grow exponentially.",2022-11-13 16:01:17
XXX,journalArticle,2020,"Tianchen Zhao, Xuefei Ning, Xiangsheng Shi, Songyi Yang, Shuang Liang, Peng Lei, Jianfei Chen, Huazhong Yang, Yu Wang",BARS: Joint Search of Cell Topology and Layout for Accurate and Efficient Binary ARchitectures,,,,,http://arxiv.org/abs/2011.10804v3,"Binary Neural Networks (BNNs) have received significant attention due to
their promising efficiency. Currently, most BNN studies directly adopt
widely-used CNN architectures, which can be suboptimal for BNNs. This paper
proposes a novel Binary ARchitecture Search (BARS) flow to discover superior
binary architecture in a large design space. Specifically, we analyze the
information bottlenecks that are related to both the topology and layout
architecture design choices. And we propose to automatically search for the
optimal information flow. To achieve that, we design a two-level (Macro &
Micro) search space tailored for BNNs and apply a differentiable neural
architecture search (NAS) to explore this search space efficiently. The
macro-level search space includes width and depth decisions, which is required
for better balancing the model performance and complexity. We also design the
micro-level search space to strengthen the information flow for BNN. %A notable
challenge of BNN architecture search lies in that binary operations exacerbate
the ""collapse"" problem of differentiable NAS, for which we incorporate various
search and derive strategies to stabilize the search process. On CIFAR-10, BARS
achieves 1.5% higher accuracy with 2/3 binary operations and 1/10
floating-point operations comparing with existing BNN NAS studies. On ImageNet,
with similar resource consumption, BARS-discovered architecture achieves a 6%
accuracy gain than hand-crafted binary ResNet-18 architectures and outperforms
other binary architectures while fully binarizing the architecture backbone.",2022-11-13 16:01:17
XXX,journalArticle,2021,"Simon Zhuang, Dylan Hadfield-Menell",Consequences of Misaligned AI,NeurIPS 2020,,,,http://arxiv.org/abs/2102.03896v1,"AI systems often rely on two key components: a specified goal or reward
function and an optimization algorithm to compute the optimal behavior for that
goal. This approach is intended to provide value for a principal: the user on
whose behalf the agent acts. The objectives given to these agents often refer
to a partial specification of the principal's goals. We consider the cost of
this incompleteness by analyzing a model of a principal and an agent in a
resource constrained world where the $L$ attributes of the state correspond to
different sources of utility for the principal. We assume that the reward
function given to the agent only has support on $J < L$ attributes. The
contributions of our paper are as follows: 1) we propose a novel model of an
incomplete principal-agent problem from artificial intelligence; 2) we provide
necessary and sufficient conditions under which indefinitely optimizing for any
incomplete proxy objective leads to arbitrarily low overall utility; and 3) we
show how modifying the setup to allow reward functions that reference the full
state or allowing the principal to update the proxy objective over time can
lead to higher utility solutions. The results in this paper argue that we
should view the design of reward functions as an interactive and dynamic
process and identifies a theoretical scenario where some degree of
interactivity is desirable.",2022-11-13 16:01:17
XXX,journalArticle,2021,"Aquib Mustafa, Majid Mazouchi, Subramanya Nageshrao, Hamidreza Modares",Assured Learning-enabled Autonomy: A Metacognitive Reinforcement Learning Framework,,,,,http://arxiv.org/abs/2103.12558v2,"Reinforcement learning (RL) agents with pre-specified reward functions cannot
provide guaranteed safety across variety of circumstances that an uncertain
system might encounter. To guarantee performance while assuring satisfaction of
safety constraints across variety of circumstances, an assured autonomous
control framework is presented in this paper by empowering RL algorithms with
metacognitive learning capabilities. More specifically, adapting the reward
function parameters of the RL agent is performed in a metacognitive
decision-making layer to assure the feasibility of RL agent. That is, to assure
that the learned policy by the RL agent satisfies safety constraints specified
by signal temporal logic while achieving as much performance as possible. The
metacognitive layer monitors any possible future safety violation under the
actions of the RL agent and employs a higher-layer Bayesian RL algorithm to
proactively adapt the reward function for the lower-layer RL agent. To minimize
the higher-layer Bayesian RL intervention, a fitness function is leveraged by
the metacognitive layer as a metric to evaluate success of the lower-layer RL
agent in satisfaction of safety and liveness specifications, and the
higher-layer Bayesian RL intervenes only if there is a risk of lower-layer RL
failure. Finally, a simulation example is provided to validate the
effectiveness of the proposed approach.",2022-11-13 16:01:18
XXX,journalArticle,2021,"Tri Minh Nguyen, Thomas P Quinn, Thin Nguyen, Truyen Tran",Counterfactual Explanation with Multi-Agent Reinforcement Learning for Drug Target Prediction,,,,,http://arxiv.org/abs/2103.12983v2,"Motivation: Many high-performance DTA models have been proposed, but they are
mostly black-box and thus lack human interpretability. Explainable AI (XAI) can
make DTA models more trustworthy, and can also enable scientists to distill
biological knowledge from the models. Counterfactual explanation is one popular
approach to explaining the behaviour of a deep neural network, which works by
systematically answering the question ""How would the model output change if the
inputs were changed in this way?"". Most counterfactual explanation methods only
operate on single input data. It remains an open problem how to extend
counterfactual-based XAI methods to DTA models, which have two inputs, one for
drug and one for target, that also happen to be discrete in nature.
  Methods: We propose a multi-agent reinforcement learning framework,
Multi-Agent Counterfactual Drug target binding Affinity (MACDA), to generate
counterfactual explanations for the drug-protein complex. Our proposed
framework provides human-interpretable counterfactual instances while
optimizing both the input drug and target for counterfactual generation at the
same time.
  Results: We benchmark the proposed MACDA framework using the Davis dataset
and find that our framework produces more parsimonious explanations with no
loss in explanation validity, as measured by encoding similarity and QED. We
then present a case study involving ABL1 and Nilotinib to demonstrate how MACDA
can explain the behaviour of a DTA model in the underlying substructure
interaction between inputs in its prediction, revealing mechanisms that align
with prior domain knowledge.",2022-11-13 16:01:18
XXX,journalArticle,2021,"Abdullah Khalili, Abdelhamid Bouchachia",Toward Building Science Discovery Machines,,,,,http://arxiv.org/abs/2103.15551v7,"The dream of building machines that can do science has inspired scientists
for decades. Remarkable advances have been made recently; however, we are still
far from achieving this goal. In this paper, we focus on the scientific
discovery process where a high level of reasoning and remarkable
problem-solving ability are required. We review different machine learning
techniques used in scientific discovery with their limitations. We survey and
discuss the main principles driving the scientific discovery process. These
principles are used in different fields and by different scientists to solve
problems and discover new knowledge. We provide many examples of the use of
these principles in different fields such as physics, mathematics, and biology.
We also review AI systems that attempt to implement some of these principles.
We argue that building science discovery machines should be guided by these
principles as an alternative to the dominant approach of current AI systems
that focuses on narrow objectives. Building machines that fully incorporate
these principles in an automated way might open the doors for many
advancements.",2022-11-13 16:01:19
XXX,journalArticle,2021,"The Anh Han, Tom Lenaerts, Francisco C. Santos, Luis Moniz Pereira",Voluntary safety commitments provide an escape from over-regulation in AI development,,,,,http://arxiv.org/abs/2104.03741v1,"With the introduction of Artificial Intelligence (AI) and related
technologies in our daily lives, fear and anxiety about their misuse as well as
the hidden biases in their creation have led to a demand for regulation to
address such issues. Yet blindly regulating an innovation process that is not
well understood, may stifle this process and reduce benefits that society may
gain from the generated technology, even under the best intentions. In this
paper, starting from a baseline model that captures the fundamental dynamics of
a race for domain supremacy using AI technology, we demonstrate how socially
unwanted outcomes may be produced when sanctioning is applied unconditionally
to risk-taking, i.e. potentially unsafe, behaviours. As an alternative to
resolve the detrimental effect of over-regulation, we propose a voluntary
commitment approach wherein technologists have the freedom of choice between
independently pursuing their course of actions or establishing binding
agreements to act safely, with sanctioning of those that do not abide to what
they pledged. Overall, this work reveals for the first time how voluntary
commitments, with sanctions either by peers or an institution, leads to
socially beneficial outcomes in all scenarios envisageable in a short-term race
towards domain supremacy through AI technology. These results are directly
relevant for the design of governance and regulatory policies that aim to
ensure an ethical and responsible AI technology development process.",2022-11-13 16:01:19
XXX,journalArticle,2021,"Dominik Dellermann, Philipp Ebel, Matthias Soellner, Jan Marco Leimeister",Hybrid Intelligence,,,,10.1007/s12599-019-00595-2,http://arxiv.org/abs/2105.00691v1,"Research has a long history of discussing what is superior in predicting
certain outcomes: statistical methods or the human brain. This debate has
repeatedly been sparked off by the remarkable technological advances in the
field of artificial intelligence (AI), such as solving tasks like object and
speech recognition, achieving significant improvements in accuracy through
deep-learning algorithms (Goodfellow et al. 2016), or combining various methods
of computational intelligence, such as fuzzy logic, genetic algorithms, and
case-based reasoning (Medsker 2012). One of the implicit promises that underlie
these advancements is that machines will 1 day be capable of performing complex
tasks or may even supersede humans in performing these tasks. This triggers new
heated debates of when machines will ultimately replace humans (McAfee and
Brynjolfsson 2017). While previous research has proved that AI performs well in
some clearly defined tasks such as playing chess, playing Go or identifying
objects on images, it is doubted that the development of an artificial general
intelligence (AGI) which is able to solve multiple tasks at the same time can
be achieved in the near future (e.g., Russell and Norvig 2016). Moreover, the
use of AI to solve complex business problems in organizational contexts occurs
scarcely, and applications for AI that solve complex problems remain mainly in
laboratory settings instead of being implemented in practice. Since the road to
AGI is still a long one, we argue that the most likely paradigm for the
division of labor between humans and machines in the next decades is Hybrid
Intelligence. This concept aims at using the complementary strengths of human
intelligence and AI, so that they can perform better than each of the two could
separately (e.g., Kamar 2016).",2022-11-13 16:01:20
XXX,journalArticle,2021,Beren Millidge,Towards a Mathematical Theory of Abstraction,,,,,http://arxiv.org/abs/2106.01826v2,"While the utility of well-chosen abstractions for understanding and
predicting the behaviour of complex systems is well appreciated, precisely what
an abstraction $\textit{is}$ has so far has largely eluded mathematical
formalization. In this paper, we aim to set out a mathematical theory of
abstraction. We provide a precise characterisation of what an abstraction is
and, perhaps more importantly, suggest how abstractions can be learnt directly
from data both for static datasets and for dynamical systems. We define an
abstraction to be a small set of `summaries' of a system which can be used to
answer a set of queries about the system or its behaviour. The difference
between the ground truth behaviour of the system on the queries and the
behaviour of the system predicted only by the abstraction provides a measure of
the `leakiness' of the abstraction which can be used as a loss function to
directly learn abstractions from data. Our approach can be considered a
generalization of classical statistics where we are not interested in
reconstructing `the data' in full, but are instead only concerned with
answering a set of arbitrary queries about the data. While highly theoretical,
our results have deep implications for statistical inference and machine
learning and could be used to develop explicit methods for learning precise
kinds of abstractions directly from data.",2022-11-13 16:01:20
XXX,journalArticle,2021,"Youri Coppens, Denis Steckelmacher, Catholijn M. Jonker, Ann Nowé",Synthesising Reinforcement Learning Policies through Set-Valued Inductive Rule Learning,"Trustworthy AI - Integrating Learning, Optimization and Reasoning
  (2021), Lecture Notes in Computer Science, vol. 12641, pp. 163-179",,,10.1007/978-3-030-73959-1_15,http://arxiv.org/abs/2106.06009v1,"Today's advanced Reinforcement Learning algorithms produce black-box
policies, that are often difficult to interpret and trust for a person. We
introduce a policy distilling algorithm, building on the CN2 rule mining
algorithm, that distills the policy into a rule-based decision system. At the
core of our approach is the fact that an RL process does not just learn a
policy, a mapping from states to actions, but also produces extra
meta-information, such as action values indicating the quality of alternative
actions. This meta-information can indicate whether more than one action is
near-optimal for a certain state. We extend CN2 to make it able to leverage
knowledge about equally-good actions to distill the policy into fewer rules,
increasing its interpretability by a person. Then, to ensure that the rules
explain a valid, non-degenerate policy, we introduce a refinement algorithm
that fine-tunes the rules to obtain good performance when executed in the
environment. We demonstrate the applicability of our algorithm on the Mario AI
benchmark, a complex task that requires modern reinforcement learning
algorithms including neural networks. The explanations we produce capture the
learned policy in only a few rules, that allow a person to understand what the
black-box agent learned. Source code:
https://gitlab.ai.vub.ac.be/yocoppen/svcn2",2022-11-13 16:01:21
XXX,journalArticle,2021,"Arwa Alanqary, Gloria Z. Lin, Joie Le, Tan Zhi-Xuan, Vikash K. Mansinghka, Joshua B. Tenenbaum",Modeling the Mistakes of Boundedly Rational Agents Within a Bayesian Theory of Mind,,,,,http://arxiv.org/abs/2106.13249v1,"When inferring the goals that others are trying to achieve, people
intuitively understand that others might make mistakes along the way. This is
crucial for activities such as teaching, offering assistance, and deciding
between blame or forgiveness. However, Bayesian models of theory of mind have
generally not accounted for these mistakes, instead modeling agents as mostly
optimal in achieving their goals. As a result, they are unable to explain
phenomena like locking oneself out of one's house, or losing a game of chess.
Here, we extend the Bayesian Theory of Mind framework to model boundedly
rational agents who may have mistaken goals, plans, and actions. We formalize
this by modeling agents as probabilistic programs, where goals may be confused
with semantically similar states, plans may be misguided due to
resource-bounded planning, and actions may be unintended due to execution
errors. We present experiments eliciting human goal inferences in two domains:
(i) a gridworld puzzle with gems locked behind doors, and (ii) a block-stacking
domain. Our model better explains human inferences than alternatives, while
generalizing across domains. These findings indicate the importance of modeling
others as bounded agents, in order to account for the full richness of human
intuitive psychology.",2022-11-13 16:01:21
XXX,journalArticle,2021,"Yisroel Mirsky, Ambra Demontis, Jaidip Kotak, Ram Shankar, Deng Gelei, Liu Yang, Xiangyu Zhang, Wenke Lee, Yuval Elovici, Battista Biggio",The Threat of Offensive AI to Organizations,,,,,http://arxiv.org/abs/2106.15764v1,"AI has provided us with the ability to automate tasks, extract information
from vast amounts of data, and synthesize media that is nearly
indistinguishable from the real thing. However, positive tools can also be used
for negative purposes. In particular, cyber adversaries can use AI (such as
machine learning) to enhance their attacks and expand their campaigns.
  Although offensive AI has been discussed in the past, there is a need to
analyze and understand the threat in the context of organizations. For example,
how does an AI-capable adversary impact the cyber kill chain? Does AI benefit
the attacker more than the defender? What are the most significant AI threats
facing organizations today and what will be their impact on the future?
  In this survey, we explore the threat of offensive AI on organizations.
First, we present the background and discuss how AI changes the adversary's
methods, strategies, goals, and overall attack model. Then, through a
literature review, we identify 33 offensive AI capabilities which adversaries
can use to enhance their attacks. Finally, through a user study spanning
industry and academia, we rank the AI threats and provide insights on the
adversaries.",2022-11-13 16:01:21
XXX,journalArticle,2021,Dorien Herremans,aiSTROM -- A roadmap for developing a successful AI strategy,"IEEE Access, 2021",,,10.1109/ACCESS.2021.3127548,http://arxiv.org/abs/2107.06071v2,"A total of 34% of AI research and development projects fails or are
abandoned, according to a recent survey by Rackspace Technology of 1,870
companies. We propose a new strategic framework, aiSTROM, that empowers
managers to create a successful AI strategy based on a thorough literature
review. This provides a unique and integrated approach that guides managers and
lead developers through the various challenges in the implementation process.
In the aiSTROM framework, we start by identifying the top n potential projects
(typically 3-5). For each of those, seven areas of focus are thoroughly
analysed. These areas include creating a data strategy that takes into account
unique cross-departmental machine learning data requirements, security, and
legal requirements. aiSTROM then guides managers to think about how to put
together an interdisciplinary artificial intelligence (AI) implementation team
given the scarcity of AI talent. Once an AI team strategy has been established,
it needs to be positioned within the organization, either cross-departmental or
as a separate division. Other considerations include AI as a service (AIaas),
or outsourcing development. Looking at new technologies, we have to consider
challenges such as bias, legality of black-box-models, and keeping humans in
the loop. Next, like any project, we need value-based key performance
indicators (KPIs) to track and validate the progress. Depending on the
company's risk-strategy, a SWOT analysis (strengths, weaknesses, opportunities,
and threats) can help further classify the shortlisted projects. Finally, we
should make sure that our strategy includes continuous education of employees
to enable a culture of adoption. This unique and comprehensive framework offers
a valuable, literature supported, tool for managers and lead developers.",2022-11-13 16:01:22
XXX,journalArticle,2021,Desmond C. Ong,An Ethical Framework for Guiding the Development of Affectively-Aware Artificial Intelligence,,,,,http://arxiv.org/abs/2107.13734v1,"The recent rapid advancements in artificial intelligence research and
deployment have sparked more discussion about the potential ramifications of
socially- and emotionally-intelligent AI. The question is not if research can
produce such affectively-aware AI, but when it will. What will it mean for
society when machines -- and the corporations and governments they serve -- can
""read"" people's minds and emotions? What should developers and operators of
such AI do, and what should they not do? The goal of this article is to
pre-empt some of the potential implications of these developments, and propose
a set of guidelines for evaluating the (moral and) ethical consequences of
affectively-aware AI, in order to guide researchers, industry professionals,
and policy-makers. We propose a multi-stakeholder analysis framework that
separates the ethical responsibilities of AI Developers vis-\`a-vis the
entities that deploy such AI -- which we term Operators. Our analysis produces
two pillars that clarify the responsibilities of each of these stakeholders:
Provable Beneficence, which rests on proving the effectiveness of the AI, and
Responsible Stewardship, which governs responsible collection, use, and storage
of data and the decisions made from such data. We end with recommendations for
researchers, developers, operators, as well as regulators and law-makers.",2022-11-13 16:01:23
XXX,journalArticle,2021,"Marianna Bergamaschi Ganapini, Murray Campbell, Francesco Fabiano, Lior Horesh, Jon Lenchner, Andrea Loreggia, Nicholas Mattei, Francesca Rossi, Biplav Srivastava, Kristen Brent Venable",Thinking Fast and Slow in AI: the Role of Metacognition,,,,,http://arxiv.org/abs/2110.01834v1,"AI systems have seen dramatic advancement in recent years, bringing many
applications that pervade our everyday life. However, we are still mostly
seeing instances of narrow AI: many of these recent developments are typically
focused on a very limited set of competencies and goals, e.g., image
interpretation, natural language processing, classification, prediction, and
many others. Moreover, while these successes can be accredited to improved
algorithms and techniques, they are also tightly linked to the availability of
huge datasets and computational power. State-of-the-art AI still lacks many
capabilities that would naturally be included in a notion of (human)
intelligence.
  We argue that a better study of the mechanisms that allow humans to have
these capabilities can help us understand how to imbue AI systems with these
competencies. We focus especially on D. Kahneman's theory of thinking fast and
slow, and we propose a multi-agent AI architecture where incoming problems are
solved by either system 1 (or ""fast"") agents, that react by exploiting only
past experience, or by system 2 (or ""slow"") agents, that are deliberately
activated when there is the need to reason and search for optimal solutions
beyond what is expected from the system 1 agent. Both kinds of agents are
supported by a model of the world, containing domain knowledge about the
environment, and a model of ""self"", containing information about past actions
of the system and solvers' skills.",2022-11-13 16:01:23
XXX,journalArticle,2021,"Shahar Avin, Haydn Belfield, Miles Brundage, Gretchen Krueger, Jasmine Wang, Adrian Weller, Markus Anderljung, Igor Krawczuk, David Krueger, Jonathan Lebensold, Tegan Maharaj, Noa Zilberman",Filling gaps in trustworthy development of AI,"Science (2021) Vol 374, Issue 6573, pp. 1327-1329",,,10.1126/science.abi7176,http://arxiv.org/abs/2112.07773v1,"The range of application of artificial intelligence (AI) is vast, as is the
potential for harm. Growing awareness of potential risks from AI systems has
spurred action to address those risks, while eroding confidence in AI systems
and the organizations that develop them. A 2019 study found over 80
organizations that published and adopted ""AI ethics principles'', and more have
joined since. But the principles often leave a gap between the ""what"" and the
""how"" of trustworthy AI development. Such gaps have enabled questionable or
ethically dubious behavior, which casts doubts on the trustworthiness of
specific organizations, and the field more broadly. There is thus an urgent
need for concrete methods that both enable AI developers to prevent harm and
allow them to demonstrate their trustworthiness through verifiable behavior.
Below, we explore mechanisms (drawn from arXiv:2004.07213) for creating an
ecosystem where AI developers can earn trust - if they are trustworthy. Better
assessment of developer trustworthiness could inform user choice, employee
actions, investment decisions, legal recourse, and emerging governance regimes.",2022-11-13 16:01:23
XXX,journalArticle,2022,"Enrique Iglesias, Samaneh Jozashoori, Maria-Esther Vidal",Scaling Up Knowledge Graph Creation to Large and Heterogeneous Data Sources,,,,,http://arxiv.org/abs/2201.09694v3,"RDF knowledge graphs (KG) are powerful data structures to represent factual
statements created from heterogeneous data sources. KG creation is laborious
and demands data management techniques to be executed efficiently. This paper
tackles the problem of the automatic generation of KG creation processes
declaratively specified; it proposes techniques for planning and transforming
heterogeneous data into RDF triples following mapping assertions specified in
the RDF Mapping Language (RML). Given a set of mapping assertions, the planner
provides an optimized execution plan by partitioning and scheduling the
execution of the assertions. First, the planner assesses an optimized number of
partitions considering the number of data sources, type of mapping assertions,
and the associations between different assertions. After providing a list of
partitions and assertions that belong to each partition, the planner determines
their execution order. A greedy algorithm is implemented to generate the
partitions' bushy tree execution plan. Bushy tree plans are translated into
operating system commands that guide the execution of the partitions of the
mapping assertions in the order indicated by the bushy tree. The proposed
optimization approach is evaluated over state-of-the-art RML-compliant engines,
and existing benchmarks of data sources and RML triples maps. Our experimental
results suggest that the performance of the studied engines can be considerably
improved, particularly in a complex setting with numerous triples maps and
large data sources. As a result, engines that time out in complex cases are
enabled to produce at least a portion of the KG applying the planner.",2022-11-13 16:01:24
XXX,journalArticle,2022,"Chacha Chen, Shi Feng, Amit Sharma, Chenhao Tan",Machine Explanations and Human Understanding,,,,,http://arxiv.org/abs/2202.04092v1,"Explanations are hypothesized to improve human understanding of machine
learning models and achieve a variety of desirable outcomes, ranging from model
debugging to enhancing human decision making. However, empirical studies have
found mixed and even negative results. An open question, therefore, is under
what conditions explanations can improve human understanding and in what way.
Using adapted causal diagrams, we provide a formal characterization of the
interplay between machine explanations and human understanding, and show how
human intuitions play a central role in enabling human understanding.
Specifically, we identify three core concepts of interest that cover all
existing quantitative measures of understanding in the context of human-AI
decision making: task decision boundary, model decision boundary, and model
error. Our key result is that without assumptions about task-specific
intuitions, explanations may potentially improve human understanding of model
decision boundary, but they cannot improve human understanding of task decision
boundary or model error. To achieve complementary human-AI performance, we
articulate possible ways on how explanations need to work with human
intuitions. For instance, human intuitions about the relevance of features
(e.g., education is more important than age in predicting a person's income)
can be critical in detecting model error. We validate the importance of human
intuitions in shaping the outcome of machine explanations with empirical
human-subject studies. Overall, our work provides a general framework along
with actionable implications for future algorithmic development and empirical
experiments of machine explanations.",2022-11-13 16:01:25
XXX,journalArticle,2022,"Kailas Vodrahalli, Tobias Gerstenberg, James Zou",Uncalibrated Models Can Improve Human-AI Collaboration,,,,,http://arxiv.org/abs/2202.05983v3,"In many practical applications of AI, an AI model is used as a decision aid
for human users. The AI provides advice that a human (sometimes) incorporates
into their decision-making process. The AI advice is often presented with some
measure of ""confidence"" that the human can use to calibrate how much they
depend on or trust the advice. In this paper, we present an initial exploration
that suggests showing AI models as more confident than they actually are, even
when the original AI is well-calibrated, can improve human-AI performance
(measured as the accuracy and confidence of the human's final prediction after
seeing the AI advice). We first train a model to predict human incorporation of
AI advice using data from thousands of human-AI interactions. This enables us
to explicitly estimate how to transform the AI's prediction confidence, making
the AI uncalibrated, in order to improve the final human prediction. We
empirically validate our results across four different tasks--dealing with
images, text and tabular data--involving hundreds of human participants. We
further support our findings with simulation analysis. Our findings suggest the
importance of jointly optimizing the human-AI system as opposed to the standard
paradigm of optimizing the AI model alone.",2022-11-13 16:01:25
XXX,journalArticle,2022,Richard S. Sutton,The Quest for a Common Model of the Intelligent Decision Maker,,,,,http://arxiv.org/abs/2202.13252v3,"The premise of the Multi-disciplinary Conference on Reinforcement Learning
and Decision Making is that multiple disciplines share an interest in
goal-directed decision making over time. The idea of this paper is to sharpen
and deepen this premise by proposing a perspective on the decision maker that
is substantive and widely held across psychology, artificial intelligence,
economics, control theory, and neuroscience, which I call the ""common model of
the intelligent agent"". The common model does not include anything specific to
any organism, world, or application domain. The common model does include
aspects of the decision maker's interaction with its world (there must be input
and output, and a goal) and internal components of the decision maker (for
perception, decision-making, internal evaluation, and a world model). I
identify these aspects and components, note that they are given different names
in different disciplines but refer essentially to the same ideas, and discuss
the challenges and benefits of devising a neutral terminology that can be used
across disciplines. It is time to recognize and build on the convergence of
multiple diverse disciplines on a substantive common model of the intelligent
agent.",2022-11-13 16:01:26
XXX,journalArticle,2022,"Sreejan Kumar, Ishita Dasgupta, Raja Marjieh, Nathaniel D. Daw, Jonathan D. Cohen, Thomas L. Griffiths",Disentangling Abstraction from Statistical Pattern Matching in Human and Machine Learning,,,,,http://arxiv.org/abs/2204.01437v2,"The ability to acquire abstract knowledge is a hallmark of human intelligence
and is believed by many to be one of the core differences between humans and
neural network models. Agents can be endowed with an inductive bias towards
abstraction through meta-learning, where they are trained on a distribution of
tasks that share some abstract structure that can be learned and applied.
However, because neural networks are hard to interpret, it can be difficult to
tell whether agents have learned the underlying abstraction, or alternatively
statistical patterns that are characteristic of that abstraction. In this work,
we compare the performance of humans and agents in a meta-reinforcement
learning paradigm in which tasks are generated from abstract rules. We define a
novel methodology for building ""task metamers"" that closely match the
statistics of the abstract tasks but use a different underlying generative
process, and evaluate performance on both abstract and metamer tasks. In our
first set of experiments, we found that humans perform better at abstract tasks
than metamer tasks whereas a widely-used meta-reinforcement learning agent
performs worse on the abstract tasks than the matched metamers. In a second set
of experiments, we base the tasks on abstractions derived directly from
empirically identified human priors. We utilize the same procedure to generate
corresponding metamer tasks, and see the same double dissociation between
humans and agents. This work provides a foundation for characterizing
differences between humans and machine learning that can be used in future work
towards developing machines with human-like behavior.",2022-11-13 16:01:26
XXX,journalArticle,2022,"Theodore R. Sumers, Robert D. Hawkins, Mark K. Ho, Thomas L. Griffiths, Dylan Hadfield-Menell",Linguistic communication as (inverse) reward design,,,,,http://arxiv.org/abs/2204.05091v1,"Natural language is an intuitive and expressive way to communicate reward
information to autonomous agents. It encompasses everything from concrete
instructions to abstract descriptions of the world. Despite this, natural
language is often challenging to learn from: it is difficult for machine
learning methods to make appropriate inferences from such a wide range of
input. This paper proposes a generalization of reward design as a unifying
principle to ground linguistic communication: speakers choose utterances to
maximize expected rewards from the listener's future behaviors. We first extend
reward design to incorporate reasoning about unknown future states in a linear
bandit setting. We then define a speaker model which chooses utterances
according to this objective. Simulations show that short-horizon speakers
(reasoning primarily about a single, known state) tend to use instructions,
while long-horizon speakers (reasoning primarily about unknown, future states)
tend to describe the reward function. We then define a pragmatic listener which
performs inverse reward design by jointly inferring the speaker's latent
horizon and rewards. Our findings suggest that this extension of reward design
to linguistic communication, including the notion of a latent speaker horizon,
is a promising direction for achieving more robust alignment outcomes from
natural language supervision.",2022-11-13 16:01:27
XXX,journalArticle,2022,"Huili Chen, Xinqiao Zhang, Ke Huang, Farinaz Koushanfar",AdaTest:Reinforcement Learning and Adaptive Sampling for On-chip Hardware Trojan Detection,,,,,http://arxiv.org/abs/2204.06117v1,"This paper proposes AdaTest, a novel adaptive test pattern generation
framework for efficient and reliable Hardware Trojan (HT) detection. HT is a
backdoor attack that tampers with the design of victim integrated circuits
(ICs). AdaTest improves the existing HT detection techniques in terms of
scalability and accuracy of detecting smaller Trojans in the presence of noise
and variations. To achieve high trigger coverage, AdaTest leverages
Reinforcement Learning (RL) to produce a diverse set of test inputs.
Particularly, we progressively generate test vectors with high reward values in
an iterative manner. In each iteration, the test set is evaluated and
adaptively expanded as needed. Furthermore, AdaTest integrates adaptive
sampling to prioritize test samples that provide more information for HT
detection, thus reducing the number of samples while improving the sample
quality for faster exploration. We develop AdaTest with a Software/Hardware
co-design principle and provide an optimized on-chip architecture solution.
AdaTest's architecture minimizes the hardware overhead in two ways:(i)
Deploying circuit emulation on programmable hardware to accelerate reward
evaluation of the test input; (ii) Pipelining each computation stage in AdaTest
by automatically constructing auxiliary circuit for test input generation,
reward evaluation, and adaptive sampling. We evaluate AdaTest's performance on
various HT benchmarks and compare it with two prior works that use logic
testing for HT detection. Experimental results show that AdaTest engenders up
to two orders of test generation speedup and two orders of test set size
reduction compared to the prior works while achieving the same level or higher
Trojan detection rate.",2022-11-13 16:01:27
XXX,journalArticle,2022,"Jennafer S. Roberts, Laura N. Montoya","Contextualizing Artificially Intelligent Morality: A Meta-Ethnography of Top-Down, Bottom-Up, and Hybrid Models for Theoretical and Applied Ethics in Artificial Intelligence",,,,,http://arxiv.org/abs/2204.07612v2,"In this meta-ethnography, we explore three different angles of ethical
artificial intelligence (AI) design implementation including the philosophical
ethical viewpoint, the technical perspective, and framing through a political
lens. Our qualitative research includes a literature review that highlights the
cross-referencing of these angles by discussing the value and drawbacks of
contrastive top-down, bottom-up, and hybrid approaches previously published.
The novel contribution to this framework is the political angle, which
constitutes ethics in AI either being determined by corporations and
governments and imposed through policies or law (coming from the top), or
ethics being called for by the people (coming from the bottom), as well as
top-down, bottom-up, and hybrid technicalities of how AI is developed within a
moral construct and in consideration of its users, with expected and unexpected
consequences and long-term impact in the world. There is a focus on
reinforcement learning as an example of a bottom-up applied technical approach
and AI ethics principles as a practical top-down approach. This investigation
includes real-world case studies to impart a global perspective, as well as
philosophical debate on the ethics of AI and theoretical future thought
experimentation based on historical facts, current world circumstances, and
possible ensuing realities.",2022-11-13 16:01:28
XXX,journalArticle,2022,"Zhenghua Chen, Min Wu, Alvin Chan, Xiaoli Li, Yew-Soon Ong",A Survey on AI Sustainability: Emerging Trends on Learning Algorithms and Research Challenges,,,,,http://arxiv.org/abs/2205.03824v1,"Artificial Intelligence (AI) is a fast-growing research and development (R&D)
discipline which is attracting increasing attention because of its promises to
bring vast benefits for consumers and businesses, with considerable benefits
promised in productivity growth and innovation. To date it has reported
significant accomplishments in many areas that have been deemed as challenging
for machines, ranging from computer vision, natural language processing, audio
analysis to smart sensing and many others. The technical trend in realizing the
successes has been towards increasing complex and large size AI models so as to
solve more complex problems at superior performance and robustness. This rapid
progress, however, has taken place at the expense of substantial environmental
costs and resources. Besides, debates on the societal impacts of AI, such as
fairness, safety and privacy, have continued to grow in intensity. These issues
have presented major concerns pertaining to the sustainable development of AI.
In this work, we review major trends in machine learning approaches that can
address the sustainability problem of AI. Specifically, we examine emerging AI
methodologies and algorithms for addressing the sustainability issue of AI in
two major aspects, i.e., environmental sustainability and social sustainability
of AI. We will also highlight the major limitations of existing studies and
propose potential research challenges and directions for the development of
next generation of sustainable AI techniques. We believe that this technical
review can help to promote a sustainable development of AI R&D activities for
the research community.",2022-11-13 16:01:29
XXX,journalArticle,2022,"Sascha Saralajew, Ammar Shaker, Zhao Xu, Kiril Gashteovski, Bhushan Kotnis, Wiem Ben Rim, Jürgen Quittek, Carolin Lawrence",A Human-Centric Assessment Framework for AI,,,,,http://arxiv.org/abs/2205.12749v2,"With the rise of AI systems in real-world applications comes the need for
reliable and trustworthy AI. An essential aspect of this are explainable AI
systems. However, there is no agreed standard on how explainable AI systems
should be assessed. Inspired by the Turing test, we introduce a human-centric
assessment framework where a leading domain expert accepts or rejects the
solutions of an AI system and another domain expert. By comparing the
acceptance rates of provided solutions, we can assess how the AI system
performs compared to the domain expert, and whether the AI system's
explanations (if provided) are human-understandable. This setup -- comparable
to the Turing test -- can serve as a framework for a wide range of
human-centric AI system assessments. We demonstrate this by presenting two
instantiations: (1) an assessment that measures the classification accuracy of
a system with the option to incorporate label uncertainties; (2) an assessment
where the usefulness of provided explanations is determined in a human-centric
manner.",2022-11-13 16:01:29
XXX,journalArticle,2022,"Yushi Cao, Zhiming Li, Tianpei Yang, Hao Zhang, Yan Zheng, Yi Li, Jianye Hao, Yang Liu",GALOIS: Boosting Deep Reinforcement Learning via Generalizable Logic Synthesis,,,,,http://arxiv.org/abs/2205.13728v1,"Despite achieving superior performance in human-level control problems,
unlike humans, deep reinforcement learning (DRL) lacks high-order intelligence
(e.g., logic deduction and reuse), thus it behaves ineffectively than humans
regarding learning and generalization in complex problems. Previous works
attempt to directly synthesize a white-box logic program as the DRL policy,
manifesting logic-driven behaviors. However, most synthesis methods are built
on imperative or declarative programming, and each has a distinct limitation,
respectively. The former ignores the cause-effect logic during synthesis,
resulting in low generalizability across tasks. The latter is strictly
proof-based, thus failing to synthesize programs with complex hierarchical
logic. In this paper, we combine the above two paradigms together and propose a
novel Generalizable Logic Synthesis (GALOIS) framework to synthesize
hierarchical and strict cause-effect logic programs. GALOIS leverages the
program sketch and defines a new sketch-based hybrid program language for
guiding the synthesis. Based on that, GALOIS proposes a sketch-based program
synthesis method to automatically generate white-box programs with
generalizable and interpretable cause-effect logic. Extensive evaluations on
various decision-making tasks with complex logic demonstrate the superiority of
GALOIS over mainstream baselines regarding the asymptotic performance,
generalizability, and great knowledge reusability across different
environments.",2022-11-13 16:01:30
XXX,journalArticle,2022,"Kuang-Huei Lee, Ofir Nachum, Mengjiao Yang, Lisa Lee, Daniel Freeman, Winnie Xu, Sergio Guadarrama, Ian Fischer, Eric Jang, Henryk Michalewski, Igor Mordatch",Multi-Game Decision Transformers,,,,,http://arxiv.org/abs/2205.15241v2,"A longstanding goal of the field of AI is a method for learning a highly
capable, generalist agent from diverse experience. In the subfields of vision
and language, this was largely achieved by scaling up transformer-based models
and training them on large, diverse datasets. Motivated by this progress, we
investigate whether the same strategy can be used to produce generalist
reinforcement learning agents. Specifically, we show that a single
transformer-based model - with a single set of weights - trained purely offline
can play a suite of up to 46 Atari games simultaneously at close-to-human
performance. When trained and evaluated appropriately, we find that the same
trends observed in language and vision hold, including scaling of performance
with model size and rapid adaptation to new games via fine-tuning. We compare
several approaches in this multi-game setting, such as online and offline RL
methods and behavioral cloning, and find that our Multi-Game Decision
Transformer models offer the best scalability and performance. We release the
pre-trained models and code to encourage further research in this direction.",2022-11-13 16:01:30
XXX,journalArticle,2022,"Claire Stevenson, Iris Smal, Matthijs Baas, Raoul Grasman, Han van der Maas",Putting GPT-3's Creativity to the (Alternative Uses) Test,,,,,http://arxiv.org/abs/2206.08932v1,"AI large language models have (co-)produced amazing written works from
newspaper articles to novels and poetry. These works meet the standards of the
standard definition of creativity: being original and useful, and sometimes
even the additional element of surprise. But can a large language model
designed to predict the next text fragment provide creative, out-of-the-box,
responses that still solve the problem at hand? We put Open AI's generative
natural language model, GPT-3, to the test. Can it provide creative solutions
to one of the most commonly used tests in creativity research? We assessed
GPT-3's creativity on Guilford's Alternative Uses Test and compared its
performance to previously collected human responses on expert ratings of
originality, usefulness and surprise of responses, flexibility of each set of
ideas as well as an automated method to measure creativity based on the
semantic distance between a response and the AUT object in question. Our
results show that -- on the whole -- humans currently outperform GPT-3 when it
comes to creative output. But, we believe it is only a matter of time before
GPT-3 catches up on this particular task. We discuss what this work reveals
about human and AI creativity, creativity testing and our definition of
creativity.",2022-11-13 16:01:31
XXX,journalArticle,2022,"Théophile Champion, Marek Grześ, Howard Bowman",Multi-Modal and Multi-Factor Branching Time Active Inference,,,,,http://arxiv.org/abs/2206.12503v1,"Active inference is a state-of-the-art framework for modelling the brain that
explains a wide range of mechanisms such as habit formation, dopaminergic
discharge and curiosity. Recently, two versions of branching time active
inference (BTAI) based on Monte-Carlo tree search have been developed to handle
the exponential (space and time) complexity class that occurs when computing
the prior over all possible policies up to the time horizon. However, those two
versions of BTAI still suffer from an exponential complexity class w.r.t the
number of observed and latent variables being modelled. In the present paper,
we resolve this limitation by first allowing the modelling of several
observations, each of them having its own likelihood mapping. Similarly, we
allow each latent state to have its own transition mapping. The inference
algorithm then exploits the factorisation of the likelihood and transition
mappings to accelerate the computation of the posterior. Those two
optimisations were tested on the dSprites environment in which the metadata of
the dSprites dataset was used as input to the model instead of the dSprites
images. On this task, $BTAI_{VMP}$ (Champion et al., 2022b,a) was able to solve
96.9\% of the task in 5.1 seconds, and $BTAI_{BF}$ (Champion et al., 2021a) was
able to solve 98.6\% of the task in 17.5 seconds. Our new approach
($BTAI_{3MF}$) outperformed both of its predecessors by solving the task
completly (100\%) in only 2.559 seconds. Finally, $BTAI_{3MF}$ has been
implemented in a flexible and easy to use (python) package, and we developed a
graphical user interface to enable the inspection of the model's beliefs,
planning process and behaviour.",2022-11-13 16:01:31
XXX,journalArticle,2022,"Hiroshi Yamakawa, Yutaka Matsuo",Recognition of All Categories of Entities by AI,,,,,http://arxiv.org/abs/2208.06590v2,"Human-level AI will have significant impacts on human society. However,
estimates for the realization time are debatable. To arrive at human-level AI,
artificial general intelligence (AGI), as opposed to AI systems that are
specialized for a specific task, was set as a technically meaningful long-term
goal. But now, propelled by advances in deep learning, that achievement is
getting much closer. Considering the recent technological developments, it
would be meaningful to discuss the completion date of human-level AI through
the ""comprehensive technology map approach,"" wherein we map human-level
capabilities at a reasonable granularity, identify the current range of
technology, and discuss the technical challenges in traversing unexplored areas
and predict when all of them will be overcome. This paper presents a new
argumentative option to view the ontological sextet, which encompasses entities
in a way that is consistent with our everyday intuition and scientific
practice, as a comprehensive technological map. Because most of the modeling of
the world, in terms of how to interpret it, by an intelligent subject is the
recognition of distal entities and the prediction of their temporal evolution,
being able to handle all distal entities is a reasonable goal. Based on the
findings of philosophy and engineering cognitive technology, we predict that in
the relatively near future, AI will be able to recognize various entities to
the same degree as humans.",2022-11-13 16:01:32
XXX,journalArticle,2022,"Gali Noti, Yiling Chen",Learning When to Advise Human Decision Makers,,,,,http://arxiv.org/abs/2209.13578v1,"Artificial intelligence (AI) systems are increasingly used for providing
advice to facilitate human decision making. While a large body of work has
explored how AI systems can be optimized to produce accurate and fair advice
and how algorithmic advice should be presented to human decision makers, in
this work we ask a different basic question: When should algorithms provide
advice? Motivated by limitations of the current practice of constantly
providing algorithmic advice, we propose the design of AI systems that interact
with the human user in a two-sided manner and provide advice only when it is
likely to be beneficial to the human in making their decision. Our AI systems
learn advising policies using past human decisions. Then, for new cases, the
learned policies utilize input from the human to identify cases where
algorithmic advice would be useful, as well as those where the human is better
off deciding alone. We conduct a large-scale experiment to evaluate our
approach by using data from the US criminal justice system on pretrial-release
decisions. In our experiment, participants were asked to assess the risk of
defendants to violate their release terms if released and were advised by
different advising approaches. The results show that our interactive-advising
approach manages to provide advice at times of need and to significantly
improve human decision making compared to fixed, non-interactive advising
approaches. Our approach has additional advantages in facilitating human
learning, preserving complementary strengths of human decision makers, and
leading to more positive responsiveness to the advice.",2022-11-13 16:01:32
XXX,journalArticle,2022,"Mu Yuan, Lan Zhang, Fengxiang He, Xueting Tong, Miao-Hui Song, Xiang-Yang Li",InFi: End-to-End Learning to Filter Input for Resource-Efficiency in Mobile-Centric Inference,,,,,http://arxiv.org/abs/2209.13873v1,"Mobile-centric AI applications have high requirements for resource-efficiency
of model inference. Input filtering is a promising approach to eliminate the
redundancy so as to reduce the cost of inference. Previous efforts have
tailored effective solutions for many applications, but left two essential
questions unanswered: (1) theoretical filterability of an inference workload to
guide the application of input filtering techniques, thereby avoiding the
trial-and-error cost for resource-constrained mobile applications; (2) robust
discriminability of feature embedding to allow input filtering to be widely
effective for diverse inference tasks and input content. To answer them, we
first formalize the input filtering problem and theoretically compare the
hypothesis complexity of inference models and input filters to understand the
optimization potential. Then we propose the first end-to-end learnable input
filtering framework that covers most state-of-the-art methods and surpasses
them in feature embedding with robust discriminability. We design and implement
InFi that supports six input modalities and multiple mobile-centric
deployments. Comprehensive evaluations confirm our theoretical results and show
that InFi outperforms strong baselines in applicability, accuracy, and
efficiency. InFi achieve 8.5x throughput and save 95% bandwidth, while keeping
over 90% accuracy, for a video analytics application on mobile platforms.",2022-11-13 16:01:33
XXX,journalArticle,2022,"Michael L. Littman, Ifeoma Ajunwa, Guy Berger, Craig Boutilier, Morgan Currie, Finale Doshi-Velez, Gillian Hadfield, Michael C. Horowitz, Charles Isbell, Hiroaki Kitano, Karen Levy, Terah Lyons, Melanie Mitchell, Julie Shah, Steven Sloman, Shannon Vallor, Toby Walsh","Gathering Strength, Gathering Storms: The One Hundred Year Study on Artificial Intelligence (AI100) 2021 Study Panel Report",,,,,http://arxiv.org/abs/2210.15767v1,"In September 2021, the ""One Hundred Year Study on Artificial Intelligence""
project (AI100) issued the second report of its planned long-term periodic
assessment of artificial intelligence (AI) and its impact on society. It was
written by a panel of 17 study authors, each of whom is deeply rooted in AI
research, chaired by Michael Littman of Brown University. The report, entitled
""Gathering Strength, Gathering Storms,"" answers a set of 14 questions probing
critical areas of AI development addressing the major risks and dangers of AI,
its effects on society, its public perception and the future of the field. The
report concludes that AI has made a major leap from the lab to people's lives
in recent years, which increases the urgency to understand its potential
negative effects. The questions were developed by the AI100 Standing Committee,
chaired by Peter Stone of the University of Texas at Austin, consisting of a
group of AI leaders with expertise in computer science, sociology, ethics,
economics, and other disciplines.",2022-11-13 16:01:33
XXX,journalArticle,2022,"David Fernández Llorca, Vicky Charisi, Ronan Hamon, Ignacio Sánchez, Emilia Gómez",Liability regimes in the age of AI: a use-case driven analysis of the burden of proof,,,,,http://arxiv.org/abs/2211.01817v1,"New emerging technologies powered by Artificial Intelligence (AI) have the
potential to disruptively transform our societies for the better. In
particular, data-driven learning approaches (i.e., Machine Learning (ML)) have
been a true revolution in the advancement of multiple technologies in various
application domains. But at the same time there is growing concerns about
certain intrinsic characteristics of these methodologies that carry potential
risks to both safety and fundamental rights. Although there are mechanisms in
the adoption process to minimize these risks (e.g., safety regulations), these
do not exclude the possibility of harm occurring, and if this happens, victims
should be able to seek compensation. Liability regimes will therefore play a
key role in ensuring basic protection for victims using or interacting with
these systems. However, the same characteristics that make AI systems
inherently risky, such as lack of causality, opacity, unpredictability or their
self and continuous learning capabilities, lead to considerable difficulties
when it comes to proving causation. This paper presents three case studies, as
well as the methodology to reach them, that illustrate these difficulties.
Specifically, we address the cases of cleaning robots, delivery drones and
robots in education. The outcome of the proposed analysis suggests the need to
revise liability regimes to alleviate the burden of proof on victims in cases
involving AI technologies.",2022-11-13 16:01:34
XXX,journalArticle,2022,"Kyle A. Kilian, Christopher J. Ventura, Mark M. Bailey",Examining the Differential Risk from High-level Artificial Intelligence and the Question of Control,,,,,http://arxiv.org/abs/2211.03157v2,"Artificial Intelligence (AI) is one of the most transformative technologies
of the 21st century. The extent and scope of future AI capabilities remain a
key uncertainty, with widespread disagreement on timelines and potential
impacts. As nations and technology companies race toward greater complexity and
autonomy in AI systems, there are concerns over the extent of integration and
oversight of opaque AI decision processes. This is especially true in the
subfield of machine learning (ML), where systems learn to optimize objectives
without human assistance. Objectives can be imperfectly specified or executed
in an unexpected or potentially harmful way. This becomes more concerning as
systems increase in power and autonomy, where an abrupt capability jump could
result in unexpected shifts in power dynamics or even catastrophic failures.
This study presents a hierarchical complex systems framework to model AI risk
and provide a template for alternative futures analysis. Survey data were
collected from domain experts in the public and private sectors to classify AI
impact and likelihood. The results show increased uncertainty over the powerful
AI agent scenario, confidence in multiagent environments, and increased concern
over AI alignment failures and influence-seeking behavior.",2022-11-13 16:01:35
XXX,journalArticle,2014,"Wiebe van der Hoek, Dirk Walther, Michael Wooldridge",Reasoning About the Transfer of Control,"Journal Of Artificial Intelligence Research, Volume 37, pages
  437-477, 2010",,,10.1613/jair.2901,http://arxiv.org/abs/1401.3825v1,"We present DCL-PC: a logic for reasoning about how the abilities of agents
and coalitions of agents are altered by transferring control from one agent to
another. The logical foundation of DCL-PC is CL-PC, a logic for reasoning about
cooperation in which the abilities of agents and coalitions of agents stem from
a distribution of atomic Boolean variables to individual agents -- the choices
available to a coalition correspond to assignments to the variables the
coalition controls. The basic modal constructs of DCL-PC are of the form
coalition C can cooperate to bring about phi. DCL-PC extends CL-PC with dynamic
logic modalities in which atomic programs are of the form agent i gives control
of variable p to agent j; as usual in dynamic logic, these atomic programs may
be combined using sequence, iteration, choice, and test operators to form
complex programs. By combining such dynamic transfer programs with cooperation
modalities, it becomes possible to reason about how the power of agents and
coalitions is affected by the transfer of control. We give two alternative
semantics for the logic: a direct semantics, in which we capture the
distributions of Boolean variables to agents; and a more conventional Kripke
semantics. We prove that these semantics are equivalent, and then present an
axiomatization for the logic. We investigate the computational complexity of
model checking and satisfiability for DCL-PC, and show that both problems are
PSPACE-complete (and hence no worse than the underlying logic CL-PC). Finally,
we investigate the characterisation of control in DCL-PC. We distinguish
between first-order control -- the ability of an agent or coalition to control
some state of affairs through the assignment of values to the variables under
the control of the agent or coalition -- and second-order control -- the
ability of an agent to exert control over the control that other agents have by
transferring variables to other agents. We give a logical characterisation of
second-order control.",2022-11-13 16:01:35
XXX,journalArticle,2015,"Stefano V. Albrecht, Jacob W. Crandall, Subramanian Ramamoorthy",Belief and Truth in Hypothesised Behaviours,,,,10.1016/j.artint.2016.02.004,http://arxiv.org/abs/1507.07688v3,"There is a long history in game theory on the topic of Bayesian or ""rational""
learning, in which each player maintains beliefs over a set of alternative
behaviours, or types, for the other players. This idea has gained increasing
interest in the artificial intelligence (AI) community, where it is used as a
method to control a single agent in a system composed of multiple agents with
unknown behaviours. The idea is to hypothesise a set of types, each specifying
a possible behaviour for the other agents, and to plan our own actions with
respect to those types which we believe are most likely, given the observed
actions of the agents. The game theory literature studies this idea primarily
in the context of equilibrium attainment. In contrast, many AI applications
have a focus on task completion and payoff maximisation. With this perspective
in mind, we identify and address a spectrum of questions pertaining to belief
and truth in hypothesised types. We formulate three basic ways to incorporate
evidence into posterior beliefs and show when the resulting beliefs are
correct, and when they may fail to be correct. Moreover, we demonstrate that
prior beliefs can have a significant impact on our ability to maximise payoffs
in the long-term, and that they can be computed automatically with consistent
performance effects. Furthermore, we analyse the conditions under which we are
able complete our task optimally, despite inaccuracies in the hypothesised
types. Finally, we show how the correctness of hypothesised types can be
ascertained during the interaction via an automated statistical analysis.",2022-11-13 16:01:36
XXX,journalArticle,2016,"Gopal P. Sarma, Nick J. Hay",Mammalian Value Systems,Informatica Vol. 41 No. 3 (2017),,,,http://arxiv.org/abs/1607.08289v4,"Characterizing human values is a topic deeply interwoven with the sciences,
humanities, art, and many other human endeavors. In recent years, a number of
thinkers have argued that accelerating trends in computer science, cognitive
science, and related disciplines foreshadow the creation of intelligent
machines which meet and ultimately surpass the cognitive abilities of human
beings, thereby entangling an understanding of human values with future
technological development. Contemporary research accomplishments suggest
sophisticated AI systems becoming widespread and responsible for managing many
aspects of the modern world, from preemptively planning users' travel schedules
and logistics, to fully autonomous vehicles, to domestic robots assisting in
daily living. The extrapolation of these trends has been most forcefully
described in the context of a hypothetical ""intelligence explosion,"" in which
the capabilities of an intelligent software agent would rapidly increase due to
the presence of feedback loops unavailable to biological organisms. The
possibility of superintelligent agents, or simply the widespread deployment of
sophisticated, autonomous AI systems, highlights an important theoretical
problem: the need to separate the cognitive and rational capacities of an agent
from the fundamental goal structure, or value system, which constrains and
guides the agent's actions. The ""value alignment problem"" is to specify a goal
structure for autonomous agents compatible with human values. In this brief
article, we suggest that recent ideas from affective neuroscience and related
disciplines aimed at characterizing neurological and behavioral universals in
the mammalian class provide important conceptual foundations relevant to
describing human values. We argue that the notion of ""mammalian value systems""
points to a potential avenue for fundamental research in AI safety and AI
ethics.",2022-11-13 16:01:36
XXX,journalArticle,2016,"Sarmimala Saikia, Lovekesh Vig, Ashwin Srinivasan, Gautam Shroff, Puneet Agarwal, Richa Rawat",Neuro-symbolic EDA-based Optimisation using ILP-enhanced DBNs,,,,,http://arxiv.org/abs/1612.06528v1,"We investigate solving discrete optimisation problems using the estimation of
distribution (EDA) approach via a novel combination of deep belief
networks(DBN) and inductive logic programming (ILP).While DBNs are used to
learn the structure of successively better feasible solutions,ILP enables the
incorporation of domain-based background knowledge related to the goodness of
solutions.Recent work showed that ILP could be an effective way to use domain
knowledge in an EDA scenario.However,in a purely ILP-based EDA,sampling
successive populations is either inefficient or not straightforward.In our
Neuro-symbolic EDA,an ILP engine is used to construct a model for good
solutions using domain-based background knowledge.These rules are introduced as
Boolean features in the last hidden layer of DBNs used for EDA-based
optimization.This incorporation of logical ILP features requires some changes
while training and sampling from DBNs: (a)our DBNs need to be trained with data
for units at the input layer as well as some units in an otherwise hidden
layer, and (b)we would like the samples generated to be drawn from instances
entailed by the logical model.We demonstrate the viability of our approach on
instances of two optimisation problems: predicting optimal depth-of-win for the
KRK endgame,and jobshop scheduling.Our results are promising: (i)On each
iteration of distribution estimation,samples obtained with an ILP-assisted DBN
have a substantially greater proportion of good solutions than samples
generated using a DBN without ILP features, and (ii)On termination of
distribution estimation,samples obtained using an ILP-assisted DBN contain more
near-optimal samples than samples from a DBN without ILP features.These results
suggest that the use of ILP-constructed theories could be useful for
incorporating complex domain-knowledge into deep models for estimation of
distribution based procedures.",2022-11-13 16:01:37
XXX,journalArticle,2016,"C. J. C. Burges, T. Hart, Z. Yang, S. Cucerzan, R. W. White, A. Pastusiak, J. Lewis",A Base Camp for Scaling AI,,,,,http://arxiv.org/abs/1612.07896v1,"Modern statistical machine learning (SML) methods share a major limitation
with the early approaches to AI: there is no scalable way to adapt them to new
domains. Human learning solves this in part by leveraging a rich, shared,
updateable world model. Such scalability requires modularity: updating part of
the world model should not impact unrelated parts. We have argued that such
modularity will require both ""correctability"" (so that errors can be corrected
without introducing new errors) and ""interpretability"" (so that we can
understand what components need correcting).
  To achieve this, one could attempt to adapt state of the art SML systems to
be interpretable and correctable; or one could see how far the simplest
possible interpretable, correctable learning methods can take us, and try to
control the limitations of SML methods by applying them only where needed. Here
we focus on the latter approach and we investigate two main ideas: ""Teacher
Assisted Learning"", which leverages crowd sourcing to learn language; and
""Factored Dialog Learning"", which factors the process of application
development into roles where the language competencies needed are isolated,
enabling non-experts to quickly create new applications.
  We test these ideas in an ""Automated Personal Assistant"" (APA) setting, with
two scenarios: that of detecting user intent from a user-APA dialog; and that
of creating a class of event reminder applications, where a non-expert
""teacher"" can then create specific apps. For the intent detection task, we use
a dataset of a thousand labeled utterances from user dialogs with Cortana, and
we show that our approach matches state of the art SML methods, but in addition
provides full transparency: the whole (editable) model can be summarized on one
human-readable page. For the reminder app task, we ran small user studies to
verify the efficacy of the approach.",2022-11-13 16:01:37
XXX,journalArticle,2017,"Naveen Sundar Govindarajulu, Selmer Bringsjord",On Automating the Doctrine of Double Effect,,,,,http://arxiv.org/abs/1703.08922v5,"The doctrine of double effect ($\mathcal{DDE}$) is a long-studied ethical
principle that governs when actions that have both positive and negative
effects are to be allowed. The goal in this paper is to automate
$\mathcal{DDE}$. We briefly present $\mathcal{DDE}$, and use a first-order
modal logic, the deontic cognitive event calculus, as our framework to
formalize the doctrine. We present formalizations of increasingly stronger
versions of the principle, including what is known as the doctrine of triple
effect. We then use our framework to simulate successfully scenarios that have
been used to test for the presence of the principle in human subjects. Our
framework can be used in two different modes: One can use it to build
$\mathcal{DDE}$-compliant autonomous systems from scratch, or one can use it to
verify that a given AI system is $\mathcal{DDE}$-compliant, by applying a
$\mathcal{DDE}$ layer on an existing system or model. For the latter mode, the
underlying AI system can be built using any architecture (planners, deep neural
networks, bayesian networks, knowledge-representation systems, or a hybrid); as
long as the system exposes a few parameters in its model, such verification is
possible. The role of the $\mathcal{DDE}$ layer here is akin to a (dynamic or
static) software verifier that examines existing software modules. Finally, we
end by presenting initial work on how one can apply our $\mathcal{DDE}$ layer
to the STRIPS-style planning model, and to a modified POMDP model.This is
preliminary work to illustrate the feasibility of the second mode, and we hope
that our initial sketches can be useful for other researchers in incorporating
DDE in their own frameworks.",2022-11-13 16:01:38
XXX,journalArticle,2019,"Stuart M. Marshall, Douglas Moore, Alastair R. G. Murray, Sara I. Walker, Leroy Cronin",Quantifying the pathways to life using assembly spaces,,,,,http://arxiv.org/abs/1907.04649v2,"We have developed the concept of pathway assembly to explore the amount of
extrinsic information required to build an object. To quantify this information
in an agnostic way, we present a method to determine the amount of pathway
assembly information contained within such an object by deconstructing the
object into its irreducible parts, and then evaluating the minimum number of
steps to reconstruct the object along any pathway. The mathematical
formalisation of this approach uses an assembly space. By finding the minimal
number of steps contained in the route by which the objects can be assembled
within that space, we can compare how much information (I) is gained from
knowing this pathway assembly index (PA) according to I_PA=log (|N|)/(|N_PA |)
where, for an end product with PA=x, N is the set of objects possible that can
be created from the same irreducible parts within x steps regardless of PA, and
NPA is the subset of those objects with the precise pathway assembly index
PA=x. Applying this formalism to objects formed in 1D, 2D and 3D space allows
us to identify objects in the world or wider Universe that have high assembly
numbers. We propose that objects with PA greater than a threshold are important
because these are uniquely identifiable as those that must have been produced
by biological or technological processes, rather than the assembly occurring
via unbiased random processes alone. We think this approach is needed to help
identify the new physical and chemical laws needed to understand what life is,
by quantifying what life does.",2022-11-13 16:01:38
XXX,journalArticle,2020,Pedro Casas,Two Decades of AI4NETS-AI/ML for Data Networks: Challenges & Research Directions,"5th IEEE/IFIP International Workshop on Analytics for Network and
  Service Management (AnNet 2020)",,,,http://arxiv.org/abs/2003.04080v1,"The popularity of Artificial Intelligence (AI) -- and of Machine Learning
(ML) as an approach to AI, has dramatically increased in the last few years,
due to its outstanding performance in various domains, notably in image, audio,
and natural language processing. In these domains, AI success-stories are
boosting the applied field. When it comes to AI/ML for data communication
Networks (AI4NETS), and despite the many attempts to turn networks into
learning agents, the successful application of AI/ML in networking is limited.
There is a strong resistance against AI/ML-based solutions, and a striking gap
between the extensive academic research and the actual deployments of such
AI/ML-based systems in operational environments. The truth is, there are still
many unsolved complex challenges associated to the analysis of networking data
through AI/ML, which hinders its acceptability and adoption in the practice. In
this positioning paper I elaborate on the most important show-stoppers in
AI4NETS, and present a research agenda to tackle some of these challenges,
enabling a natural adoption of AI/ML for networking. In particular, I focus the
future research in AI4NETS around three major pillars: (i) to make AI/ML
immediately applicable in networking problems through the concepts of effective
learning, turning it into a useful and reliable way to deal with complex
data-driven networking problems; (ii) to boost the adoption of AI/ML at the
large scale by learning from the Internet-paradigm itself, conceiving novel
distributed and hierarchical learning approaches mimicking the distributed
topological principles and operation of the Internet itself; and (iii) to
exploit the softwarization and distribution of networks to conceive
AI/ML-defined Networks (AIDN), relying on the distributed generation and
re-usage of knowledge through novel Knowledge Delivery Networks (KDNs).",2022-11-13 16:01:39
XXX,journalArticle,2021,"Dongbo Xi, Zhen Chen, Peng Yan, Yinger Zhang, Yongchun Zhu, Fuzhen Zhuang, Yu Chen",Modeling the Sequential Dependence among Audience Multi-step Conversions with Multi-task Learning in Targeted Display Advertising,,,,,http://arxiv.org/abs/2105.08489v2,"In most real-world large-scale online applications (e.g., e-commerce or
finance), customer acquisition is usually a multi-step conversion process of
audiences. For example, an impression->click->purchase process is usually
performed of audiences for e-commerce platforms. However, it is more difficult
to acquire customers in financial advertising (e.g., credit card advertising)
than in traditional advertising. On the one hand, the audience multi-step
conversion path is longer. On the other hand, the positive feedback is sparser
(class imbalance) step by step, and it is difficult to obtain the final
positive feedback due to the delayed feedback of activation. Multi-task
learning is a typical solution in this direction. While considerable multi-task
efforts have been made in this direction, a long-standing challenge is how to
explicitly model the long-path sequential dependence among audience multi-step
conversions for improving the end-to-end conversion. In this paper, we propose
an Adaptive Information Transfer Multi-task (AITM) framework, which models the
sequential dependence among audience multi-step conversions via the Adaptive
Information Transfer (AIT) module. The AIT module can adaptively learn what and
how much information to transfer for different conversion stages. Besides, by
combining the Behavioral Expectation Calibrator in the loss function, the AITM
framework can yield more accurate end-to-end conversion identification. The
proposed framework is deployed in Meituan app, which utilizes it to real-timely
show a banner to the audience with a high end-to-end conversion rate for
Meituan Co-Branded Credit Cards. Offline experimental results on both
industrial and public real-world datasets clearly demonstrate that the proposed
framework achieves significantly better performance compared with
state-of-the-art baselines.",2022-11-13 16:01:40
XXX,journalArticle,2021,Yongfeng Zhang,Problem Learning: Towards the Free Will of Machines,,,,,http://arxiv.org/abs/2109.00177v1,"A machine intelligence pipeline usually consists of six components: problem,
representation, model, loss, optimizer and metric. Researchers have worked hard
trying to automate many components of the pipeline. However, one key component
of the pipeline--problem definition--is still left mostly unexplored in terms
of automation. Usually, it requires extensive efforts from domain experts to
identify, define and formulate important problems in an area. However,
automatically discovering research or application problems for an area is
beneficial since it helps to identify valid and potentially important problems
hidden in data that are unknown to domain experts, expand the scope of tasks
that we can do in an area, and even inspire completely new findings.
  This paper describes Problem Learning, which aims at learning to discover and
define valid and ethical problems from data or from the machine's interaction
with the environment. We formalize problem learning as the identification of
valid and ethical problems in a problem space and introduce several possible
approaches to problem learning. In a broader sense, problem learning is an
approach towards the free will of intelligent machines. Currently, machines are
still limited to solving the problems defined by humans, without the ability or
flexibility to freely explore various possible problems that are even unknown
to humans. Though many machine learning techniques have been developed and
integrated into intelligent systems, they still focus on the means rather than
the purpose in that machines are still solving human defined problems. However,
proposing good problems is sometimes even more important than solving problems,
because a good problem can help to inspire new ideas and gain deeper
understandings. The paper also discusses the ethical implications of problem
learning under the background of Responsible AI.",2022-11-13 16:01:40
XXX,journalArticle,2021,"Théophile Champion, Howard Bowman, Marek Grześ",Branching Time Active Inference: empirical study and complexity class analysis,,,,,http://arxiv.org/abs/2111.11276v2,"Active inference is a state-of-the-art framework for modelling the brain that
explains a wide range of mechanisms such as habit formation, dopaminergic
discharge and curiosity. However, recent implementations suffer from an
exponential complexity class when computing the prior over all the possible
policies up to the time horizon. Fountas et al (2020) used Monte Carlo tree
search to address this problem, leading to very good results in two different
tasks. Additionally, Champion et al (2021a) proposed a tree search approach
based on (temporal) structure learning. This was enabled by the development of
a variational message passing approach to active inference, which enables
compositional construction of Bayesian networks for active inference. However,
this message passing tree search approach, which we call branching-time active
inference (BTAI), has never been tested empirically. In this paper, we present
an experimental study of BTAI in the context of a maze solving agent. In this
context, we show that both improved prior preferences and deeper search help
mitigate the vulnerability to local minima. Then, we compare BTAI to standard
active inference (AcI) on a graph navigation task. We show that for small
graphs, both BTAI and AcI successfully solve the task. For larger graphs, AcI
exhibits an exponential (space) complexity class, making the approach
intractable. However, BTAI explores the space of policies more efficiently,
successfully scaling to larger graphs. Then, BTAI was compared to the POMCP
algorithm on the frozen lake environment. The experiments suggest that BTAI and
the POMCP algorithm accumulate a similar amount of reward. Also, we describe
when BTAI receives more rewards than the POMCP agent, and when the opposite is
true. Finally, we compared BTAI to the approach of Fountas et al (2020) on the
dSprites dataset, and we discussed the pros and cons of each approach.",2022-11-13 16:01:41
XXX,journalArticle,2022,"David Leslie, Christopher Burr, Mhairi Aitken, Michael Katell, Morgan Briggs, Cami Rincon","Human rights, democracy, and the rule of law assurance framework for AI systems: A proposal",,,,10.5281/zenodo.5981676,http://arxiv.org/abs/2202.02776v1,"Following on from the publication of its Feasibility Study in December 2020,
the Council of Europe's Ad Hoc Committee on Artificial Intelligence (CAHAI) and
its subgroups initiated efforts to formulate and draft its Possible Elements of
a Legal Framework on Artificial Intelligence, based on the Council of Europe's
standards on human rights, democracy, and the rule of law. This document was
ultimately adopted by the CAHAI plenary in December 2021. To support this
effort, The Alan Turing Institute undertook a programme of research that
explored the governance processes and practical tools needed to operationalise
the integration of human right due diligence with the assurance of trustworthy
AI innovation practices.
  The resulting framework was completed and submitted to the Council of Europe
in September 2021. It presents an end-to-end approach to the assurance of AI
project lifecycles that integrates context-based risk analysis and appropriate
stakeholder engagement with comprehensive impact assessment, and transparent
risk management, impact mitigation, and innovation assurance practices. Taken
together, these interlocking processes constitute a Human Rights, Democracy and
the Rule of Law Assurance Framework (HUDERAF). The HUDERAF combines the
procedural requirements for principles-based human rights due diligence with
the governance mechanisms needed to set up technical and socio-technical
guardrails for responsible and trustworthy AI innovation practices. Its purpose
is to provide an accessible and user-friendly set of mechanisms for
facilitating compliance with a binding legal framework on artificial
intelligence, based on the Council of Europe's standards on human rights,
democracy, and the rule of law, and to ensure that AI innovation projects are
carried out with appropriate levels of public accountability, transparency, and
democratic governance.",2022-11-13 16:01:41
XXX,journalArticle,2022,"Anu K. Myne, Kevin J. Leahy, Ryan J. Soklaski",Knowledge-Integrated Informed AI for National Security,,,,,http://arxiv.org/abs/2202.03188v1,"The state of artificial intelligence technology has a rich history that dates
back decades and includes two fall-outs before the explosive resurgence of
today, which is credited largely to data-driven techniques. While AI technology
has and continues to become increasingly mainstream with impact across domains
and industries, it's not without several drawbacks, weaknesses, and potential
to cause undesired effects. AI techniques are numerous with many approaches and
variants, but they can be classified simply based on the degree of knowledge
they capture and how much data they require; two broad categories emerge as
prominent across AI to date: (1) techniques that are primarily, and often
solely, data-driven while leveraging little to no knowledge and (2) techniques
that primarily leverage knowledge and depend less on data. Now, a third
category is starting to emerge that leverages both data and knowledge, that
some refer to as ""informed AI."" This third category can be a game changer
within the national security domain where there is ample scientific and
domain-specific knowledge that stands ready to be leveraged, and where purely
data-driven AI can lead to serious unwanted consequences.
  This report shares findings from a thorough exploration of AI approaches that
exploit data as well as principled and/or practical knowledge, which we refer
to as ""knowledge-integrated informed AI."" Specifically, we review illuminating
examples of knowledge integrated in deep learning and reinforcement learning
pipelines, taking note of the performance gains they provide. We also discuss
an apparent trade space across variants of knowledge-integrated informed AI,
along with observed and prominent issues that suggest worthwhile future
research directions. Most importantly, this report suggests how the advantages
of knowledge-integrated informed AI stand to benefit the national security
domain.",2022-11-13 16:01:42
XXX,journalArticle,2022,"Sam Clarke, Ben Cottier, Aryeh Englander, Daniel Eth, David Manheim, Samuel Dylan Martin, Issa Rice",Modeling Transformative AI Risks (MTAIR) Project -- Summary Report,,,,,http://arxiv.org/abs/2206.09360v1,"This report outlines work by the Modeling Transformative AI Risk (MTAIR)
project, an attempt to map out the key hypotheses, uncertainties, and
disagreements in debates about catastrophic risks from advanced AI, and the
relationships between them. This builds on an earlier diagram by Ben Cottier
and Rohin Shah which laid out some of the crucial disagreements (""cruxes"")
visually, with some explanation. Based on an extensive literature review and
engagement with experts, the report explains a model of the issues involved,
and the initial software-based implementation that can incorporate probability
estimates or other quantitative factors to enable exploration, planning, and/or
decision support. By gathering information from various debates and discussions
into a single more coherent presentation, we hope to enable better discussions
and debates about the issues involved.
  The model starts with a discussion of reasoning via analogies and general
prior beliefs about artificial intelligence. Following this, it lays out a
model of different paths and enabling technologies for high-level machine
intelligence, and a model of how advances in the capabilities of these systems
might proceed, including debates about self-improvement, discontinuous
improvements, and the possibility of distributed, non-agentic high-level
intelligence or slower improvements. The model also looks specifically at the
question of learned optimization, and whether machine learning systems will
create mesa-optimizers. The impact of different safety research on the previous
sets of questions is then examined, to understand whether and how research
could be useful in enabling safer systems. Finally, we discuss a model of
different failure modes and loss of control or takeover scenarios.",2022-11-13 16:01:42
XXX,journalArticle,2022,"Paulo Pirozelli, Ais B. R. Castro, Ana Luiza C. de Oliveira, André S. Oliveira, Flávio N. Cação, Igor C. Silveira, João G. M. Campos, Laura C. Motheo, Leticia F. Figueiredo, Lucas F. A. O. Pellicer, Marcelo A. José, Marcos M. José, Pedro de M. Ligabue, Ricardo S. Grava, Rodrigo M. Tavares, Vinícius B. Matos, Yan V. Sym, Anna H. R. Costa, Anarosa A. F. Brandão, Denis D. Mauá, Fabio G. Cozman, Sarajane M. Peres",The BLue Amazon Brain (BLAB): A Modular Architecture of Services about the Brazilian Maritime Territory,"AI: Modeling Oceans and Climate Change (IJCAI-ECAI), 2022",,,,http://arxiv.org/abs/2209.07928v1,"We describe the first steps in the development of an artificial agent focused
on the Brazilian maritime territory, a large region within the South Atlantic
also known as the Blue Amazon. The ""BLue Amazon Brain"" (BLAB) integrates a
number of services aimed at disseminating information about this region and its
importance, functioning as a tool for environmental awareness. The main service
provided by BLAB is a conversational facility that deals with complex questions
about the Blue Amazon, called BLAB-Chat; its central component is a controller
that manages several task-oriented natural language processing modules (e.g.,
question answering and summarizer systems). These modules have access to an
internal data lake as well as to third-party databases. A news reporter
(BLAB-Reporter) and a purposely-developed wiki (BLAB-Wiki) are also part of the
BLAB service architecture. In this paper, we describe our current version of
BLAB's architecture (interface, backend, web services, NLP modules, and
resources) and comment on the challenges we have faced so far, such as the lack
of training data and the scattered state of domain information. Solving these
issues presents a considerable challenge in the development of artificial
intelligence for technical domains.",2022-11-13 16:01:43
XXX,journalArticle,2022,"Lin Guan, Karthik Valmeekam, Subbarao Kambhampati",Relative Behavioral Attributes: Filling the Gap between Symbolic Goal Specification and Reward Learning from Human Preferences,,,,,http://arxiv.org/abs/2210.15906v1,"Generating complex behaviors from goals specified by non-expert users is a
crucial aspect of intelligent agents. Interactive reward learning from
trajectory comparisons is one way to allow non-expert users to convey complex
objectives by expressing preferences over short clips of agent behaviors. Even
though this method can encode complex tacit knowledge present in the underlying
tasks, it implicitly assumes that the human is unable to provide rich-form
feedback other than binary preference labels, leading to extremely high
feedback complexity and poor user experience. While providing a detailed
symbolic specification of the objectives might be tempting, it is not always
feasible even for an expert user. However, in most cases, humans are aware of
how the agent should change its behavior along meaningful axes to fulfill the
underlying purpose, even if they are not able to fully specify task objectives
symbolically. Using this as motivation, we introduce the notion of Relative
Behavioral Attributes, which acts as a middle ground, between exact goal
specification and reward learning purely from preference labels, by enabling
the users to tweak the agent's behavior through nameable concepts (e.g.,
increasing the softness of the movement of a two-legged ""sneaky"" agent). We
propose two different parametric methods that can potentially encode any kind
of behavioral attributes from ordered behavior clips. We demonstrate the
effectiveness of our methods on 4 tasks with 9 different behavioral attributes
and show that once the attributes are learned, end users can effortlessly
produce desirable agent behaviors, by providing feedback just around 10 times.
The feedback complexity of our approach is over 10 times less than the
learning-from-human-preferences baseline and this demonstrates that our
approach is readily applicable in real-world applications.",2022-11-13 16:01:43
XXX,journalArticle,2008,"Amanda Whitbrook, Uwe Aickelin, Jonathan Garibaldi",Idiotypic Immune Networks in Mobile Robot Control,"IEEE Transactions on Systems, Man and Cybernetics, Part B, 37(6),
  1581- 1598, 2007",,,10.1109/TSMCB.2007.907334,http://arxiv.org/abs/0803.2981v1,"Jerne's idiotypic network theory postulates that the immune response involves
inter-antibody stimulation and suppression as well as matching to antigens. The
theory has proved the most popular Artificial Immune System (ais) model for
incorporation into behavior-based robotics but guidelines for implementing
idiotypic selection are scarce. Furthermore, the direct effects of employing
the technique have not been demonstrated in the form of a comparison with
non-idiotypic systems. This paper aims to address these issues. A method for
integrating an idiotypic ais network with a Reinforcement Learning based
control system (rl) is described and the mechanisms underlying antibody
stimulation and suppression are explained in detail. Some hypotheses that
account for the network advantage are put forward and tested using three
systems with increasing idiotypic complexity. The basic rl, a simplified hybrid
ais-rl that implements idiotypic selection independently of derived
concentration levels and a full hybrid ais-rl scheme are examined. The test bed
takes the form of a simulated Pioneer robot that is required to navigate
through maze worlds detecting and tracking door markers.",2022-11-13 16:01:44
XXX,journalArticle,2009,Elena S. Vishnevskaya,Building the information kernel and the problem of recognition,,,,,http://arxiv.org/abs/0903.4513v7,"At this point in time there is a need for a new representation of different
information, to identify and organize descending its characteristics. Today,
science is a powerful tool for the description of reality - the numbers. Why
the most important property of numbers. Suppose we have a number 0.2351734, it
is clear that the figures are there in order of importance. If necessary, we
can round the number up to some value, eg 0.235. Arguably, the 0,235 - the most
important information of 0.2351734. Thus, we can reduce the size of numbers is
not losing much with the accuracy. Clearly, if learning to provide a graphical
or audio information kernel, we can provide the most relevant information,
discarding the rest. Introduction of various kinds of information in an
information kernel, is an important task, to solve many problems in artificial
intelligence and information theory.",2022-11-13 16:01:44
XXX,journalArticle,2010,"Louise A. Dennis, Michael Fisher, Nicholas Lincoln, Alexei Lisitsa, Sandor M. Veres",Agent Based Approaches to Engineering Autonomous Space Software,"EPTCS 20, 2010, pp. 63-67",,,10.4204/EPTCS.20.6,http://arxiv.org/abs/1003.0617v1,"Current approaches to the engineering of space software such as satellite
control systems are based around the development of feedback controllers using
packages such as MatLab's Simulink toolbox. These provide powerful tools for
engineering real time systems that adapt to changes in the environment but are
limited when the controller itself needs to be adapted.
  We are investigating ways in which ideas from temporal logics and agent
programming can be integrated with the use of such control systems to provide a
more powerful layer of autonomous decision making. This paper will discuss our
initial approaches to the engineering of such systems.",2022-11-13 16:01:45
XXX,journalArticle,2010,"David H. Wolpert, Gregory Benford",What does Newcomb's paradox teach us?,,,,,http://arxiv.org/abs/1003.1343v1,"In Newcomb's paradox you choose to receive either the contents of a
particular closed box, or the contents of both that closed box and another one.
Before you choose, a prediction algorithm deduces your choice, and fills the
two boxes based on that deduction. Newcomb's paradox is that game theory
appears to provide two conflicting recommendations for what choice you should
make in this scenario. We analyze Newcomb's paradox using a recent extension of
game theory in which the players set conditional probability distributions in a
Bayes net. We show that the two game theory recommendations in Newcomb's
scenario have different presumptions for what Bayes net relates your choice and
the algorithm's prediction. We resolve the paradox by proving that these two
Bayes nets are incompatible. We also show that the accuracy of the algorithm's
prediction, the focus of much previous work, is irrelevant. In addition we show
that Newcomb's scenario only provides a contradiction between game theory's
expected utility and dominance principles if one is sloppy in specifying the
underlying Bayes net. We also show that Newcomb's paradox is time-reversal
invariant; both the paradox and its resolution are unchanged if the algorithm
makes its `prediction' after you make your choice rather than before.",2022-11-13 16:01:45
XXX,journalArticle,2011,Stuart Armstrong,Anthropic decision theory,,,,,http://arxiv.org/abs/1110.6437v3,"This paper sets out to resolve how agents ought to act in the Sleeping Beauty
problem and various related anthropic (self-locating belief) problems, not
through the calculation of anthropic probabilities, but through finding the
correct decision to make. It creates an anthropic decision theory (ADT) that
decides these problems from a small set of principles. By doing so, it
demonstrates that the attitude of agents with regards to each other (selfish or
altruistic) changes the decisions they reach, and that it is very important to
take this into account. To illustrate ADT, it is then applied to two major
anthropic problems and paradoxes, the Presumptuous Philosopher and Doomsday
problems, thus resolving some issues about the probability of human extinction.",2022-11-13 16:01:46
XXX,journalArticle,2012,"Léon Bottou, Jonas Peters, Joaquin Quiñonero-Candela, Denis X. Charles, D. Max Chickering, Elon Portugaly, Dipankar Ray, Patrice Simard, Ed Snelson",Counterfactual Reasoning and Learning Systems,,,,,http://arxiv.org/abs/1209.2355v5,"This work shows how to leverage causal inference to understand the behavior
of complex learning systems interacting with their environment and predict the
consequences of changes to the system. Such predictions allow both humans and
algorithms to select changes that improve both the short-term and long-term
performance of such systems. This work is illustrated by experiments carried
out on the ad placement system associated with the Bing search engine.",2022-11-13 16:01:46
XXX,journalArticle,2012,"Abhimanu Kumar, Jason Baldridge, Matthew Lease, Joydeep Ghosh",Dating Texts without Explicit Temporal Cues,,,,,http://arxiv.org/abs/1211.2290v1,"This paper tackles temporal resolution of documents, such as determining when
a document is about or when it was written, based only on its text. We apply
techniques from information retrieval that predict dates via language models
over a discretized timeline. Unlike most previous works, we rely {\it solely}
on temporal cues implicit in the text. We consider both document-likelihood and
divergence based techniques and several smoothing methods for both of them. Our
best model predicts the mid-point of individuals' lives with a median of 22 and
mean error of 36 years for Wikipedia biographies from 3800 B.C. to the present
day. We also show that this approach works well when training on such
biographies and predicting dates both for non-biographical Wikipedia pages
about specific years (500 B.C. to 2010 A.D.) and for publication dates of short
stories (1798 to 2008). Together, our work shows that, even in absence of
temporal extraction resources, it is possible to achieve remarkable temporal
locality across a diverse set of texts.",2022-11-13 16:01:47
XXX,journalArticle,2012,"Daniel Faria, Catia Pesquita, Emanuel Santos, Francisco M. Couto, Cosmin Stroe, Isabel F. Cruz",Testing the AgreementMaker System in the Anatomy Task of OAEI 2012,,,,,http://arxiv.org/abs/1212.1625v1,"The AgreementMaker system was the leading system in the anatomy task of the
Ontology Alignment Evaluation Initiative (OAEI) competition in 2011. While
AgreementMaker did not compete in OAEI 2012, here we report on its performance
in the 2012 anatomy task, using the same configurations of AgreementMaker
submitted to OAEI 2011. Additionally, we also test AgreementMaker using an
updated version of the UBERON ontology as a mediating ontology, and otherwise
identical configurations. AgreementMaker achieved an F-measure of 91.8% with
the 2011 configurations, and an F-measure of 92.2% with the updated UBERON
ontology. Thus, AgreementMaker would have been the second best system had it
competed in the anatomy task of OAEI 2012, and only 0.1% below the F-measure of
the best system.",2022-11-13 16:01:47
XXX,journalArticle,2013,"Yoram Moses, Marcia K. Shamo",A Knowledge-based Treatment of Human-Automation Systems,,,,,http://arxiv.org/abs/1307.2191v1,"In a supervisory control system the human agent knowledge of past, current,
and future system behavior is critical for system performance. Being able to
reason about that knowledge in a precise and structured manner is central to
effective system design. In this paper we introduce the application of a
well-established formal approach to reasoning about knowledge to the modeling
and analysis of complex human-automation systems. An intuitive notion of
knowledge in human-automation systems is sketched and then cast as a formal
model. We present a case study in which the approach is used to model and
reason about a classic problem from the human-automation systems literature;
the results of our analysis provide evidence for the validity and value of
reasoning about complex systems in terms of the knowledge of the system agents.
To conclude, we discuss research directions that will extend this approach, and
note several systems in the aviation and human-robot team domains that are of
particular interest.",2022-11-13 16:01:48
XXX,journalArticle,2013,"Christoph Benzmüller, Bruno Woltzenlogel Paleo","Formalization, Mechanization and Automation of Gödel's Proof of God's Existence","Frontiers in Artificial Intelligence and Applications, Volume 263:
  ECAI 2014",,,10.3233/978-1-61499-419-0-93,http://arxiv.org/abs/1308.4526v5,"G\""odel's ontological proof has been analysed for the first-time with an
unprecedent degree of detail and formality with the help of higher-order
theorem provers. The following has been done (and in this order): A detailed
natural deduction proof. A formalization of the axioms, definitions and
theorems in the TPTP THF syntax. Automatic verification of the consistency of
the axioms and definitions with Nitpick. Automatic demonstration of the
theorems with the provers LEO-II and Satallax. A step-by-step formalization
using the Coq proof assistant. A formalization using the Isabelle proof
assistant, where the theorems (and some additional lemmata) have been automated
with Sledgehammer and Metis.",2022-11-13 16:01:48
XXX,journalArticle,2014,"Yaakov Gal, Avi Pfeffer",Networks of Influence Diagrams: A Formalism for Representing Agents' Beliefs and Decision-Making Processes,"Journal Of Artificial Intelligence Research, Volume 33, pages
  109-147, 2008",,,10.1613/jair.2503,http://arxiv.org/abs/1401.3426v1,"This paper presents Networks of Influence Diagrams (NID), a compact, natural
and highly expressive language for reasoning about agents beliefs and
decision-making processes. NIDs are graphical structures in which agents mental
models are represented as nodes in a network; a mental model for an agent may
itself use descriptions of the mental models of other agents. NIDs are
demonstrated by examples, showing how they can be used to describe conflicting
and cyclic belief structures, and certain forms of bounded rationality. In an
opponent modeling domain, NIDs were able to outperform other computational
agents whose strategies were not known in advance. NIDs are equivalent in
representation to Bayesian games but they are more compact and structured than
this formalism. In particular, the equilibrium definition for NIDs makes an
explicit distinction between agents optimal strategies, and how they actually
behave in reality.",2022-11-13 16:01:49
XXX,journalArticle,2014,"Wei Bai, Emmanuel M. Tadjouddine, Yu Guo",Enabling Automatic Certification of Online Auctions,"EPTCS 147, 2014, pp. 123-132",,,10.4204/EPTCS.147.9,http://arxiv.org/abs/1404.0854v1,"We consider the problem of building up trust in a network of online auctions
by software agents. This requires agents to have a deeper understanding of
auction mechanisms and be able to verify desirable properties of a given
mechanism. We have shown how these mechanisms can be formalised as semantic web
services in OWL-S, a good enough expressive machine-readable formalism enabling
software agents, to discover, invoke, and execute a web service. We have also
used abstract interpretation to translate the auction's specifications from
OWL-S, based on description logic, to COQ, based on typed lambda calculus, in
order to enable automatic verification of desirable properties of the auction
by the software agents. For this language translation, we have discussed the
syntactic transformation as well as the semantics connections between both
concrete and abstract domains. This work contributes to the implementation of
the vision of agent-mediated e-commerce systems.",2022-11-13 16:01:50
XXX,journalArticle,2014,"Matthias Englert, Sandra Siebert, Martin Ziegler",Logical Limitations to Machine Ethics with Consequences to Lethal Autonomous Weapons,,,,,http://arxiv.org/abs/1411.2842v1,"Lethal Autonomous Weapons promise to revolutionize warfare -- and raise a
multitude of ethical and legal questions. It has thus been suggested to program
values and principles of conduct (such as the Geneva Conventions) into the
machines' control, thereby rendering them both physically and morally superior
to human combatants.
  We employ mathematical logic and theoretical computer science to explore
fundamental limitations to the moral behaviour of intelligent machines in a
series of ""Gedankenexperiments"": Refining and sharpening variants of the
Trolley Problem leads us to construct an (admittedly artificial but) fully
deterministic situation where a robot is presented with two choices: one
morally clearly preferable over the other -- yet, based on the undecidability
of the Halting problem, it provably cannot decide algorithmically which one.
Our considerations have surprising implications to the question of
responsibility and liability for an autonomous system's actions and lead to
specific technical recommendations.",2022-11-13 16:01:50
XXX,journalArticle,2015,"Krzysztof Chalupka, Pietro Perona, Frederick Eberhardt",Multi-Level Cause-Effect Systems,,,,,http://arxiv.org/abs/1512.07942v1,"We present a domain-general account of causation that applies to settings in
which macro-level causal relations between two systems are of interest, but the
relevant causal features are poorly understood and have to be aggregated from
vast arrays of micro-measurements. Our approach generalizes that of Chalupka et
al. (2015) to the setting in which the macro-level effect is not specified. We
formalize the connection between micro- and macro-variables in such situations
and provide a coherent framework describing causal relations at multiple levels
of analysis. We present an algorithm that discovers macro-variable causes and
effects from micro-level measurements obtained from an experiment. We further
show how to design experiments to discover macro-variables from observational
micro-variable data. Finally, we show that under specific conditions, one can
identify multiple levels of causal structure. Throughout the article, we use a
simulated neuroscience multi-unit recording experiment to illustrate the ideas
and the algorithms.",2022-11-13 16:01:51
XXX,journalArticle,2016,"Siddharth Reddy, Igor Labutov, Thorsten Joachims",Latent Skill Embedding for Personalized Lesson Sequence Recommendation,,,,,http://arxiv.org/abs/1602.07029v1,"Students in online courses generate large amounts of data that can be used to
personalize the learning process and improve quality of education. In this
paper, we present the Latent Skill Embedding (LSE), a probabilistic model of
students and educational content that can be used to recommend personalized
sequences of lessons with the goal of helping students prepare for specific
assessments. Akin to collaborative filtering for recommender systems, the
algorithm does not require students or content to be described by features, but
it learns a representation using access traces. We formulate this problem as a
regularized maximum-likelihood embedding of students, lessons, and assessments
from historical student-content interactions. An empirical evaluation on
large-scale data from Knewton, an adaptive learning technology company, shows
that this approach predicts assessment results competitively with benchmark
models and is able to discriminate between lesson sequences that lead to
mastery and failure.",2022-11-13 16:01:51
XXX,journalArticle,2016,"Kai Arulkumaran, Nat Dilokthanakul, Murray Shanahan, Anil Anthony Bharath",Classifying Options for Deep Reinforcement Learning,,,,,http://arxiv.org/abs/1604.08153v3,"In this paper we combine one method for hierarchical reinforcement learning -
the options framework - with deep Q-networks (DQNs) through the use of
different ""option heads"" on the policy network, and a supervisory network for
choosing between the different options. We utilise our setup to investigate the
effects of architectural constraints in subtasks with positive and negative
transfer, across a range of network capacities. We empirically show that our
augmented DQN has lower sample complexity when simultaneously learning subtasks
with negative transfer, without degrading performance when learning subtasks
with positive transfer.",2022-11-13 16:01:52
XXX,journalArticle,2016,"Michael Crosscombe, Jonathan Lawry",Exploiting Vagueness for Multi-Agent Consensus,,,,10.1007/978-981-10-2564-8_5,http://arxiv.org/abs/1607.05540v2,"A framework for consensus modelling is introduced using Kleene's three valued
logic as a means to express vagueness in agents' beliefs. Explicitly borderline
cases are inherent to propositions involving vague concepts where sentences of
a propositional language may be absolutely true, absolutely false or
borderline. By exploiting these intermediate truth values, we can allow agents
to adopt a more vague interpretation of underlying concepts in order to weaken
their beliefs and reduce the levels of inconsistency, so as to achieve
consensus. We consider a consensus combination operation which results in
agents adopting the borderline truth value as a shared viewpoint if they are in
direct conflict. Simulation experiments are presented which show that applying
this operator to agents chosen at random (subject to a consistency threshold)
from a population, with initially diverse opinions, results in convergence to a
smaller set of more precise shared beliefs. Furthermore, if the choice of
agents for combination is dependent on the payoff of their beliefs, this acting
as a proxy for performance or usefulness, then the system converges to beliefs
which, on average, have higher payoff.",2022-11-13 16:01:52
XXX,journalArticle,2016,"Ethan Fast, Eric Horvitz",Long-Term Trends in the Public Perception of Artificial Intelligence,,,,,http://arxiv.org/abs/1609.04904v2,"Analyses of text corpora over time can reveal trends in beliefs, interest,
and sentiment about a topic. We focus on views expressed about artificial
intelligence (AI) in the New York Times over a 30-year period. General
interest, awareness, and discussion about AI has waxed and waned since the
field was founded in 1956. We present a set of measures that captures levels of
engagement, measures of pessimism and optimism, the prevalence of specific
hopes and concerns, and topics that are linked to discussions about AI over
decades. We find that discussion of AI has increased sharply since 2009, and
that these discussions have been consistently more optimistic than pessimistic.
However, when we examine specific concerns, we find that worries of loss of
control of AI, ethical concerns for AI, and the negative impact of AI on work
have grown in recent years. We also find that hopes for AI in healthcare and
education have increased over time.",2022-11-13 16:01:53
XXX,journalArticle,2016,"Paolo Izzo, Hongyang Qu, Sandor M. Veres",A stochastically verifiable autonomous control architecture with reasoning,,,,,http://arxiv.org/abs/1611.03372v1,"A new agent architecture called Limited Instruction Set Agent (LISA) is
introduced for autonomous control. The new architecture is based on previous
implementations of AgentSpeak and it is structurally simpler than its
predecessors with the aim of facilitating design-time and run-time verification
methods. The process of abstracting the LISA system to two different types of
discrete probabilistic models (DTMC and MDP) is investigated and illustrated.
The LISA system provides a tool for complete modelling of the agent and the
environment for probabilistic verification. The agent program can be
automatically compiled into a DTMC or a MDP model for verification with Prism.
The automatically generated Prism model can be used for both design-time and
run-time verification. The run-time verification is investigated and
illustrated in the LISA system as an internal modelling mechanism for
prediction of future outcomes.",2022-11-13 16:01:53
XXX,journalArticle,2016,"Ofir Nachum, Mohammad Norouzi, Dale Schuurmans",Improving Policy Gradient by Exploring Under-appreciated Rewards,,,,,http://arxiv.org/abs/1611.09321v3,"This paper presents a novel form of policy gradient for model-free
reinforcement learning (RL) with improved exploration properties. Current
policy-based methods use entropy regularization to encourage undirected
exploration of the reward landscape, which is ineffective in high dimensional
spaces with sparse rewards. We propose a more directed exploration strategy
that promotes exploration of under-appreciated reward regions. An action
sequence is considered under-appreciated if its log-probability under the
current policy under-estimates its resulting reward. The proposed exploration
strategy is easy to implement, requiring small modifications to an
implementation of the REINFORCE algorithm. We evaluate the approach on a set of
algorithmic tasks that have long challenged RL methods. Our approach reduces
hyper-parameter sensitivity and demonstrates significant improvements over
baseline methods. Our algorithm successfully solves a benchmark multi-digit
addition task and generalizes to long sequences. This is, to our knowledge, the
first time that a pure RL method has solved addition using only reward
feedback.",2022-11-13 16:01:53
XXX,journalArticle,2017,"Andrew Slavin Ross, Michael C. Hughes, Finale Doshi-Velez",Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations,,,,,http://arxiv.org/abs/1703.03717v2,"Neural networks are among the most accurate supervised learning methods in
use today, but their opacity makes them difficult to trust in critical
applications, especially when conditions in training differ from those in test.
Recent work on explanations for black-box models has produced tools (e.g. LIME)
to show the implicit rules behind predictions, which can help us identify when
models are right for the wrong reasons. However, these methods do not scale to
explaining entire datasets and cannot correct the problems they reveal. We
introduce a method for efficiently explaining and regularizing differentiable
models by examining and selectively penalizing their input gradients, which
provide a normal to the decision boundary. We apply these penalties both based
on expert annotation and in an unsupervised fashion that encourages diverse
models with qualitatively different decision boundaries for the same
classification problem. On multiple datasets, we show our approach generates
faithful explanations and models that generalize much better when conditions
differ between training and test.",2022-11-13 16:01:54
XXX,journalArticle,2017,"Tambet Matiisen, Avital Oliver, Taco Cohen, John Schulman",Teacher-Student Curriculum Learning,,,,,http://arxiv.org/abs/1707.00183v2,"We propose Teacher-Student Curriculum Learning (TSCL), a framework for
automatic curriculum learning, where the Student tries to learn a complex task
and the Teacher automatically chooses subtasks from a given set for the Student
to train on. We describe a family of Teacher algorithms that rely on the
intuition that the Student should practice more those tasks on which it makes
the fastest progress, i.e. where the slope of the learning curve is highest. In
addition, the Teacher algorithms address the problem of forgetting by also
choosing tasks where the Student's performance is getting worse. We demonstrate
that TSCL matches or surpasses the results of carefully hand-crafted curricula
in two tasks: addition of decimal numbers with LSTM and navigation in
Minecraft. Using our automatically generated curriculum enabled to solve a
Minecraft maze that could not be solved at all when training directly on
solving the maze, and the learning was an order of magnitude faster than
uniform sampling of subtasks.",2022-11-13 16:01:55
XXX,journalArticle,2017,"Mohammed Alshiekh, Roderick Bloem, Ruediger Ehlers, Bettina Könighofer, Scott Niekum, Ufuk Topcu",Safe Reinforcement Learning via Shielding,,,,,http://arxiv.org/abs/1708.08611v2,"Reinforcement learning algorithms discover policies that maximize reward, but
do not necessarily guarantee safety during learning or execution phases. We
introduce a new approach to learn optimal policies while enforcing properties
expressed in temporal logic. To this end, given the temporal logic
specification that is to be obeyed by the learning system, we propose to
synthesize a reactive system called a shield. The shield is introduced in the
traditional learning process in two alternative ways, depending on the location
at which the shield is implemented. In the first one, the shield acts each time
the learning agent is about to make a decision and provides a list of safe
actions. In the second way, the shield is introduced after the learning agent.
The shield monitors the actions from the learner and corrects them only if the
chosen action causes a violation of the specification. We discuss which
requirements a shield must meet to preserve the convergence guarantees of the
learner. Finally, we demonstrate the versatility of our approach on several
challenging reinforcement learning scenarios.",2022-11-13 16:01:55
XXX,journalArticle,2018,Daniel Estrada,"Value Alignment, Fair Play, and the Rights of Service Robots",ACM/AIES 2018,,,10.1145/3278721.3278730,http://arxiv.org/abs/1803.02852v1,"Ethics and safety research in artificial intelligence is increasingly framed
in terms of ""alignment"" with human values and interests. I argue that Turing's
call for ""fair play for machines"" is an early and often overlooked contribution
to the alignment literature. Turing's appeal to fair play suggests a need to
correct human behavior to accommodate our machines, a surprising inversion of
how value alignment is treated today. Reflections on ""fair play"" motivate a
novel interpretation of Turing's notorious ""imitation game"" as a condition not
of intelligence but instead of value alignment: a machine demonstrates a
minimal degree of alignment (with the norms of conversation, for instance) when
it can go undetected when interrogated by a human. I carefully distinguish this
interpretation from the Moral Turing Test, which is not motivated by a
principle of fair play, but instead depends on imitation of human moral
behavior. Finally, I consider how the framework of fair play can be used to
situate the debate over robot rights within the alignment literature. I argue
that extending rights to service robots operating in public spaces is ""fair"" in
precisely the sense that it encourages an alignment of interests between humans
and machines.",2022-11-13 16:01:56
XXX,journalArticle,2018,"Jade Shi, Rhiju Das, Vijay S. Pande",SentRNA: Improving computational RNA design by incorporating a prior of human design strategies,,,,,http://arxiv.org/abs/1803.03146v2,"Solving the RNA inverse folding problem is a critical prerequisite to RNA
design, an emerging field in bioengineering with a broad range of applications
from reaction catalysis to cancer therapy. Although significant progress has
been made in developing machine-based inverse RNA folding algorithms, current
approaches still have difficulty designing sequences for large or complex
targets. On the other hand, human players of the online RNA design game EteRNA
have consistently shown superior performance in this regard, being able to
readily design sequences for targets that are challenging for machine
algorithms. Here we present a novel approach to the RNA design problem,
SentRNA, a design agent consisting of a fully-connected neural network trained
end-to-end using human-designed RNA sequences. We show that through this
approach, SentRNA can solve complex targets previously unsolvable by any
machine-based approach and achieve state-of-the-art performance on two separate
challenging test sets. Our results demonstrate that incorporating human design
strategies into a design algorithm can significantly boost machine performance
and suggests a new paradigm for machine-based RNA design.",2022-11-13 16:01:56
XXX,journalArticle,2018,"Henrik Aslund, El Mahdi El Mhamdi, Rachid Guerraoui, Alexandre Maurer",Virtuously Safe Reinforcement Learning,,,,,http://arxiv.org/abs/1805.11447v1,"We show that when a third party, the adversary, steps into the two-party
setting (agent and operator) of safely interruptible reinforcement learning, a
trade-off has to be made between the probability of following the optimal
policy in the limit, and the probability of escaping a dangerous situation
created by the adversary. So far, the work on safely interruptible agents has
assumed a perfect perception of the agent about its environment (no adversary),
and therefore implicitly set the second probability to zero, by explicitly
seeking a value of one for the first probability. We show that (1) agents can
be made both interruptible and adversary-resilient, and (2) the
interruptibility can be made safe in the sense that the agent itself will not
seek to avoid it. We also solve the problem that arises when the agent does not
go completely greedy, i.e. issues with safe exploration in the limit.
Resilience to perturbed perception, safe exploration in the limit, and safe
interruptibility are the three pillars of what we call \emph{virtuously safe
reinforcement learning}.",2022-11-13 16:01:57
XXX,journalArticle,2018,"Victoria Krakovna, Laurent Orseau, Ramana Kumar, Miljan Martic, Shane Legg",Penalizing side effects using stepwise relative reachability,,,,,http://arxiv.org/abs/1806.01186v2,"How can we design safe reinforcement learning agents that avoid unnecessary
disruptions to their environment? We show that current approaches to penalizing
side effects can introduce bad incentives, e.g. to prevent any irreversible
changes in the environment, including the actions of other agents. To isolate
the source of such undesirable incentives, we break down side effects penalties
into two components: a baseline state and a measure of deviation from this
baseline state. We argue that some of these incentives arise from the choice of
baseline, and others arise from the choice of deviation measure. We introduce a
new variant of the stepwise inaction baseline and a new deviation measure based
on relative reachability of states. The combination of these design choices
avoids the given undesirable incentives, while simpler baselines and the
unreachability measure fail. We demonstrate this empirically by comparing
different combinations of baseline and deviation measure choices on a set of
gridworld experiments designed to illustrate possible bad incentives.",2022-11-13 16:01:58
XXX,journalArticle,2018,"Cinjon Resnick, Roberta Raileanu, Sanyam Kapoor, Alexander Peysakhovich, Kyunghyun Cho, Joan Bruna","Backplay: ""Man muss immer umkehren""",,,,,http://arxiv.org/abs/1807.06919v5,"Model-free reinforcement learning (RL) requires a large number of trials to
learn a good policy, especially in environments with sparse rewards. We explore
a method to improve the sample efficiency when we have access to
demonstrations. Our approach, Backplay, uses a single demonstration to
construct a curriculum for a given task. Rather than starting each training
episode in the environment's fixed initial state, we start the agent near the
end of the demonstration and move the starting point backwards during the
course of training until we reach the initial state. Our contributions are that
we analytically characterize the types of environments where Backplay can
improve training speed, demonstrate the effectiveness of Backplay both in large
grid worlds and a complex four player zero-sum game (Pommerman), and show that
Backplay compares favorably to other competitive methods known to improve
sample efficiency. This includes reward shaping, behavioral cloning, and
reverse curriculum generation.",2022-11-13 16:01:58
XXX,journalArticle,2018,"Jonathan Lacotte, Mohammad Ghavamzadeh, Yinlam Chow, Marco Pavone",Risk-Sensitive Generative Adversarial Imitation Learning,,,,,http://arxiv.org/abs/1808.04468v2,"We study risk-sensitive imitation learning where the agent's goal is to
perform at least as well as the expert in terms of a risk profile. We first
formulate our risk-sensitive imitation learning setting. We consider the
generative adversarial approach to imitation learning (GAIL) and derive an
optimization problem for our formulation, which we call it risk-sensitive GAIL
(RS-GAIL). We then derive two different versions of our RS-GAIL optimization
problem that aim at matching the risk profiles of the agent and the expert
w.r.t. Jensen-Shannon (JS) divergence and Wasserstein distance, and develop
risk-sensitive generative adversarial imitation learning algorithms based on
these optimization problems. We evaluate the performance of our algorithms and
compare them with GAIL and the risk-averse imitation learning (RAIL) algorithms
in two MuJoCo and two OpenAI classical control tasks.",2022-11-13 16:01:59
XXX,journalArticle,2018,"Christopher Iliffe Sprague, Petter Ögren",Adding Neural Network Controllers to Behavior Trees without Destroying Performance Guarantees,,,,,http://arxiv.org/abs/1809.10283v3,"In this paper, we show how Behavior Trees that have performance guarantees,
in terms of safety and goal convergence, can be extended with components that
were designed using machine learning, without destroying those performance
guarantees.
  Machine learning approaches such as reinforcement learning or learning from
demonstration can be very appealing to AI designers that want efficient and
realistic behaviors in their agents. However, those algorithms seldom provide
guarantees for solving the given task in all different situations while keeping
the agent safe. Instead, such guarantees are often easier to find for manually
designed model-based approaches. In this paper we exploit the modularity of
behavior trees to extend a given design with an efficient, but possibly
unreliable, machine learning component in a way that preserves the guarantees.
The approach is illustrated with an inverted pendulum example.",2022-11-13 16:02:00
XXX,journalArticle,2018,Andrew Slavin Ross,Training Machine Learning Models by Regularizing their Explanations,,,,,http://arxiv.org/abs/1810.00869v1,"Neural networks are among the most accurate supervised learning methods in
use today. However, their opacity makes them difficult to trust in critical
applications, especially when conditions in training may differ from those in
practice. Recent efforts to develop explanations for neural networks and
machine learning models more generally have produced tools to shed light on the
implicit rules behind predictions. These tools can help us identify when models
are right for the wrong reasons. However, they do not always scale to
explaining predictions for entire datasets, are not always at the right level
of abstraction, and most importantly cannot correct the problems they reveal.
In this thesis, we explore the possibility of training machine learning models
(with a particular focus on neural networks) using explanations themselves. We
consider approaches where models are penalized not only for making incorrect
predictions but also for providing explanations that are either inconsistent
with domain knowledge or overly complex. These methods let us train models
which can not only provide more interpretable rationales for their predictions
but also generalize better when training data is confounded or meaningfully
different from test data (even adversarially so).",2022-11-13 16:02:00
XXX,journalArticle,2018,"Brett W Israelsen, Nisar R Ahmed, Eric Frew, Dale Lawrence, Brian Argrow",Factorized Machine Self-Confidence for Decision-Making Agents,,,,,http://arxiv.org/abs/1810.06519v2,"Algorithmic assurances from advanced autonomous systems assist human users in
understanding, trusting, and using such systems appropriately. Designing these
systems with the capacity of assessing their own capabilities is one approach
to creating an algorithmic assurance. The idea of `machine self-confidence' is
introduced for autonomous systems. Using a factorization based framework for
self-confidence assessment, one component of self-confidence, called
`solver-quality', is discussed in the context of Markov decision processes for
autonomous systems. Markov decision processes underlie much of the theory of
reinforcement learning, and are commonly used for planning and decision making
under uncertainty in robotics and autonomous systems. A `solver quality' metric
is formally defined in the context of decision making algorithms based on
Markov decision processes. A method for assessing solver quality is then
derived, drawing inspiration from empirical hardness models. Finally, numerical
experiments for an unmanned autonomous vehicle navigation problem under
different solver, parameter, and environment conditions indicate that the
self-confidence metric exhibits the desired properties. Discussion of results,
and avenues for future investigation are included.",2022-11-13 16:02:00
XXX,journalArticle,2018,"Guansong Lu, Zhiming Zhou, Yuxuan Song, Kan Ren, Yong Yu",Guiding the One-to-one Mapping in CycleGAN via Optimal Transport,,,,,http://arxiv.org/abs/1811.06284v1,"CycleGAN is capable of learning a one-to-one mapping between two data
distributions without paired examples, achieving the task of unsupervised data
translation. However, there is no theoretical guarantee on the property of the
learned one-to-one mapping in CycleGAN. In this paper, we experimentally find
that, under some circumstances, the one-to-one mapping learned by CycleGAN is
just a random one within the large feasible solution space. Based on this
observation, we explore to add extra constraints such that the one-to-one
mapping is controllable and satisfies more properties related to specific
tasks. We propose to solve an optimal transport mapping restrained by a
task-specific cost function that reflects the desired properties, and use the
barycenters of optimal transport mapping to serve as references for CycleGAN.
Our experiments indicate that the proposed algorithm is capable of learning a
one-to-one mapping with the desired properties.",2022-11-13 16:02:01
XXX,journalArticle,2018,"Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, Shane Legg",Scalable agent alignment via reward modeling: a research direction,,,,,http://arxiv.org/abs/1811.07871v1,"One obstacle to applying reinforcement learning algorithms to real-world
problems is the lack of suitable reward functions. Designing such reward
functions is difficult in part because the user only has an implicit
understanding of the task objective. This gives rise to the agent alignment
problem: how do we create agents that behave in accordance with the user's
intentions? We outline a high-level research direction to solve the agent
alignment problem centered around reward modeling: learning a reward function
from interaction with the user and optimizing the learned reward function with
reinforcement learning. We discuss the key challenges we expect to face when
scaling reward modeling to complex and general domains, concrete approaches to
mitigate these challenges, and ways to establish trust in the resulting agents.",2022-11-13 16:02:01
XXX,journalArticle,2019,"Thanh Thi Nguyen, Ngoc Duy Nguyen, Fernando Bello, Saeid Nahavandi",A New Tensioning Method using Deep Reinforcement Learning for Surgical Pattern Cutting,2019 IEEE International Conference on Industrial Technology (ICIT),,,10.1109/ICIT.2019.8755235,http://arxiv.org/abs/1901.03327v1,"Surgeons normally need surgical scissors and tissue grippers to cut through a
deformable surgical tissue. The cutting accuracy depends on the skills to
manipulate these two tools. Such skills are part of basic surgical skills
training as in the Fundamentals of Laparoscopic Surgery. The gripper is used to
pinch a point on the surgical sheet and pull the tissue to a certain direction
to maintain the tension while the scissors cut through a trajectory. As the
surgical materials are deformable, it requires a comprehensive tensioning
policy to yield appropriate tensioning direction at each step of the cutting
process. Automating a tensioning policy for a given cutting trajectory will
support not only the human surgeons but also the surgical robots to improve the
cutting accuracy and reliability. This paper presents a multiple pinch point
approach to modelling an autonomous tensioning planner based on a deep
reinforcement learning algorithm. Experiments on a simulator show that the
proposed method is superior to existing methods in terms of both performance
and robustness.",2022-11-13 16:02:02
XXX,journalArticle,2019,"Ismail Akrout, Amal Feriani, Mohamed Akrout",Hacking Google reCAPTCHA v3 using Reinforcement Learning,,,,,http://arxiv.org/abs/1903.01003v3,"We present a Reinforcement Learning (RL) methodology to bypass Google
reCAPTCHA v3. We formulate the problem as a grid world where the agent learns
how to move the mouse and click on the reCAPTCHA button to receive a high
score. We study the performance of the agent when we vary the cell size of the
grid world and show that the performance drops when the agent takes big steps
toward the goal. Finally, we used a divide and conquer strategy to defeat the
reCAPTCHA system for any grid resolution. Our proposed method achieves a
success rate of 97.4% on a 100x100 grid and 96.7% on a 1000x1000 screen
resolution.",2022-11-13 16:02:02
XXX,journalArticle,2019,Scott H. Hawley,Challenges for an Ontology of Artificial Intelligence,,,,,http://arxiv.org/abs/1903.03171v1,"Of primary importance in formulating a response to the increasing prevalence
and power of artificial intelligence (AI) applications in society are questions
of ontology. Questions such as: What ""are"" these systems? How are they to be
regarded? How does an algorithm come to be regarded as an agent? We discuss
three factors which hinder discussion and obscure attempts to form a clear
ontology of AI: (1) the various and evolving definitions of AI, (2) the
tendency for pre-existing technologies to be assimilated and regarded as
""normal,"" and (3) the tendency of human beings to anthropomorphize. This list
is not intended as exhaustive, nor is it seen to preclude entirely a clear
ontology, however, these challenges are a necessary set of topics for
consideration. Each of these factors is seen to present a 'moving target' for
discussion, which poses a challenge for both technical specialists and
non-practitioners of AI systems development (e.g., philosophers and
theologians) to speak meaningfully given that the corpus of AI structures and
capabilities evolves at a rapid pace. Finally, we present avenues for moving
forward, including opportunities for collaborative synthesis for scholars in
philosophy and science.",2022-11-13 16:02:03
XXX,journalArticle,2019,"Yash Goyal, Ziyan Wu, Jan Ernst, Dhruv Batra, Devi Parikh, Stefan Lee",Counterfactual Visual Explanations,,,,,http://arxiv.org/abs/1904.07451v2,"In this work, we develop a technique to produce counterfactual visual
explanations. Given a 'query' image $I$ for which a vision system predicts
class $c$, a counterfactual visual explanation identifies how $I$ could change
such that the system would output a different specified class $c'$. To do this,
we select a 'distractor' image $I'$ that the system predicts as class $c'$ and
identify spatial regions in $I$ and $I'$ such that replacing the identified
region in $I$ with the identified region in $I'$ would push the system towards
classifying $I$ as $c'$. We apply our approach to multiple image classification
datasets generating qualitative results showcasing the interpretability and
discriminativeness of our counterfactual explanations. To explore the
effectiveness of our explanations in teaching humans, we present machine
teaching experiments for the task of fine-grained bird classification. We find
that users trained to distinguish bird species fare better when given access to
counterfactual explanations in addition to training examples.",2022-11-13 16:02:03
XXX,journalArticle,2019,Mario Gleirscher,Risk Structures: Towards Engineering Risk-aware Autonomous Systems,,,,10.1007/s00165-021-00545-4,http://arxiv.org/abs/1904.10386v1,"Inspired by widely-used techniques of causal modelling in risk, failure, and
accident analysis, this work discusses a compositional framework for risk
modelling. Risk models capture fragments of the space of risky events likely to
occur when operating a machine in a given environment. Moreover, one can build
such models into machines such as autonomous robots, to equip them with the
ability of risk-aware perception, monitoring, decision making, and control.
With the notion of a risk factor as the modelling primitive, the framework
provides several means to construct and shape risk models. Relational and
algebraic properties are investigated and proofs support the validity and
consistency of these properties over the corresponding models. Several examples
throughout the discussion illustrate the applicability of the concepts.
Overall, this work focuses on the qualitative treatment of risk with the
outlook of transferring these results to probabilistic refinements of the
discussed framework.",2022-11-13 16:02:04
XXX,journalArticle,2019,"Radoslaw Martin Cichy, Gemma Roig, Alex Andonian, Kshitij Dwivedi, Benjamin Lahner, Alex Lascelles, Yalda Mohsenzadeh, Kandan Ramakrishnan, Aude Oliva",The Algonauts Project: A Platform for Communication between the Sciences of Biological and Artificial Intelligence,,,,,http://arxiv.org/abs/1905.05675v1,"In the last decade, artificial intelligence (AI) models inspired by the brain
have made unprecedented progress in performing real-world perceptual tasks like
object classification and speech recognition. Recently, researchers of natural
intelligence have begun using those AI models to explore how the brain performs
such tasks. These developments suggest that future progress will benefit from
increased interaction between disciplines. Here we introduce the Algonauts
Project as a structured and quantitative communication channel for
interdisciplinary interaction between natural and artificial intelligence
researchers. The project's core is an open challenge with a quantitative
benchmark whose goal is to account for brain data through computational models.
This project has the potential to provide better models of natural intelligence
and to gather findings that advance AI. The 2019 Algonauts Project focuses on
benchmarking computational models predicting human brain activity when people
look at pictures of objects. The 2019 edition of the Algonauts Project is
available online: http://algonauts.csail.mit.edu/.",2022-11-13 16:02:04
XXX,journalArticle,2019,"Sebastian Tschiatschek, Ahana Ghosh, Luis Haug, Rati Devidze, Adish Singla",Learner-aware Teaching: Inverse Reinforcement Learning with Preferences and Constraints,,,,,http://arxiv.org/abs/1906.00429v2,"Inverse reinforcement learning (IRL) enables an agent to learn complex
behavior by observing demonstrations from a (near-)optimal policy. The typical
assumption is that the learner's goal is to match the teacher's demonstrated
behavior. In this paper, we consider the setting where the learner has its own
preferences that it additionally takes into consideration. These preferences
can for example capture behavioral biases, mismatched worldviews, or physical
constraints. We study two teaching approaches: learner-agnostic teaching, where
the teacher provides demonstrations from an optimal policy ignoring the
learner's preferences, and learner-aware teaching, where the teacher accounts
for the learner's preferences. We design learner-aware teaching algorithms and
show that significant performance improvements can be achieved over
learner-agnostic teaching.",2022-11-13 16:02:05
XXX,journalArticle,2019,"Noel C. F. Codella, Michael Hind, Karthikeyan Natesan Ramamurthy, Murray Campbell, Amit Dhurandhar, Kush R. Varshney, Dennis Wei, Aleksandra Mojsilović",Teaching AI to Explain its Decisions Using Embeddings and Multi-Task Learning,,,,,http://arxiv.org/abs/1906.02299v1,"Using machine learning in high-stakes applications often requires predictions
to be accompanied by explanations comprehensible to the domain user, who has
ultimate responsibility for decisions and outcomes. Recently, a new framework
for providing explanations, called TED, has been proposed to provide meaningful
explanations for predictions. This framework augments training data to include
explanations elicited from domain users, in addition to features and labels.
This approach ensures that explanations for predictions are tailored to the
complexity expectations and domain knowledge of the consumer. In this paper, we
build on this foundational work, by exploring more sophisticated instantiations
of the TED framework and empirically evaluate their effectiveness in two
diverse domains, chemical odor and skin cancer prediction. Results demonstrate
that meaningful explanations can be reliably taught to machine learning
algorithms, and in some cases, improving modeling accuracy.",2022-11-13 16:02:05
XXX,journalArticle,2019,"Angela Daly, Thilo Hagendorff, Li Hui, Monique Mann, Vidushi Marda, Ben Wagner, Wei Wang, Saskia Witteborn",Artificial Intelligence Governance and Ethics: Global Perspectives,,,,,http://arxiv.org/abs/1907.03848v1,"Artificial intelligence (AI) is a technology which is increasingly being
utilised in society and the economy worldwide, and its implementation is
planned to become more prevalent in coming years. AI is increasingly being
embedded in our lives, supplementing our pervasive use of digital technologies.
But this is being accompanied by disquiet over problematic and dangerous
implementations of AI, or indeed, even AI itself deciding to do dangerous and
problematic actions, especially in fields such as the military, medicine and
criminal justice. These developments have led to concerns about whether and how
AI systems adhere, and will adhere to ethical standards. These concerns have
stimulated a global conversation on AI ethics, and have resulted in various
actors from different countries and sectors issuing ethics and governance
initiatives and guidelines for AI. Such developments form the basis for our
research in this report, combining our international and interdisciplinary
expertise to give an insight into what is happening in Australia, China,
Europe, India and the US.",2022-11-13 16:02:06
XXX,journalArticle,2019,"Joseph Y. Halpern, Rafael Pass","A Conceptually Well-Founded Characterization of Iterated Admissibility Using an ""All I Know"" Operator","EPTCS 297, 2019, pp. 221-232",,,10.4204/EPTCS.297.15,http://arxiv.org/abs/1907.09106v1,"Brandenburger, Friedenberg, and Keisler provide an epistemic characterization
of iterated admissibility (IA), also known as iterated deletion of weakly
dominated strategies, where uncertainty is represented using LPSs
(lexicographic probability sequences). Their characterization holds in a rich
structure called a complete structure, where all types are possible. In earlier
work, we gave a characterization of iterated admissibility using an ""all I
know"" operator, that captures the intuition that ""all the agent knows"" is that
agents satisfy the appropriate rationality assumptions. That characterization
did not need complete structures and used probability structures, not LPSs.
However, that characterization did not deal with Samuelson's conceptual concern
regarding IA, namely, that at higher levels, players do not consider possible
strategies that were used to justify their choice of strategy at lower levels.
In this paper, we give a characterization of IA using the all I know operator
that does deal with Samuelson's concern. However, it uses LPSs. We then show
how to modify the characterization using notions of ""approximate belief"" and
""approximately all I know"" so as to deal with Samuelson's concern while still
working with probability structures.",2022-11-13 16:02:06
XXX,journalArticle,2019,"Alexander Serb, Themistoklis Prodromakis",A system of different layers of abstraction for artificial intelligence,,,,,http://arxiv.org/abs/1907.10508v1,"The field of artificial intelligence (AI) represents an enormous endeavour of
humankind that is currently transforming our societies down to their very
foundations. Its task, building truly intelligent systems, is underpinned by a
vast array of subfields ranging from the development of new electronic
components to mathematical formulations of highly abstract and complex
reasoning. This breadth of subfields renders it often difficult to understand
how they all fit together into a bigger picture and hides the multi-faceted,
multi-layered conceptual structure that in a sense can be said to be what AI
truly is. In this perspective we propose a system of five levels/layers of
abstraction that underpin many AI implementations. We further posit that each
layer is subject to a complexity-performance trade-off whilst different layers
are interlocked with one another in a control-complexity trade-off. This
overview provides a conceptual map that can help to identify how and where
innovation should be targeted in order to achieve different levels of
functionality, assure them for safety, optimise performance under various
operating constraints and map the opportunity space for social and economic
exploitation.",2022-11-13 16:02:07
XXX,journalArticle,2019,"Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, Igor Mordatch",Emergent Tool Use From Multi-Agent Autocurricula,,,,,http://arxiv.org/abs/1909.07528v2,"Through multi-agent competition, the simple objective of hide-and-seek, and
standard reinforcement learning algorithms at scale, we find that agents create
a self-supervised autocurriculum inducing multiple distinct rounds of emergent
strategy, many of which require sophisticated tool use and coordination. We
find clear evidence of six emergent phases in agent strategy in our
environment, each of which creates a new pressure for the opposing team to
adapt; for instance, agents learn to build multi-object shelters using moveable
boxes which in turn leads to agents discovering that they can overcome
obstacles using ramps. We further provide evidence that multi-agent competition
may scale better with increasing environment complexity and leads to behavior
that centers around far more human-relevant skills than other self-supervised
reinforcement learning methods such as intrinsic motivation. Finally, we
propose transfer and fine-tuning as a way to quantitatively evaluate targeted
capabilities, and we compare hide-and-seek agents to both intrinsic motivation
and random initialization baselines in a suite of domain-specific intelligence
tests.",2022-11-13 16:02:08
XXX,journalArticle,2019,F. Richard Yu,From the Internet of Information to the Internet of Intelligence,,,,,http://arxiv.org/abs/1909.08068v1,"In the era of the Internet of information, we have gone through layering,
cross-layer, and cross-system design paradigms. Recently, the ``curse of
modeling"" and ``curse of dimensionality"" of the cross-system design paradigm
have resulted in the popularity of using artificial intelligence (AI) to
optimize the Internet of information. However, many significant research
challenges remain to be addressed for the AI approach, including the lack of
high-quality training data due to privacy and resources constraints in this
data-driven approach. To address these challenges, we need to take a look at
humans' cooperation in a larger time scale. To facilitate cooperation in modern
history, we have built three major technologies: ``grid of transportation"",
``grid of energy"", and ``the Internet of information"". In this paper, we argue
that the next cooperation paradigm could be the ``Internet of intelligence
(Intelligence-Net)"", where intelligence can be easily obtained like energy and
information, enabled by the recent advances in blockchain technology. We
present some recent advances in these areas, and discuss some open issues and
challenges that need to be addressed in the future.",2022-11-13 16:02:08
XXX,journalArticle,2019,"Ahana Ghosh, Sebastian Tschiatschek, Hamed Mahdavi, Adish Singla",Towards Deployment of Robust AI Agents for Human-Machine Partnerships,,,,,http://arxiv.org/abs/1910.02330v2,"We study the problem of designing AI agents that can robustly cooperate with
people in human-machine partnerships. Our work is inspired by real-life
scenarios in which an AI agent, e.g., a virtual assistant, has to cooperate
with new users after its deployment. We model this problem via a parametric MDP
framework where the parameters correspond to a user's type and characterize her
behavior. In the test phase, the AI agent has to interact with a user of
unknown type. Our approach to designing a robust AI agent relies on observing
the user's actions to make inferences about the user's type and adapting its
policy to facilitate efficient cooperation. We show that without being
adaptive, an AI agent can end up performing arbitrarily bad in the test phase.
We develop two algorithms for computing policies that automatically adapt to
the user in the test phase. We demonstrate the effectiveness of our approach in
solving a two-agent collaborative task.",2022-11-13 16:02:08
XXX,journalArticle,2019,"Louis Kirsch, Sjoerd van Steenkiste, Jürgen Schmidhuber",Improving Generalization in Meta Reinforcement Learning using Learned Objectives,,,,,http://arxiv.org/abs/1910.04098v2,"Biological evolution has distilled the experiences of many learners into the
general learning algorithms of humans. Our novel meta reinforcement learning
algorithm MetaGenRL is inspired by this process. MetaGenRL distills the
experiences of many complex agents to meta-learn a low-complexity neural
objective function that decides how future individuals will learn. Unlike
recent meta-RL algorithms, MetaGenRL can generalize to new environments that
are entirely different from those used for meta-training. In some cases, it
even outperforms human-engineered RL algorithms. MetaGenRL uses off-policy
second-order gradients during meta-training that greatly increase its sample
efficiency.",2022-11-13 16:02:09
XXX,journalArticle,2019,"Erdem Bıyık, Malayandi Palan, Nicholas C. Landolfi, Dylan P. Losey, Dorsa Sadigh",Asking Easy Questions: A User-Friendly Approach to Active Reward Learning,,,,,http://arxiv.org/abs/1910.04365v1,"Robots can learn the right reward function by querying a human expert.
Existing approaches attempt to choose questions where the robot is most
uncertain about the human's response; however, they do not consider how easy it
will be for the human to answer! In this paper we explore an information gain
formulation for optimally selecting questions that naturally account for the
human's ability to answer. Our approach identifies questions that optimize the
trade-off between robot and human uncertainty, and determines when these
questions become redundant or costly. Simulations and a user study show our
method not only produces easy questions, but also ultimately results in faster
reward learning.",2022-11-13 16:02:09
XXX,journalArticle,2019,"D. Verma, S. Calo",Using AI/ML to gain situational understanding from passive network observations,,,,,http://arxiv.org/abs/1910.06266v1,"The data available in the network traffic fromany Government building
contains a significant amount ofinformation. An analysis of the traffic can
yield insightsand situational understanding about what is happening inthe
building. However, the use of traditional network packet inspection, either
deep or shallow, is useful for only a limited understanding of the environment,
with applicability limited to some aspects of network and security management.
If weuse AI/ML based techniques to understand the network traffic, we can gain
significant insights which increase our situational awareness of what is
happening in the environment.At IBM, we have created a system which uses a
combination of network domain knowledge and machine learning techniques to
convert network traffic into actionable insights about the on premise
environment. These insights include characterization of the communicating
devices, discovering unauthorized devices that may violate policy requirements,
identifying hidden components and vulnerability points, detecting leakage of
sensitive information, and identifying the presence of people and devices.In
this paper, we will describe the overall design of this system, the major
use-cases that have been identified for it, and the lessons learnt when
deploying this system for some of those use-cases",2022-11-13 16:02:10
XXX,journalArticle,2019,"Bairavi Venkatesh, Tosha Shah, Antong Chen, Soheil Ghafurian",Restoration of marker occluded hematoxylin and eosin stained whole slide histology images using generative adversarial networks,,,,,http://arxiv.org/abs/1910.06428v1,"It is common for pathologists to annotate specific regions of the tissue,
such as tumor, directly on the glass slide with markers. Although this practice
was helpful prior to the advent of histology whole slide digitization, it often
occludes important details which are increasingly relevant to immuno-oncology
due to recent advancements in digital pathology imaging techniques. The current
work uses a generative adversarial network with cycle loss to remove these
annotations while still maintaining the underlying structure of the tissue by
solving an image-to-image translation problem. We train our network on up to
300 whole slide images with marker inks and show that 70% of the corrected
image patches are indistinguishable from originally uncontaminated image tissue
to a human expert. This portion increases 97% when we replace the human expert
with a deep residual network. We demonstrated the fidelity of the method to the
original image by calculating the correlation between image gradient
magnitudes. We observed a revival of up to 94,000 nuclei per slide in our
dataset, the majority of which were located on tissue border.",2022-11-13 16:02:10
XXX,journalArticle,2019,"Dongge Han, Wendelin Boehmer, Michael Wooldridge, Alex Rogers",Multi-agent Hierarchical Reinforcement Learning with Dynamic Termination,,,,10.1007/978-3-030-29911-8_7,http://arxiv.org/abs/1910.09508v1,"In a multi-agent system, an agent's optimal policy will typically depend on
the policies chosen by others. Therefore, a key issue in multi-agent systems
research is that of predicting the behaviours of others, and responding
promptly to changes in such behaviours. One obvious possibility is for each
agent to broadcast their current intention, for example, the currently executed
option in a hierarchical reinforcement learning framework. However, this
approach results in inflexibility of agents if options have an extended
duration and are dynamic. While adjusting the executed option at each step
improves flexibility from a single-agent perspective, frequent changes in
options can induce inconsistency between an agent's actual behaviour and its
broadcast intention. In order to balance flexibility and predictability, we
propose a dynamic termination Bellman equation that allows the agents to
flexibly terminate their options. We evaluate our model empirically on a set of
multi-agent pursuit and taxi tasks, and show that our agents learn to adapt
flexibly across scenarios that require different termination behaviours.",2022-11-13 16:02:11
XXX,journalArticle,2019,"Yuval Heffetz, Roman Vainstein, Gilad Katz, Lior Rokach",DeepLine: AutoML Tool for Pipelines Generation using Deep Reinforcement Learning and Hierarchical Actions Filtering,,,,,http://arxiv.org/abs/1911.00061v1,"Automatic machine learning (AutoML) is an area of research aimed at
automating machine learning (ML) activities that currently require human
experts. One of the most challenging tasks in this field is the automatic
generation of end-to-end ML pipelines: combining multiple types of ML
algorithms into a single architecture used for end-to-end analysis of
previously-unseen data. This task has two challenging aspects: the first is the
need to explore a large search space of algorithms and pipeline architectures.
The second challenge is the computational cost of training and evaluating
multiple pipelines. In this study we present DeepLine, a reinforcement learning
based approach for automatic pipeline generation. Our proposed approach
utilizes an efficient representation of the search space and leverages past
knowledge gained from previously-analyzed datasets to make the problem more
tractable. Additionally, we propose a novel hierarchical-actions algorithm that
serves as a plugin, mediating the environment-agent interaction in deep
reinforcement learning problems. The plugin significantly speeds up the
training process of our model. Evaluation on 56 datasets shows that DeepLine
outperforms state-of-the-art approaches both in accuracy and in computational
cost.",2022-11-13 16:02:11
XXX,journalArticle,2019,"Daniel Kasenberg, Antonio Roque, Ravenna Thielstrom, Meia Chita-Tegmark, Matthias Scheutz",Generating Justifications for Norm-Related Agent Decisions,,,,,http://arxiv.org/abs/1911.00226v1,"We present an approach to generating natural language justifications of
decisions derived from norm-based reasoning. Assuming an agent which maximally
satisfies a set of rules specified in an object-oriented temporal logic, the
user can ask factual questions (about the agent's rules, actions, and the
extent to which the agent violated the rules) as well as ""why"" questions that
require the agent comparing actual behavior to counterfactual trajectories with
respect to these rules. To produce natural-sounding explanations, we focus on
the subproblem of producing natural language clauses from statements in a
fragment of temporal logic, and then describe how to embed these clauses into
explanatory sentences. We use a human judgment evaluation on a testbed task to
compare our approach to variants in terms of intelligibility, mental model and
perceived trust.",2022-11-13 16:02:12
XXX,journalArticle,2019,"Daniel Furelos-Blanco, Mark Law, Alessandra Russo, Krysia Broda, Anders Jonsson",Induction of Subgoal Automata for Reinforcement Learning,,,,,http://arxiv.org/abs/1911.13152v1,"In this work we present ISA, a novel approach for learning and exploiting
subgoals in reinforcement learning (RL). Our method relies on inducing an
automaton whose transitions are subgoals expressed as propositional formulas
over a set of observable events. A state-of-the-art inductive logic programming
system is used to learn the automaton from observation traces perceived by the
RL agent. The reinforcement learning and automaton learning processes are
interleaved: a new refined automaton is learned whenever the RL agent generates
a trace not recognized by the current automaton. We evaluate ISA in several
gridworld problems and show that it performs similarly to a method for which
automata are given in advance. We also show that the learned automata can be
exploited to speed up convergence through reward shaping and transfer learning
across multiple tasks. Finally, we analyze the running time and the number of
traces that ISA needs to learn an automata, and the impact that the number of
observable events has on the learner's performance.",2022-11-13 16:02:12
XXX,journalArticle,2019,"Ross Gruetzemacher, Jess Whittlestone",The Transformative Potential of Artificial Intelligence,,,,,http://arxiv.org/abs/1912.00747v3,"The terms 'human-level artificial intelligence' and 'artificial general
intelligence' are widely used to refer to the possibility of advanced
artificial intelligence (AI) with potentially extreme impacts on society. These
terms are poorly defined and do not necessarily indicate what is most important
with respect to future societal impacts. We suggest that the term
'transformative AI' is a helpful alternative, reflecting the possibility that
advanced AI systems could have very large impacts on society without reaching
human-level cognitive abilities. To be most useful, however, more analysis of
what it means for AI to be 'transformative' is needed. In this paper, we
propose three different levels on which AI might be said to be transformative,
associated with different levels of societal change. We suggest that these
distinctions would improve conversations between policy makers and decision
makers concerning the mid- to long-term impacts of advances in AI. Further, we
feel this would have a positive effect on strategic foresight efforts involving
advanced AI, which we expect to illuminate paths to alternative futures. We
conclude with a discussion of the benefits of our new framework and by
highlighting directions for future work in this area.",2022-11-13 16:02:13
XXX,journalArticle,2019,"Ehsan Toreini, Mhairi Aitken, Kovila Coopamootoo, Karen Elliott, Carlos Gonzalez Zelaya, Aad van Moorsel",The relationship between trust in AI and trustworthy machine learning technologies,,,,,http://arxiv.org/abs/1912.00782v2,"To build AI-based systems that users and the public can justifiably trust one
needs to understand how machine learning technologies impact trust put in these
services. To guide technology developments, this paper provides a systematic
approach to relate social science concepts of trust with the technologies used
in AI-based services and products. We conceive trust as discussed in the ABI
(Ability, Benevolence, Integrity) framework and use a recently proposed mapping
of ABI on qualities of technologies. We consider four categories of machine
learning technologies, namely these for Fairness, Explainability, Auditability
and Safety (FEAS) and discuss if and how these possess the required qualities.
Trust can be impacted throughout the life cycle of AI-based systems, and we
introduce the concept of Chain of Trust to discuss technological needs for
trust in different stages of the life cycle. FEAS has obvious relations with
known frameworks and therefore we relate FEAS to a variety of international
Principled AI policy and technology frameworks that have emerged in recent
years.",2022-11-13 16:02:13
XXX,journalArticle,2019,"Kevin Lu, Igor Mordatch, Pieter Abbeel",Adaptive Online Planning for Continual Lifelong Learning,,,,,http://arxiv.org/abs/1912.01188v2,"We study learning control in an online reset-free lifelong learning scenario,
where mistakes can compound catastrophically into the future and the underlying
dynamics of the environment may change. Traditional model-free policy learning
methods have achieved successes in difficult tasks due to their broad
flexibility, but struggle in this setting, as they can activate failure modes
early in their lifetimes which are difficult to recover from and face
performance degradation as dynamics change. On the other hand, model-based
planning methods learn and adapt quickly, but require prohibitive levels of
computational resources. We present a new algorithm, Adaptive Online Planning
(AOP), that achieves strong performance in this setting by combining
model-based planning with model-free learning. By approximating the uncertainty
of the model-free components and the planner performance, AOP is able to call
upon more extensive planning only when necessary, leading to reduced
computation times, while still gracefully adapting behaviors in the face of
unpredictable changes in the world -- even when traditional RL fails.",2022-11-13 16:02:14
XXX,journalArticle,2019,"Mustafa Mert Çelikok, Tomi Peltola, Pedram Daee, Samuel Kaski",Interactive AI with a Theory of Mind,,,,,http://arxiv.org/abs/1912.05284v1,"Understanding each other is the key to success in collaboration. For humans,
attributing mental states to others, the theory of mind, provides the crucial
advantage. We argue for formulating human--AI interaction as a multi-agent
problem, endowing AI with a computational theory of mind to understand and
anticipate the user. To differentiate the approach from previous work, we
introduce a categorisation of user modelling approaches based on the level of
agency learnt in the interaction. We describe our recent work in using nested
multi-agent modelling to formulate user models for multi-armed bandit based
interactive AI systems, including a proof-of-concept user study.",2022-11-13 16:02:14
XXX,journalArticle,2019,"Krishn Bera, Yash Mandilwar, Bapi Raju",Value-of-Information based Arbitration between Model-based and Model-free Control,,,,,http://arxiv.org/abs/1912.05453v1,"There have been numerous attempts in explaining the general learning
behaviours using model-based and model-free methods. While the model-based
control is flexible yet computationally expensive in planning, the model-free
control is quick but inflexible. The model-based control is therefore immune
from reward devaluation and contingency degradation. Multiple arbitration
schemes have been suggested to achieve the data efficiency and computational
efficiency of model-based and model-free control respectively. In this context,
we propose a quantitative 'value of information' based arbitration between both
the controllers in order to establish a general computational framework for
skill learning. The interacting model-based and model-free reinforcement
learning processes are arbitrated using an uncertainty-based value of
information. We further show that our algorithm performs better than Q-learning
as well as Q-learning with experience replay.",2022-11-13 16:02:15
XXX,journalArticle,2019,Thomas Bartz-Beielstein,Why we need an AI-resilient society,,,,,http://arxiv.org/abs/1912.08786v1,"Artificial intelligence is considered as a key technology. It has a huge
impact on our society. Besides many positive effects, there are also some
negative effects or threats. Some of these threats to society are well-known,
e.g., weapons or killer robots. But there are also threats that are ignored.
These unknown-knowns or blind spots affect privacy, and facilitate manipulation
and mistaken identities. We cannot trust data, audio, video, and identities any
more. Democracies are able to cope with known threats, the known-knowns.
Transforming unknown-knowns to known-knowns is one important cornerstone of
resilient societies. An AI-resilient society is able to transform threats
caused by new AI tecchnologies such as generative adversarial networks.
Resilience can be seen as a positive adaptation of these threats. We propose
three strategies how this adaptation can be achieved: awareness, agreements,
and red flags. This article accompanies the TEDx talk ""Why we urgently need an
AI-resilient society"", see https://youtu.be/f6c2ngp7rqY.",2022-11-13 16:02:15
XXX,journalArticle,2019,"P. M. Krafft, Meg Young, Michael Katell, Karen Huang, Ghislain Bugingo",Defining AI in Policy versus Practice,,,,,http://arxiv.org/abs/1912.11095v1,"Recent concern about harms of information technologies motivate consideration
of regulatory action to forestall or constrain certain developments in the
field of artificial intelligence (AI). However, definitional ambiguity hampers
the possibility of conversation about this urgent topic of public concern.
Legal and regulatory interventions require agreed-upon definitions, but
consensus around a definition of AI has been elusive, especially in policy
conversations. With an eye towards practical working definitions and a broader
understanding of positions on these issues, we survey experts and review
published policy documents to examine researcher and policy-maker conceptions
of AI. We find that while AI researchers favor definitions of AI that emphasize
technical functionality, policy-makers instead use definitions that compare
systems to human thinking and behavior. We point out that definitions adhering
closely to the functionality of AI systems are more inclusive of technologies
in use today, whereas definitions that emphasize human-like capabilities are
most applicable to hypothetical future technologies. As a result of this gap,
ethical and regulatory efforts may overemphasize concern about future
technologies at the expense of pressing issues with existing deployed
technologies.",2022-11-13 16:02:15
XXX,journalArticle,2020,Haydn Belfield,Activism by the AI Community: Analysing Recent Achievements and Future Prospects,,,,,http://arxiv.org/abs/2001.06528v1,"The artificial intelligence community (AI) has recently engaged in activism
in relation to their employers, other members of the community, and their
governments in order to shape the societal and ethical implications of AI. It
has achieved some notable successes, but prospects for further political
organising and activism are uncertain. We survey activism by the AI community
over the last six years; apply two analytical frameworks drawing upon the
literature on epistemic communities, and worker organising and bargaining; and
explore what they imply for the future prospects of the AI community. Success
thus far has hinged on a coherent shared culture, and high bargaining power due
to the high demand for a limited supply of AI talent. Both are crucial to the
future of AI activism and worthy of sustained attention.",2022-11-13 16:02:16
XXX,journalArticle,2020,"Christian Kästner, Eunsuk Kang",Teaching Software Engineering for AI-Enabled Systems,,,,,http://arxiv.org/abs/2001.06691v1,"Software engineers have significant expertise to offer when building
intelligent systems, drawing on decades of experience and methods for building
systems that are scalable, responsive and robust, even when built on unreliable
components. Systems with artificial-intelligence or machine-learning (ML)
components raise new challenges and require careful engineering. We designed a
new course to teach software-engineering skills to students with a background
in ML. We specifically go beyond traditional ML courses that teach modeling
techniques under artificial conditions and focus, in lecture and assignments,
on realism with large and changing datasets, robust and evolvable
infrastructure, and purposeful requirements engineering that considers ethics
and fairness as well. We describe the course and our infrastructure and share
experience and all material from teaching the course for the first time.",2022-11-13 16:02:16
XXX,journalArticle,2020,"Martin Lindvall, Jesper Molin",Designing for the Long Tail of Machine Learning,,,,,http://arxiv.org/abs/2001.07455v1,"Recent technical advances has made machine learning (ML) a promising
component to include in end user facing systems. However, user experience (UX)
practitioners face challenges in relating ML to existing user-centered design
processes and how to navigate the possibilities and constraints of this design
space. Drawing on our own experience, we characterize designing within this
space as navigating trade-offs between data gathering, model development and
designing valuable interactions for a given model performance. We suggest that
the theoretical description of how machine learning performance scales with
training data can guide designers in these trade-offs as well as having
implications for prototyping. We exemplify the learning curve's usage by
arguing that a useful pattern is to design an initial system in a bootstrap
phase that aims to exploit the training effect of data collected at increasing
orders of magnitude.",2022-11-13 16:02:17
XXX,journalArticle,2020,"Jan Bosch, Ivica Crnkovic, Helena Holmström Olsson",Engineering AI Systems: A Research Agenda,,,,,http://arxiv.org/abs/2001.07522v2,"Artificial intelligence (AI) and machine learning (ML) are increasingly
broadly adopted in industry, However, based on well over a dozen case studies,
we have learned that deploying industry-strength, production quality ML models
in systems proves to be challenging. Companies experience challenges related to
data quality, design methods and processes, performance of models as well as
deployment and compliance. We learned that a new, structured engineering
approach is required to construct and evolve systems that contain ML/DL
components. In this paper, we provide a conceptualization of the typical
evolution patterns that companies experience when employing ML as well as an
overview of the key problems experienced by the companies that we have studied.
The main contribution of the paper is a research agenda for AI engineering that
provides an overview of the key engineering challenges surrounding ML solutions
and an overview of open items that need to be addressed by the research
community at large.",2022-11-13 16:02:17
XXX,journalArticle,2020,"Shikha Singh, Deepak Khemani",Subjective Knowledge and Reasoning about Agents in Multi-Agent Systems,,,,,http://arxiv.org/abs/2001.08016v1,"Though a lot of work in multi-agent systems is focused on reasoning about
knowledge and beliefs of artificial agents, an explicit representation and
reasoning about the presence/absence of agents, especially in the scenarios
where agents may be unaware of other agents joining in or going offline in a
multi-agent system, leading to partial knowledge/asymmetric knowledge of the
agents is mostly overlooked by the MAS community. Such scenarios lay the
foundations of cases where an agent can influence other agents' mental states
by (mis)informing them about the presence/absence of collaborators or
adversaries. In this paper, we investigate how Kripke structure-based epistemic
models can be extended to express the above notion based on an agent's
subjective knowledge and we discuss the challenges that come along.",2022-11-13 16:02:18
XXX,journalArticle,2020,"Devleena Das, Sonia Chernova",Leveraging Rationales to Improve Human Task Performance,,,,10.1145/3377325.3377512,http://arxiv.org/abs/2002.04202v1,"Machine learning (ML) systems across many application areas are increasingly
demonstrating performance that is beyond that of humans. In response to the
proliferation of such models, the field of Explainable AI (XAI) has sought to
develop techniques that enhance the transparency and interpretability of
machine learning methods. In this work, we consider a question not previously
explored within the XAI and ML communities: Given a computational system whose
performance exceeds that of its human user, can explainable AI capabilities be
leveraged to improve the performance of the human? We study this question in
the context of the game of Chess, for which computational game engines that
surpass the performance of the average player are widely available. We
introduce the Rationale-Generating Algorithm, an automated technique for
generating rationales for utility-based computational methods, which we
evaluate with a multi-day user study against two baselines. The results show
that our approach produces rationales that lead to statistically significant
improvement in human task performance, demonstrating that rationales
automatically generated from an AI's internal task model can be used not only
to explain what the system is doing, but also to instruct the user and
ultimately improve their task performance.",2022-11-13 16:02:18
XXX,journalArticle,2020,"Mohammadhosein Hasanbeig, Alessandro Abate, Daniel Kroening",Cautious Reinforcement Learning with Logical Constraints,,,,,http://arxiv.org/abs/2002.12156v2,"This paper presents the concept of an adaptive safe padding that forces
Reinforcement Learning (RL) to synthesise optimal control policies while
ensuring safety during the learning process. Policies are synthesised to
satisfy a goal, expressed as a temporal logic formula, with maximal
probability. Enforcing the RL agent to stay safe during learning might limit
the exploration, however we show that the proposed architecture is able to
automatically handle the trade-off between efficient progress in exploration
(towards goal satisfaction) and ensuring safety. Theoretical guarantees are
available on the optimality of the synthesised policies and on the convergence
of the learning algorithm. Experimental results are provided to showcase the
performance of the proposed method.",2022-11-13 16:02:18
XXX,journalArticle,2020,"Abhishek Kumar, Benjamin Finley, Tristan Braud, Sasu Tarkoma, Pan Hui",Marketplace for AI Models,,,,,http://arxiv.org/abs/2003.01593v1,"Artificial intelligence shows promise for solving many practical societal
problems in areas such as healthcare and transportation. However, the current
mechanisms for AI model diffusion such as Github code repositories, academic
project webpages, and commercial AI marketplaces have some limitations; for
example, a lack of monetization methods, model traceability, and model
auditabilty. In this work, we sketch guidelines for a new AI diffusion method
based on a decentralized online marketplace. We consider the technical,
economic, and regulatory aspects of such a marketplace including a discussion
of solutions for problems in these areas. Finally, we include a comparative
analysis of several current AI marketplaces that are already available or in
development. We find that most of these marketplaces are centralized commercial
marketplaces with relatively few models.",2022-11-13 16:02:19
XXX,journalArticle,2020,"Gabriel Lima, Meeyoung Cha, Chihyung Jeon, Kyungsin Park",The Conflict Between People's Urge to Punish AI and Legal Systems,,,,10.3389/frobt.2021.756242,http://arxiv.org/abs/2003.06507v3,"Regulating artificial intelligence (AI) has become necessary in light of its
deployment in high-risk scenarios. This paper explores the proposal to extend
legal personhood to AI and robots, which had not yet been examined through the
lens of the general public. We present two studies (N = 3,559) to obtain
people's views of electronic legal personhood vis-\`a-vis existing liability
models. Our study reveals people's desire to punish automated agents even
though these entities are not recognized any mental state. Furthermore, people
did not believe automated agents' punishment would fulfill deterrence nor
retribution and were unwilling to grant them legal punishment preconditions,
namely physical independence and assets. Collectively, these findings suggest a
conflict between the desire to punish automated agents and its perceived
impracticability. We conclude by discussing how future design and legal
decisions may influence how the public reacts to automated agents' wrongdoings.",2022-11-13 16:02:20
XXX,journalArticle,2020,Spyridon Samothrakis,Open Loop In Natura Economic Planning,,,,,http://arxiv.org/abs/2005.01539v2,"The debate between the optimal way of allocating societal surplus (i.e.
products and services) has been raging, in one form or another, practically
forever; following the collapse of the Soviet Union in 1991, the market became
the only legitimate form of organisation -- there was no other alternative.
Working within the tradition of Marx, Leontief, Kantorovich, Beer and
Cockshott, we propose what we deem an automated planning system that aims to
operate on unit level (e.g., factories and citizens), rather than on aggregate
demand and sectors. We explain why it is both a viable and desirable
alternative to current market conditions and position our solution within
current societal structures. Our experiments show that it would be trivial to
plan for up to 50K industrial goods and 5K final goods in commodity hardware.",2022-11-13 16:02:20
XXX,journalArticle,2020,"J. K. Terry, Nathaniel Grammel",Multi-Agent Informational Learning Processes,,,,,http://arxiv.org/abs/2006.06870v4,"We introduce a new mathematical model of multi-agent reinforcement learning,
the Multi-Agent Informational Learning Processor ""MAILP"" model. The model is
based on the notion that agents have policies for a certain amount of
information, models how this information iteratively evolves and propagates
through many agents. This model is very general, and the only meaningful
assumption made is that learning for individual agents progressively slows over
time.",2022-11-13 16:02:21
XXX,journalArticle,2020,"Mirka Snyder Caron, Abhishek Gupta",The Social Contract for AI,,,,,http://arxiv.org/abs/2006.08140v1,"Like any technology, AI systems come with inherent risks and potential
benefits. It comes with potential disruption of established norms and methods
of work, societal impacts and externalities. One may think of the adoption of
technology as a form of social contract, which may evolve or fluctuate in time,
scale, and impact. It is important to keep in mind that for AI, meeting the
expectations of this social contract is critical, because recklessly driving
the adoption and implementation of unsafe, irresponsible, or unethical AI
systems may trigger serious backlash against industry and academia involved
which could take decades to resolve, if not actually seriously harm society.
For the purpose of this paper, we consider that a social contract arises when
there is sufficient consensus within society to adopt and implement this new
technology. As such, to enable a social contract to arise for the adoption and
implementation of AI, developing: 1) A socially accepted purpose, through 2) A
safe and responsible method, with 3) A socially aware level of risk involved,
for 4) A socially beneficial outcome, is key.",2022-11-13 16:02:22
XXX,journalArticle,2020,"Nathan Fulton, Nathan Hunt, Nghia Hoang, Subhro Das",Formal Verification of End-to-End Learning in Cyber-Physical Systems: Progress and Challenges,,,,,http://arxiv.org/abs/2006.09181v1,"Autonomous systems -- such as self-driving cars, autonomous drones, and
automated trains -- must come with strong safety guarantees. Over the past
decade, techniques based on formal methods have enjoyed some success in
providing strong correctness guarantees for large software systems including
operating system kernels, cryptographic protocols, and control software for
drones. These successes suggest it might be possible to ensure the safety of
autonomous systems by constructing formal, computer-checked correctness proofs.
This paper identifies three assumptions underlying existing formal verification
techniques, explains how each of these assumptions limits the applicability of
verification in autonomous systems, and summarizes preliminary work toward
improving the strength of evidence provided by formal verification.",2022-11-13 16:02:22
XXX,journalArticle,2020,"Alexander I. Cowen-Rivers, Daniel Palenicek, Vincent Moens, Mohammed Abdullah, Aivar Sootla, Jun Wang, Haitham Ammar",SAMBA: Safe Model-Based & Active Reinforcement Learning,,,,,http://arxiv.org/abs/2006.09436v1,"In this paper, we propose SAMBA, a novel framework for safe reinforcement
learning that combines aspects from probabilistic modelling, information
theory, and statistics. Our method builds upon PILCO to enable active
exploration using novel(semi-)metrics for out-of-sample Gaussian process
evaluation optimised through a multi-objective problem that supports
conditional-value-at-risk constraints. We evaluate our algorithm on a variety
of safe dynamical system benchmarks involving both low and high-dimensional
state representations. Our results show orders of magnitude reductions in
samples and violations compared to state-of-the-art methods. Lastly, we provide
intuition as to the effectiveness of the framework by a detailed analysis of
our active metrics and safety constraints.",2022-11-13 16:02:23
XXX,journalArticle,2020,"Hossein Hajipour, Mateusz Malinowski, Mario Fritz",IReEn: Reverse-Engineering of Black-Box Functions via Iterative Neural Program Synthesis,,,,,http://arxiv.org/abs/2006.10720v2,"In this work, we investigate the problem of revealing the functionality of a
black-box agent. Notably, we are interested in the interpretable and formal
description of the behavior of such an agent. Ideally, this description would
take the form of a program written in a high-level language. This task is also
known as reverse engineering and plays a pivotal role in software engineering,
computer security, but also most recently in interpretability. In contrast to
prior work, we do not rely on privileged information on the black box, but
rather investigate the problem under a weaker assumption of having only access
to inputs and outputs of the program. We approach this problem by iteratively
refining a candidate set using a generative neural program synthesis approach
until we arrive at a functionally equivalent program. We assess the performance
of our approach on the Karel dataset. Our results show that the proposed
approach outperforms the state-of-the-art on this challenge by finding an
approximately functional equivalent program in 78% of cases -- even exceeding
prior work that had privileged information on the black-box.",2022-11-13 16:02:24
XXX,journalArticle,2020,"Deepak Gopinath, Mahdieh Nejati Javaremi, Brenna D. Argall",Customized Handling of Unintended Interface Operation in Assistive Robots,,,,,http://arxiv.org/abs/2007.02092v2,"We present an assistance system that reasons about a human's intended actions
during robot teleoperation in order to provide appropriate corrections for
unintended behavior. We model the human's physical interaction with a control
interface during robot teleoperation and distinguish between intended and
measured physical actions explicitly. By reasoning over the unobserved
intentions using model-based inference techniques, our assistive system
provides customized corrections on a user's issued commands. We validate our
algorithm with a 10-person human subject study in which we evaluate the
performance of the proposed assistance paradigms. Our results show that the
assistance paradigms helped to significantly reduce task completion time,
number of mode switches, cognitive workload, and user frustration and improve
overall user satisfaction.",2022-11-13 16:02:25
XXX,journalArticle,2020,"Alvaro Ovalle, Simon M. Lucas",Modulation of viability signals for self-regulatory control,,,,,http://arxiv.org/abs/2007.09297v2,"We revisit the role of instrumental value as a driver of adaptive behavior.
In active inference, instrumental or extrinsic value is quantified by the
information-theoretic surprisal of a set of observations measuring the extent
to which those observations conform to prior beliefs or preferences. That is,
an agent is expected to seek the type of evidence that is consistent with its
own model of the world. For reinforcement learning tasks, the distribution of
preferences replaces the notion of reward. We explore a scenario in which the
agent learns this distribution in a self-supervised manner. In particular, we
highlight the distinction between observations induced by the environment and
those pertaining more directly to the continuity of an agent in time. We
evaluate our methodology in a dynamic environment with discrete time and
actions. First with a surprisal minimizing model-free agent (in the RL sense)
and then expanding to the model-based case to minimize the expected free
energy.",2022-11-13 16:02:25
XXX,journalArticle,2020,"Gabriel Paludo Licks, Felipe Meneguzzi",Automated Database Indexing using Model-free Reinforcement Learning,,,,,http://arxiv.org/abs/2007.14244v1,"Configuring databases for efficient querying is a complex task, often carried
out by a database administrator. Solving the problem of building indexes that
truly optimize database access requires a substantial amount of database and
domain knowledge, the lack of which often results in wasted space and memory
for irrelevant indexes, possibly jeopardizing database performance for querying
and certainly degrading performance for updating. We develop an architecture to
solve the problem of automatically indexing a database by using reinforcement
learning to optimize queries by indexing data throughout the lifetime of a
database. In our experimental evaluation, our architecture shows superior
performance compared to related work on reinforcement learning and genetic
algorithms, maintaining near-optimal index configurations and efficiently
scaling to large databases.",2022-11-13 16:02:26
XXX,journalArticle,2020,"Chidiebere Onyedinma, Patrick Gavigan, Babak Esfandiari",Toward Campus Mail Delivery Using BDI,"EPTCS 319, 2020, pp. 127-143",,,10.4204/EPTCS.319.10,http://arxiv.org/abs/2007.16089v1,"Autonomous systems developed with the Belief-Desire-Intention (BDI)
architecture are usually mostly implemented in simulated environments. In this
project we sought to build a BDI agent for use in the real world for campus
mail delivery in the tunnel system at Carleton University. Ideally, the robot
should receive a delivery order via a mobile application, pick up the mail at a
station, navigate the tunnels to the destination station, and notify the
recipient.
  We linked the Robot Operating System (ROS) with a BDI reasoning system to
achieve a subset of the required use cases. ROS handles the low-level sensing
and actuation, while the BDI reasoning system handles the high-level reasoning
and decision making. Sensory data is orchestrated and sent from ROS to the
reasoning system as perceptions. These perceptions are then deliberated upon,
and an action string is sent back to ROS for interpretation and driving of the
necessary actuator for the action to be performed.
  In this paper we present our current implementation, which closes the loop on
the hardware-software integration, and implements a subset of the use cases
required for the full system.",2022-11-13 16:02:26
XXX,journalArticle,2020,"Gabriel Lima, Changyeon Kim, Seungho Ryu, Chihyung Jeon, Meeyoung Cha",Collecting the Public Perception of AI and Robot Rights,,,,,http://arxiv.org/abs/2008.01339v1,"Whether to give rights to artificial intelligence (AI) and robots has been a
sensitive topic since the European Parliament proposed advanced robots could be
granted ""electronic personalities."" Numerous scholars who favor or disfavor its
feasibility have participated in the debate. This paper presents an experiment
(N=1270) that 1) collects online users' first impressions of 11 possible rights
that could be granted to autonomous electronic agents of the future and 2)
examines whether debunking common misconceptions on the proposal modifies one's
stance toward the issue. The results indicate that even though online users
mainly disfavor AI and robot rights, they are supportive of protecting
electronic agents from cruelty (i.e., favor the right against cruel treatment).
Furthermore, people's perceptions became more positive when given information
about rights-bearing non-human entities or myth-refuting statements. The style
used to introduce AI and robot rights significantly affected how the
participants perceived the proposal, similar to the way metaphors function in
creating laws. For robustness, we repeated the experiment over a more
representative sample of U.S. residents (N=164) and found that perceptions
gathered from online users and those by the general population are similar.",2022-11-13 16:02:27
XXX,journalArticle,2020,"Alexandra Luccioni, Joseph Bullock, Katherine Hoffmann Pham, Cynthia Sin Nga Lam, Miguel Luengo-Oroz","Considerations, Good Practices, Risks and Pitfalls in Developing AI Solutions Against COVID-19","Harvard CRCS Workshop on AI for Social Good, United States, 2020",,,,http://arxiv.org/abs/2008.09043v1,"The COVID-19 pandemic has been a major challenge to humanity, with 12.7
million confirmed cases as of July 13th, 2020 [1]. In previous work, we
described how Artificial Intelligence can be used to tackle the pandemic with
applications at the molecular, clinical, and societal scales [2]. In the
present follow-up article, we review these three research directions, and
assess the level of maturity and feasibility of the approaches used, as well as
their potential for operationalization. We also summarize some commonly
encountered risks and practical pitfalls, as well as guidelines and best
practices for formulating and deploying AI applications at different scales.",2022-11-13 16:02:27
XXX,journalArticle,2020,"Kishor Jothimurugan, Rajeev Alur, Osbert Bastani",A Composable Specification Language for Reinforcement Learning Tasks,"In Advances in Neural Information Processing Systems, pp.
  13041-13051. 2019",,,,http://arxiv.org/abs/2008.09293v2,"Reinforcement learning is a promising approach for learning control policies
for robot tasks. However, specifying complex tasks (e.g., with multiple
objectives and safety constraints) can be challenging, since the user must
design a reward function that encodes the entire task. Furthermore, the user
often needs to manually shape the reward to ensure convergence of the learning
algorithm. We propose a language for specifying complex control tasks, along
with an algorithm that compiles specifications in our language into a reward
function and automatically performs reward shaping. We implement our approach
in a tool called SPECTRL, and show that it outperforms several state-of-the-art
baselines.",2022-11-13 16:02:28
XXX,journalArticle,2020,"Sandhya Saisubramanian, Shlomo Zilberstein, Ece Kamar",Avoiding Negative Side Effects due to Incomplete Knowledge of AI Systems,,,,,http://arxiv.org/abs/2008.12146v3,"Autonomous agents acting in the real-world often operate based on models that
ignore certain aspects of the environment. The incompleteness of any given
model -- handcrafted or machine acquired -- is inevitable due to practical
limitations of any modeling technique for complex real-world settings. Due to
the limited fidelity of its model, an agent's actions may have unexpected,
undesirable consequences during execution. Learning to recognize and avoid such
negative side effects of an agent's actions is critical to improve the safety
and reliability of autonomous systems. Mitigating negative side effects is an
emerging research topic that is attracting increased attention due to the rapid
growth in the deployment of AI systems and their broad societal impacts. This
article provides a comprehensive overview of different forms of negative side
effects and the recent research efforts to address them. We identify key
characteristics of negative side effects, highlight the challenges in avoiding
negative side effects, and discuss recently developed approaches, contrasting
their benefits and limitations. The article concludes with a discussion of open
questions and suggestions for future research directions.",2022-11-13 16:02:29
XXX,journalArticle,2020,"Guy Clarke Marshall, André Freitas, Caroline Jay",How Researchers Use Diagrams in Communicating Neural Network Systems,,,,,http://arxiv.org/abs/2008.12566v2,"Neural networks are a prevalent and effective machine learning component, and
their application is leading to significant scientific progress in many
domains. As the field of neural network systems is fast growing, it is
important to understand how advances are communicated. Diagrams are key to
this, appearing in almost all papers describing novel systems. This paper
reports on a study into the use of neural network system diagrams, through
interviews, card sorting, and qualitative feedback structured around
ecologically-derived examples. We find high diversity of usage, perception and
preference in both creation and interpretation of diagrams, examining this in
the context of existing design, information visualisation, and user experience
guidelines. Considering the interview data alongside existing guidance, we
propose guidelines aiming to improve the way in which neural network system
diagrams are constructed.",2022-11-13 16:02:29
XXX,journalArticle,2020,"Alexandre Galashov, Jakub Sygnowski, Guillaume Desjardins, Jan Humplik, Leonard Hasenclever, Rae Jeong, Yee Whye Teh, Nicolas Heess",Importance Weighted Policy Learning and Adaptation,,,,,http://arxiv.org/abs/2009.04875v2,"The ability to exploit prior experience to solve novel problems rapidly is a
hallmark of biological learning systems and of great practical importance for
artificial ones. In the meta reinforcement learning literature much recent work
has focused on the problem of optimizing the learning process itself. In this
paper we study a complementary approach which is conceptually simple, general,
modular and built on top of recent improvements in off-policy learning. The
framework is inspired by ideas from the probabilistic inference literature and
combines robust off-policy learning with a behavior prior, or default behavior
that constrains the space of solutions and serves as a bias for exploration; as
well as a representation for the value function, both of which are easily
learned from a number of training tasks in a multi-task scenario. Our approach
achieves competitive adaptation performance on hold-out tasks compared to meta
reinforcement learning baselines and can scale to complex sparse-reward
scenarios.",2022-11-13 16:02:30
XXX,journalArticle,2020,"Pallavi Bagga, Nicola Paoletti, Kostas Stathis",Learnable Strategies for Bilateral Agent Negotiation over Multiple Issues,,,,,http://arxiv.org/abs/2009.08302v2,"We present a novel bilateral negotiation model that allows a self-interested
agent to learn how to negotiate over multiple issues in the presence of user
preference uncertainty. The model relies upon interpretable strategy templates
representing the tactics the agent should employ during the negotiation and
learns template parameters to maximize the average utility received over
multiple negotiations, thus resulting in optimal bid acceptance and generation.
Our model also uses deep reinforcement learning to evaluate threshold utility
values, for those tactics that require them, thereby deriving optimal utilities
for every environment state. To handle user preference uncertainty, the model
relies on a stochastic search to find user model that best agrees with a given
partial preference profile. Multi-objective optimization and multi-criteria
decision-making methods are applied at negotiation time to generate
Pareto-optimal outcomes thereby increasing the number of successful (win-win)
negotiations. Rigorous experimental evaluations show that the agent employing
our model outperforms the winning agents of the 10th Automated Negotiating
Agents Competition (ANAC'19) in terms of individual as well as social-welfare
utilities.",2022-11-13 16:02:30
XXX,journalArticle,2020,Johannes Schneider,Humans learn too: Better Human-AI Interaction using Optimized Human Inputs,,,,,http://arxiv.org/abs/2009.09266v1,"Humans rely more and more on systems with AI components. The AI community
typically treats human inputs as a given and optimizes AI models only. This
thinking is one-sided and it neglects the fact that humans can learn, too. In
this work, human inputs are optimized for better interaction with an AI model
while keeping the model fixed. The optimized inputs are accompanied by
instructions on how to create them. They allow humans to save time and cut on
errors, while keeping required changes to original inputs limited. We propose
continuous and discrete optimization methods modifying samples in an iterative
fashion. Our quantitative and qualitative evaluation including a human study on
different hand-generated inputs shows that the generated proposals lead to
lower error rates, require less effort to create and differ only modestly from
the original samples.",2022-11-13 16:02:31
XXX,journalArticle,2020,U. Kerzel,Enterprise AI Canvas -- Integrating Artificial Intelligence into Business,,,,10.1080/08839514.2020.1826146,http://arxiv.org/abs/2009.11190v1,"Artificial Intelligence (AI) and Machine Learning have enormous potential to
transform businesses and disrupt entire industry sectors. However, companies
wishing to integrate algorithmic decisions into their face multiple challenges:
They have to identify use-cases in which artificial intelligence can create
value, as well as decisions that can be supported or executed automatically.
Furthermore, the organization will need to be transformed to be able to
integrate AI based systems into their human work-force. Furthermore, the more
technical aspects of the underlying machine learning model have to be discussed
in terms of how they impact the various units of a business: Where do the
relevant data come from, which constraints have to be considered, how is the
quality of the data and the prediction evaluated?
  The Enterprise AI canvas is designed to bring Data Scientist and business
expert together to discuss and define all relevant aspects which need to be
clarified in order to integrate AI based systems into a digital enterprise. It
consists of two parts where part one focuses on the business view and
organizational aspects, whereas part two focuses on the underlying machine
learning model and the data it uses.",2022-11-13 16:02:32
XXX,journalArticle,2020,"Kai-En Yang, Chia-Yu Tsai, Hung-Hao Shen, Chen-Feng Chiang, Feng-Ming Tsai, Chung-An Wang, Yiju Ting, Chia-Shun Yeh, Chin-Tang Lai",Trust-Region Method with Deep Reinforcement Learning in Analog Design Space Exploration,,,,10.1109/DAC18074.2021.9586087,http://arxiv.org/abs/2009.13772v4,"This paper introduces new perspectives on analog design space search. To
minimize the time-to-market, this endeavor better cast as constraint
satisfaction problem than global optimization defined in prior arts. We
incorporate model-based agents, contrasted with model-free learning, to
implement a trust-region strategy. As such, simple feed-forward networks can be
trained with supervised learning, where the convergence is relatively trivial.
Experiment results demonstrate orders of magnitude improvement on search
iterations. Additionally, the unprecedented consideration of PVT conditions are
accommodated. On circuits with TSMC 5/6nm process, our method achieve
performance surpassing human designers. Furthermore, this framework is in
production in industrial settings.",2022-11-13 16:02:32
XXX,journalArticle,2020,"Xusen Yin, Ralph Weischedel, Jonathan May",Learning to Generalize for Sequential Decision Making,,,,,http://arxiv.org/abs/2010.02229v1,"We consider problems of making sequences of decisions to accomplish tasks,
interacting via the medium of language. These problems are often tackled with
reinforcement learning approaches. We find that these models do not generalize
well when applied to novel task domains. However, the large amount of
computation necessary to adequately train and explore the search space of
sequential decision making, under a reinforcement learning paradigm, precludes
the inclusion of large contextualized language models, which might otherwise
enable the desired generalization ability. We introduce a teacher-student
imitation learning methodology and a means of converting a reinforcement
learning model into a natural language understanding model. Together, these
methodologies enable the introduction of contextualized language models into
the sequential decision making problem space. We show that models can learn
faster and generalize more, leveraging both the imitation learning and the
reformulation. Our models exceed teacher performance on various held-out
decision problems, by up to 7% on in-domain problems and 24% on out-of-domain
problems.",2022-11-13 16:02:33
XXX,journalArticle,2020,"Daniel Nemirovsky, Nicolas Thiebaut, Ye Xu, Abhishek Gupta",Providing Actionable Feedback in Hiring Marketplaces using Generative Adversarial Networks,,,,,http://arxiv.org/abs/2010.02419v1,"Machine learning predictors have been increasingly applied in production
settings, including in one of the world's largest hiring platforms, Hired, to
provide a better candidate and recruiter experience. The ability to provide
actionable feedback is desirable for candidates to improve their chances of
achieving success in the marketplace. Until recently, however, methods aimed at
providing actionable feedback have been limited in terms of realism and
latency. In this work, we demonstrate how, by applying a newly introduced
method based on Generative Adversarial Networks (GANs), we are able to overcome
these limitations and provide actionable feedback in real-time to candidates in
production settings. Our experimental results highlight the significant
benefits of utilizing a GAN-based approach on our dataset relative to two other
state-of-the-art approaches (including over 1000x latency gains). We also
illustrate the potential impact of this approach in detail on two real
candidate profile examples.",2022-11-13 16:02:33
XXX,journalArticle,2020,"Abdulmajid Murad, Frank Alexander Kraemer, Kerstin Bach, Gavin Taylor",Information-Driven Adaptive Sensing Based on Deep Reinforcement Learning,"10th International Conference on the Internet of Things (IoT20),
  October 6-9, 2020, Malmo, Sweden",,,10.1145/3410992.3411001,http://arxiv.org/abs/2010.04112v1,"In order to make better use of deep reinforcement learning in the creation of
sensing policies for resource-constrained IoT devices, we present and study a
novel reward function based on the Fisher information value. This reward
function enables IoT sensor devices to learn to spend available energy on
measurements at otherwise unpredictable moments, while conserving energy at
times when measurements would provide little new information. This is a highly
general approach, which allows for a wide range of use cases without
significant human design effort or hyper-parameter tuning. We illustrate the
approach in a scenario of workplace noise monitoring, where results show that
the learned behavior outperforms a uniform sampling strategy and comes close to
a near-optimal oracle solution.",2022-11-13 16:02:34
XXX,journalArticle,2020,"Alessandro Giuseppi, Antonio Pietrabissa",Chance-Constrained Control with Lexicographic Deep Reinforcement Learning,"IEEE Control Systems Letters, vol. 4, no. 3, pp. 755-760, July
  2020",,,10.1109/LCSYS.2020.2979635,http://arxiv.org/abs/2010.09468v1,"This paper proposes a lexicographic Deep Reinforcement Learning
(DeepRL)-based approach to chance-constrained Markov Decision Processes, in
which the controller seeks to ensure that the probability of satisfying the
constraint is above a given threshold. Standard DeepRL approaches require i)
the constraints to be included as additional weighted terms in the cost
function, in a multi-objective fashion, and ii) the tuning of the introduced
weights during the training phase of the Deep Neural Network (DNN) according to
the probability thresholds. The proposed approach, instead, requires to
separately train one constraint-free DNN and one DNN associated to each
constraint and then, at each time-step, to select which DNN to use depending on
the system observed state. The presented solution does not require any
hyper-parameter tuning besides the standard DNN ones, even if the probability
thresholds changes. A lexicographic version of the well-known DeepRL algorithm
DQN is also proposed and validated via simulations.",2022-11-13 16:02:34
XXX,journalArticle,2020,"Niya Stoimenova, Rebecca Price",Exploring the Nuances of Designing (with/for) Artificial Intelligence,"Design Issues, 36(4), 45-55 (2020)",,,10.1162/desi_a_00613,http://arxiv.org/abs/2010.15578v1,"Solutions relying on artificial intelligence are devised to predict data
patterns and answer questions that are clearly defined, involve an enumerable
set of solutions, clear rules, and inherently binary decision mechanisms. Yet,
as they become exponentially implemented in our daily activities, they begin to
transcend these initial boundaries and to affect the larger sociotechnical
system in which they are situated. In this arrangement, a solution is under
pressure to surpass true or false criteria and move to an ethical evaluation of
right and wrong. Neither algorithmic solutions, nor purely humanistic ones will
be enough to fully mitigate undesirable outcomes in the narrow state of AI or
its future incarnations. We must take a holistic view. In this paper we explore
the construct of infrastructure as a means to simultaneously address
algorithmic and societal issues when designing AI.",2022-11-13 16:02:35
XXX,journalArticle,2020,"Paul Schwerdtner, Florens Greßner, Nikhil Kapoor, Felix Assion, René Sass, Wiebke Günther, Fabian Hüger, Peter Schlicht",Risk Assessment for Machine Learning Models,,,,,http://arxiv.org/abs/2011.04328v1,"In this paper we propose a framework for assessing the risk associated with
deploying a machine learning model in a specified environment. For that we
carry over the risk definition from decision theory to machine learning. We
develop and implement a method that allows to define deployment scenarios, test
the machine learning model under the conditions specified in each scenario, and
estimate the damage associated with the output of the machine learning model
under test. Using the likelihood of each scenario together with the estimated
damage we define \emph{key risk indicators} of a machine learning model.
  The definition of scenarios and weighting by their likelihood allows for
standardized risk assessment in machine learning throughout multiple domains of
application. In particular, in our framework, the robustness of a machine
learning model to random input corruptions, distributional shifts caused by a
changing environment, and adversarial perturbations can be assessed.",2022-11-13 16:02:35
XXX,journalArticle,2020,"Annie Xie, Dylan P. Losey, Ryan Tolsma, Chelsea Finn, Dorsa Sadigh",Learning Latent Representations to Influence Multi-Agent Interaction,,,,,http://arxiv.org/abs/2011.06619v1,"Seamlessly interacting with humans or robots is hard because these agents are
non-stationary. They update their policy in response to the ego agent's
behavior, and the ego agent must anticipate these changes to co-adapt. Inspired
by humans, we recognize that robots do not need to explicitly model every
low-level action another agent will make; instead, we can capture the latent
strategy of other agents through high-level representations. We propose a
reinforcement learning-based framework for learning latent representations of
an agent's policy, where the ego agent identifies the relationship between its
behavior and the other agent's future strategy. The ego agent then leverages
these latent dynamics to influence the other agent, purposely guiding them
towards policies suitable for co-adaptation. Across several simulated domains
and a real-world air hockey game, our approach outperforms the alternatives and
learns to influence the other agent.",2022-11-13 16:02:36
XXX,journalArticle,2020,"Ramana Kumar, Jonathan Uesato, Richard Ngo, Tom Everitt, Victoria Krakovna, Shane Legg",REALab: An Embedded Perspective on Tampering,,,,,http://arxiv.org/abs/2011.08820v1,"This paper describes REALab, a platform for embedded agency research in
reinforcement learning (RL). REALab is designed to model the structure of
tampering problems that may arise in real-world deployments of RL. Standard
Markov Decision Process (MDP) formulations of RL and simulated environments
mirroring the MDP structure assume secure access to feedback (e.g., rewards).
This may be unrealistic in settings where agents are embedded and can corrupt
the processes producing feedback (e.g., human supervisors, or an implemented
reward function). We describe an alternative Corrupt Feedback MDP formulation
and the REALab environment platform, which both avoid the secure feedback
assumption. We hope the design of REALab provides a useful perspective on
tampering problems, and that the platform may serve as a unit test for the
presence of tampering incentives in RL agent designs.",2022-11-13 16:02:36
XXX,journalArticle,2020,"Pedro Freire, Adam Gleave, Sam Toyer, Stuart Russell",DERAIL: Diagnostic Environments for Reward And Imitation Learning,,,,,http://arxiv.org/abs/2012.01365v1,"The objective of many real-world tasks is complex and difficult to
procedurally specify. This makes it necessary to use reward or imitation
learning algorithms to infer a reward or policy directly from human data.
Existing benchmarks for these algorithms focus on realism, testing in complex
environments. Unfortunately, these benchmarks are slow, unreliable and cannot
isolate failures. As a complementary approach, we develop a suite of simple
diagnostic tasks that test individual facets of algorithm performance in
isolation. We evaluate a range of common reward and imitation learning
algorithms on our tasks. Our results confirm that algorithm performance is
highly sensitive to implementation details. Moreover, in a case-study into a
popular preference-based reward learning implementation, we illustrate how the
suite can pinpoint design flaws and rapidly evaluate candidate solutions. The
environments are available at https://github.com/HumanCompatibleAI/seals .",2022-11-13 16:02:37
XXX,journalArticle,2020,"Pratyay Banerjee, Chitta Baral, Man Luo, Arindam Mitra, Kuntal Pal, Tran C. Son, Neeraj Varshney",Can Transformers Reason About Effects of Actions?,,,,,http://arxiv.org/abs/2012.09938v1,"A recent work has shown that transformers are able to ""reason"" with facts and
rules in a limited setting where the rules are natural language expressions of
conjunctions of conditions implying a conclusion. Since this suggests that
transformers may be used for reasoning with knowledge given in natural
language, we do a rigorous evaluation of this with respect to a common form of
knowledge and its corresponding reasoning -- the reasoning about effects of
actions. Reasoning about action and change has been a top focus in the
knowledge representation subfield of AI from the early days of AI and more
recently it has been a highlight aspect in common sense question answering. We
consider four action domains (Blocks World, Logistics, Dock-Worker-Robots and a
Generic Domain) in natural language and create QA datasets that involve
reasoning about the effects of actions in these domains. We investigate the
ability of transformers to (a) learn to reason in these domains and (b)
transfer that learning from the generic domains to the other domains.",2022-11-13 16:02:37
XXX,journalArticle,2020,"Jerry Zikun Chen, Shi Yu, Haoran Wang",Exploring Fluent Query Reformulations with Text-to-Text Transformers and Reinforcement Learning,,,,,http://arxiv.org/abs/2012.10033v2,"Query reformulation aims to alter noisy or ambiguous text sequences into
coherent ones closer to natural language questions. This is to prevent errors
from propagating in a client-facing pipeline and promote better communication
with users. Besides, it is crucial to maintain performance in downstream
environments like question answering when rephrased queries are given as input.
We show that under the previous framework (AQA), attempts to alter RL
algorithms do not bring significant benefits to either reward acquisition or
sequence fluency. Instead, we leverage a query-reformulating text-to-text
transformer (QRT5) and apply policy-based RL algorithms to further nudge this
reformulator and obtain better answers downstream by generating
reward-acquiring query trajectories. QRT5 shows better sample efficiency in RL
to achieve the same level of QA performance as the previous approach. It can
generate reformulations with more readability based on query well-formedness
evaluations and can generalize to out-of-sample data. Our framework is
demonstrated to be flexible, allowing reward signals to be sourced from
different downstream environments such as intent classification.",2022-11-13 16:02:38
XXX,journalArticle,2020,"Zelin Zhao, Chuang Gan, Jiajun Wu, Xiaoxiao Guo, Joshua B. Tenenbaum",Augmenting Policy Learning with Routines Discovered from a Single Demonstration,,,,,http://arxiv.org/abs/2012.12469v4,"Humans can abstract prior knowledge from very little data and use it to boost
skill learning. In this paper, we propose routine-augmented policy learning
(RAPL), which discovers routines composed of primitive actions from a single
demonstration and uses discovered routines to augment policy learning. To
discover routines from the demonstration, we first abstract routine candidates
by identifying grammar over the demonstrated action trajectory. Then, the best
routines measured by length and frequency are selected to form a routine
library. We propose to learn policy simultaneously at primitive-level and
routine-level with discovered routines, leveraging the temporal structure of
routines. Our approach enables imitating expert behavior at multiple temporal
scales for imitation learning and promotes reinforcement learning exploration.
Extensive experiments on Atari games demonstrate that RAPL improves the
state-of-the-art imitation learning method SQIL and reinforcement learning
method A2C. Further, we show that discovered routines can generalize to unseen
levels and difficulties on the CoinRun benchmark.",2022-11-13 16:02:38
XXX,journalArticle,2020,Lance Eliot,Antitrust and Artificial Intelligence (AAI): Antitrust Vigilance Lifecycle and AI Legal Reasoning Autonomy,,,,,http://arxiv.org/abs/2012.13016v1,"There is an increasing interest in the entwining of the field of antitrust
with the field of Artificial Intelligence (AI), frequently referred to jointly
as Antitrust and AI (AAI) in the research literature. This study focuses on the
synergies entangling antitrust and AI, doing so to extend the literature by
proffering the primary ways that these two fields intersect, consisting of: (1)
the application of antitrust to AI, and (2) the application of AI to antitrust.
To date, most of the existing research on this intermixing has concentrated on
the former, namely the application of antitrust to AI, entailing how the
marketplace will be altered by the advent of AI and the potential for adverse
antitrust behaviors arising accordingly. Opting to explore more deeply the
other side of this coin, this research closely examines the application of AI
to antitrust and establishes an antitrust vigilance lifecycle to which AI is
predicted to be substantively infused for purposes of enabling and bolstering
antitrust detection, enforcement, and post-enforcement monitoring. Furthermore,
a gradual and incremental injection of AI into antitrust vigilance is
anticipated to occur as significant advances emerge amidst the Levels of
Autonomy (LoA) for AI Legal Reasoning (AILR).",2022-11-13 16:02:39
XXX,journalArticle,2020,"Arnaud Fickinger, Simon Zhuang, Andrew Critch, Dylan Hadfield-Menell, Stuart Russell",Multi-Principal Assistance Games: Definition and Collegial Mechanisms,,,,,http://arxiv.org/abs/2012.14536v1,"We introduce the concept of a multi-principal assistance game (MPAG), and
circumvent an obstacle in social choice theory, Gibbard's theorem, by using a
sufficiently collegial preference inference mechanism. In an MPAG, a single
agent assists N human principals who may have widely different preferences.
MPAGs generalize assistance games, also known as cooperative inverse
reinforcement learning games. We analyze in particular a generalization of
apprenticeship learning in which the humans first perform some work to obtain
utility and demonstrate their preferences, and then the robot acts to further
maximize the sum of human payoffs. We show in this setting that if the game is
sufficiently collegial, i.e. if the humans are responsible for obtaining a
sufficient fraction of the rewards through their own actions, then their
preferences are straightforwardly revealed through their work. This revelation
mechanism is non-dictatorial, does not limit the possible outcomes to two
alternatives, and is dominant-strategy incentive-compatible.",2022-11-13 16:02:39
XXX,journalArticle,2021,"Engkarat Techapanurak, Anh-Chuong Dang, Takayuki Okatani",Bridging In- and Out-of-distribution Samples for Their Better Discriminability,,,,,http://arxiv.org/abs/2101.02500v1,"This paper proposes a method for OOD detection. Questioning the premise of
previous studies that ID and OOD samples are separated distinctly, we consider
samples lying in the intermediate of the two and use them for training a
network. We generate such samples using multiple image transformations that
corrupt inputs in various ways and with different severity levels. We estimate
where the generated samples by a single image transformation lie between ID and
OOD using a network trained on clean ID samples. To be specific, we make the
network classify the generated samples and calculate their mean classification
accuracy, using which we create a soft target label for them. We train the same
network from scratch using the original ID samples and the generated samples
with the soft labels created for them. We detect OOD samples by thresholding
the entropy of the predicted softmax probability. The experimental results show
that our method outperforms the previous state-of-the-art in the standard
benchmark tests. We also analyze the effect of the number and particular
combinations of image corrupting transformations on the performance.",2022-11-13 16:02:40
XXX,journalArticle,2021,"Vivek Veeriah, Tom Zahavy, Matteo Hessel, Zhongwen Xu, Junhyuk Oh, Iurii Kemaev, Hado van Hasselt, David Silver, Satinder Singh",Discovery of Options via Meta-Learned Subgoals,,,,,http://arxiv.org/abs/2102.06741v1,"Temporal abstractions in the form of options have been shown to help
reinforcement learning (RL) agents learn faster. However, despite prior work on
this topic, the problem of discovering options through interaction with an
environment remains a challenge. In this paper, we introduce a novel
meta-gradient approach for discovering useful options in multi-task RL
environments. Our approach is based on a manager-worker decomposition of the RL
agent, in which a manager maximises rewards from the environment by learning a
task-dependent policy over both a set of task-independent discovered-options
and primitive actions. The option-reward and termination functions that define
a subgoal for each option are parameterised as neural networks and trained via
meta-gradients to maximise their usefulness. Empirical analysis on gridworld
and DeepMind Lab tasks show that: (1) our approach can discover meaningful and
diverse temporally-extended options in multi-task RL domains, (2) the
discovered options are frequently used by the agent while learning to solve the
training tasks, and (3) that the discovered options help a randomly initialised
manager learn faster in completely new tasks.",2022-11-13 16:02:41
XXX,journalArticle,2021,"Khanh Nguyen, Dipendra Misra, Robert Schapire, Miro Dudík, Patrick Shafto",Interactive Learning from Activity Description,,,,,http://arxiv.org/abs/2102.07024v2,"We present a novel interactive learning protocol that enables training
request-fulfilling agents by verbally describing their activities. Unlike
imitation learning (IL), our protocol allows the teaching agent to provide
feedback in a language that is most appropriate for them. Compared with reward
in reinforcement learning (RL), the description feedback is richer and allows
for improved sample complexity. We develop a probabilistic framework and an
algorithm that practically implements our protocol. Empirical results in two
challenging request-fulfilling problems demonstrate the strengths of our
approach: compared with RL baselines, it is more sample-efficient; compared
with IL baselines, it achieves competitive success rates without requiring the
teaching agent to be able to demonstrate the desired behavior using the
learning agent's actions. Apart from empirical evaluation, we also provide
theoretical guarantees for our algorithm under certain assumptions about the
teacher and the environment.",2022-11-13 16:02:41
XXX,journalArticle,2021,"Giuliano Lorenzoni, Paulo Alencar, Nathalia Nascimento, Donald Cowan",Machine Learning Model Development from a Software Engineering Perspective: A Systematic Literature Review,,,,,http://arxiv.org/abs/2102.07574v1,"Data scientists often develop machine learning models to solve a variety of
problems in the industry and academy but not without facing several challenges
in terms of Model Development. The problems regarding Machine Learning
Development involves the fact that such professionals do not realize that they
usually perform ad-hoc practices that could be improved by the adoption of
activities presented in the Software Engineering Development Lifecycle. Of
course, since machine learning systems are different from traditional Software
systems, some differences in their respective development processes are to be
expected. In this context, this paper is an effort to investigate the
challenges and practices that emerge during the development of ML models from
the software engineering perspective by focusing on understanding how software
developers could benefit from applying or adapting the traditional software
engineering process to the Machine Learning workflow.",2022-11-13 16:02:42
XXX,journalArticle,2021,"Selmer Bringsjord, Naveen Sundar Govindarajulu, Michael Giancola","AI Can Stop Mass Shootings, and More",,,,,http://arxiv.org/abs/2102.09343v1,"We propose to build directly upon our longstanding, prior r&d in AI/machine
ethics in order to attempt to make real the blue-sky idea of AI that can thwart
mass shootings, by bringing to bear its ethical reasoning. The r&d in question
is overtly and avowedly logicist in form, and since we are hardly the only ones
who have established a firm foundation in the attempt to imbue AI's with their
own ethical sensibility, the pursuit of our proposal by those in different
methodological camps should, we believe, be considered as well. We seek herein
to make our vision at least somewhat concrete by anchoring our exposition to
two simulations, one in which the AI saves the lives of innocents by locking
out a malevolent human's gun, and a second in which this malevolent agent is
allowed by the AI to be neutralized by law enforcement. Along the way, some
objections are anticipated, and rebutted.",2022-11-13 16:02:42
XXX,journalArticle,2021,"Chao-Han Huck Yang, I-Te Danny Hung, Yi Ouyang, Pin-Yu Chen",Training a Resilient Q-Network against Observational Interference,,,,,http://arxiv.org/abs/2102.09677v3,"Deep reinforcement learning (DRL) has demonstrated impressive performance in
various gaming simulators and real-world applications. In practice, however, a
DRL agent may receive faulty observation by abrupt interferences such as
black-out, frozen-screen, and adversarial perturbation. How to design a
resilient DRL algorithm against these rare but mission-critical and
safety-crucial scenarios is an essential yet challenging task. In this paper,
we consider a deep q-network (DQN) framework training with an auxiliary task of
observational interferences such as artificial noises. Inspired by causal
inference for observational interference, we propose a causal inference based
DQN algorithm called causal inference Q-network (CIQ). We evaluate the
performance of CIQ in several benchmark DQN environments with different types
of interferences as auxiliary labels. Our experimental results show that the
proposed CIQ method could achieve higher performance and more resilience
against observational interferences.",2022-11-13 16:02:43
XXX,journalArticle,2021,"Solon Barocas, Anhong Guo, Ece Kamar, Jacquelyn Krones, Meredith Ringel Morris, Jennifer Wortman Vaughan, Duncan Wadsworth, Hanna Wallach","Designing Disaggregated Evaluations of AI Systems: Choices, Considerations, and Tradeoffs",,,,,http://arxiv.org/abs/2103.06076v2,"Disaggregated evaluations of AI systems, in which system performance is
assessed and reported separately for different groups of people, are
conceptually simple. However, their design involves a variety of choices. Some
of these choices influence the results that will be obtained, and thus the
conclusions that can be drawn; others influence the impacts -- both beneficial
and harmful -- that a disaggregated evaluation will have on people, including
the people whose data is used to conduct the evaluation. We argue that a deeper
understanding of these choices will enable researchers and practitioners to
design careful and conclusive disaggregated evaluations. We also argue that
better documentation of these choices, along with the underlying considerations
and tradeoffs that have been made, will help others when interpreting an
evaluation's results and conclusions.",2022-11-13 16:02:43
XXX,journalArticle,2021,"Matteo Camilli, Michael Felderer, Andrea Giusti, Dominik T. Matt, Anna Perini, Barbara Russo, Angelo Susi",Towards Risk Modeling for Collaborative AI,,,,,http://arxiv.org/abs/2103.07460v1,"Collaborative AI systems aim at working together with humans in a shared
space to achieve a common goal. This setting imposes potentially hazardous
circumstances due to contacts that could harm human beings. Thus, building such
systems with strong assurances of compliance with requirements domain specific
standards and regulations is of greatest importance. Challenges associated with
the achievement of this goal become even more severe when such systems rely on
machine learning components rather than such as top-down rule-based AI. In this
paper, we introduce a risk modeling approach tailored to Collaborative AI
systems. The risk model includes goals, risk events and domain specific
indicators that potentially expose humans to hazards. The risk model is then
leveraged to drive assurance methods that feed in turn the risk model through
insights extracted from run-time evidence. Our envisioned approach is described
by means of a running example in the domain of Industry 4.0, where a robotic
arm endowed with a visual perception component, implemented with machine
learning, collaborates with a human operator for a production-relevant task.",2022-11-13 16:02:44
XXX,journalArticle,2021,"Harshit Sikchi, Wenxuan Zhou, David Held",Lyapunov Barrier Policy Optimization,,,,,http://arxiv.org/abs/2103.09230v1,"Deploying Reinforcement Learning (RL) agents in the real-world require that
the agents satisfy safety constraints. Current RL agents explore the
environment without considering these constraints, which can lead to damage to
the hardware or even other agents in the environment. We propose a new method,
LBPO, that uses a Lyapunov-based barrier function to restrict the policy update
to a safe set for each training iteration. Our method also allows the user to
control the conservativeness of the agent with respect to the constraints in
the environment. LBPO significantly outperforms state-of-the-art baselines in
terms of the number of constraint violations during training while being
competitive in terms of performance. Further, our analysis reveals that
baselines like CPO and SDDPG rely mostly on backtracking to ensure safety
rather than safe projection, which provides insight into why previous methods
might not have effectively limit the number of constraint violations.",2022-11-13 16:02:44
XXX,journalArticle,2021,"Yuanhao Xie, Luís Cruz, Petra Heck, Jan S. Rellermeyer",Systematic Mapping Study on the Machine Learning Lifecycle,,,,,http://arxiv.org/abs/2103.10248v1,"The development of artificial intelligence (AI) has made various industries
eager to explore the benefits of AI. There is an increasing amount of research
surrounding AI, most of which is centred on the development of new AI
algorithms and techniques. However, the advent of AI is bringing an increasing
set of practical problems related to AI model lifecycle management that need to
be investigated. We address this gap by conducting a systematic mapping study
on the lifecycle of AI model. Through quantitative research, we provide an
overview of the field, identify research opportunities, and provide suggestions
for future research. Our study yields 405 publications published from 2005 to
2020, mapped in 5 different main research topics, and 31 sub-topics. We observe
that only a minority of publications focus on data management and model
production problems, and that more studies should address the AI lifecycle from
a holistic perspective.",2022-11-13 16:02:45
XXX,journalArticle,2021,"Dmitrii Krasheninnikov, Rohin Shah, Herke van Hoof",Combining Reward Information from Multiple Sources,,,,,http://arxiv.org/abs/2103.12142v1,"Given two sources of evidence about a latent variable, one can combine the
information from both by multiplying the likelihoods of each piece of evidence.
However, when one or both of the observation models are misspecified, the
distributions will conflict. We study this problem in the setting with two
conflicting reward functions learned from different sources. In such a setting,
we would like to retreat to a broader distribution over reward functions, in
order to mitigate the effects of misspecification. We assume that an agent will
maximize expected reward given this distribution over reward functions, and
identify four desiderata for this setting. We propose a novel algorithm,
Multitask Inverse Reward Design (MIRD), and compare it to a range of simple
baselines. While all methods must trade off between conservatism and
informativeness, through a combination of theory and empirical results on a toy
environment, we find that MIRD and its variant MIRD-IF strike a good balance
between the two.",2022-11-13 16:02:45
XXX,journalArticle,2021,"Francesco Ponzio, Enrico Macii, Elisa Ficarra, Santa Di Cataldo",W2WNet: a two-module probabilistic Convolutional Neural Network with embedded data cleansing functionality,,,,,http://arxiv.org/abs/2103.13107v1,"Convolutional Neural Networks (CNNs) are supposed to be fed with only
high-quality annotated datasets. Nonetheless, in many real-world scenarios,
such high quality is very hard to obtain, and datasets may be affected by any
sort of image degradation and mislabelling issues. This negatively impacts the
performance of standard CNNs, both during the training and the inference phase.
To address this issue we propose Wise2WipedNet (W2WNet), a new two-module
Convolutional Neural Network, where a Wise module exploits Bayesian inference
to identify and discard spurious images during the training, and a Wiped module
takes care of the final classification while broadcasting information on the
prediction confidence at inference time. The goodness of our solution is
demonstrated on a number of public benchmarks addressing different image
classification tasks, as well as on a real-world case study on histological
image analysis. Overall, our experiments demonstrate that W2WNet is able to
identify image degradation and mislabelling issues both at training and at
inference time, with a positive impact on the final classification accuracy.",2022-11-13 16:02:46
XXX,journalArticle,2021,"Faraz Torabi, Garrett Warnell, Peter Stone",DEALIO: Data-Efficient Adversarial Learning for Imitation from Observation,,,,,http://arxiv.org/abs/2104.00163v1,"In imitation learning from observation IfO, a learning agent seeks to imitate
a demonstrating agent using only observations of the demonstrated behavior
without access to the control signals generated by the demonstrator. Recent
methods based on adversarial imitation learning have led to state-of-the-art
performance on IfO problems, but they typically suffer from high sample
complexity due to a reliance on data-inefficient, model-free reinforcement
learning algorithms. This issue makes them impractical to deploy in real-world
settings, where gathering samples can incur high costs in terms of time,
energy, and risk. In this work, we hypothesize that we can incorporate ideas
from model-based reinforcement learning with adversarial methods for IfO in
order to increase the data efficiency of these methods without sacrificing
performance. Specifically, we consider time-varying linear Gaussian policies,
and propose a method that integrates the linear-quadratic regulator with path
integral policy improvement into an existing adversarial IfO framework. The
result is a more data-efficient IfO algorithm with better performance, which we
show empirically in four simulation domains: using far fewer interactions with
the environment, the proposed method exhibits similar or better performance
than the existing technique.",2022-11-13 16:02:46
XXX,journalArticle,2021,"Brittany Davis Pierson, Justine Ventura, Matthew E. Taylor",The Atari Data Scraper,,,,,http://arxiv.org/abs/2104.04893v1,"Reinforcement learning has made great strides in recent years due to the
success of methods using deep neural networks. However, such neural networks
act as a black box, obscuring the inner workings. While reinforcement learning
has the potential to solve unique problems, a lack of trust and understanding
of reinforcement learning algorithms could prevent their widespread adoption.
Here, we present a library that attaches a ""data scraper"" to deep reinforcement
learning agents, acting as an observer, and then show how the data collected by
the Atari Data Scraper can be used to understand and interpret deep
reinforcement learning agents. The code for the Atari Data Scraper can be found
here: https://github.com/IRLL/Atari-Data-Scraper",2022-11-13 16:02:47
XXX,journalArticle,2021,"Ercument Ilhan, Jeremy Gow, Diego Perez-Liebana",Action Advising with Advice Imitation in Deep Reinforcement Learning,,,,,http://arxiv.org/abs/2104.08441v1,"Action advising is a peer-to-peer knowledge exchange technique built on the
teacher-student paradigm to alleviate the sample inefficiency problem in deep
reinforcement learning. Recently proposed student-initiated approaches have
obtained promising results. However, due to being in the early stages of
development, these also have some substantial shortcomings. One of the
abilities that are absent in the current methods is further utilising advice by
reusing, which is especially crucial in the practical settings considering the
budget and cost constraints in peer-to-peer. In this study, we present an
approach to enable the student agent to imitate previously acquired advice to
reuse them directly in its exploration policy, without any interventions in the
learning mechanism itself. In particular, we employ a behavioural cloning
module to imitate the teacher policy and use dropout regularisation to have a
notion of epistemic uncertainty to keep track of which state-advice pairs are
actually collected. As the results of experiments we conducted in three Atari
games show, advice reusing via generalisation is indeed a feasible option in
deep RL and our approach can successfully achieve this while significantly
improving the learning performance, even when paired with a simple early
advising heuristic.",2022-11-13 16:02:47
XXX,journalArticle,2021,Lambert Hogenhout,A Framework for Ethical AI at the United Nations,,,,,http://arxiv.org/abs/2104.12547v1,"This paper aims to provide an overview of the ethical concerns in artificial
intelligence (AI) and the framework that is needed to mitigate those risks, and
to suggest a practical path to ensure the development and use of AI at the
United Nations (UN) aligns with our ethical values. The overview discusses how
AI is an increasingly powerful tool with potential for good, albeit one with a
high risk of negative side-effects that go against fundamental human rights and
UN values. It explains the need for ethical principles for AI aligned with
principles for data governance, as data and AI are tightly interwoven. It
explores different ethical frameworks that exist and tools such as assessment
lists. It recommends that the UN develop a framework consisting of ethical
principles, architectural standards, assessment methods, tools and
methodologies, and a policy to govern the implementation and adherence to this
framework, accompanied by an education program for staff.",2022-11-13 16:02:48
XXX,journalArticle,2021,"Anirudhan Badrinath, Frederic Wang, Zachary Pardos",pyBKT: An Accessible Python Library of Bayesian Knowledge Tracing Models,,,,,http://arxiv.org/abs/2105.00385v2,"Bayesian Knowledge Tracing, a model used for cognitive mastery estimation,
has been a hallmark of adaptive learning research and an integral component of
deployed intelligent tutoring systems (ITS). In this paper, we provide a brief
history of knowledge tracing model research and introduce pyBKT, an accessible
and computationally efficient library of model extensions from the literature.
The library provides data generation, fitting, prediction, and cross-validation
routines, as well as a simple to use data helper interface to ingest typical
tutor log dataset formats. We evaluate the runtime with various dataset sizes
and compare to past implementations. Additionally, we conduct sanity checks of
the model using experiments with simulated data to evaluate the accuracy of its
EM parameter learning and use real-world data to validate its predictions,
comparing pyBKT's supported model variants with results from the papers in
which they were originally introduced. The library is open source and open
license for the purpose of making knowledge tracing more accessible to
communities of research and practice and to facilitate progress in the field
through easier replication of past approaches.",2022-11-13 16:02:48
XXX,journalArticle,2021,"Sarah Dean, Thomas Krendl Gilbert, Nathan Lambert, Tom Zick",Axes for Sociotechnical Inquiry in AI Research,,,,10.1109/TTS.2021.3074097,http://arxiv.org/abs/2105.06551v1,"The development of artificial intelligence (AI) technologies has far exceeded
the investigation of their relationship with society. Sociotechnical inquiry is
needed to mitigate the harms of new technologies whose potential impacts remain
poorly understood. To date, subfields of AI research develop primarily
individual views on their relationship with sociotechnics, while tools for
external investigation, comparison, and cross-pollination are lacking. In this
paper, we propose four directions for inquiry into new and evolving areas of
technological development: value--what progress and direction does a field
promote, optimization--how the defined system within a problem formulation
relates to broader dynamics, consensus--how agreement is achieved and who is
included in building it, and failure--what methods are pursued when the problem
specification is found wanting. The paper provides a lexicon for sociotechnical
inquiry and illustrates it through the example of consumer drone technology.",2022-11-13 16:02:49
XXX,journalArticle,2021,"Michael Crosscombe, Jonathan Lawry",The Impact of Network Connectivity on Collective Learning,,,,,http://arxiv.org/abs/2106.00655v2,"In decentralised autonomous systems it is the interactions between individual
agents which govern the collective behaviours of the system. These local-level
interactions are themselves often governed by an underlying network structure.
These networks are particularly important for collective learning and
decision-making whereby agents must gather evidence from their environment and
propagate this information to other agents in the system. Models for collective
behaviours may often rely upon the assumption of total connectivity between
agents to provide effective information sharing within the system, but this
assumption may be ill-advised. In this paper we investigate the impact that the
underlying network has on performance in the context of collective learning.
Through simulations we study small-world networks with varying levels of
connectivity and randomness and conclude that totally-connected networks result
in higher average error when compared to networks with less connectivity.
Furthermore, we show that networks of high regularity outperform networks with
increasing levels of random connectivity.",2022-11-13 16:02:49
XXX,journalArticle,2021,"Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch",Decision Transformer: Reinforcement Learning via Sequence Modeling,,,,,http://arxiv.org/abs/2106.01345v2,"We introduce a framework that abstracts Reinforcement Learning (RL) as a
sequence modeling problem. This allows us to draw upon the simplicity and
scalability of the Transformer architecture, and associated advances in
language modeling such as GPT-x and BERT. In particular, we present Decision
Transformer, an architecture that casts the problem of RL as conditional
sequence modeling. Unlike prior approaches to RL that fit value functions or
compute policy gradients, Decision Transformer simply outputs the optimal
actions by leveraging a causally masked Transformer. By conditioning an
autoregressive model on the desired return (reward), past states, and actions,
our Decision Transformer model can generate future actions that achieve the
desired return. Despite its simplicity, Decision Transformer matches or exceeds
the performance of state-of-the-art model-free offline RL baselines on Atari,
OpenAI Gym, and Key-to-Door tasks.",2022-11-13 16:02:50
XXX,journalArticle,2021,"Stephen McAleer, John Lanier, Michael Dennis, Pierre Baldi, Roy Fox",Improving Social Welfare While Preserving Autonomy via a Pareto Mediator,,,,,http://arxiv.org/abs/2106.03927v1,"Machine learning algorithms often make decisions on behalf of agents with
varied and sometimes conflicting interests. In domains where agents can choose
to take their own action or delegate their action to a central mediator, an
open question is how mediators should take actions on behalf of delegating
agents. The main existing approach uses delegating agents to punish
non-delegating agents in an attempt to get all agents to delegate, which tends
to be costly for all. We introduce a Pareto Mediator which aims to improve
outcomes for delegating agents without making any of them worse off. Our
experiments in random normal form games, a restaurant recommendation game, and
a reinforcement learning sequential social dilemma show that the Pareto
Mediator greatly increases social welfare. Also, even when the Pareto Mediator
is based on an incorrect model of agent utility, performance gracefully
degrades to the pre-intervention level, due to the individual autonomy
preserved by the voluntary mediator.",2022-11-13 16:02:50
XXX,journalArticle,2021,"Gaurav Yengera, Rati Devidze, Parameswaran Kamalaruban, Adish Singla",Curriculum Design for Teaching via Demonstrations: Theory and Applications,,,,,http://arxiv.org/abs/2106.04696v3,"We consider the problem of teaching via demonstrations in sequential
decision-making settings. In particular, we study how to design a personalized
curriculum over demonstrations to speed up the learner's convergence. We
provide a unified curriculum strategy for two popular learner models: Maximum
Causal Entropy Inverse Reinforcement Learning (MaxEnt-IRL) and Cross-Entropy
Behavioral Cloning (CrossEnt-BC). Our unified strategy induces a ranking over
demonstrations based on a notion of difficulty scores computed w.r.t. the
teacher's optimal policy and the learner's current policy. Compared to the
state of the art, our strategy doesn't require access to the learner's internal
dynamics and still enjoys similar convergence guarantees under mild technical
conditions. Furthermore, we adapt our curriculum strategy to the setting where
no teacher agent is present using task-specific difficulty scores. Experiments
on a synthetic car driving environment and navigation-based environments
demonstrate the effectiveness of our curriculum strategy.",2022-11-13 16:02:51
XXX,journalArticle,2021,"Mythreyi Velmurugan, Chun Ouyang, Catarina Moreira, Renuka Sindhgatta",Developing a Fidelity Evaluation Approach for Interpretable Machine Learning,,,,,http://arxiv.org/abs/2106.08492v1,"Although modern machine learning and deep learning methods allow for complex
and in-depth data analytics, the predictive models generated by these methods
are often highly complex, and lack transparency. Explainable AI (XAI) methods
are used to improve the interpretability of these complex models, and in doing
so improve transparency. However, the inherent fitness of these explainable
methods can be hard to evaluate. In particular, methods to evaluate the
fidelity of the explanation to the underlying black box require further
development, especially for tabular data. In this paper, we (a) propose a three
phase approach to developing an evaluation method; (b) adapt an existing
evaluation method primarily for image and text data to evaluate models trained
on tabular data; and (c) evaluate two popular explainable methods using this
evaluation method. Our evaluations suggest that the internal mechanism of the
underlying predictive model, the internal mechanism of the explainable method
used and model and data complexity all affect explanation fidelity. Given that
explanation fidelity is so sensitive to context and tools and data used, we
could not clearly identify any specific explainable method as being superior to
another.",2022-11-13 16:02:51
XXX,journalArticle,2021,"Dominik Sisejkovic, Lennart M. Reimann, Elmira Moussavi, Farhad Merchant, Rainer Leupers",Logic Locking at the Frontiers of Machine Learning: A Survey on Developments and Opportunities,"2021 IFIP/IEEE 29th International Conference on Very Large Scale
  Integration (VLSI-SoC)",,,10.1109/VLSI-SoC53125.2021.9606979,http://arxiv.org/abs/2107.01915v4,"In the past decade, a lot of progress has been made in the design and
evaluation of logic locking; a premier technique to safeguard the integrity of
integrated circuits throughout the electronics supply chain. However, the
widespread proliferation of machine learning has recently introduced a new
pathway to evaluating logic locking schemes. This paper summarizes the recent
developments in logic locking attacks and countermeasures at the frontiers of
contemporary machine learning models. Based on the presented work, the key
takeaways, opportunities, and challenges are highlighted to offer
recommendations for the design of next-generation logic locking.",2022-11-13 16:02:52
XXX,journalArticle,2021,"Raphael Y. Cohen, Aaron D. Sodickson",An Orchestration Platform that Puts Radiologists in the Driver's Seat of AI Innovation: A Methodological Approach,,,,,http://arxiv.org/abs/2107.04409v1,"Current AI-driven research in radiology requires resources and expertise that
are often inaccessible to small and resource-limited labs. The clinicians who
are able to participate in AI research are frequently well-funded,
well-staffed, and either have significant experience with AI and computing, or
have access to colleagues or facilities that do. Current imaging data is
clinician-oriented and is not easily amenable to machine learning initiatives,
resulting in inefficient, time consuming, and costly efforts that rely upon a
crew of data engineers and machine learning scientists, and all too often
preclude radiologists from driving AI research and innovation. We present the
system and methodology we have developed to address infrastructure and platform
needs, while reducing the staffing and resource barriers to entry. We emphasize
a data-first and modular approach that streamlines the AI development and
deployment process while providing efficient and familiar interfaces for
radiologists, such that they can be the drivers of new AI innovations.",2022-11-13 16:02:52
XXX,journalArticle,2021,"Stepan Makarenko, Dmitry Sorokin, Alexander Ulanov, A. I. Lvovsky",Aligning an optical interferometer with beam divergence control and continuous action space,,,,,http://arxiv.org/abs/2107.04457v2,"Reinforcement learning is finding its way to real-world problem application,
transferring from simulated environments to physical setups. In this work, we
implement vision-based alignment of an optical Mach-Zehnder interferometer with
a confocal telescope in one arm, which controls the diameter and divergence of
the corresponding beam. We use a continuous action space; exponential scaling
enables us to handle actions within a range of over two orders of magnitude.
Our agent trains only in a simulated environment with domain randomizations. In
an experimental evaluation, the agent significantly outperforms an existing
solution and a human expert.",2022-11-13 16:02:53
XXX,journalArticle,2021,"Jesse David Dinneen, Helen Bubinger","Not Quite 'Ask a Librarian': AI on the Nature, Value, and Future of LIS",,,,,http://arxiv.org/abs/2107.05383v1,"AI language models trained on Web data generate prose that reflects human
knowledge and public sentiments, but can also contain novel insights and
predictions. We asked the world's best language model, GPT-3, fifteen difficult
questions about the nature, value, and future of library and information
science (LIS), topics that receive perennial attention from LIS scholars. We
present highlights from its 45 different responses, which range from platitudes
and caricatures to interesting perspectives and worrisome visions of the
future, thus providing an LIS-tailored demonstration of the current performance
of AI language models. We also reflect on the viability of using AI to forecast
or generate research ideas in this way today. Finally, we have shared the full
response log online for readers to consider and evaluate for themselves.",2022-11-13 16:02:54
XXX,journalArticle,2021,"Ariyan Bighashdel, Panagiotis Meletis, Pavol Jancura, Gijs Dubbelman",Deep Adaptive Multi-Intention Inverse Reinforcement Learning,,,,,http://arxiv.org/abs/2107.06692v1,"This paper presents a deep Inverse Reinforcement Learning (IRL) framework
that can learn an a priori unknown number of nonlinear reward functions from
unlabeled experts' demonstrations. For this purpose, we employ the tools from
Dirichlet processes and propose an adaptive approach to simultaneously account
for both complex and unknown number of reward functions. Using the conditional
maximum entropy principle, we model the experts' multi-intention behaviors as a
mixture of latent intention distributions and derive two algorithms to estimate
the parameters of the deep reward network along with the number of experts'
intentions from unlabeled demonstrations. The proposed algorithms are evaluated
on three benchmarks, two of which have been specifically extended in this study
for multi-intention IRL, and compared with well-known baselines. We demonstrate
through several experiments the advantages of our algorithms over the existing
approaches and the benefits of online inferring, rather than fixing beforehand,
the number of expert's intentions.",2022-11-13 16:02:55
XXX,journalArticle,2021,"Sunder Ali Khowaja, Kapal Dev, Nawab Muhammad Faseeh Qureshi, Parus Khuwaja, Luca Foschini",Towards Industrial Private AI: A two-tier framework for data and model security,IEEE Wireless Communications 2022,,,,http://arxiv.org/abs/2107.12806v2,"With the advances in 5G and IoT devices, the industries are vastly adopting
artificial intelligence (AI) techniques for improving classification and
prediction-based services. However, the use of AI also raises concerns
regarding privacy and security that can be misused or leaked. Private AI was
recently coined to address the data security issue by combining AI with
encryption techniques, but existing studies have shown that model inversion
attacks can be used to reverse engineer the images from model parameters. In
this regard, we propose a Federated Learning and Encryption-based Private
(FLEP) AI framework that provides two-tier security for data and model
parameters in an IIoT environment. We proposed a three-layer encryption method
for data security and provide a hypothetical method to secure the model
parameters. Experimental results show that the proposed method achieves better
encryption quality at the expense of slightly increased execution time. We also
highlight several open issues and challenges regarding the FLEP AI framework's
realization.",2022-11-13 16:02:55
XXX,journalArticle,2021,"Ahmad Hammoudeh, Sara Tedmori, Nadim Obeid",A Reflection on Learning from Data: Epistemology Issues and Limitations,,,,,http://arxiv.org/abs/2107.13270v1,"Although learning from data is effective and has achieved significant
milestones, it has many challenges and limitations. Learning from data starts
from observations and then proceeds to broader generalizations. This framework
is controversial in science, yet it has achieved remarkable engineering
successes. This paper reflects on some epistemological issues and some of the
limitations of the knowledge discovered in data. The document discusses the
common perception that getting more data is the key to achieving better machine
learning models from theoretical and practical perspectives. The paper sheds
some light on the shortcomings of using generic mathematical theories to
describe the process. It further highlights the need for theories specialized
in learning from data. While more data leverages the performance of machine
learning models in general, the relation in practice is shown to be logarithmic
at its best; After a specific limit, more data stabilize or degrade the machine
learning models. Recent work in reinforcement learning showed that the trend is
shifting away from data-oriented approaches and relying more on algorithms. The
paper concludes that learning from data is hindered by many limitations. Hence
an approach that has an intensional orientation is needed.",2022-11-13 16:02:56
XXX,journalArticle,2021,Susan von Struensee,"The Role of Social Movements, Coalitions, and Workers in Resisting Harmful Artificial Intelligence and Contributing to the Development of Responsible AI",,,,,http://arxiv.org/abs/2107.14052v1,"There is mounting public concern over the influence that AI based systems has
in our society. Coalitions in all sectors are acting worldwide to resist hamful
applications of AI. From indigenous people addressing the lack of reliable
data, to smart city stakeholders, to students protesting the academic
relationships with sex trafficker and MIT donor Jeffery Epstein, the
questionable ethics and values of those heavily investing in and profiting from
AI are under global scrutiny. There are biased, wrongful, and disturbing
assumptions embedded in AI algorithms that could get locked in without
intervention. Our best human judgment is needed to contain AI's harmful impact.
Perhaps one of the greatest contributions of AI will be to make us ultimately
understand how important human wisdom truly is in life on earth.",2022-11-13 16:02:57
XXX,journalArticle,2021,Ajay Kulkarni,Towards Understanding the Impact of Real-Time AI-Powered Educational Dashboards (RAED) on Providing Guidance to Instructors,,,,,http://arxiv.org/abs/2107.14414v1,"The objectives of this ongoing research are to build Real-Time AI-Powered
Educational Dashboard (RAED) as a decision support tool for instructors, and to
measure its impact on them while making decisions. Current developments in AI
can be combined with the educational dashboards to make them AI-Powered. Thus,
AI can help in providing recommendations based on the students' performances.
AI-Powered educational dashboards can also assist instructors in tracking
real-time student activities. In this ongoing research, our aim is to develop
the AI component as well as improve the existing design component of the RAED.
Further, we will conduct experiments to study its impact on instructors, and
understand how much they trust RAED to guide them while making decisions. This
paper elaborates on the ongoing research and future direction.",2022-11-13 16:02:58
XXX,journalArticle,2021,"Jiahao Chen, Victor Storchan, Eren Kurshan",Beyond Fairness Metrics: Roadblocks and Challenges for Ethical AI in Practice,,,,,http://arxiv.org/abs/2108.06217v1,"We review practical challenges in building and deploying ethical AI at the
scale of contemporary industrial and societal uses. Apart from the purely
technical concerns that are the usual focus of academic research, the
operational challenges of inconsistent regulatory pressures, conflicting
business goals, data quality issues, development processes, systems integration
practices, and the scale of deployment all conspire to create new ethical
risks. Such ethical concerns arising from these practical considerations are
not adequately addressed by existing research results. We argue that a holistic
consideration of ethics in the development and deployment of AI systems is
necessary for building ethical AI in practice, and exhort researchers to
consider the full operational contexts of AI systems when assessing ethical
risks.",2022-11-13 16:02:58
XXX,journalArticle,2021,Benjamin Cedric Larsen,A Framework for Understanding AI-Induced Field Change: How AI Technologies are Legitimized and Institutionalized,"In Proceedings of the 2021 AAAI ACM Conference on AI Ethics and
  Society",,,10.1145/3461702.3462591,http://arxiv.org/abs/2108.07804v1,"Artificial intelligence (AI) systems operate in increasingly diverse areas,
from healthcare to facial recognition, the stock market, autonomous vehicles,
and so on. While the underlying digital infrastructure of AI systems is
developing rapidly, each area of implementation is subject to different degrees
and processes of legitimization. By combining elements from institutional
theory and information systems-theory, this paper presents a conceptual
framework to analyze and understand AI-induced field-change. The introduction
of novel AI-agents into new or existing fields creates a dynamic in which
algorithms (re)shape organizations and institutions while existing
institutional infrastructures determine the scope and speed at which
organizational change is allowed to occur. Where institutional infrastructure
and governance arrangements, such as standards, rules, and regulations, still
are unelaborate, the field can move fast but is also more likely to be
contested. The institutional infrastructure surrounding AI-induced fields is
generally little elaborated, which could be an obstacle to the broader
institutionalization of AI-systems going forward.",2022-11-13 16:02:59
XXX,journalArticle,2021,"Paolo Bova, Jonas Emanuel Müller, Benjamin Harack",Safe Transformative AI via a Windfall Clause,,,,,http://arxiv.org/abs/2108.09404v2,"Society could soon see transformative artificial intelligence (TAI). Models
of competition for TAI show firms face strong competitive pressure to deploy
TAI systems before they are safe. This paper explores a proposed solution to
this problem, a Windfall Clause, where developers commit to donating a
significant portion of any eventual extremely large profits to good causes.
However, a key challenge for a Windfall Clause is that firms must have reason
to join one. Firms must also believe these commitments are credible. We extend
a model of TAI competition with a Windfall Clause to show how firms and
policymakers can design a Windfall Clause which overcomes these challenges.
Encouragingly, firms benefit from joining a Windfall Clause under a wide range
of scenarios. We also find that firms join the Windfall Clause more often when
the competition is more dangerous. Even when firms learn each other's
capabilities, firms rarely wish to withdraw their support for the Windfall
Clause. These three findings strengthen the case for using a Windfall Clause to
promote the safe development of TAI.",2022-11-13 16:03:00
XXX,journalArticle,2021,"Shachaf Poran, Gil Amsalem, Amit Beka, Dmitri Goldenberg",With One Voice: Composing a Travel Voice Assistant from Re-purposed Models,"2nd International Workshop on Industrial Recommendation Systems @
  KDD 2021",,,,http://arxiv.org/abs/2108.11463v1,"Voice assistants provide users a new way of interacting with digital
products, allowing them to retrieve information and complete tasks with an
increased sense of control and flexibility. Such products are comprised of
several machine learning models, like Speech-to-Text transcription, Named
Entity Recognition and Resolution, and Text Classification. Building a voice
assistant from scratch takes the prolonged efforts of several teams
constructing numerous models and orchestrating between components. Alternatives
such as using third-party vendors or re-purposing existing models may be
considered to shorten time-to-market and development costs. However, each
option has its benefits and drawbacks. We present key insights from building a
voice search assistant for Booking.com search and recommendation system. Our
paper compares the achieved performance and development efforts in dedicated
tailor-made solutions against existing re-purposed models. We share and discuss
our data-driven decisions about implementation trade-offs and their estimated
outcomes in hindsight, showing that a fully functional machine learning product
can be built from existing models.",2022-11-13 16:03:00
XXX,journalArticle,2021,"Jess Whittlestone, Jack Clark",Why and How Governments Should Monitor AI Development,,,,,http://arxiv.org/abs/2108.12427v2,"In this paper we outline a proposal for improving the governance of
artificial intelligence (AI) by investing in government capacity to
systematically measure and monitor the capabilities and impacts of AI systems.
If adopted, this would give governments greater information about the AI
ecosystem, equipping them to more effectively direct AI development and
deployment in the most societally and economically beneficial directions. It
would also create infrastructure that could rapidly identify potential threats
or harms that could occur as a consequence of changes in the AI ecosystem, such
as the emergence of strategically transformative capabilities, or the
deployment of harmful systems.
  We begin by outlining the problem which motivates this proposal: in brief,
traditional governance approaches struggle to keep pace with the speed of
progress in AI. We then present our proposal for addressing this problem:
governments must invest in measurement and monitoring infrastructure. We
discuss this proposal in detail, outlining what specific things governments
could focus on measuring and monitoring, and the kinds of benefits this would
generate for policymaking. Finally, we outline some potential pilot projects
and some considerations for implementing this in practice.",2022-11-13 16:03:01
XXX,journalArticle,2021,"Sebastian Flennerhag, Yannick Schroecker, Tom Zahavy, Hado van Hasselt, David Silver, Satinder Singh",Bootstrapped Meta-Learning,,,,,http://arxiv.org/abs/2109.04504v2,"Meta-learning empowers artificial intelligence to increase its efficiency by
learning how to learn. Unlocking this potential involves overcoming a
challenging meta-optimisation problem. We propose an algorithm that tackles
this problem by letting the meta-learner teach itself. The algorithm first
bootstraps a target from the meta-learner, then optimises the meta-learner by
minimising the distance to that target under a chosen (pseudo-)metric. Focusing
on meta-learning with gradients, we establish conditions that guarantee
performance improvements and show that the metric can control
meta-optimisation. Meanwhile, the bootstrapping mechanism can extend the
effective meta-learning horizon without requiring backpropagation through all
updates. We achieve a new state-of-the art for model-free agents on the Atari
ALE benchmark and demonstrate that it yields both performance and efficiency
gains in multi-task meta-learning. Finally, we explore how bootstrapping opens
up new possibilities and find that it can meta-learn efficient exploration in
an epsilon-greedy Q-learning agent, without backpropagating through the update
rule.",2022-11-13 16:03:02
XXX,journalArticle,2021,"Oliver Eigner, Sebastian Eresheim, Peter Kieseberg, Lukas Daniel Klausner, Martin Pirker, Torsten Priebe, Simon Tjoa, Fiammetta Marulli, Francesco Mercaldo",Towards Resilient Artificial Intelligence: Survey and Research Issues,"Proceedings of the 2021 IEEE International Conference on Cyber
  Security and Resilience (CSR 2021), 2021, 536-542",,,10.1109/CSR51186.2021.9527986,http://arxiv.org/abs/2109.08904v1,"Artificial intelligence (AI) systems are becoming critical components of
today's IT landscapes. Their resilience against attacks and other environmental
influences needs to be ensured just like for other IT assets. Considering the
particular nature of AI, and machine learning (ML) in particular, this paper
provides an overview of the emerging field of resilient AI and presents
research issues the authors identify as potential future work.",2022-11-13 16:03:02
XXX,journalArticle,2021,"Dan Hendrycks, Nicholas Carlini, John Schulman, Jacob Steinhardt",Unsolved Problems in ML Safety,,,,,http://arxiv.org/abs/2109.13916v5,"Machine learning (ML) systems are rapidly increasing in size, are acquiring
new capabilities, and are increasingly deployed in high-stakes settings. As
with other powerful technologies, safety for ML should be a leading research
priority. In response to emerging safety challenges in ML, such as those
introduced by recent large-scale models, we provide a new roadmap for ML Safety
and refine the technical problems that the field needs to address. We present
four problems ready for research, namely withstanding hazards (""Robustness""),
identifying hazards (""Monitoring""), reducing inherent model hazards
(""Alignment""), and reducing systemic hazards (""Systemic Safety""). Throughout,
we clarify each problem's motivation and provide concrete research directions.",2022-11-13 16:03:03
XXX,journalArticle,2021,"Jing Bi, Jiebo Luo, Chenliang Xu",Procedure Planning in Instructional Videos via Contextual Modeling and Model-based Policy Learning,,,,,http://arxiv.org/abs/2110.01770v2,"Learning new skills by observing humans' behaviors is an essential capability
of AI. In this work, we leverage instructional videos to study humans'
decision-making processes, focusing on learning a model to plan goal-directed
actions in real-life videos. In contrast to conventional action recognition,
goal-directed actions are based on expectations of their outcomes requiring
causal knowledge of potential consequences of actions. Thus, integrating the
environment structure with goals is critical for solving this task. Previous
works learn a single world model will fail to distinguish various tasks,
resulting in an ambiguous latent space; planning through it will gradually
neglect the desired outcomes since the global information of the future goal
degrades quickly as the procedure evolves. We address these limitations with a
new formulation of procedure planning and propose novel algorithms to model
human behaviors through Bayesian Inference and model-based Imitation Learning.
Experiments conducted on real-world instructional videos show that our method
can achieve state-of-the-art performance in reaching the indicated goals.
Furthermore, the learned contextual information presents interesting features
for planning in a latent space.",2022-11-13 16:03:03
XXX,journalArticle,2021,"Tian Dong, Han Qiu, Tianwei Zhang, Jiwei Li, Hewu Li, Jialiang Lu",Fingerprinting Multi-exit Deep Neural Network Models via Inference Time,,,,,http://arxiv.org/abs/2110.03175v1,"Transforming large deep neural network (DNN) models into the multi-exit
architectures can overcome the overthinking issue and distribute a large DNN
model on resource-constrained scenarios (e.g. IoT frontend devices and backend
servers) for inference and transmission efficiency. Nevertheless, intellectual
property (IP) protection for the multi-exit models in the wild is still an
unsolved challenge. Previous efforts to verify DNN model ownership mainly rely
on querying the model with specific samples and checking the responses, e.g.,
DNN watermarking and fingerprinting. However, they are vulnerable to
adversarial settings such as adversarial training and are not suitable for the
IP verification for multi-exit DNN models. In this paper, we propose a novel
approach to fingerprint multi-exit models via inference time rather than
inference predictions. Specifically, we design an effective method to generate
a set of fingerprint samples to craft the inference process with a unique and
robust inference time cost as the evidence for model ownership. We conduct
extensive experiments to prove the uniqueness and robustness of our method on
three structures (ResNet-56, VGG-16, and MobileNet) and three datasets
(CIFAR-10, CIFAR-100, and Tiny-ImageNet) under comprehensive adversarial
settings.",2022-11-13 16:03:04
XXX,journalArticle,2021,"Andreas Madsen, Nicholas Meade, Vaibhav Adlakha, Siva Reddy",Evaluating the Faithfulness of Importance Measures in NLP by Recursively Masking Allegedly Important Tokens and Retraining,,,,,http://arxiv.org/abs/2110.08412v3,"To explain NLP models a popular approach is to use importance measures, such
as attention, which inform input tokens are important for making a prediction.
However, an open question is how well these explanations accurately reflect a
model's logic, a property called faithfulness.
  To answer this question, we propose Recursive ROAR, a new faithfulness
metric. This works by recursively masking allegedly important tokens and then
retraining the model. The principle is that this should result in worse model
performance compared to masking random tokens. The result is a performance
curve given a masking-ratio. Furthermore, we propose a summarizing metric using
relative area-between-curves (RACU), which allows for easy comparison across
papers, models, and tasks.
  We evaluate 4 different importance measures on 8 different datasets, using
both LSTM-attention models and RoBERTa models. We find that the faithfulness of
importance measures is both model-dependent and task-dependent. This conclusion
contradicts previous evaluations in both computer vision and faithfulness of
attention literature.",2022-11-13 16:03:05
XXX,journalArticle,2021,"Su Lin Blodgett, Michael Madaio",Risks of AI Foundation Models in Education,,,,,http://arxiv.org/abs/2110.10024v1,"If the authors of a recent Stanford report (Bommasani et al., 2021) on the
opportunities and risks of ""foundation models"" are to be believed, these models
represent a paradigm shift for AI and for the domains in which they will
supposedly be used, including education. Although the name is new (and
contested (Field, 2021)), the term describes existing types of algorithmic
models that are ""trained on broad data at scale"" and ""fine-tuned"" (i.e.,
adapted) for particular downstream tasks, and is intended to encompass large
language models such as BERT or GPT-3 and computer vision models such as CLIP.
Such technologies have the potential for harm broadly speaking (e.g., Bender et
al., 2021), but their use in the educational domain is particularly fraught,
despite the potential benefits for learners claimed by the authors. In section
3.3 of the Stanford report, Malik et al. argue that achieving the goal of
providing education for all learners requires more efficient computational
approaches that can rapidly scale across educational domains and across
educational contexts, for which they argue foundation models are uniquely
well-suited. However, evidence suggests that not only are foundation models not
likely to achieve the stated benefits for learners, but their use may also
introduce new risks for harm.",2022-11-13 16:03:05
XXX,journalArticle,2021,"Muhammad Usman, Divya Gopinath, Corina S. Păsăreanu",QuantifyML: How Good is my Machine Learning Model?,"EPTCS 348, 2021, pp. 92-100",,,10.4204/EPTCS.348.6,http://arxiv.org/abs/2110.12588v1,"The efficacy of machine learning models is typically determined by computing
their accuracy on test data sets. However, this may often be misleading, since
the test data may not be representative of the problem that is being studied.
With QuantifyML we aim to precisely quantify the extent to which machine
learning models have learned and generalized from the given data. Given a
trained model, QuantifyML translates it into a C program and feeds it to the
CBMC model checker to produce a formula in Conjunctive Normal Form (CNF). The
formula is analyzed with off-the-shelf model counters to obtain precise counts
with respect to different model behavior. QuantifyML enables i) evaluating
learnability by comparing the counts for the outputs to ground truth, expressed
as logical predicates, ii) comparing the performance of models built with
different machine learning algorithms (decision-trees vs. neural networks), and
iii) quantifying the safety and robustness of models.",2022-11-13 16:03:06
XXX,journalArticle,2021,"Zeerak Talat, Hagen Blix, Josef Valvoda, Maya Indira Ganesh, Ryan Cotterell, Adina Williams",A Word on Machine Ethics: A Response to Jiang et al. (2021),,,,,http://arxiv.org/abs/2111.04158v1,"Ethics is one of the longest standing intellectual endeavors of humanity. In
recent years, the fields of AI and NLP have attempted to wrangle with how
learning systems that interact with humans should be constrained to behave
ethically. One proposal in this vein is the construction of morality models
that can take in arbitrary text and output a moral judgment about the situation
described. In this work, we focus on a single case study of the recently
proposed Delphi model and offer a critique of the project's proposed method of
automating morality judgments. Through an audit of Delphi, we examine broader
issues that would be applicable to any similar attempt. We conclude with a
discussion of how machine ethics could usefully proceed, by focusing on current
and near-future uses of technology, in a way that centers around transparency,
democratic values, and allows for straightforward accountability.",2022-11-13 16:03:06
XXX,journalArticle,2021,"Patric M. Fulop, Vincent Danos",Efficient estimates of optimal transport via low-dimensional embeddings,,,,,http://arxiv.org/abs/2111.04838v1,"Optimal transport distances (OT) have been widely used in recent work in
Machine Learning as ways to compare probability distributions. These are costly
to compute when the data lives in high dimension. Recent work by Paty et al.,
2019, aims specifically at reducing this cost by computing OT using low-rank
projections of the data (seen as discrete measures). We extend this approach
and show that one can approximate OT distances by using more general families
of maps provided they are 1-Lipschitz. The best estimate is obtained by
maximising OT over the given family. As OT calculations are done after mapping
data to a lower dimensional space, our method scales well with the original
data dimension. We demonstrate the idea with neural networks.",2022-11-13 16:03:07
XXX,journalArticle,2021,"Ying Zhang, Matthew A. Gitzendanner, Dan S. Maxwell, Justin W. Richardson, Kaleb E. Smith, Eric A. Stubbs, Brian J. Stucky, Jingchao Zhang, Erik Deumens",Building an AI-ready RSE Workforce,,,,,http://arxiv.org/abs/2111.04916v1,"Artificial Intelligence has been transforming industries and academic
research across the globe, and research software development is no exception.
Machine learning and deep learning are being applied in every aspect of the
research software development lifecycles, from new algorithm design paradigms
to software development processes. In this paper, we discuss our views on
today's challenges and opportunities that AI has presented on research software
development and engineers, and the approaches we, at the University of Florida,
are taking to prepare our workforce for the new era of AI.",2022-11-13 16:03:07
XXX,journalArticle,2021,"Waddah Saeed, Christian Omlin",Explainable AI (XAI): A Systematic Meta-Survey of Current Challenges and Future Opportunities,,,,,http://arxiv.org/abs/2111.06420v1,"The past decade has seen significant progress in artificial intelligence
(AI), which has resulted in algorithms being adopted for resolving a variety of
problems. However, this success has been met by increasing model complexity and
employing black-box AI models that lack transparency. In response to this need,
Explainable AI (XAI) has been proposed to make AI more transparent and thus
advance the adoption of AI in critical domains. Although there are several
reviews of XAI topics in the literature that identified challenges and
potential research directions in XAI, these challenges and research directions
are scattered. This study, hence, presents a systematic meta-survey for
challenges and future research directions in XAI organized in two themes: (1)
general challenges and research directions in XAI and (2) challenges and
research directions in XAI based on machine learning life cycle's phases:
design, development, and deployment. We believe that our meta-survey
contributes to XAI literature by providing a guide for future exploration in
the XAI area.",2022-11-13 16:03:08
XXX,journalArticle,2021,"Alex Kearney, Anna Koop, Johannes Günther, Patrick M. Pilarski",Finding Useful Predictions by Meta-gradient Descent to Improve Decision-making,"NeurIPS 2021 Workshop on Self-Supervised Learning: Theory and
  Practice",,,,http://arxiv.org/abs/2111.11212v1,"In computational reinforcement learning, a growing body of work seeks to
express an agent's model of the world through predictions about future
sensations. In this manuscript we focus on predictions expressed as General
Value Functions: temporally extended estimates of the accumulation of a future
signal. One challenge is determining from the infinitely many predictions that
the agent could possibly make which might support decision-making. In this
work, we contribute a meta-gradient descent method by which an agent can
directly specify what predictions it learns, independent of designer
instruction. To that end, we introduce a partially observable domain suited to
this investigation. We then demonstrate that through interaction with the
environment an agent can independently select predictions that resolve the
partial-observability, resulting in performance similar to expertly chosen
value functions. By learning, rather than manually specifying these
predictions, we enable the agent to identify useful predictions in a
self-supervised manner, taking a step towards truly autonomous systems.",2022-11-13 16:03:09
XXX,journalArticle,2021,Shashank Yadav,Machines & Influence: An Information Systems Lens,,,,,http://arxiv.org/abs/2111.13365v4,"Policymakers face a broader challenge of how to view AI capabilities today
and where does society stand in terms of those capabilities. This paper surveys
AI capabilities and tackles this very issue, exploring it in context of
political security in digitally networked societies. We extend the ideas of
Information Management to better understand contemporary AI systems as part of
a larger and more complex information system. Comprehensively reviewing AI
capabilities and contemporary man-machine interactions, we undertake conceptual
development to suggest that better information management could allow states to
more optimally offset the risks of AI enabled influence and better utilise the
emerging capabilities which these systems have to offer to policymakers and
political institutions across the world. Hopefully this long essay will actuate
further debates and discussions over these ideas, and prove to be a useful
contribution towards governing the future of AI.",2022-11-13 16:03:09
XXX,journalArticle,2021,"Pablo Villanueva-Domingo, Francisco Villaescusa-Navarro, Shy Genel, Daniel Anglés-Alcázar, Lars Hernquist, Federico Marinacci, David N. Spergel, Mark Vogelsberger, Desika Narayanan",Weighing the Milky Way and Andromeda with Artificial Intelligence,,,,,http://arxiv.org/abs/2111.14874v1,"We present new constraints on the masses of the halos hosting the Milky Way
and Andromeda galaxies derived using graph neural networks. Our models, trained
on thousands of state-of-the-art hydrodynamic simulations of the CAMELS
project, only make use of the positions, velocities and stellar masses of the
galaxies belonging to the halos, and are able to perform likelihood-free
inference on halo masses while accounting for both cosmological and
astrophysical uncertainties. Our constraints are in agreement with estimates
from other traditional methods.",2022-11-13 16:03:10
XXX,journalArticle,2021,"Inioluwa Deborah Raji, Emily M. Bender, Amandalynne Paullada, Emily Denton, Alex Hanna",AI and the Everything in the Whole Wide World Benchmark,,,,,http://arxiv.org/abs/2111.15366v1,"There is a tendency across different subfields in AI to valorize a small
collection of influential benchmarks. These benchmarks operate as stand-ins for
a range of anointed common problems that are frequently framed as foundational
milestones on the path towards flexible and generalizable AI systems.
State-of-the-art performance on these benchmarks is widely understood as
indicative of progress towards these long-term goals. In this position paper,
we explore the limits of such benchmarks in order to reveal the construct
validity issues in their framing as the functionally ""general"" broad measures
of progress they are set up to be.",2022-11-13 16:03:10
XXX,journalArticle,2021,"Weichao Zhou, Wenchao Li",Programmatic Reward Design by Example,,,,,http://arxiv.org/abs/2112.08438v2,"Reward design is a fundamental problem in reinforcement learning (RL). A
misspecified or poorly designed reward can result in low sample efficiency and
undesired behaviors. In this paper, we propose the idea of programmatic reward
design, i.e. using programs to specify the reward functions in RL environments.
Programs allow human engineers to express sub-goals and complex task scenarios
in a structured and interpretable way. The challenge of programmatic reward
design, however, is that while humans can provide the high-level structures,
properly setting the low-level details, such as the right amount of reward for
a specific sub-task, remains difficult. A major contribution of this paper is a
probabilistic framework that can infer the best candidate programmatic reward
function from expert demonstrations. Inspired by recent generative-adversarial
approaches, our framework searches for the most likely programmatic reward
function under which the optimally generated trajectories cannot be
differentiated from the demonstrated trajectories. Experimental results show
that programmatic reward functionslearned using this framework can
significantly outperform those learned using existing reward learning
algo-rithms, and enable RL agents to achieve state-of-the-artperformance on
highly complex tasks.",2022-11-13 16:03:11
XXX,journalArticle,2021,Immanuel Trummer,"DB-BERT: a Database Tuning Tool that ""Reads the Manual""",,,,,http://arxiv.org/abs/2112.10925v1,"DB-BERT is a database tuning tool that exploits information gained via
natural language analysis of manuals and other relevant text documents. It uses
text to identify database system parameters to tune as well as recommended
parameter values. DB-BERT applies large, pre-trained language models
(specifically, the BERT model) for text analysis. During an initial training
phase, it fine-tunes model weights in order to translate natural language hints
into recommended settings. At run time, DB-BERT learns to aggregate, adapt, and
prioritize hints to achieve optimal performance for a specific database system
and benchmark. Both phases are iterative and use reinforcement learning to
guide the selection of tuning settings to evaluate (penalizing settings that
the database system rejects while rewarding settings that improve performance).
In our experiments, we leverage hundreds of text documents about database
tuning as input for DB-BERT. We compare DB-BERT against various baselines,
considering different benchmarks (TPC-C and TPC-H), metrics (throughput and run
time), as well as database systems (Postgres and MySQL). In all cases, DB-BERT
finds the best parameter settings among all compared methods. The code of
DB-BERT is available online at https://itrummer.github.io/dbbert/.",2022-11-13 16:03:12
XXX,journalArticle,2022,"Wei Ye, Francesco Bullo, Noah Friedkin, Ambuj K Singh",Modeling Human-AI Team Decision Making,,,,,http://arxiv.org/abs/2201.02759v1,"AI and humans bring complementary skills to group deliberations. Modeling
this group decision making is especially challenging when the deliberations
include an element of risk and an exploration-exploitation process of
appraising the capabilities of the human and AI agents. To investigate this
question, we presented a sequence of intellective issues to a set of human
groups aided by imperfect AI agents. A group's goal was to appraise the
relative expertise of the group's members and its available AI agents, evaluate
the risks associated with different actions, and maximize the overall reward by
reaching consensus. We propose and empirically validate models of human-AI team
decision making under such uncertain circumstances, and show the value of
socio-cognitive constructs of prospect theory, influence dynamics, and Bayesian
learning in predicting the behavior of human-AI groups.",2022-11-13 16:03:12
XXX,journalArticle,2022,"Alexander Pan, Kush Bhatia, Jacob Steinhardt",The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models,,,,,http://arxiv.org/abs/2201.03544v2,"Reward hacking -- where RL agents exploit gaps in misspecified reward
functions -- has been widely observed, but not yet systematically studied. To
understand how reward hacking arises, we construct four RL environments with
misspecified rewards. We investigate reward hacking as a function of agent
capabilities: model capacity, action space resolution, observation space noise,
and training time. More capable agents often exploit reward misspecifications,
achieving higher proxy reward and lower true reward than less capable agents.
Moreover, we find instances of phase transitions: capability thresholds at
which the agent's behavior qualitatively shifts, leading to a sharp decrease in
the true reward. Such phase transitions pose challenges to monitoring the
safety of ML systems. To address this, we propose an anomaly detection task for
aberrant policies and offer several baseline detectors.",2022-11-13 16:03:13
XXX,journalArticle,2022,"Yitzhak Spielberg, Amos Azaria",The Concept of Criticality in AI Safety,,,,,http://arxiv.org/abs/2201.04632v1,"When AI agents don't align their actions with human values they may cause
serious harm. One way to solve the value alignment problem is by including a
human operator who monitors all of the agent's actions. Despite the fact, that
this solution guarantees maximal safety, it is very inefficient, since it
requires the human operator to dedicate all of his attention to the agent. In
this paper, we propose a much more efficient solution that allows an operator
to be engaged in other activities without neglecting his monitoring task. In
our approach the AI agent requests permission from the operator only for
critical actions, that is, potentially harmful actions. We introduce the
concept of critical actions with respect to AI safety and discuss how to build
a model that measures action criticality. We also discuss how the operator's
feedback could be used to make the agent smarter.",2022-11-13 16:03:14
XXX,journalArticle,2022,"Yitzhak Spielberg, Amos Azaria",Revelation of Task Difficulty in AI-aided Education,,,,,http://arxiv.org/abs/2201.04633v1,"When a student is asked to perform a given task, her subjective estimate of
the difficulty of that task has a strong influence on her performance. There
exists a rich literature on the impact of perceived task difficulty on
performance and motivation. Yet, there is another topic that is closely related
to the subject of the influence of perceived task difficulty that did not
receive any attention in previous research - the influence of revealing the
true difficulty of a task to the student. This paper investigates the impact of
revealing the task difficulty on the student's performance, motivation,
self-efficacy and subjective task value via an experiment in which workers are
asked to solve matchstick riddles. Furthermore, we discuss how the experiment
results might be relevant for AI-aided education. Specifically, we elaborate on
the question of how a student's learning experience might be improved by
supporting her with two types of AI systems: an AI system that predicts task
difficulty and an AI system that determines when task difficulty should be
revealed and when not.",2022-11-13 16:03:14
XXX,journalArticle,2022,"Biplav Srivastava, Tarmo Koppel, Ronak Shah, Owen Bond, Sai Teja Paladi, Rohit Sharma, Austin Hetherington",ULTRA: A Data-driven Approach for Recommending Team Formation in Response to Proposal Calls,,,,,http://arxiv.org/abs/2201.05646v1,"We introduce an emerging AI-based approach and prototype system for assisting
team formation when researchers respond to calls for proposals from funding
agencies. This is an instance of the general problem of building teams when
demand opportunities come periodically and potential members may vary over
time. The novelties of our approach are that we: (a) extract technical skills
needed about researchers and calls from multiple data sources and normalize
them using Natural Language Processing (NLP) techniques, (b) build a prototype
solution based on matching and teaming based on constraints, (c) describe
initial feedback about system from researchers at a University to deploy, and
(d) create and publish a dataset that others can use.",2022-11-13 16:03:15
XXX,journalArticle,2022,"Ryan Soklaski, Justin Goodwin, Olivia Brown, Michael Yee, Jason Matterer",Tools and Practices for Responsible AI Engineering,,,,,http://arxiv.org/abs/2201.05647v1,"Responsible Artificial Intelligence (AI) - the practice of developing,
evaluating, and maintaining accurate AI systems that also exhibit essential
properties such as robustness and explainability - represents a multifaceted
challenge that often stretches standard machine learning tooling, frameworks,
and testing methods beyond their limits. In this paper, we present two new
software libraries - hydra-zen and the rAI-toolbox - that address critical
needs for responsible AI engineering. hydra-zen dramatically simplifies the
process of making complex AI applications configurable, and their behaviors
reproducible. The rAI-toolbox is designed to enable methods for evaluating and
enhancing the robustness of AI-models in a way that is scalable and that
composes naturally with other popular ML frameworks. We describe the design
principles and methodologies that make these tools effective, including the use
of property-based testing to bolster the reliability of the tools themselves.
Finally, we demonstrate the composability and flexibility of the tools by
showing how various use cases from adversarial robustness and explainable AI
can be concisely implemented with familiar APIs.",2022-11-13 16:03:15
XXX,journalArticle,2022,"Julius Frost, Olivia Watkins, Eric Weiner, Pieter Abbeel, Trevor Darrell, Bryan Plummer, Kate Saenko",Explaining Reinforcement Learning Policies through Counterfactual Trajectories,,,,,http://arxiv.org/abs/2201.12462v2,"In order for humans to confidently decide where to employ RL agents for
real-world tasks, a human developer must validate that the agent will perform
well at test-time. Some policy interpretability methods facilitate this by
capturing the policy's decision making in a set of agent rollouts. However,
even the most informative trajectories of training time behavior may give
little insight into the agent's behavior out of distribution. In contrast, our
method conveys how the agent performs under distribution shifts by showing the
agent's behavior across a wider trajectory distribution. We generate these
trajectories by guiding the agent to more diverse unseen states and showing the
agent's behavior there. In a user study, we demonstrate that our method enables
users to score better than baseline methods on one of two agent validation
tasks.",2022-11-13 16:03:16
XXX,journalArticle,2022,"Michael Laskin, Hao Liu, Xue Bin Peng, Denis Yarats, Aravind Rajeswaran, Pieter Abbeel",CIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery,,,,,http://arxiv.org/abs/2202.00161v3,"We introduce Contrastive Intrinsic Control (CIC), an algorithm for
unsupervised skill discovery that maximizes the mutual information between
state-transitions and latent skill vectors. CIC utilizes contrastive learning
between state-transitions and skills to learn behavior embeddings and maximizes
the entropy of these embeddings as an intrinsic reward to encourage behavioral
diversity. We evaluate our algorithm on the Unsupervised Reinforcement Learning
Benchmark, which consists of a long reward-free pre-training phase followed by
a short adaptation phase to downstream tasks with extrinsic rewards. CIC
substantially improves over prior methods in terms of adaptation efficiency,
outperforming prior unsupervised skill discovery methods by 1.79x and the next
leading overall exploration algorithm by 1.18x.",2022-11-13 16:03:17
XXX,journalArticle,2022,Ben Green,Technology Ethics in Action: Critical and Interdisciplinary Perspectives,Special Issue of the Journal of Social Computing (2021),,,,http://arxiv.org/abs/2202.01351v2,"This special issue interrogates the meaning and impacts of ""tech ethics"": the
embedding of ethics into digital technology research, development, use, and
governance. In response to concerns about the social harms associated with
digital technologies, many individuals and institutions have articulated the
need for a greater emphasis on ethics in digital technology. Yet as more groups
embrace the concept of ethics, critical discourses have emerged questioning
whose ethics are being centered, whether ""ethics"" is the appropriate frame for
improving technology, and what it means to develop ""ethical"" technology in
practice. This interdisciplinary issue takes up these questions, interrogating
the relationships among ethics, technology, and society in action. This special
issue engages with the normative and contested notions of ethics itself, how
ethics has been integrated with technology across domains, and potential paths
forward to support more just and egalitarian technology. Rather than starting
from philosophical theories, the authors in this issue orient their articles
around the real-world discourses and impacts of tech ethics--i.e., tech ethics
in action.",2022-11-13 16:03:17
XXX,journalArticle,2022,Remy Demichelis,Science Facing Interoperability as a Necessary Condition of Success and Evil,,,,,http://arxiv.org/abs/2202.02540v1,"Artificial intelligence (AI) systems, such as machine learning algorithms,
have allowed scientists, marketers and governments to shed light on
correlations that remained invisible until now. Beforehand, the dots that we
had to connect in order to imagine a new knowledge were either too numerous,
too sparse or not even detected. Sometimes, the information was not stored in
the same data lake or format and was not able to communicate. But in creating
new bridges with AI, many problems appeared such as bias reproduction, unfair
inferences or mass surveillance. Our aim is to show that, on one hand, the AI's
deep ethical problem lays essentially in these new connections made possible by
systems interoperability. In connecting the spheres of our life, these systems
undermine the notion of justice particular to each of them, because the new
interactions create dominances of social goods from a sphere to another. These
systems make therefore spheres permeable to one another and, in doing so, they
open to progress as well as to tyranny. On another hand, however, we would like
to emphasize that the act to connect what used to seem a priori disjoint is a
necessary move of knowledge and scientific progress.",2022-11-13 16:03:18
XXX,journalArticle,2022,"Ronny Luss, Amit Dhurandhar, Miao Liu",Local Explanations for Reinforcement Learning,,,,,http://arxiv.org/abs/2202.03597v1,"Many works in explainable AI have focused on explaining black-box
classification models. Explaining deep reinforcement learning (RL) policies in
a manner that could be understood by domain users has received much less
attention. In this paper, we propose a novel perspective to understanding RL
policies based on identifying important states from automatically learned
meta-states. The key conceptual difference between our approach and many
previous ones is that we form meta-states based on locality governed by the
expert policy dynamics rather than based on similarity of actions, and that we
do not assume any particular knowledge of the underlying topology of the state
space. Theoretically, we show that our algorithm to find meta-states converges
and the objective that selects important states from each meta-state is
submodular leading to efficient high quality greedy selection. Experiments on
four domains (four rooms, door-key, minipacman, and pong) and a carefully
conducted user study illustrate that our perspective leads to better
understanding of the policy. We conjecture that this is a result of our
meta-states being more intuitive in that the corresponding important states are
strong indicators of tractable intermediate goals that are easier for humans to
interpret and follow.",2022-11-13 16:03:18
XXX,journalArticle,2022,"Qinqing Zheng, Amy Zhang, Aditya Grover",Online Decision Transformer,,,,,http://arxiv.org/abs/2202.05607v2,"Recent work has shown that offline reinforcement learning (RL) can be
formulated as a sequence modeling problem (Chen et al., 2021; Janner et al.,
2021) and solved via approaches similar to large-scale language modeling.
However, any practical instantiation of RL also involves an online component,
where policies pretrained on passive offline datasets are finetuned via
taskspecific interactions with the environment. We propose Online Decision
Transformers (ODT), an RL algorithm based on sequence modeling that blends
offline pretraining with online finetuning in a unified framework. Our
framework uses sequence-level entropy regularizers in conjunction with
autoregressive modeling objectives for sample-efficient exploration and
finetuning. Empirically, we show that ODT is competitive with the
state-of-the-art in absolute performance on the D4RL benchmark but shows much
more significant gains during the finetuning procedure.",2022-11-13 16:03:19
XXX,journalArticle,2022,Christoph Benzmüller,A Simplified Variant of Gödel's Ontological Argument,,,,,http://arxiv.org/abs/2202.06264v2,"A simplified variant of G\""odel's ontological argument is presented. The
simplified argument is valid already in basic modal logics K or KT, it does not
suffer from modal collapse, and it avoids the rather complex predicates of
essence (Ess.) and necessary existence (NE) as used by G\""odel. The variant
presented has been obtained as a side result of a series of theory
simplification experiments conducted in interaction with a modern proof
assistant system. The starting point for these experiments was the computer
encoding of G\""odel's argument, and then automated reasoning techniques were
systematically applied to arrive at the simplified variant presented. The
presented work thus exemplifies a fruitful human-computer interaction in
computational metaphysics. Whether the presented result increases or decreases
the attractiveness and persuasiveness of the ontological argument is a question
I would like to pass on to philosophy and theology.",2022-11-13 16:03:20
XXX,journalArticle,2022,"Sebastiaan De Peuter, Samuel Kaski",Zero-Shot Assistance in Novel Decision Problems,,,,,http://arxiv.org/abs/2202.07364v2,"We consider the problem of creating assistants that can help agents - often
humans - solve novel sequential decision problems, assuming the agent is not
able to specify the reward function explicitly to the assistant. Instead of
aiming to automate, and act in place of the agent as in current approaches, we
give the assistant an advisory role and keep the agent in the loop as the main
decision maker. The difficulty is that we must account for potential biases
induced by limitations or constraints of the agent which may cause it to
seemingly irrationally reject advice. To do this we introduce a novel
formalization of assistance that models these biases, allowing the assistant to
infer and adapt to them. We then introduce a new method for planning the
assistant's advice which can scale to large decision making problems. Finally,
we show experimentally that our approach adapts to these agent biases, and
results in higher cumulative reward for the agent than automation-based
alternatives.",2022-11-13 16:03:20
XXX,journalArticle,2022,"Kanak Tekwani, Manojkumar Parmar",Critical Checkpoints for Evaluating Defence Models Against Adversarial Attack and Robustness,,,,,http://arxiv.org/abs/2202.09039v1,"From past couple of years there is a cycle of researchers proposing a defence
model for adversaries in machine learning which is arguably defensible to most
of the existing attacks in restricted condition (they evaluate on some bounded
inputs or datasets). And then shortly another set of researcher finding the
vulnerabilities in that defence model and breaking it by proposing a stronger
attack model. Some common flaws are been noticed in the past defence models
that were broken in very short time. Defence models being broken so easily is a
point of concern as decision of many crucial activities are taken with the help
of machine learning models. So there is an utter need of some defence
checkpoints that any researcher should keep in mind while evaluating the
soundness of technique and declaring it to be decent defence technique. In this
paper, we have suggested few checkpoints that should be taken into
consideration while building and evaluating the soundness of defence models.
All these points are recommended after observing why some past defence models
failed and how some model remained adamant and proved their soundness against
some of the very strong attacks.",2022-11-13 16:03:21
XXX,journalArticle,2022,"Alihan Hüyük, William R. Zame, Mihaela van der Schaar",Inferring Lexicographically-Ordered Rewards from Preferences,,,,,http://arxiv.org/abs/2202.10153v2,"Modeling the preferences of agents over a set of alternatives is a principal
concern in many areas. The dominant approach has been to find a single
reward/utility function with the property that alternatives yielding higher
rewards are preferred over alternatives yielding lower rewards. However, in
many settings, preferences are based on multiple, often competing, objectives;
a single reward function is not adequate to represent such preferences. This
paper proposes a method for inferring multi-objective reward-based
representations of an agent's observed preferences. We model the agent's
priorities over different objectives as entering lexicographically, so that
objectives with lower priorities matter only when the agent is indifferent with
respect to objectives with higher priorities. We offer two example applications
in healthcare, one inspired by cancer treatment, the other inspired by organ
transplantation, to illustrate how the lexicographically-ordered rewards we
learn can provide a better understanding of a decision-maker's preferences and
help improve policies when used in reinforcement learning.",2022-11-13 16:03:21
XXX,journalArticle,2022,"Kai Arulkumaran, Dylan R. Ashley, Jürgen Schmidhuber, Rupesh K. Srivastava",All You Need Is Supervised Learning: From Imitation Learning to Meta-RL With Upside Down RL,,,,,http://arxiv.org/abs/2202.11960v1,"Upside down reinforcement learning (UDRL) flips the conventional use of the
return in the objective function in RL upside down, by taking returns as input
and predicting actions. UDRL is based purely on supervised learning, and
bypasses some prominent issues in RL: bootstrapping, off-policy corrections,
and discount factors. While previous work with UDRL demonstrated it in a
traditional online RL setting, here we show that this single algorithm can also
work in the imitation learning and offline RL settings, be extended to the
goal-conditioned RL setting, and even the meta-RL setting. With a general agent
architecture, a single UDRL agent can learn across all paradigms.",2022-11-13 16:03:22
XXX,journalArticle,2022,"Ali Furkan Biten, Rubèn Tito, Lluis Gomez, Ernest Valveny, Dimosthenis Karatzas",OCR-IDL: OCR Annotations for Industry Document Library Dataset,,,,,http://arxiv.org/abs/2202.12985v1,"Pretraining has proven successful in Document Intelligence tasks where deluge
of documents are used to pretrain the models only later to be finetuned on
downstream tasks. One of the problems of the pretraining approaches is the
inconsistent usage of pretraining data with different OCR engines leading to
incomparable results between models. In other words, it is not obvious whether
the performance gain is coming from diverse usage of amount of data and
distinct OCR engines or from the proposed models. To remedy the problem, we
make public the OCR annotations for IDL documents using commercial OCR engine
given their superior performance over open source OCR models. The contributed
dataset (OCR-IDL) has an estimated monetary value over 20K US$. It is our hope
that OCR-IDL can be a starting point for future works on Document Intelligence.
All of our data and its collection process with the annotations can be found in
https://github.com/furkanbiten/idl_data.",2022-11-13 16:03:23
XXX,journalArticle,2022,"Hongzhi Wen, Jiayuan Ding, Wei Jin, Yiqi Wang, Yuying Xie, Jiliang Tang",Graph Neural Networks for Multimodal Single-Cell Data Integration,,,,10.1145/3534678.3539213,http://arxiv.org/abs/2203.01884v3,"Recent advances in multimodal single-cell technologies have enabled
simultaneous acquisitions of multiple omics data from the same cell, providing
deeper insights into cellular states and dynamics. However, it is challenging
to learn the joint representations from the multimodal data, model the
relationship between modalities, and, more importantly, incorporate the vast
amount of single-modality datasets into the downstream analyses. To address
these challenges and correspondingly facilitate multimodal single-cell data
analyses, three key tasks have been introduced: $\textit{modality prediction}$,
$\textit{modality matching}$ and $\textit{joint embedding}$. In this work, we
present a general Graph Neural Network framework $\textit{scMoGNN}$ to tackle
these three tasks and show that $\textit{scMoGNN}$ demonstrates superior
results in all three tasks compared with the state-of-the-art and conventional
approaches. Our method is an official winner in the overall ranking of
$\textit{Modality prediction}$ from NeurIPS 2021 Competition, and all
implementations of our methods have been integrated into DANCE
package~\url{https://github.com/OmicsML/dance}.",2022-11-13 16:03:23
XXX,journalArticle,2022,"Armin Moin, Ukrit Wattanavaekin, Alexandra Lungu, Moharram Challenger, Atta Badii, Stephan Günnemann",Enabling Automated Machine Learning for Model-Driven AI Engineering,,,,,http://arxiv.org/abs/2203.02927v1,"Developing smart software services requires both Software Engineering and
Artificial Intelligence (AI) skills. AI practitioners, such as data scientists
often focus on the AI side, for example, creating and training Machine Learning
(ML) models given a specific use case and data. They are typically not
concerned with the entire software development life-cycle, architectural
decisions for the system and performance issues beyond the predictive ML models
(e.g., regarding the security, privacy, throughput, scalability, availability,
as well as ethical, legal and regulatory compliance). In this manuscript, we
propose a novel approach to enable Model-Driven Software Engineering and
Model-Driven AI Engineering. In particular, we support Automated ML, thus
assisting software engineers without deep AI knowledge in developing
AI-intensive systems by choosing the most appropriate ML model, algorithm and
techniques with suitable hyper-parameters for the task at hand. To validate our
work, we carry out a case study in the smart energy domain.",2022-11-13 16:03:24
XXX,journalArticle,2022,"Felix Friedrich, Wolfgang Stammer, Patrick Schramowski, Kristian Kersting",A Typology to Explore the Mitigation of Shortcut Behavior,,,,,http://arxiv.org/abs/2203.03668v2,"As machine learning models become increasingly larger, trained weakly
supervised on large, possibly uncurated data sets, it becomes increasingly
important to establish mechanisms for inspecting, interacting, and revising
models to mitigate learning shortcuts and guarantee their learned knowledge is
aligned with human knowledge. The recently proposed XIL framework was developed
for this purpose, and several such methods have been introduced, each with
individual motivations and methodological details. In this work, we provide a
unification of various XIL methods into a single typology by establishing a
common set of basic modules. In doing so, we pave the way for a principled
comparison of existing, but, importantly, also future XIL approaches. In
addition, we discuss existing and introduce novel measures and benchmarks for
evaluating the overall abilities of a XIL method. Given this extensive toolbox,
including our typology, measures, and benchmarks, we finally compare several
recent XIL methods methodologically and quantitatively. In our evaluations, all
methods prove to revise a model successfully. However, we found remarkable
differences in individual benchmark tasks, revealing valuable
application-relevant aspects for integrating these benchmarks in developing
future methods.",2022-11-13 16:03:25
XXX,journalArticle,2022,"Yuan Gong, Sameer Khurana, Andrew Rouditchenko, James Glass",CMKD: CNN/Transformer-Based Cross-Model Knowledge Distillation for Audio Classification,,,,,http://arxiv.org/abs/2203.06760v1,"Audio classification is an active research area with a wide range of
applications. Over the past decade, convolutional neural networks (CNNs) have
been the de-facto standard building block for end-to-end audio classification
models. Recently, neural networks based solely on self-attention mechanisms
such as the Audio Spectrogram Transformer (AST) have been shown to outperform
CNNs. In this paper, we find an intriguing interaction between the two very
different models - CNN and AST models are good teachers for each other. When we
use either of them as the teacher and train the other model as the student via
knowledge distillation (KD), the performance of the student model noticeably
improves, and in many cases, is better than the teacher model. In our
experiments with this CNN/Transformer Cross-Model Knowledge Distillation (CMKD)
method we achieve new state-of-the-art performance on FSD50K, AudioSet, and
ESC-50.",2022-11-13 16:03:26
XXX,journalArticle,2022,"Michael Bohlke-Schneider, Shubham Kapoor, Tim Januschowski",Resilient Neural Forecasting Systems,,,,,http://arxiv.org/abs/2203.08492v1,"Industrial machine learning systems face data challenges that are often
under-explored in the academic literature. Common data challenges are data
distribution shifts, missing values and anomalies. In this paper, we discuss
data challenges and solutions in the context of a Neural Forecasting
application on labor planning.We discuss how to make this forecasting system
resilient to these data challenges. We address changes in data distribution
with a periodic retraining scheme and discuss the critical importance of model
stability in this setting. Furthermore, we show how our deep learning model
deals with missing values natively without requiring imputation. Finally, we
describe how we detect anomalies in the input data and mitigate their effect
before they impact the forecasts. This results in a fully autonomous
forecasting system that compares favorably to a hybrid system consisting of the
algorithm and human overrides.",2022-11-13 16:03:26
XXX,journalArticle,2022,"Qinghua Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Zhenchang Xing",Towards a Roadmap on Software Engineering for Responsible AI,,,,,http://arxiv.org/abs/2203.08594v1,"Although AI is transforming the world, there are serious concerns about its
ability to behave and make decisions responsibly. Many ethical regulations,
principles, and frameworks for responsible AI have been issued recently.
However, they are high level and difficult to put into practice. On the other
hand, most AI researchers focus on algorithmic solutions, while the responsible
AI challenges actually crosscut the entire engineering lifecycle and components
of AI systems. To close the gap in operationalizing responsible AI, this paper
aims to develop a roadmap on software engineering for responsible AI. The
roadmap focuses on (i) establishing multi-level governance for responsible AI
systems, (ii) setting up the development processes incorporating
process-oriented practices for responsible AI systems, and (iii) building
responsible-AI-by-design into AI systems through system-level architectural
style, patterns and techniques.",2022-11-13 16:03:27
XXX,journalArticle,2022,"Zhe Zhang, Yaozhong Gan, Xiaoyang Tan",Robust Action Gap Increasing with Clipped Advantage Learning,,,,,http://arxiv.org/abs/2203.11677v1,"Advantage Learning (AL) seeks to increase the action gap between the optimal
action and its competitors, so as to improve the robustness to estimation
errors. However, the method becomes problematic when the optimal action induced
by the approximated value function does not agree with the true optimal action.
In this paper, we present a novel method, named clipped Advantage Learning
(clipped AL), to address this issue. The method is inspired by our observation
that increasing the action gap blindly for all given samples while not taking
their necessities into account could accumulate more errors in the performance
loss bound, leading to a slow value convergence, and to avoid that, we should
adjust the advantage value adaptively. We show that our simple clipped AL
operator not only enjoys fast convergence guarantee but also retains proper
action gaps, hence achieving a good balance between the large action gap and
the fast convergence. The feasibility and effectiveness of the proposed method
are verified empirically on several RL benchmarks with promising performance.",2022-11-13 16:03:27
XXX,journalArticle,2022,"Markus Borg, Johan Bengtsson, Harald Österling, Alexander Hagelborn, Isabella Gagner, Piotr Tomaszewski",Quality Assurance of Generative Dialog Models in an Evolving Conversational Agent Used for Swedish Language Practice,,,,,http://arxiv.org/abs/2203.15414v1,"Due to the migration megatrend, efficient and effective second-language
acquisition is vital. One proposed solution involves AI-enabled conversational
agents for person-centered interactive language practice. We present results
from ongoing action research targeting quality assurance of proprietary
generative dialog models trained for virtual job interviews. The action team
elicited a set of 38 requirements for which we designed corresponding automated
test cases for 15 of particular interest to the evolving solution. Our results
show that six of the test case designs can detect meaningful differences
between candidate models. While quality assurance of natural language
processing applications is complex, we provide initial steps toward an
automated framework for machine learning model selection in the context of an
evolving conversational agent. Future work will focus on model selection in an
MLOps setting.",2022-11-13 16:03:28
XXX,journalArticle,2022,"Daniel Jarne Ornia, Manuel Mazo Jr",Robust Event-Driven Interactions in Cooperative Multi-Agent Learning,"Formal Modeling and Analysis of Timed Systems. FORMATS 2022.
  Lecture Notes in Computer Science, vol 13465. Springer, Cham",,,10.1007/978-3-031-15839-1_16,http://arxiv.org/abs/2204.03361v2,"We present an approach to reduce the communication required between agents in
a Multi-Agent learning system by exploiting the inherent robustness of the
underlying Markov Decision Process. We compute so-called robustness surrogate
functions (off-line), that give agents a conservative indication of how far
their state measurements can deviate before they need to update other agents in
the system. This results in fully distributed decision functions, enabling
agents to decide when it is necessary to update others. We derive bounds on the
optimality of the resulting systems in terms of the discounted sum of rewards
obtained, and show these bounds are a function of the design parameters.
Additionally, we extend the results for the case where the robustness surrogate
functions are learned from data, and present experimental results demonstrating
a significant reduction in communication events between agents.",2022-11-13 16:03:28
XXX,journalArticle,2022,"Chao Li, Jia Ning, Han Hu, Kun He","Enhancing the Robustness, Efficiency, and Diversity of Differentiable Architecture Search",,,,,http://arxiv.org/abs/2204.04681v1,"Differentiable architecture search (DARTS) has attracted much attention due
to its simplicity and significant improvement in efficiency. However, the
excessive accumulation of the skip connection makes it suffer from long-term
weak stability and low robustness. Many works attempt to restrict the
accumulation of skip connections by indicators or manual design, however, these
methods are susceptible to thresholds and human priors. In this work, we
suggest a more subtle and direct approach that removes skip connections from
the operation space. Then, by introducing an adaptive channel allocation
strategy, we redesign the DARTS framework to automatically refill the skip
connections in the evaluation stage, resolving the performance degradation
caused by the absence of skip connections. Our method, dubbed
Adaptive-Channel-Allocation-DARTS (ACA-DRATS), could eliminate the
inconsistency in operation strength and significantly expand the architecture
diversity. We continue to explore smaller search space under our framework, and
offer a direct search on the entire ImageNet dataset. Experiments show that
ACA-DRATS improves the search stability and significantly speeds up DARTS by
more than ten times while yielding higher accuracy.",2022-11-13 16:03:29
XXX,journalArticle,2022,"Fu-Chieh Chang, Yu-Wei Tseng, Ya-Wen Yu, Ssu-Rui Lee, Alexandru Cioba, I-Lun Tseng, Da-shan Shiu, Jhih-Wei Hsu, Cheng-Yuan Wang, Chien-Yi Yang, Ren-Chu Wang, Yao-Wen Chang, Tai-Chen Chen, Tung-Chieh Chen",Flexible Multiple-Objective Reinforcement Learning for Chip Placement,,,,,http://arxiv.org/abs/2204.06407v1,"Recently, successful applications of reinforcement learning to chip placement
have emerged. Pretrained models are necessary to improve efficiency and
effectiveness. Currently, the weights of objective metrics (e.g., wirelength,
congestion, and timing) are fixed during pretraining. However, fixed-weighed
models cannot generate the diversity of placements required for engineers to
accommodate changing requirements as they arise. This paper proposes flexible
multiple-objective reinforcement learning (MORL) to support objective functions
with inference-time variable weights using just a single pretrained model. Our
macro placement results show that MORL can generate the Pareto frontier of
multiple objectives effectively.",2022-11-13 16:03:29
XXX,journalArticle,2022,"Sahir, Ercüment İlhan, Srijita Das, Matthew E. Taylor",Methodical Advice Collection and Reuse in Deep Reinforcement Learning,,,,,http://arxiv.org/abs/2204.07254v1,"Reinforcement learning (RL) has shown great success in solving many
challenging tasks via use of deep neural networks. Although using deep learning
for RL brings immense representational power, it also causes a well-known
sample-inefficiency problem. This means that the algorithms are data-hungry and
require millions of training samples to converge to an adequate policy. One way
to combat this issue is to use action advising in a teacher-student framework,
where a knowledgeable teacher provides action advice to help the student. This
work considers how to better leverage uncertainties about when a student should
ask for advice and if the student can model the teacher to ask for less advice.
The student could decide to ask for advice when it is uncertain or when both it
and its model of the teacher are uncertain. In addition to this investigation,
this paper introduces a new method to compute uncertainty for a deep RL agent
using a secondary neural network. Our empirical results show that using dual
uncertainties to drive advice collection and reuse may improve learning
performance across several Atari games.",2022-11-13 16:03:30
XXX,journalArticle,2022,"Anna Yeaton, Rahul G. Krishnan, Rebecca Mieloszyk, David Alvarez-Melis, Grace Huynh",Hierarchical Optimal Transport for Comparing Histopathology Datasets,,,,,http://arxiv.org/abs/2204.08324v2,"Scarcity of labeled histopathology data limits the applicability of deep
learning methods to under-profiled cancer types and labels. Transfer learning
allows researchers to overcome the limitations of small datasets by
pre-training machine learning models on larger datasets similar to the small
target dataset. However, similarity between datasets is often determined
heuristically. In this paper, we propose a principled notion of distance
between histopathology datasets based on a hierarchical generalization of
optimal transport distances. Our method does not require any training, is
agnostic to model type, and preserves much of the hierarchical structure in
histopathology datasets imposed by tiling. We apply our method to H&E stained
slides from The Cancer Genome Atlas from six different cancer types. We show
that our method outperforms a baseline distance in a cancer-type prediction
task. Our results also show that our optimal transport distance predicts
difficulty of transferability in a tumor vs.normal prediction setting.",2022-11-13 16:03:31
XXX,journalArticle,2022,"Anton Korinek, Avital Balwit",Aligned with Whom? Direct and social goals for AI systems,,,,,http://arxiv.org/abs/2205.04279v1,"As artificial intelligence (AI) becomes more powerful and widespread, the AI
alignment problem - how to ensure that AI systems pursue the goals that we want
them to pursue - has garnered growing attention. This article distinguishes two
types of alignment problems depending on whose goals we consider, and analyzes
the different solutions necessitated by each. The direct alignment problem
considers whether an AI system accomplishes the goals of the entity operating
it. In contrast, the social alignment problem considers the effects of an AI
system on larger groups or on society more broadly. In particular, it also
considers whether the system imposes externalities on others. Whereas solutions
to the direct alignment problem center around more robust implementation,
social alignment problems typically arise because of conflicts between
individual and group-level goals, elevating the importance of AI governance to
mediate such conflicts. Addressing the social alignment problem requires both
enforcing existing norms on their developers and operators and designing new
norms that apply directly to AI systems.",2022-11-13 16:03:31
XXX,journalArticle,2022,Virginia Dignum,Responsible Artificial Intelligence -- from Principles to Practice,,,,,http://arxiv.org/abs/2205.10785v1,"The impact of Artificial Intelligence does not depend only on fundamental
research and technological developments, but for a large part on how these
systems are introduced into society and used in everyday situations. AI is
changing the way we work, live and solve challenges but concerns about
fairness, transparency or privacy are also growing. Ensuring responsible,
ethical AI is more than designing systems whose result can be trusted. It is
about the way we design them, why we design them, and who is involved in
designing them. In order to develop and use AI responsibly, we need to work
towards technical, societal, institutional and legal methods and tools which
provide concrete support to AI practitioners, as well as awareness and training
to enable participation of all, to ensure the alignment of AI systems with our
societies' principles and values.",2022-11-13 16:03:32
XXX,journalArticle,2022,"Giovanni De Toni, Paolo Viappiani, Bruno Lepri, Andrea Passerini",Generating personalized counterfactual interventions for algorithmic recourse by eliciting user preferences,,,,,http://arxiv.org/abs/2205.13743v1,"Counterfactual interventions are a powerful tool to explain the decisions of
a black-box decision process, and to enable algorithmic recourse. They are a
sequence of actions that, if performed by a user, can overturn an unfavourable
decision made by an automated decision system. However, most of the current
methods provide interventions without considering the user's preferences. For
example, a user might prefer doing certain actions with respect to others. In
this work, we present the first human-in-the-loop approach to perform
algorithmic recourse by eliciting user preferences. We introduce a polynomial
procedure to ask choice-set questions which maximize the Expected Utility of
Selection (EUS), and use it to iteratively refine our cost estimates in a
Bayesian setting. We integrate this preference elicitation strategy into a
reinforcement learning agent coupled with Monte Carlo Tree Search for efficient
exploration, so as to provide personalized interventions achieving algorithmic
recourse. An experimental evaluation on synthetic and real-world datasets shows
that a handful of queries allows to achieve a substantial reduction in the cost
of interventions with respect to user-independent alternatives.",2022-11-13 16:03:32
XXX,journalArticle,2022,"Xuan Bac Nguyen, Apoorva Bisht, Hugh Churchill, Khoa Luu",Two-Dimensional Quantum Material Identification via Self-Attention and Soft-labeling in Deep Learning,,,,,http://arxiv.org/abs/2205.15948v1,"In quantum machine field, detecting two-dimensional (2D) materials in Silicon
chips is one of the most critical problems. Instance segmentation can be
considered as a potential approach to solve this problem. However, similar to
other deep learning methods, the instance segmentation requires a large scale
training dataset and high quality annotation in order to achieve a considerable
performance. In practice, preparing the training dataset is a challenge since
annotators have to deal with a large image, e.g 2K resolution, and extremely
dense objects in this problem. In this work, we present a novel method to
tackle the problem of missing annotation in instance segmentation in 2D quantum
material identification. We propose a new mechanism for automatically detecting
false negative objects and an attention based loss strategy to reduce the
negative impact of these objects contributing to the overall loss function. We
experiment on the 2D material detection datasets, and the experiments show our
method outperforms previous works.",2022-11-13 16:03:33
XXX,journalArticle,2022,"Omer Antverg, Eyal Ben-David, Yonatan Belinkov",IDANI: Inference-time Domain Adaptation via Neuron-level Interventions,,,,,http://arxiv.org/abs/2206.00259v1,"Large pre-trained models are usually fine-tuned on downstream task data, and
tested on unseen data. When the train and test data come from different
domains, the model is likely to struggle, as it is not adapted to the test
domain. We propose a new approach for domain adaptation (DA), using
neuron-level interventions: We modify the representation of each test example
in specific neurons, resulting in a counterfactual example from the source
domain, which the model is more familiar with. The modified example is then fed
back into the model. While most other DA methods are applied during training
time, ours is applied during inference only, making it more efficient and
applicable. Our experiments show that our method improves performance on unseen
domains.",2022-11-13 16:03:33
XXX,journalArticle,2022,"Tero Karras, Miika Aittala, Timo Aila, Samuli Laine",Elucidating the Design Space of Diffusion-Based Generative Models,,,,,http://arxiv.org/abs/2206.00364v2,"We argue that the theory and practice of diffusion-based generative models
are currently unnecessarily convoluted and seek to remedy the situation by
presenting a design space that clearly separates the concrete design choices.
This lets us identify several changes to both the sampling and training
processes, as well as preconditioning of the score networks. Together, our
improvements yield new state-of-the-art FID of 1.79 for CIFAR-10 in a
class-conditional setting and 1.97 in an unconditional setting, with much
faster sampling (35 network evaluations per image) than prior designs. To
further demonstrate their modular nature, we show that our design changes
dramatically improve both the efficiency and quality obtainable with
pre-trained score networks from previous work, including improving the FID of a
previously trained ImageNet-64 model from 2.07 to near-SOTA 1.55, and after
re-training with our proposed improvements to a new SOTA of 1.36.",2022-11-13 16:03:34
XXX,journalArticle,2022,"Bao-Sinh Nguyen, Quang-Bach Tran, Tuan-Anh Nguyen Dang, Duc Nguyen, Hung Le",HYCEDIS: HYbrid Confidence Engine for Deep Document Intelligence System,,,,,http://arxiv.org/abs/2206.02628v2,"Measuring the confidence of AI models is critical for safely deploying AI in
real-world industrial systems. One important application of confidence
measurement is information extraction from scanned documents. However, there
exists no solution to provide reliable confidence score for current
state-of-the-art deep-learning-based information extractors. In this paper, we
propose a complete and novel architecture to measure confidence of current deep
learning models in document information extraction task. Our architecture
consists of a Multi-modal Conformal Predictor and a Variational
Cluster-oriented Anomaly Detector, trained to faithfully estimate its
confidence on its outputs without the need of host models modification. We
evaluate our architecture on real-wold datasets, not only outperforming
competing confidence estimators by a huge margin but also demonstrating
generalization ability to out-of-distribution data.",2022-11-13 16:03:35
XXX,journalArticle,2022,"Benjamin Eysenbach, Soumith Udatha, Sergey Levine, Ruslan Salakhutdinov",Imitating Past Successes can be Very Suboptimal,,,,,http://arxiv.org/abs/2206.03378v1,"Prior work has proposed a simple strategy for reinforcement learning (RL):
label experience with the outcomes achieved in that experience, and then
imitate the relabeled experience. These outcome-conditioned imitation learning
methods are appealing because of their simplicity, strong performance, and
close ties with supervised learning. However, it remains unclear how these
methods relate to the standard RL objective, reward maximization. In this
paper, we prove that existing outcome-conditioned imitation learning methods do
not necessarily improve the policy; rather, in some settings they can decrease
the expected reward. Nonetheless, we show that a simple modification results in
a method that does guarantee policy improvement, under some assumptions. Our
aim is not to develop an entirely new method, but rather to explain how a
variant of outcome-conditioned imitation learning can be used to maximize
rewards.",2022-11-13 16:03:35
XXX,journalArticle,2022,"Chengyang Ying, Xinning Zhou, Hang Su, Dong Yan, Ning Chen, Jun Zhu",Towards Safe Reinforcement Learning via Constraining Conditional Value-at-Risk,IJCAI 2022,,,,http://arxiv.org/abs/2206.04436v2,"Though deep reinforcement learning (DRL) has obtained substantial success, it
may encounter catastrophic failures due to the intrinsic uncertainty of both
transition and observation. Most of the existing methods for safe reinforcement
learning can only handle transition disturbance or observation disturbance
since these two kinds of disturbance affect different parts of the agent;
besides, the popular worst-case return may lead to overly pessimistic policies.
To address these issues, we first theoretically prove that the performance
degradation under transition disturbance and observation disturbance depends on
a novel metric of Value Function Range (VFR), which corresponds to the gap in
the value function between the best state and the worst state. Based on the
analysis, we adopt conditional value-at-risk (CVaR) as an assessment of risk
and propose a novel reinforcement learning algorithm of
CVaR-Proximal-Policy-Optimization (CPPO) which formalizes the risk-sensitive
constrained optimization problem by keeping its CVaR under a given threshold.
Experimental results show that CPPO achieves a higher cumulative reward and is
more robust against both observation and transition disturbances on a series of
continuous control tasks in MuJoCo.",2022-11-13 16:03:36
XXX,journalArticle,2022,"Yakov Miron, Chana Ross, Yuval Goldfracht, Chen Tessler, Dotan Di Castro",Towards Autonomous Grading In The Real World,,,,,http://arxiv.org/abs/2206.06091v2,"In this work, we aim to tackle the problem of autonomous grading, where a
dozer is required to flatten an uneven area. In addition, we explore methods
for bridging the gap between a simulated environment and real scenarios. We
design both a realistic physical simulation and a scaled real prototype
environment mimicking the real dozer dynamics and sensory information. We
establish heuristics and learning strategies in order to solve the problem.
Through extensive experimentation, we show that although heuristics are capable
of tackling the problem in a clean and noise-free simulated environment, they
fail catastrophically when facing real world scenarios. As the heuristics are
capable of successfully solving the task in the simulated environment, we show
they can be leveraged to guide a learning agent which can generalize and solve
the task both in simulation and in a scaled prototype environment.",2022-11-13 16:03:36
XXX,journalArticle,2022,"Maribeth Rauh, John Mellor, Jonathan Uesato, Po-Sen Huang, Johannes Welbl, Laura Weidinger, Sumanth Dathathri, Amelia Glaese, Geoffrey Irving, Iason Gabriel, William Isaac, Lisa Anne Hendricks",Characteristics of Harmful Text: Towards Rigorous Benchmarking of Language Models,,,,,http://arxiv.org/abs/2206.08325v2,"Large language models produce human-like text that drive a growing number of
applications. However, recent literature and, increasingly, real world
observations, have demonstrated that these models can generate language that is
toxic, biased, untruthful or otherwise harmful. Though work to evaluate
language model harms is under way, translating foresight about which harms may
arise into rigorous benchmarks is not straightforward. To facilitate this
translation, we outline six ways of characterizing harmful text which merit
explicit consideration when designing new benchmarks. We then use these
characteristics as a lens to identify trends and gaps in existing benchmarks.
Finally, we apply them in a case study of the Perspective API, a toxicity
classifier that is widely used in harm benchmarks. Our characteristics provide
one piece of the bridge that translates between foresight and effective
evaluation.",2022-11-13 16:03:37
XXX,journalArticle,2022,"Tengyang Xie, Akanksha Saran, Dylan J. Foster, Lekan Molu, Ida Momennejad, Nan Jiang, Paul Mineiro, John Langford",Interaction-Grounded Learning with Action-inclusive Feedback,,,,,http://arxiv.org/abs/2206.08364v2,"Consider the problem setting of Interaction-Grounded Learning (IGL), in which
a learner's goal is to optimally interact with the environment with no explicit
reward to ground its policies. The agent observes a context vector, takes an
action, and receives a feedback vector, using this information to effectively
optimize a policy with respect to a latent reward function. Prior analyzed
approaches fail when the feedback vector contains the action, which
significantly limits IGL's success in many potential scenarios such as
Brain-computer interface (BCI) or Human-computer interface (HCI) applications.
We address this by creating an algorithm and analysis which allows IGL to work
even when the feedback vector contains the action, encoded in any fashion. We
provide theoretical guarantees and large-scale experiments based on supervised
datasets to demonstrate the effectiveness of the new approach.",2022-11-13 16:03:38
XXX,journalArticle,2022,"Jean-Stanislas Denain, Jacob Steinhardt",Auditing Visualizations: Transparency Methods Struggle to Detect Anomalous Behavior,,,,,http://arxiv.org/abs/2206.13498v1,"Transparency methods such as model visualizations provide information that
outputs alone might miss, since they describe the internals of neural networks.
But can we trust that model explanations reflect model behavior? For instance,
can they diagnose abnormal behavior such as backdoors or shape bias? To
evaluate model explanations, we define a model as anomalous if it differs from
a reference set of normal models, and we test whether transparency methods
assign different explanations to anomalous and normal models. We find that
while existing methods can detect stark anomalies such as shape bias or
adversarial training, they struggle to identify more subtle anomalies such as
models trained on incomplete data. Moreover, they generally fail to distinguish
the inputs that induce anomalous behavior, e.g. images containing a backdoor
trigger. These results reveal new blind spots in existing model explanations,
pointing to the need for further method development.",2022-11-13 16:03:38
XXX,journalArticle,2022,"James McCarthy, Rahul Nair, Elizabeth Daly, Radu Marinescu, Ivana Dusparic",Boolean Decision Rules for Reinforcement Learning Policy Summarisation,,,,,http://arxiv.org/abs/2207.08651v1,"Explainability of Reinforcement Learning (RL) policies remains a challenging
research problem, particularly when considering RL in a safety context.
Understanding the decisions and intentions of an RL policy offer avenues to
incorporate safety into the policy by limiting undesirable actions. We propose
the use of a Boolean Decision Rules model to create a post-hoc rule-based
summary of an agent's policy. We evaluate our proposed approach using a DQN
agent trained on an implementation of a lava gridworld and show that, given a
hand-crafted feature representation of this gridworld, simple generalised rules
can be created, giving a post-hoc explainable summary of the agent's policy. We
discuss possible avenues to introduce safety into a RL agent's policy by using
rules generated by this rule-based model as constraints imposed on the agent's
policy, as well as discuss how creating simple rule summaries of an agent's
policy may help in the debugging process of RL agents.",2022-11-13 16:03:39
XXX,journalArticle,2022,"Stalin Muñoz Gutiérrez, Gerald Steinbauer-Wagner",The Need for a Meta-Architecture for Robot Autonomy,"EPTCS 362, 2022, pp. 81-97",,,10.4204/EPTCS.362.9,http://arxiv.org/abs/2207.09712v1,"Long-term autonomy of robotic systems implicitly requires dependable
platforms that are able to naturally handle hardware and software faults,
problems in behaviors, or lack of knowledge. Model-based dependable platforms
additionally require the application of rigorous methodologies during the
system development, including the use of correct-by-construction techniques to
implement robot behaviors. As the level of autonomy in robots increases, so do
the cost of offering guarantees about the dependability of the system.
Certifiable dependability of autonomous robots, we argue, can benefit from
formal models of the integration of several cognitive functions, knowledge
processing, reasoning, and meta-reasoning. Here we put forward the case for a
generative model of cognitive architectures for autonomous robotic agents that
subscribes to the principles of model-based engineering and certifiable
dependability, autonomic computing, and knowledge-enabled robotics.",2022-11-13 16:03:39
XXX,journalArticle,2022,"Haoran Xu, Xianyuan Zhan, Honglei Yin, Huiling Qin",Discriminator-Weighted Offline Imitation Learning from Suboptimal Demonstrations,,,,,http://arxiv.org/abs/2207.10050v1,"We study the problem of offline Imitation Learning (IL) where an agent aims
to learn an optimal expert behavior policy without additional online
environment interactions. Instead, the agent is provided with a supplementary
offline dataset from suboptimal behaviors. Prior works that address this
problem either require that expert data occupies the majority proportion of the
offline dataset, or need to learn a reward function and perform offline
reinforcement learning (RL) afterwards. In this paper, we aim to address the
problem without additional steps of reward learning and offline RL training for
the case when demonstrations contain a large proportion of suboptimal data.
Built upon behavioral cloning (BC), we introduce an additional discriminator to
distinguish expert and non-expert data. We propose a cooperation framework to
boost the learning of both tasks, Based on this framework, we design a new IL
algorithm, where the outputs of discriminator serve as the weights of the BC
loss. Experimental results show that our proposed algorithm achieves higher
returns and faster training speed compared to baseline algorithms.",2022-11-13 16:03:40
XXX,journalArticle,2022,"Kenneth Holstein, Maria De-Arteaga, Lakshmi Tumati, Yanghuidi Cheng",Toward Supporting Perceptual Complementarity in Human-AI Collaboration via Reflection on Unobservables,,,,,http://arxiv.org/abs/2207.13834v1,"In many real world contexts, successful human-AI collaboration requires
humans to productively integrate complementary sources of information into
AI-informed decisions. However, in practice human decision-makers often lack
understanding of what information an AI model has access to in relation to
themselves. There are few available guidelines regarding how to effectively
communicate about unobservables: features that may influence the outcome, but
which are unavailable to the model. In this work, we conducted an online
experiment to understand whether and how explicitly communicating potentially
relevant unobservables influences how people integrate model outputs and
unobservables when making predictions. Our findings indicate that presenting
prompts about unobservables can change how humans integrate model outputs and
unobservables, but do not necessarily lead to improved performance.
Furthermore, the impacts of these prompts can vary depending on
decision-makers' prior domain expertise. We conclude by discussing implications
for future research and design of AI-based decision support tools.",2022-11-13 16:03:40
XXX,journalArticle,2022,"Corban Rivera, Chace Ashcraft, Alexander New, James Schmidt, Gautam Vallabha",Latent Properties of Lifelong Learning Systems,,,,,http://arxiv.org/abs/2207.14378v1,"Creating artificial intelligence (AI) systems capable of demonstrating
lifelong learning is a fundamental challenge, and many approaches and metrics
have been proposed to analyze algorithmic properties. However, for existing
lifelong learning metrics, algorithmic contributions are confounded by task and
scenario structure. To mitigate this issue, we introduce an algorithm-agnostic
explainable surrogate-modeling approach to estimate latent properties of
lifelong learning algorithms. We validate the approach for estimating these
properties via experiments on synthetic data. To validate the structure of the
surrogate model, we analyze real performance data from a collection of popular
lifelong learning approaches and baselines adapted for lifelong classification
and lifelong reinforcement learning.",2022-11-13 16:03:41
XXX,journalArticle,2022,Jamie Harris,The History of AI Rights Research,,,,,http://arxiv.org/abs/2208.04714v2,"This report documents the history of research on AI rights and other moral
consideration of artificial entities. It highlights key intellectual influences
on this literature as well as research and academic discussion addressing the
topic more directly. We find that researchers addressing AI rights have often
seemed to be unaware of the work of colleagues whose interests overlap with
their own. Academic interest in this topic has grown substantially in recent
years; this reflects wider trends in academic research, but it seems that
certain influential publications, the gradual, accumulating ubiquity of AI and
robotic technology, and relevant news events may all have encouraged increased
academic interest in this specific topic. We suggest four levers that, if
pulled on in the future, might increase interest further: the adoption of
publication strategies similar to those of the most successful previous
contributors; increased engagement with adjacent academic fields and debates;
the creation of specialized journals, conferences, and research institutions;
and more exploration of legal rights for artificial entities.",2022-11-13 16:03:42
XXX,journalArticle,2022,"Olusola T. Odeyomi, Olubiyi O. Akintade, Temitayo O. Olowu, Gergely Zaruba",A Review of the Convergence of 5G/6G Architecture and Deep Learning,,,,,http://arxiv.org/abs/2208.07643v1,"The convergence of 5G architecture and deep learning has gained a lot of
research interests in both the fields of wireless communication and artificial
intelligence. This is because deep learning technologies have been identified
to be the potential driver of the 5G technologies, that make up the 5G
architecture. Hence, there have been extensive surveys on the convergence of 5G
architecture and deep learning. However, most of the existing survey papers
mainly focused on how deep learning can converge with a specific 5G technology,
thus, not covering the full spectrum of the 5G architecture. Although there is
a recent survey paper that appears to be robust, a review of that paper shows
that it is not well structured to specifically cover the convergence of deep
learning and the 5G technologies. Hence, this paper provides a robust overview
of the convergence of the key 5G technologies and deep learning. The challenges
faced by such convergence are discussed. In addition, a brief overview of the
future 6G architecture, and how it can converge with deep learning is also
discussed.",2022-11-13 16:03:42
XXX,journalArticle,2022,"Charlotte Siegmann, Markus Anderljung",The Brussels Effect and Artificial Intelligence: How EU regulation will impact the global AI market,,,,,http://arxiv.org/abs/2208.12645v1,"The European Union is likely to introduce among the first, most stringent,
and most comprehensive AI regulatory regimes of the world's major
jurisdictions. In this report, we ask whether the EU's upcoming regulation for
AI will diffuse globally, producing a so-called ""Brussels Effect"". Building on
and extending Anu Bradford's work, we outline the mechanisms by which such
regulatory diffusion may occur. We consider both the possibility that the EU's
AI regulation will incentivise changes in products offered in non-EU countries
(a de facto Brussels Effect) and the possibility it will influence regulation
adopted by other jurisdictions (a de jure Brussels Effect). Focusing on the
proposed EU AI Act, we tentatively conclude that both de facto and de jure
Brussels effects are likely for parts of the EU regulatory regime. A de facto
effect is particularly likely to arise in large US tech companies with AI
systems that the AI Act terms ""high-risk"". We argue that the upcoming
regulation might be particularly important in offering the first and most
influential operationalisation of what it means to develop and deploy
trustworthy or human-centred AI. If the EU regime is likely to see significant
diffusion, ensuring it is well-designed becomes a matter of global importance.",2022-11-13 16:03:43
XXX,journalArticle,2022,"Satwik Patnaik, Vasudev Gohil, Hao Guo, Jeyavijayan, Rajendran","Reinforcement Learning for Hardware Security: Opportunities, Developments, and Challenges",,,,,http://arxiv.org/abs/2208.13885v1,"Reinforcement learning (RL) is a machine learning paradigm where an
autonomous agent learns to make an optimal sequence of decisions by interacting
with the underlying environment. The promise demonstrated by RL-guided
workflows in unraveling electronic design automation problems has encouraged
hardware security researchers to utilize autonomous RL agents in solving
domain-specific problems. From the perspective of hardware security, such
autonomous agents are appealing as they can generate optimal actions in an
unknown adversarial environment. On the other hand, the continued globalization
of the integrated circuit supply chain has forced chip fabrication to
off-shore, untrustworthy entities, leading to increased concerns about the
security of the hardware. Furthermore, the unknown adversarial environment and
increasing design complexity make it challenging for defenders to detect subtle
modifications made by attackers (a.k.a. hardware Trojans). In this brief, we
outline the development of RL agents in detecting hardware Trojans, one of the
most challenging hardware security problems. Additionally, we outline potential
opportunities and enlist the challenges of applying RL to solve hardware
security problems.",2022-11-13 16:03:43
XXX,journalArticle,2022,"James R. Kirk, Robert E. Wray, Peter Lindes, John E. Laird",Improving Language Model Prompting in Support of Semi-autonomous Task Learning,,,,,http://arxiv.org/abs/2209.07636v1,"Language models (LLMs) offer potential as a source of knowledge for agents
that need to acquire new task competencies within a performance environment. We
describe efforts toward a novel agent capability that can construct cues (or
""prompts"") that result in useful LLM responses for an agent learning a new
task. Importantly, responses must not only be ""reasonable"" (a measure used
commonly in research on knowledge extraction from LLMs) but also specific to
the agent's task context and in a form that the agent can interpret given its
native language capacities. We summarize a series of empirical investigations
of prompting strategies and evaluate responses against the goals of targeted
and actionable responses for task learning. Our results demonstrate that
actionable task knowledge can be obtained from LLMs in support of online agent
task learning.",2022-11-13 16:03:43
XXX,journalArticle,2022,"Hosein Hasanbeig, Daniel Kroening, Alessandro Abate",LCRL: Certified Policy Synthesis via Logically-Constrained Reinforcement Learning,,,,,http://arxiv.org/abs/2209.10341v1,"LCRL is a software tool that implements model-free Reinforcement Learning
(RL) algorithms over unknown Markov Decision Processes (MDPs), synthesising
policies that satisfy a given linear temporal specification with maximal
probability. LCRL leverages partially deterministic finite-state machines known
as Limit Deterministic Buchi Automata (LDBA) to express a given linear temporal
specification. A reward function for the RL algorithm is shaped on-the-fly,
based on the structure of the LDBA. Theoretical guarantees under proper
assumptions ensure the convergence of the RL algorithm to an optimal policy
that maximises the satisfaction probability. We present case studies to
demonstrate the applicability, ease of use, scalability, and performance of
LCRL. Owing to the LDBA-guided exploration and LCRL model-free architecture, we
observe robust performance, which also scales well when compared to standard RL
approaches (whenever applicable to LTL specifications). Full instructions on
how to execute all the case studies in this paper are provided on a GitHub page
that accompanies the LCRL distribution www.github.com/grockious/lcrl.",2022-11-13 16:03:44
XXX,journalArticle,2022,"Lunjun Zhang, Bradly C. Stadie",Understanding Hindsight Goal Relabeling Requires Rethinking Divergence Minimization,,,,,http://arxiv.org/abs/2209.13046v1,"Hindsight goal relabeling has become a foundational technique for multi-goal
reinforcement learning (RL). The idea is quite simple: any arbitrary trajectory
can be seen as an expert demonstration for reaching the trajectory's end state.
Intuitively, this procedure trains a goal-conditioned policy to imitate a
sub-optimal expert. However, this connection between imitation and hindsight
relabeling is not well understood. Modern imitation learning algorithms are
described in the language of divergence minimization, and yet it remains an
open problem how to recast hindsight goal relabeling into that framework. In
this work, we develop a unified objective for goal-reaching that explains such
a connection, from which we can derive goal-conditioned supervised learning
(GCSL) and the reward function in hindsight experience replay (HER) from first
principles. Experimentally, we find that despite recent advances in
goal-conditioned behaviour cloning (BC), multi-goal Q-learning can still
outperform BC-like methods; moreover, a vanilla combination of both actually
hurts model performance. Under our framework, we study when BC is expected to
help, and empirically validate our findings. Our work further bridges
goal-reaching and generative modeling, illustrating the nuances and new
pathways of extending the success of generative models to RL.",2022-11-13 16:03:45
XXX,journalArticle,2022,"Jialu Zhang, José Cambronero, Sumit Gulwani, Vu Le, Ruzica Piskac, Gustavo Soares, Gust Verbruggen",Repairing Bugs in Python Assignments Using Large Language Models,,,,,http://arxiv.org/abs/2209.14876v1,"Students often make mistakes on their introductory programming assignments as
part of their learning process. Unfortunately, providing custom repairs for
these mistakes can require a substantial amount of time and effort from class
instructors. Automated program repair (APR) techniques can be used to
synthesize such fixes. Prior work has explored the use of symbolic and neural
techniques for APR in the education domain. Both types of approaches require
either substantial engineering efforts or large amounts of data and training.
We propose to use a large language model trained on code, such as Codex, to
build an APR system -- MMAPR -- for introductory Python programming
assignments. Our system can fix both syntactic and semantic mistakes by
combining multi-modal prompts, iterative querying, test-case-based selection of
few-shots, and program chunking. We evaluate MMAPR on 286 real student programs
and compare to a baseline built by combining a state-of-the-art Python syntax
repair engine, BIFI, and state-of-the-art Python semantic repair engine for
student assignments, Refactory. We find that MMAPR can fix more programs and
produce smaller patches on average.",2022-11-13 16:03:46
XXX,journalArticle,2022,"Burcu Sayin, Fabio Casati, Andrea Passerini, Jie Yang, Xinyue Chen",Rethinking and Recomputing the Value of ML Models,,,,,http://arxiv.org/abs/2209.15157v1,"In this paper, we argue that the way we have been training and evaluating ML
models has largely forgotten the fact that they are applied in an organization
or societal context as they provide value to people. We show that with this
perspective we fundamentally change how we evaluate, select and deploy ML
models - and to some extent even what it means to learn. Specifically, we
stress that the notion of value plays a central role in learning and
evaluating, and different models may require different learning practices and
provide different values based on the application context they are applied. We
also show that this concretely impacts how we select and embed models into
human workflows based on experimental datasets. Nothing of what is presented
here is hard: to a large extent is a series of fairly trivial observations with
massive practical implications.",2022-11-13 16:03:46
XXX,journalArticle,2022,"Arjun Krishnakumar, Colin White, Arber Zela, Renbo Tu, Mahmoud Safari, Frank Hutter",NAS-Bench-Suite-Zero: Accelerating Research on Zero Cost Proxies,,,,,http://arxiv.org/abs/2210.03230v1,"Zero-cost proxies (ZC proxies) are a recent architecture performance
prediction technique aiming to significantly speed up algorithms for neural
architecture search (NAS). Recent work has shown that these techniques show
great promise, but certain aspects, such as evaluating and exploiting their
complementary strengths, are under-studied. In this work, we create
NAS-Bench-Suite: we evaluate 13 ZC proxies across 28 tasks, creating by far the
largest dataset (and unified codebase) for ZC proxies, enabling
orders-of-magnitude faster experiments on ZC proxies, while avoiding
confounding factors stemming from different implementations. To demonstrate the
usefulness of NAS-Bench-Suite, we run a large-scale analysis of ZC proxies,
including a bias analysis, and the first information-theoretic analysis which
concludes that ZC proxies capture substantial complementary information.
Motivated by these findings, we present a procedure to improve the performance
of ZC proxies by reducing biases such as cell size, and we also show that
incorporating all 13 ZC proxies into the surrogate models used by NAS
algorithms can improve their predictive performance by up to 42%. Our code and
datasets are available at https://github.com/automl/naslib/tree/zerocost.",2022-11-13 16:03:47
XXX,journalArticle,2022,"Zih-Yun Chiu, Yi-Lin Tuan, William Yang Wang, Michael C. Yip",Knowledge-Grounded Reinforcement Learning,,,,,http://arxiv.org/abs/2210.03729v1,"Receiving knowledge, abiding by laws, and being aware of regulations are
common behaviors in human society. Bearing in mind that reinforcement learning
(RL) algorithms benefit from mimicking humanity, in this work, we propose that
an RL agent can act on external guidance in both its learning process and model
deployment, making the agent more socially acceptable. We introduce the
concept, Knowledge-Grounded RL (KGRL), with a formal definition that an agent
learns to follow external guidelines and develop its own policy. Moving towards
the goal of KGRL, we propose a novel actor model with an embedding-based
attention mechanism that can attend to either a learnable internal policy or
external knowledge. The proposed method is orthogonal to training algorithms,
and the external knowledge can be flexibly recomposed, rearranged, and reused
in both training and inference stages. Through experiments on tasks with
discrete and continuous action space, our KGRL agent is shown to be more sample
efficient and generalizable, and it has flexibly rearrangeable knowledge
embeddings and interpretable behaviors.",2022-11-13 16:03:47
XXX,journalArticle,2022,"Maitrey Gramopadhye, Daniel Szafir",Generating Executable Action Plans with Environmentally-Aware Language Models,,,,,http://arxiv.org/abs/2210.04964v1,"Large Language Models (LLMs) trained using massive text datasets have
recently shown promise in generating action plans for robotic agents from high
level text queries. However, these models typically do not consider the robot's
environment, resulting in generated plans that may not actually be executable
due to ambiguities in the planned actions or environmental constraints. In this
paper, we propose an approach to generate environmentally-aware action plans
that can be directly mapped to executable agent actions. Our approach involves
integrating environmental objects and object relations as additional inputs
into LLM action plan generation to provide the system with an awareness of its
surroundings, resulting in plans where each generated action is mapped to
objects present in the scene. We also design a novel scoring function that,
along with generating the action steps and associating them with objects, helps
the system disambiguate among object instances and take into account their
states. We evaluate our approach using the VirtualHome simulator and the
ActivityPrograms knowledge base. Our results show that the action plans
generated from our system outperform prior work in terms of their correctness
and executability by 5.3% and 8.9% respectively.",2022-11-13 16:03:47
XXX,journalArticle,2022,"Vaibhav Bajaj, Guni Sharon, Peter Stone",Task Phasing: Automated Curriculum Learning from Demonstrations,,,,,http://arxiv.org/abs/2210.10999v1,"Applying reinforcement learning (RL) to sparse reward domains is notoriously
challenging due to insufficient guiding signals. Common techniques for
addressing such domains include (1) learning from demonstrations and (2)
curriculum learning. While these two approaches have been studied in detail,
they have rarely been considered together. This paper aims to do so by
introducing a principled task phasing approach that uses demonstrations to
automatically generate a curriculum sequence. Using inverse RL from
(suboptimal) demonstrations we define a simple initial task. Our task phasing
approach then provides a framework to gradually increase the complexity of the
task all the way to the target task, while retuning the RL agent in each
phasing iteration. Two approaches for phasing are considered: (1) gradually
increasing the proportion of time steps an RL agent is in control, and (2)
phasing out a guiding informative reward function. We present conditions that
guarantee the convergence of these approaches to an optimal policy.
Experimental results on 3 sparse reward domains demonstrate that our task
phasing approaches outperform state-of-the-art approaches with respect to their
asymptotic performance.",2022-11-13 16:03:48
XXX,journalArticle,2022,Yanick Schraner,Teacher-student curriculum learning for reinforcement learning,,,,,http://arxiv.org/abs/2210.17368v1,"Reinforcement learning (rl) is a popular paradigm for sequential decision
making problems. The past decade's advances in rl have led to breakthroughs in
many challenging domains such as video games, board games, robotics, and chip
design. The sample inefficiency of deep reinforcement learning methods is a
significant obstacle when applying rl to real-world problems. Transfer learning
has been applied to reinforcement learning such that the knowledge gained in
one task can be applied when training in a new task. Curriculum learning is
concerned with sequencing tasks or data samples such that knowledge can be
transferred between those tasks to learn a target task that would otherwise be
too difficult to solve. Designing a curriculum that improves sample efficiency
is a complex problem. In this thesis, we propose a teacher-student curriculum
learning setting where we simultaneously train a teacher that selects tasks for
the student while the student learns how to solve the selected task. Our method
is independent of human domain knowledge and manual curriculum design. We
evaluated our methods on two reinforcement learning benchmarks: grid world and
the challenging Google Football environment. With our method, we can improve
the sample efficiency and generality of the student compared to tabula-rasa
reinforcement learning.",2022-11-13 16:03:48
XXX,journalArticle,2011,"S. S. Fatima, N. R. Jennings, M. J. Wooldridge",Multi-Issue Negotiation with Deadlines,"Journal Of Artificial Intelligence Research, Volume 27, pages
  381-417, 2006",,,10.1613/jair.2056,http://arxiv.org/abs/1110.2765v1,"This paper studies bilateral multi-issue negotiation between self-interested
autonomous agents. Now, there are a number of different procedures that can be
used for this process; the three main ones being the package deal procedure in
which all the issues are bundled and discussed together, the simultaneous
procedure in which the issues are discussed simultaneously but independently of
each other, and the sequential procedure in which the issues are discussed one
after another. Since each of them yields a different outcome, a key problem is
to decide which one to use in which circumstances. Specifically, we consider
this question for a model in which the agents have time constraints (in the
form of both deadlines and discount factors) and information uncertainty (in
that the agents do not know the opponents utility function). For this model, we
consider issues that are both independent and those that are interdependent and
determine equilibria for each case for each procedure. In so doing, we show
that the package deal is in fact the optimal procedure for each party. We then
go on to show that, although the package deal may be computationally more
complex than the other two procedures, it generates Pareto optimal outcomes
(unlike the other two), it has similar earliest and latest possible times of
agreement to the simultaneous procedure (which is better than the sequential
procedure), and that it (like the other two procedures) generates a unique
outcome only under certain conditions (which we define).",2022-11-13 16:03:49
XXX,journalArticle,2012,"Umar Syed, Robert E. Schapire",Imitation Learning with a Value-Based Prior,,,,,http://arxiv.org/abs/1206.5290v1,"The goal of imitation learning is for an apprentice to learn how to behave in
a stochastic environment by observing a mentor demonstrating the correct
behavior. Accurate prior knowledge about the correct behavior can reduce the
need for demonstrations from the mentor. We present a novel approach to
encoding prior knowledge about the correct behavior, where we assume that this
prior knowledge takes the form of a Markov Decision Process (MDP) that is used
by the apprentice as a rough and imperfect model of the mentor's behavior.
Specifically, taking a Bayesian approach, we treat the value of a policy in
this modeling MDP as the log prior probability of the policy. In other words,
we assume a priori that the mentor's behavior is likely to be a high value
policy in the modeling MDP, though quite possibly different from the optimal
policy. We describe an efficient algorithm that, given a modeling MDP and a set
of demonstrations by a mentor, provably converges to a stationary point of the
log posterior of the mentor's policy, where the posterior is computed with
respect to the ""value based"" prior. We also present empirical evidence that
this prior does in fact speed learning of the mentor's policy, and is an
improvement in our experiments over similar previous methods.",2022-11-13 16:03:49
XXX,journalArticle,2016,"Ben McCamish, Vahid Ghadakchi, Arash Termehchy, Behrouz Touri",A Signaling Game Approach to Databases Querying and Interaction,,,,,http://arxiv.org/abs/1603.04068v5,"As most users do not precisely know the structure and/or the content of
databases, their queries do not exactly reflect their information needs. The
database management systems (DBMS) may interact with users and use their
feedback on the returned results to learn the information needs behind their
queries. Current query interfaces assume that users do not learn and modify the
way way they express their information needs in form of queries during their
interaction with the DBMS. Using a real-world interaction workload, we show
that users learn and modify how to express their information needs during their
interactions with the DBMS and their learning is accurately modeled by a
well-known reinforcement learning mechanism. As current data interaction
systems assume that users do not modify their strategies, they cannot discover
the information needs behind users' queries effectively. We model the
interaction between users and DBMS as a game with identical interest between
two rational agents whose goal is to establish a common language for
representing information needs in form of queries. We propose a reinforcement
learning method that learns and answers the information needs behind queries
and adapts to the changes in users' strategies and prove that it improves the
effectiveness of answering queries stochastically speaking. We propose two
efficient implementation of this method over large relational databases. Our
extensive empirical studies over real-world query workloads indicate that our
algorithms are efficient and effective.",2022-11-13 16:03:50
XXX,journalArticle,2017,"Azalia Mirhoseini, Hieu Pham, Quoc V. Le, Benoit Steiner, Rasmus Larsen, Yuefeng Zhou, Naveen Kumar, Mohammad Norouzi, Samy Bengio, Jeff Dean",Device Placement Optimization with Reinforcement Learning,,,,,http://arxiv.org/abs/1706.04972v2,"The past few years have witnessed a growth in size and computational
requirements for training and inference with neural networks. Currently, a
common approach to address these requirements is to use a heterogeneous
distributed environment with a mixture of hardware devices such as CPUs and
GPUs. Importantly, the decision of placing parts of the neural models on
devices is often made by human experts based on simple heuristics and
intuitions. In this paper, we propose a method which learns to optimize device
placement for TensorFlow computational graphs. Key to our method is the use of
a sequence-to-sequence model to predict which subsets of operations in a
TensorFlow graph should run on which of the available devices. The execution
time of the predicted placements is then used as the reward signal to optimize
the parameters of the sequence-to-sequence model. Our main result is that on
Inception-V3 for ImageNet classification, and on RNN LSTM, for language
modeling and neural machine translation, our model finds non-trivial device
placements that outperform hand-crafted heuristics and traditional algorithmic
methods.",2022-11-13 16:03:50
XXX,journalArticle,2017,"Anirban Santara, Abhishek Naik, Balaraman Ravindran, Dipankar Das, Dheevatsa Mudigere, Sasikanth Avancha, Bharat Kaul",RAIL: Risk-Averse Imitation Learning,,,,,http://arxiv.org/abs/1707.06658v4,"Imitation learning algorithms learn viable policies by imitating an expert's
behavior when reward signals are not available. Generative Adversarial
Imitation Learning (GAIL) is a state-of-the-art algorithm for learning policies
when the expert's behavior is available as a fixed set of trajectories. We
evaluate in terms of the expert's cost function and observe that the
distribution of trajectory-costs is often more heavy-tailed for GAIL-agents
than the expert at a number of benchmark continuous-control tasks. Thus,
high-cost trajectories, corresponding to tail-end events of catastrophic
failure, are more likely to be encountered by the GAIL-agents than the expert.
This makes the reliability of GAIL-agents questionable when it comes to
deployment in risk-sensitive applications like robotic surgery and autonomous
driving. In this work, we aim to minimize the occurrence of tail-end events by
minimizing tail risk within the GAIL framework. We quantify tail risk by the
Conditional-Value-at-Risk (CVaR) of trajectories and develop the Risk-Averse
Imitation Learning (RAIL) algorithm. We observe that the policies learned with
RAIL show lower tail-end risk than those of vanilla GAIL. Thus the proposed
RAIL algorithm appears as a potent alternative to GAIL for improved reliability
in risk-sensitive applications.",2022-11-13 16:03:51
XXX,journalArticle,2018,"Abhinav Verma, Vijayaraghavan Murali, Rishabh Singh, Pushmeet Kohli, Swarat Chaudhuri",Programmatically Interpretable Reinforcement Learning,PMLR 80:5045-5054,,,,http://arxiv.org/abs/1804.02477v3,"We present a reinforcement learning framework, called Programmatically
Interpretable Reinforcement Learning (PIRL), that is designed to generate
interpretable and verifiable agent policies. Unlike the popular Deep
Reinforcement Learning (DRL) paradigm, which represents policies by neural
networks, PIRL represents policies using a high-level, domain-specific
programming language. Such programmatic policies have the benefits of being
more easily interpreted than neural networks, and being amenable to
verification by symbolic methods. We propose a new method, called Neurally
Directed Program Search (NDPS), for solving the challenging nonsmooth
optimization problem of finding a programmatic policy with maximal reward. NDPS
works by first learning a neural policy network using DRL, and then performing
a local search over programmatic policies that seeks to minimize a distance
from this neural ""oracle"". We evaluate NDPS on the task of learning to drive a
simulated car in the TORCS car-racing environment. We demonstrate that NDPS is
able to discover human-readable policies that pass some significant performance
bars. We also show that PIRL policies can have smoother trajectories, and can
be more easily transferred to environments not encountered during training,
than corresponding policies discovered by DRL.",2022-11-13 16:03:51
XXX,journalArticle,2018,"Nanqing Dong, Michael Kampffmeyer, Xiaodan Liang, Zeya Wang, Wei Dai, Eric P. Xing",Reinforced Auto-Zoom Net: Towards Accurate and Fast Breast Cancer Segmentation in Whole-slide Images,,,,,http://arxiv.org/abs/1807.11113v1,"Convolutional neural networks have led to significant breakthroughs in the
domain of medical image analysis. However, the task of breast cancer
segmentation in whole-slide images (WSIs) is still underexplored. WSIs are
large histopathological images with extremely high resolution. Constrained by
the hardware and field of view, using high-magnification patches can slow down
the inference process and using low-magnification patches can cause the loss of
information. In this paper, we aim to achieve two seemingly conflicting goals
for breast cancer segmentation: accurate and fast prediction. We propose a
simple yet efficient framework Reinforced Auto-Zoom Net (RAZN) to tackle this
task. Motivated by the zoom-in operation of a pathologist using a digital
microscope, RAZN learns a policy network to decide whether zooming is required
in a given region of interest. Because the zoom-in action is selective, RAZN is
robust to unbalanced and noisy ground truth labels and can efficiently reduce
overfitting. We evaluate our method on a public breast cancer dataset. RAZN
outperforms both single-scale and multi-scale baseline approaches, achieving
better accuracy at low inference cost.",2022-11-13 16:03:52
XXX,journalArticle,2018,"Nir Douer, Joachim Meyer",The Responsibility Quantification (ResQu) Model of Human Interaction with Automation,"IEEE Transactions on Automation Science and Engineering, 17 (2),
  1044-1060 (2020)",,,10.1109/TASE.2020.2965466,http://arxiv.org/abs/1810.12644v4,"Intelligent systems and advanced automation are involved in information
collection and evaluation, in decision-making and in the implementation of
chosen actions. In such systems, human responsibility becomes equivocal.
Understanding human casual responsibility is particularly important when
intelligent autonomous systems can harm people, as with autonomous vehicles or,
most notably, with autonomous weapon systems (AWS). Using Information Theory,
we develop a responsibility quantification (ResQu) model of human involvement
in intelligent automated systems and demonstrate its applications on decisions
regarding AWS. The analysis reveals that human comparative responsibility to
outcomes is often low, even when major functions are allocated to the human.
Thus, broadly stated policies of keeping humans in the loop and having
meaningful human control are misleading and cannot truly direct decisions on
how to involve humans in intelligent systems and advanced automation. The
current model is an initial step in the complex goal to create a comprehensive
responsibility model, that will enable quantification of human causal
responsibility. It assumes stationarity, full knowledge regarding the
characteristic of the human and automation and ignores temporal aspects.
Despite these limitations, it can aid in the analysis of systems designs
alternatives and policy decisions regarding human responsibility in intelligent
systems and advanced automation.",2022-11-13 16:03:52
XXX,journalArticle,2018,Alexander Peysakhovich,Reinforcement Learning and Inverse Reinforcement Learning with System 1 and System 2,,,,,http://arxiv.org/abs/1811.08549v2,"Inferring a person's goal from their behavior is an important problem in
applications of AI (e.g. automated assistants, recommender systems). The
workhorse model for this task is the rational actor model - this amounts to
assuming that people have stable reward functions, discount the future
exponentially, and construct optimal plans. Under the rational actor assumption
techniques such as inverse reinforcement learning (IRL) can be used to infer a
person's goals from their actions. A competing model is the dual-system model.
Here decisions are the result of an interplay between a fast, automatic,
heuristic-based system 1 and a slower, deliberate, calculating system 2. We
generalize the dual system framework to the case of Markov decision problems
and show how to compute optimal plans for dual-system agents. We show that
dual-system agents exhibit behaviors that are incompatible with rational actor
assumption. We show that naive applications of rational-actor IRL to the
behavior of dual-system agents can generate wrong inference about the agents'
goals and suggest interventions that actually reduce the agent's overall
utility. Finally, we adapt a simple IRL algorithm to correctly infer the goals
of dual system decision-makers. This allows us to make interventions that help,
rather than hinder, the dual-system agent's ability to reach their true goals.",2022-11-13 16:03:53
XXX,journalArticle,2018,"Julia A. Meister, Raja Naeem Akram, Konstantinos Markantonakis",Deep Learning Application in Security and Privacy -- Theory and Practice: A Position Paper,,,,,http://arxiv.org/abs/1812.00190v1,"Technology is shaping our lives in a multitude of ways. This is fuelled by a
technology infrastructure, both legacy and state of the art, composed of a
heterogeneous group of hardware, software, services and organisations. Such
infrastructure faces a diverse range of challenges to its operations that
include security, privacy, resilience, and quality of services. Among these,
cybersecurity and privacy are taking the centre-stage, especially since the
General Data Protection Regulation (GDPR) came into effect. Traditional
security and privacy techniques are overstretched and adversarial actors have
evolved to design exploitation techniques that circumvent protection. With the
ever-increasing complexity of technology infrastructure, security and
privacy-preservation specialists have started to look for adaptable and
flexible protection methods that can evolve (potentially autonomously) as the
adversarial actor changes its techniques. For this, Artificial Intelligence
(AI), Machine Learning (ML) and Deep Learning (DL) were put forward as
saviours. In this paper, we look at the promises of AI, ML, and DL stated in
academic and industrial literature and evaluate how realistic they are. We also
put forward potential challenges a DL based security and privacy protection
technique has to overcome. Finally, we conclude the paper with a discussion on
what steps the DL and the security and privacy-preservation community have to
take to ensure that DL is not just going to be hype, but an opportunity to
build a secure, reliable, and trusted technology infrastructure on which we can
rely on for so much in our lives.",2022-11-13 16:03:53
XXX,journalArticle,2018,"Minghao Guo, Zhao Zhong, Wei Wu, Dahua Lin, Junjie Yan",IRLAS: Inverse Reinforcement Learning for Architecture Search,,,,,http://arxiv.org/abs/1812.05285v5,"In this paper, we propose an inverse reinforcement learning method for
architecture search (IRLAS), which trains an agent to learn to search network
structures that are topologically inspired by human-designed network. Most
existing architecture search approaches totally neglect the topological
characteristics of architectures, which results in complicated architecture
with a high inference latency. Motivated by the fact that human-designed
networks are elegant in topology with a fast inference speed, we propose a
mirror stimuli function inspired by biological cognition theory to extract the
abstract topological knowledge of an expert human-design network (ResNeXt). To
avoid raising a too strong prior over the search space, we introduce inverse
reinforcement learning to train the mirror stimuli function and exploit it as a
heuristic guidance for architecture search, easily generalized to different
architecture search algorithms. On CIFAR-10, the best architecture searched by
our proposed IRLAS achieves 2.60% error rate. For ImageNet mobile setting, our
model achieves a state-of-the-art top-1 accuracy 75.28%, while being 2~4x
faster than most auto-generated architectures. A fast version of this model
achieves 10% faster than MobileNetV2, while maintaining a higher accuracy.",2022-11-13 16:03:54
XXX,journalArticle,2019,"Ross Gruetzemacher, David Paradice, Kang Bok Lee",Forecasting Transformative AI: An Expert Survey,,,,,http://arxiv.org/abs/1901.08579v2,"Transformative AI technologies have the potential to reshape critical aspects
of society in the near future. However, in order to properly prepare policy
initiatives for the arrival of such technologies accurate forecasts and
timelines are necessary. A survey was administered to attendees of three AI
conferences during the summer of 2018 (ICML, IJCAI and the HLAI conference).
The survey included questions for estimating AI capabilities over the next
decade, questions for forecasting five scenarios of transformative AI and
questions concerning the impact of computational resources in AI research.
Respondents indicated a median of 21.5% of human tasks (i.e., all tasks that
humans are currently paid to do) can be feasibly automated now, and that this
figure would rise to 40% in 5 years and 60% in 10 years. Median forecasts
indicated a 50% probability of AI systems being capable of automating 90% of
current human tasks in 25 years and 99% of current human tasks in 50 years. The
conference of attendance was found to have a statistically significant impact
on all forecasts, with attendees of HLAI providing more optimistic timelines
with less uncertainty. These findings suggest that AI experts expect major
advances in AI technology to continue over the next decade to a degree that
will likely have profound transformative impacts on society.",2022-11-13 16:03:55
XXX,journalArticle,2019,"Laura von Rueden, Sebastian Mayer, Katharina Beckh, Bogdan Georgiev, Sven Giesselbach, Raoul Heese, Birgit Kirsch, Julius Pfrommer, Annika Pick, Rajkumar Ramamurthy, Michal Walczak, Jochen Garcke, Christian Bauckhage, Jannis Schuecker",Informed Machine Learning -- A Taxonomy and Survey of Integrating Knowledge into Learning Systems,,,,10.1109/TKDE.2021.3079836,http://arxiv.org/abs/1903.12394v3,"Despite its great success, machine learning can have its limits when dealing
with insufficient training data. A potential solution is the additional
integration of prior knowledge into the training process which leads to the
notion of informed machine learning. In this paper, we present a structured
overview of various approaches in this field. We provide a definition and
propose a concept for informed machine learning which illustrates its building
blocks and distinguishes it from conventional machine learning. We introduce a
taxonomy that serves as a classification framework for informed machine
learning approaches. It considers the source of knowledge, its representation,
and its integration into the machine learning pipeline. Based on this taxonomy,
we survey related research and describe how different knowledge representations
such as algebraic equations, logic rules, or simulation results can be used in
learning systems. This evaluation of numerous papers on the basis of our
taxonomy uncovers key methods in the field of informed machine learning.",2022-11-13 16:03:55
XXX,journalArticle,2019,"Alex Kearney, Patrick M. Pilarski",When is a Prediction Knowledge?,,,,,http://arxiv.org/abs/1904.09024v1,"Within Reinforcement Learning, there is a growing collection of research
which aims to express all of an agent's knowledge of the world through
predictions about sensation, behaviour, and time. This work can be seen not
only as a collection of architectural proposals, but also as the beginnings of
a theory of machine knowledge in reinforcement learning. Recent work has
expanded what can be expressed using predictions, and developed applications
which use predictions to inform decision-making on a variety of synthetic and
real-world problems. While promising, we here suggest that the notion of
predictions as knowledge in reinforcement learning is as yet underdeveloped:
some work explicitly refers to predictions as knowledge, what the requirements
are for considering a prediction to be knowledge have yet to be well explored.
This specification of the necessary and sufficient conditions of knowledge is
important; even if claims about the nature of knowledge are left implicit in
technical proposals, the underlying assumptions of such claims have
consequences for the systems we design. These consequences manifest in both the
way we choose to structure predictive knowledge architectures, and how we
evaluate them. In this paper, we take a first step to formalizing predictive
knowledge by discussing the relationship of predictive knowledge learning
methods to existing theories of knowledge in epistemology. Specifically, we
explore the relationships between Generalized Value Functions and epistemic
notions of Justification and Truth.",2022-11-13 16:03:56
XXX,journalArticle,2019,"Chenglong Wang, Rudy Bunel, Krishnamurthy Dvijotham, Po-Sen Huang, Edward Grefenstette, Pushmeet Kohli",Knowing When to Stop: Evaluation and Verification of Conformity to Output-size Specifications,,,,,http://arxiv.org/abs/1904.12004v1,"Models such as Sequence-to-Sequence and Image-to-Sequence are widely used in
real world applications. While the ability of these neural architectures to
produce variable-length outputs makes them extremely effective for problems
like Machine Translation and Image Captioning, it also leaves them vulnerable
to failures of the form where the model produces outputs of undesirable length.
This behavior can have severe consequences such as usage of increased
computation and induce faults in downstream modules that expect outputs of a
certain length. Motivated by the need to have a better understanding of the
failures of these models, this paper proposes and studies the novel output-size
modulation problem and makes two key technical contributions. First, to
evaluate model robustness, we develop an easy-to-compute differentiable proxy
objective that can be used with gradient-based algorithms to find
output-lengthening inputs. Second and more importantly, we develop a
verification approach that can formally verify whether a network always
produces outputs within a certain length. Experimental results on Machine
Translation and Image Captioning show that our output-lengthening approach can
produce outputs that are 50 times longer than the input, while our verification
approach can, given a model and input domain, prove that the output length is
below a certain size.",2022-11-13 16:03:56
XXX,journalArticle,2019,"Otello Ardovino, Jacopo Arpetti, Marco Delmastro",Regulating AI: do we need new tools?,,,,,http://arxiv.org/abs/1904.12134v1,"The Artificial Intelligence paradigm (hereinafter referred to as ""AI"") builds
on the analysis of data able, among other things, to snap pictures of the
individuals' behaviors and preferences. Such data represent the most valuable
currency in the digital ecosystem, where their value derives from their being a
fundamental asset in order to train machines with a view to developing AI
applications. In this environment, online providers attract users by offering
them services for free and getting in exchange data generated right through the
usage of such services. This swap, characterized by an implicit nature,
constitutes the focus of the present paper, in the light of the disequilibria,
as well as market failures, that it may bring about. We use mobile apps and the
related permission system as an ideal environment to explore, via econometric
tools, those issues. The results, stemming from a dataset of over one million
observations, show that both buyers and sellers are aware that access to
digital services implicitly implies an exchange of data, although this does not
have a considerable impact neither on the level of downloads (demand), nor on
the level of the prices (supply). In other words, the implicit nature of this
exchange does not allow market indicators to work efficiently. We conclude that
current policies (e.g. transparency rules) may be inherently biased and we put
forward suggestions for a new approach.",2022-11-13 16:03:57
XXX,journalArticle,2019,"Lihi Dery, Svetlana Obraztsova, Zinovi Rabinovich, Meir Kalech",Lie on the Fly: Strategic Voting in an Iterative Preference Elicitation Process,,,,10.1007/s10726-019-09637-2,http://arxiv.org/abs/1905.04933v1,"A voting center is in charge of collecting and aggregating voter preferences.
In an iterative process, the center sends comparison queries to voters,
requesting them to submit their preference between two items. Voters might
discuss the candidates among themselves, figuring out during the elicitation
process which candidates stand a chance of winning and which do not.
Consequently, strategic voters might attempt to manipulate by deviating from
their true preferences and instead submit a different response in order to
attempt to maximize their profit. We provide a practical algorithm for
strategic voters which computes the best manipulative vote and maximizes the
voter's selfish outcome when such a vote exists. We also provide a careful
voting center which is aware of the possible manipulations and avoids
manipulative queries when possible. In an empirical study on four real-world
domains, we show that in practice manipulation occurs in a low percentage of
settings and has a low impact on the final outcome. The careful voting center
reduces manipulation even further, thus allowing for a non-distorted group
decision process to take place. We thus provide a core technology study of a
voting process that can be adopted in opinion or information aggregation
systems and in crowdsourcing applications, e.g., peer grading in Massive Open
Online Courses (MOOCs).",2022-11-13 16:03:57
XXX,journalArticle,2019,"Jessica Morley, Luciano Floridi, Libby Kinsey, Anat Elhalal","From What to How: An Initial Review of Publicly Available AI Ethics Tools, Methods and Research to Translate Principles into Practices",,,,,http://arxiv.org/abs/1905.06876v2,"The debate about the ethical implications of Artificial Intelligence dates
from the 1960s. However, in recent years symbolic AI has been complemented and
sometimes replaced by Neural Networks and Machine Learning techniques. This has
vastly increased its potential utility and impact on society, with the
consequence that the ethical debate has gone mainstream. Such debate has
primarily focused on principles - the what of AI ethics - rather than on
practices, the how. Awareness of the potential issues is increasing at a fast
rate, but the AI community's ability to take action to mitigate the associated
risks is still at its infancy. Therefore, our intention in presenting this
research is to contribute to closing the gap between principles and practices
by constructing a typology that may help practically-minded developers apply
ethics at each stage of the pipeline, and to signal to researchers where
further work is needed. The focus is exclusively on Machine Learning, but it is
hoped that the results of this research may be easily applicable to other
branches of AI. The article outlines the research method for creating this
typology, the initial findings, and provides a summary of future research
needs.",2022-11-13 16:03:58
XXX,journalArticle,2019,"Rohin Shah, Noah Gundotra, Pieter Abbeel, Anca D. Dragan","On the Feasibility of Learning, Rather than Assuming, Human Biases for Reward Inference",,,,,http://arxiv.org/abs/1906.09624v1,"Our goal is for agents to optimize the right reward function, despite how
difficult it is for us to specify what that is. Inverse Reinforcement Learning
(IRL) enables us to infer reward functions from demonstrations, but it usually
assumes that the expert is noisily optimal. Real people, on the other hand,
often have systematic biases: risk-aversion, myopia, etc. One option is to try
to characterize these biases and account for them explicitly during learning.
But in the era of deep learning, a natural suggestion researchers make is to
avoid mathematical models of human behavior that are fraught with specific
assumptions, and instead use a purely data-driven approach. We decided to put
this to the test -- rather than relying on assumptions about which specific
bias the demonstrator has when planning, we instead learn the demonstrator's
planning algorithm that they use to generate demonstrations, as a
differentiable planner. Our exploration yielded mixed findings: on the one
hand, learning the planner can lead to better reward inference than relying on
the wrong assumption; on the other hand, this benefit is dwarfed by the loss we
incur by going from an exact to a differentiable planner. This suggest that at
least for the foreseeable future, agents need a middle ground between the
flexibility of data-driven methods and the useful bias of known human biases.
Code is available at https://tinyurl.com/learningbiases.",2022-11-13 16:03:58
XXX,journalArticle,2019,"Akira Kinose, Tadahiro Taniguchi",Integration of Imitation Learning using GAIL and Reinforcement Learning using Task-achievement Rewards via Probabilistic Graphical Model,,,,,http://arxiv.org/abs/1907.02140v2,"Integration of reinforcement learning and imitation learning is an important
problem that has been studied for a long time in the field of intelligent
robotics. Reinforcement learning optimizes policies to maximize the cumulative
reward, whereas imitation learning attempts to extract general knowledge about
the trajectories demonstrated by experts, i.e., demonstrators. Because each of
them has their own drawbacks, methods combining them and compensating for each
set of drawbacks have been explored thus far. However, many of the methods are
heuristic and do not have a solid theoretical basis. In this paper, we present
a new theory for integrating reinforcement and imitation learning by extending
the probabilistic generative model framework for reinforcement learning, {\it
plan by inference}. We develop a new probabilistic graphical model for
reinforcement learning with multiple types of rewards and a probabilistic
graphical model for Markov decision processes with multiple optimality
emissions (pMDP-MO). Furthermore, we demonstrate that the integrated learning
method of reinforcement learning and imitation learning can be formulated as a
probabilistic inference of policies on pMDP-MO by considering the output of the
discriminator in generative adversarial imitation learning as an additional
optimal emission observation. We adapt the generative adversarial imitation
learning and task-achievement reward to our proposed framework, achieving
significantly better performance than agents trained with reinforcement
learning or imitation learning alone. Experiments demonstrate that our
framework successfully integrates imitation and reinforcement learning even
when the number of demonstrators is only a few.",2022-11-13 16:03:59
XXX,journalArticle,2019,"Pedro Fernandes, Francisco C. Santos, Manuel Lopes",Norms for Beneficial A.I.: A Computational Analysis of the Societal Value Alignment Problem,"AI Communications, vol. 33, no. 3-6, pp. 155-171, 2020",,,10.3233/AIC-201502,http://arxiv.org/abs/1907.03843v2,"The rise of artificial intelligence (A.I.) based systems is already offering
substantial benefits to the society as a whole. However, these systems may also
enclose potential conflicts and unintended consequences. Notably, people will
tend to adopt an A.I. system if it confers them an advantage, at which point
non-adopters might push for a strong regulation if that advantage for adopters
is at a cost for them. Here we propose an agent-based game-theoretical model
for these conflicts, where agents may decide to resort to A.I. to use and
acquire additional information on the payoffs of a stochastic game, striving to
bring insights from simulation to what has been, hitherto, a mostly
philosophical discussion. We frame our results under the current discussion on
ethical A.I. and the conflict between individual and societal gains: the
societal value alignment problem. We test the arising equilibria in the
adoption of A.I. technology under different norms followed by artificial
agents, their ensuing benefits, and the emergent levels of wealth inequality.
We show that without any regulation, purely selfish A.I. systems will have the
strongest advantage, even when a utilitarian A.I. provides significant benefits
for the individual and the society. Nevertheless, we show that it is possible
to develop A.I. systems following human conscious policies that, when
introduced in society, lead to an equilibrium where the gains for the adopters
are not at a cost for non-adopters, thus increasing the overall wealth of the
population and lowering inequality. However, as shown, a self-organised
adoption of such policies would require external regulation.",2022-11-13 16:03:59
XXX,journalArticle,2019,"Felix Leibfried, Sergio Pascual-Diaz, Jordi Grau-Moya",A Unified Bellman Optimality Principle Combining Reward Maximization and Empowerment,,,,,http://arxiv.org/abs/1907.12392v5,"Empowerment is an information-theoretic method that can be used to
intrinsically motivate learning agents. It attempts to maximize an agent's
control over the environment by encouraging visiting states with a large number
of reachable next states. Empowered learning has been shown to lead to complex
behaviors, without requiring an explicit reward signal. In this paper, we
investigate the use of empowerment in the presence of an extrinsic reward
signal. We hypothesize that empowerment can guide reinforcement learning (RL)
agents to find good early behavioral solutions by encouraging highly empowered
states. We propose a unified Bellman optimality principle for empowered reward
maximization. Our empowered reward maximization approach generalizes both
Bellman's optimality principle as well as recent information-theoretical
extensions to it. We prove uniqueness of the empowered values and show
convergence to the optimal solution. We then apply this idea to develop
off-policy actor-critic RL algorithms which we validate in high-dimensional
continuous robotics domains (MuJoCo). Our methods demonstrate improved initial
and competitive final performance compared to model-free state-of-the-art
techniques.",2022-11-13 16:04:00
XXX,journalArticle,2019,"Úlfar Erlingsson, Ilya Mironov, Ananth Raghunathan, Shuang Song",That which we call private,,,,,http://arxiv.org/abs/1908.03566v2,"The guarantees of security and privacy defenses are often strengthened by
relaxing the assumptions made about attackers or the context in which defenses
are deployed. Such relaxations can be a highly worthwhile topic of
exploration---even though they typically entail assuming a weaker, less
powerful adversary---because there may indeed be great variability in both
attackers' powers and their context.
  However, no weakening or contextual discounting of attackers' power is
assumed for what some have called ""relaxed definitions"" in the analysis of
differential-privacy guarantees. Instead, the definitions so named are the
basis of refinements and more advanced analyses of the worst-case implications
of attackers---without any change assumed in attackers' powers.
  Because they more precisely bound the worst-case privacy loss, these improved
analyses can greatly strengthen the differential-privacy upper-bound
guarantees---sometimes lowering the differential-privacy epsilon by
orders-of-magnitude. As such, to the casual eye, these analyses may appear to
imply a reduced privacy loss. This is a false perception: the privacy loss of
any concrete mechanism cannot change with the choice of a worst-case-loss
upper-bound analysis technique. Practitioners must be careful not to equate
real-world privacy with differential-privacy epsilon values, at least not
without full consideration of the context.",2022-11-13 16:04:00
XXX,journalArticle,2019,"Sebastien Racaniere, Andrew K. Lampinen, Adam Santoro, David P. Reichert, Vlad Firoiu, Timothy P. Lillicrap",Automated curricula through setter-solver interactions,"International Conference on Learning Representations, 2020",,,,http://arxiv.org/abs/1909.12892v2,"Reinforcement learning algorithms use correlations between policies and
rewards to improve agent performance. But in dynamic or sparsely rewarding
environments these correlations are often too small, or rewarding events are
too infrequent to make learning feasible. Human education instead relies on
curricula--the breakdown of tasks into simpler, static challenges with dense
rewards--to build up to complex behaviors. While curricula are also useful for
artificial agents, hand-crafting them is time consuming. This has lead
researchers to explore automatic curriculum generation. Here we explore
automatic curriculum generation in rich, dynamic environments. Using a
setter-solver paradigm we show the importance of considering goal validity,
goal feasibility, and goal coverage to construct useful curricula. We
demonstrate the success of our approach in rich but sparsely rewarding 2D and
3D environments, where an agent is tasked to achieve a single goal selected
from a set of possible goals that varies between episodes, and identify
challenges for future work. Finally, we demonstrate the value of a novel
technique that guides agents towards a desired goal distribution. Altogether,
these results represent a substantial step towards applying automatic task
curricula to learn complex, otherwise unlearnable goals, and to our knowledge
are the first to demonstrate automated curriculum generation for
goal-conditioned agents in environments where the possible goals vary between
episodes.",2022-11-13 16:04:01
XXX,journalArticle,2019,"Vicki Bier, Paul B. Kantor, Gary Lupyan, Xiaojin Zhu",Can We Distinguish Machine Learning from Human Learning?,,,,,http://arxiv.org/abs/1910.03466v1,"What makes a task relatively more or less difficult for a machine compared to
a human? Much AI/ML research has focused on expanding the range of tasks that
machines can do, with a focus on whether machines can beat humans. Allowing for
differences in scale, we can seek interesting (anomalous) pairs of tasks T, T'.
We define interesting in this way: The ""harder to learn"" relation is reversed
when comparing human intelligence (HI) to AI. While humans seems to be able to
understand problems by formulating rules, ML using neural networks does not
rely on constructing rules. We discuss a novel approach where the challenge is
to ""perform well under rules that have been created by human beings."" We
suggest that this provides a rigorous and precise pathway for understanding the
difference between the two kinds of learning. Specifically, we suggest a large
and extensible class of learning tasks, formulated as learning under rules.
With these tasks, both the AI and HI will be studied with rigor and precision.
The immediate goal is to find interesting groundtruth rule pairs. In the long
term, the goal will be to understand, in a generalizable way, what
distinguishes interesting pairs from ordinary pairs, and to define saliency
behind interesting pairs. This may open new ways of thinking about AI, and
provide unexpected insights into human learning.",2022-11-13 16:04:01
XXX,journalArticle,2019,"Andreas Sedlmeier, Thomas Gabor, Thomy Phan, Lenz Belzner, Claudia Linnhoff-Popien",Uncertainty-Based Out-of-Distribution Classification in Deep Reinforcement Learning,"Proceedings of the 12th International Conference on Agents and
  Artificial Intelligence - Volume 2: ICAART, 2020, ISBN 978-989-758-395-7,
  pages 522-529",,,10.5220/0008949905220529,http://arxiv.org/abs/2001.00496v1,"Robustness to out-of-distribution (OOD) data is an important goal in building
reliable machine learning systems. Especially in autonomous systems, wrong
predictions for OOD inputs can cause safety critical situations. As a first
step towards a solution, we consider the problem of detecting such data in a
value-based deep reinforcement learning (RL) setting. Modelling this problem as
a one-class classification problem, we propose a framework for
uncertainty-based OOD classification: UBOOD. It is based on the effect that an
agent's epistemic uncertainty is reduced for situations encountered during
training (in-distribution), and thus lower than for unencountered (OOD)
situations. Being agnostic towards the approach used for estimating epistemic
uncertainty, combinations with different uncertainty estimation methods, e.g.
approximate Bayesian inference methods or ensembling techniques are possible.
We further present a first viable solution for calculating a dynamic
classification threshold, based on the uncertainty distribution of the training
data. Evaluation shows that the framework produces reliable classification
results when combined with ensemble-based estimators, while the combination
with concrete dropout-based estimators fails to reliably detect OOD situations.
In summary, UBOOD presents a viable approach for OOD classification in deep RL
settings by leveraging the epistemic uncertainty of the agent's value function.",2022-11-13 16:04:02
XXX,journalArticle,2020,"Roozbeh Yousefzadeh, Dianne P. O'Leary",Auditing and Debugging Deep Learning Models via Decision Boundaries: Individual-level and Group-level Analysis,,,,,http://arxiv.org/abs/2001.00682v1,"Deep learning models have been criticized for their lack of easy
interpretation, which undermines confidence in their use for important
applications. Nevertheless, they are consistently utilized in many
applications, consequential to humans' lives, mostly because of their better
performance. Therefore, there is a great need for computational methods that
can explain, audit, and debug such models. Here, we use flip points to
accomplish these goals for deep learning models with continuous output scores
(e.g., computed by softmax), used in social applications. A flip point is any
point that lies on the boundary between two output classes: e.g. for a model
with a binary yes/no output, a flip point is any input that generates equal
scores for ""yes"" and ""no"". The flip point closest to a given input is of
particular importance because it reveals the least changes in the input that
would change a model's classification, and we show that it is the solution to a
well-posed optimization problem. Flip points also enable us to systematically
study the decision boundaries of a deep learning classifier. The resulting
insight into the decision boundaries of a deep model can clearly explain the
model's output on the individual-level, via an explanation report that is
understandable by non-experts. We also develop a procedure to understand and
audit model behavior towards groups of people. Flip points can also be used to
alter the decision boundaries in order to improve undesirable behaviors. We
demonstrate our methods by investigating several models trained on standard
datasets used in social applications of machine learning. We also identify the
features that are most responsible for particular classifications and
misclassifications.",2022-11-13 16:04:03
XXX,journalArticle,2020,"Shakkeel Ahmed, Ravi S. Mula, Soma S. Dhavala",A Framework for Democratizing AI,,,,,http://arxiv.org/abs/2001.00818v1,"Machine Learning and Artificial Intelligence are considered an integral part
of the Fourth Industrial Revolution. Their impact, and far-reaching
consequences, while acknowledged, are yet to be comprehended. These
technologies are very specialized, and few organizations and select highly
trained professionals have the wherewithal, in terms of money, manpower, and
might, to chart the future. However, concentration of power can lead to
marginalization, causing severe inequalities. Regulatory agencies and
governments across the globe are creating national policies, and laws around
these technologies to protect the rights of the digital citizens, as well as to
empower them. Even private, not-for-profit organizations are also contributing
to democratizing the technologies by making them \emph{accessible} and
\emph{affordable}. However, accessibility and affordability are all but a few
of the facets of democratizing the field. Others include, but not limited to,
\emph{portability}, \emph{explainability}, \emph{credibility}, \emph{fairness},
among others. As one can imagine, democratizing AI is a multi-faceted problem,
and it requires advancements in science, technology and policy. At
\texttt{mlsquare}, we are developing scientific tools in this space.
Specifically, we introduce an opinionated, extensible, \texttt{Python}
framework that provides a single point of interface to a variety of solutions
in each of the categories mentioned above. We present the design details, APIs
of the framework, reference implementations, road map for development, and
guidelines for contributions.",2022-11-13 16:04:03
XXX,journalArticle,2020,"Carlos Fernández-Loría, Foster Provost, Xintian Han",Explaining Data-Driven Decisions made by AI Systems: The Counterfactual Approach,,,,,http://arxiv.org/abs/2001.07417v5,"We examine counterfactual explanations for explaining the decisions made by
model-based AI systems. The counterfactual approach we consider defines an
explanation as a set of the system's data inputs that causally drives the
decision (i.e., changing the inputs in the set changes the decision) and is
irreducible (i.e., changing any subset of the inputs does not change the
decision). We (1) demonstrate how this framework may be used to provide
explanations for decisions made by general, data-driven AI systems that may
incorporate features with arbitrary data types and multiple predictive models,
and (2) propose a heuristic procedure to find the most useful explanations
depending on the context. We then contrast counterfactual explanations with
methods that explain model predictions by weighting features according to their
importance (e.g., SHAP, LIME) and present two fundamental reasons why we should
carefully consider whether importance-weight explanations are well-suited to
explain system decisions. Specifically, we show that (i) features that have a
large importance weight for a model prediction may not affect the corresponding
decision, and (ii) importance weights are insufficient to communicate whether
and how features influence decisions. We demonstrate this with several concise
examples and three detailed case studies that compare the counterfactual
approach with SHAP to illustrate various conditions under which counterfactual
explanations explain data-driven decisions better than importance weights.",2022-11-13 16:04:04
XXX,journalArticle,2020,"Yehia Elrakaiby, Paola Spoletini, Bashar Nuseibeh",Optimal by Design: Model-Driven Synthesis of Adaptation Strategies for Autonomous Systems,,,,,http://arxiv.org/abs/2001.08525v1,"Many software systems have become too large and complex to be managed
efficiently by human administrators, particularly when they operate in
uncertain and dynamic environments and require frequent changes.
Requirements-driven adaptation techniques have been proposed to endow systems
with the necessary means to autonomously decide ways to satisfy their
requirements. However, many current approaches rely on general-purpose
languages, models and/or frameworks to design, develop and analyze autonomous
systems. Unfortunately, these tools are not tailored towards the
characteristics of adaptation problems in autonomous systems. In this paper, we
present Optimal by Design (ObD ), a framework for model-based
requirements-driven synthesis of optimal adaptation strategies for autonomous
systems. ObD proposes a model (and a language) for the high-level description
of the basic elements of self-adaptive systems, namely the system,
capabilities, requirements and environment. Based on those elements, a Markov
Decision Process (MDP) is constructed to compute the optimal strategy or the
most rewarding system behaviour. Furthermore, this defines a reflex controller
that can ensure timely responses to changes. One novel feature of the framework
is that it benefits both from goal-oriented techniques, developed for
requirement elicitation, refinement and analysis, and synthesis capabilities
and extensive research around MDPs, their extensions and tools. Our preliminary
evaluation results demonstrate the practicality and advantages of the
framework.",2022-11-13 16:04:04
XXX,journalArticle,2020,"Mislav Juric, Agneza Sandic, Mario Brcic",AI safety: state of the field through quantitative lens,,,,,http://arxiv.org/abs/2002.05671v2,"Last decade has seen major improvements in the performance of artificial
intelligence which has driven wide-spread applications. Unforeseen effects of
such mass-adoption has put the notion of AI safety into the public eye. AI
safety is a relatively new field of research focused on techniques for building
AI beneficial for humans. While there exist survey papers for the field of AI
safety, there is a lack of a quantitative look at the research being conducted.
The quantitative aspect gives a data-driven insight about the emerging trends,
knowledge gaps and potential areas for future research. In this paper,
bibliometric analysis of the literature finds significant increase in research
activity since 2015. Also, the field is so new that most of the technical
issues are open, including: explainability with its long-term utility, and
value alignment which we have identified as the most important long-term
research topic. Equally, there is a severe lack of research into concrete
policies regarding AI. As we expect AI to be the one of the main driving forces
of changes in society, AI safety is the field under which we need to decide the
direction of humanity's future.",2022-11-13 16:04:05
XXX,journalArticle,2020,"Akanksha Saran, Ruohan Zhang, Elaine Schaertl Short, Scott Niekum",Efficiently Guiding Imitation Learning Agents with Human Gaze,,,,,http://arxiv.org/abs/2002.12500v4,"Human gaze is known to be an intention-revealing signal in human
demonstrations of tasks. In this work, we use gaze cues from human
demonstrators to enhance the performance of agents trained via three popular
imitation learning methods -- behavioral cloning (BC), behavioral cloning from
observation (BCO), and Trajectory-ranked Reward EXtrapolation (T-REX). Based on
similarities between the attention of reinforcement learning agents and human
gaze, we propose a novel approach for utilizing gaze data in a computationally
efficient manner, as part of an auxiliary loss function, which guides a network
to have higher activations in image regions where the human's gaze fixated.
This work is a step towards augmenting any existing convolutional imitation
learning agent's training with auxiliary gaze data. Our auxiliary
coverage-based gaze loss (CGL) guides learning toward a better reward function
or policy, without adding any additional learnable parameters and without
requiring gaze data at test time. We find that our proposed approach improves
the performance by 95% for BC, 343% for BCO, and 390% for T-REX, averaged over
20 different Atari games. We also find that compared to a prior
state-of-the-art imitation learning method assisted by human gaze (AGIL), our
method achieves better performance, and is more efficient in terms of learning
with fewer demonstrations. We further interpret trained CGL agents with a
saliency map visualization method to explain their performance. At last, we
show that CGL can help alleviate a well-known causal confusion problem in
imitation learning.",2022-11-13 16:04:05
XXX,journalArticle,2020,"Peter Hase, Mohit Bansal",Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?,,,,,http://arxiv.org/abs/2005.01831v1,"Algorithmic approaches to interpreting machine learning models have
proliferated in recent years. We carry out human subject tests that are the
first of their kind to isolate the effect of algorithmic explanations on a key
aspect of model interpretability, simulatability, while avoiding important
confounding experimental factors. A model is simulatable when a person can
predict its behavior on new inputs. Through two kinds of simulation tests
involving text and tabular data, we evaluate five explanations methods: (1)
LIME, (2) Anchor, (3) Decision Boundary, (4) a Prototype model, and (5) a
Composite approach that combines explanations from each method. Clear evidence
of method effectiveness is found in very few cases: LIME improves
simulatability in tabular classification, and our Prototype method is effective
in counterfactual simulation tests. We also collect subjective ratings of
explanations, but we do not find that ratings are predictive of how helpful
explanations are. Our results provide the first reliable and comprehensive
estimates of how explanations influence simulatability across a variety of
explanation methods and data domains. We show that (1) we need to be careful
about the metrics we use to evaluate explanation methods, and (2) there is
significant room for improvement in current methods. All our supporting code,
data, and models are publicly available at:
https://github.com/peterbhase/InterpretableNLP-ACL2020",2022-11-13 16:04:06
XXX,journalArticle,2020,"Michael K. Cohen, Elliot Catt, Marcus Hutter",Curiosity Killed or Incapacitated the Cat and the Asymptotically Optimal Agent,Journal of Selected Areas in Information Theory 2 (2021),,,,http://arxiv.org/abs/2006.03357v2,"Reinforcement learners are agents that learn to pick actions that lead to
high reward. Ideally, the value of a reinforcement learner's policy approaches
optimality--where the optimal informed policy is the one which maximizes
reward. Unfortunately, we show that if an agent is guaranteed to be
""asymptotically optimal"" in any (stochastically computable) environment, then
subject to an assumption about the true environment, this agent will be either
""destroyed"" or ""incapacitated"" with probability 1. Much work in reinforcement
learning uses an ergodicity assumption to avoid this problem. Often, doing
theoretical research under simplifying assumptions prepares us to provide
practical solutions even in the absence of those assumptions, but the
ergodicity assumption in reinforcement learning may have led us entirely astray
in preparing safe and effective exploration strategies for agents in dangerous
environments. Rather than assuming away the problem, we present an agent,
Mentee, with the modest guarantee of approaching the performance of a mentor,
doing safe exploration instead of reckless exploration. Critically, Mentee's
exploration probability depends on the expected information gain from
exploring. In a simple non-ergodic environment with a weak mentor, we find
Mentee outperforms existing asymptotically optimal agents and its mentor.",2022-11-13 16:04:06
XXX,journalArticle,2020,"Matteo Turchetta, Andrey Kolobov, Shital Shah, Andreas Krause, Alekh Agarwal",Safe Reinforcement Learning via Curriculum Induction,,,,,http://arxiv.org/abs/2006.12136v2,"In safety-critical applications, autonomous agents may need to learn in an
environment where mistakes can be very costly. In such settings, the agent
needs to behave safely not only after but also while learning. To achieve this,
existing safe reinforcement learning methods make an agent rely on priors that
let it avoid dangerous situations during exploration with high probability, but
both the probabilistic guarantees and the smoothness assumptions inherent in
the priors are not viable in many scenarios of interest such as autonomous
driving. This paper presents an alternative approach inspired by human
teaching, where an agent learns under the supervision of an automatic
instructor that saves the agent from violating constraints during learning. In
this model, we introduce the monitor that neither needs to know how to do well
at the task the agent is learning nor needs to know how the environment works.
Instead, it has a library of reset controllers that it activates when the agent
starts behaving dangerously, preventing it from doing damage. Crucially, the
choices of which reset controller to apply in which situation affect the speed
of agent learning. Based on observing agents' progress, the teacher itself
learns a policy for choosing the reset controllers, a curriculum, to optimize
the agent's final policy reward. Our experiments use this framework in two
environments to induce curricula for safe and efficient learning.",2022-11-13 16:04:07
XXX,journalArticle,2020,"Paul Barde, Julien Roy, Wonseok Jeon, Joelle Pineau, Christopher Pal, Derek Nowrouzezahrai",Adversarial Soft Advantage Fitting: Imitation Learning without Policy Optimization,Advances in Neural Information Processing Systems 33 (2020),,,,http://arxiv.org/abs/2006.13258v6,"Adversarial Imitation Learning alternates between learning a discriminator --
which tells apart expert's demonstrations from generated ones -- and a
generator's policy to produce trajectories that can fool this discriminator.
This alternated optimization is known to be delicate in practice since it
compounds unstable adversarial training with brittle and sample-inefficient
reinforcement learning. We propose to remove the burden of the policy
optimization steps by leveraging a novel discriminator formulation.
Specifically, our discriminator is explicitly conditioned on two policies: the
one from the previous generator's iteration and a learnable policy. When
optimized, this discriminator directly learns the optimal generator's policy.
Consequently, our discriminator's update solves the generator's optimization
problem for free: learning a policy that imitates the expert does not require
an additional optimization loop. This formulation effectively cuts by half the
implementation and computational burden of Adversarial Imitation Learning
algorithms by removing the Reinforcement Learning phase altogether. We show on
a variety of tasks that our simpler approach is competitive to prevalent
Imitation Learning methods.",2022-11-13 16:04:07
XXX,journalArticle,2020,"Ehsan Toreini, Mhairi Aitken, Kovila P. L. Coopamootoo, Karen Elliott, Vladimiro Gonzalez Zelaya, Paolo Missier, Magdalene Ng, Aad van Moorsel",Technologies for Trustworthy Machine Learning: A Survey in a Socio-Technical Context,,,,,http://arxiv.org/abs/2007.08911v3,"Concerns about the societal impact of AI-based services and systems has
encouraged governments and other organisations around the world to propose AI
policy frameworks to address fairness, accountability, transparency and related
topics. To achieve the objectives of these frameworks, the data and software
engineers who build machine-learning systems require knowledge about a variety
of relevant supporting tools and techniques. In this paper we provide an
overview of technologies that support building trustworthy machine learning
systems, i.e., systems whose properties justify that people place trust in
them. We argue that four categories of system properties are instrumental in
achieving the policy objectives, namely fairness, explainability, auditability
and safety & security (FEAS). We discuss how these properties need to be
considered across all stages of the machine learning life cycle, from data
collection through run-time model inference. As a consequence, we survey in
this paper the main technologies with respect to all four of the FEAS
properties, for data-centric as well as model-centric stages of the machine
learning system life cycle. We conclude with an identification of open research
problems, with a particular focus on the connection between trustworthy machine
learning technologies and their implications for individuals and society.",2022-11-13 16:04:08
XXX,journalArticle,2020,"Luca Weihs, Unnat Jain, Iou-Jen Liu, Jordi Salvador, Svetlana Lazebnik, Aniruddha Kembhavi, Alexander Schwing",Bridging the Imitation Gap by Adaptive Insubordination,,,,,http://arxiv.org/abs/2007.12173v3,"In practice, imitation learning is preferred over pure reinforcement learning
whenever it is possible to design a teaching agent to provide expert
supervision. However, we show that when the teaching agent makes decisions with
access to privileged information that is unavailable to the student, this
information is marginalized during imitation learning, resulting in an
""imitation gap"" and, potentially, poor results. Prior work bridges this gap via
a progression from imitation learning to reinforcement learning. While often
successful, gradual progression fails for tasks that require frequent switches
between exploration and memorization. To better address these tasks and
alleviate the imitation gap we propose 'Adaptive Insubordination' (ADVISOR).
ADVISOR dynamically weights imitation and reward-based reinforcement learning
losses during training, enabling on-the-fly switching between imitation and
exploration. On a suite of challenging tasks set within gridworlds, multi-agent
particle environments, and high-fidelity 3D simulators, we show that on-the-fly
switching with ADVISOR outperforms pure imitation, pure reinforcement learning,
as well as their sequential and parallel combinations.",2022-11-13 16:04:08
XXX,journalArticle,2020,"Evan Zheran Liu, Aditi Raghunathan, Percy Liang, Chelsea Finn",Decoupling Exploration and Exploitation for Meta-Reinforcement Learning without Sacrifices,,,,,http://arxiv.org/abs/2008.02790v4,"The goal of meta-reinforcement learning (meta-RL) is to build agents that can
quickly learn new tasks by leveraging prior experience on related tasks.
Learning a new task often requires both exploring to gather task-relevant
information and exploiting this information to solve the task. In principle,
optimal exploration and exploitation can be learned end-to-end by simply
maximizing task performance. However, such meta-RL approaches struggle with
local optima due to a chicken-and-egg problem: learning to explore requires
good exploitation to gauge the exploration's utility, but learning to exploit
requires information gathered via exploration. Optimizing separate objectives
for exploration and exploitation can avoid this problem, but prior meta-RL
exploration objectives yield suboptimal policies that gather information
irrelevant to the task. We alleviate both concerns by constructing an
exploitation objective that automatically identifies task-relevant information
and an exploration objective to recover only this information. This avoids
local optima in end-to-end training, without sacrificing optimal exploration.
Empirically, DREAM substantially outperforms existing approaches on complex
meta-RL problems, such as sparse-reward 3D visual navigation. Videos of DREAM:
https://ezliu.github.io/dream/",2022-11-13 16:04:09
XXX,journalArticle,2020,"Eiji Uchibe, Kenji Doya",Forward and inverse reinforcement learning sharing network weights and hyperparameters,"Neural Networks, December 2021, Pages 138-153",,,10.1016/j.neunet.2021.08.017,http://arxiv.org/abs/2008.07284v2,"This paper proposes model-free imitation learning named Entropy-Regularized
Imitation Learning (ERIL) that minimizes the reverse Kullback-Leibler (KL)
divergence. ERIL combines forward and inverse reinforcement learning (RL) under
the framework of an entropy-regularized Markov decision process. An inverse RL
step computes the log-ratio between two distributions by evaluating two binary
discriminators. The first discriminator distinguishes the state generated by
the forward RL step from the expert's state. The second discriminator, which is
structured by the theory of entropy regularization, distinguishes the
state-action-next-state tuples generated by the learner from the expert ones.
One notable feature is that the second discriminator shares hyperparameters
with the forward RL, which can be used to control the discriminator's ability.
A forward RL step minimizes the reverse KL estimated by the inverse RL step. We
show that minimizing the reverse KL divergence is equivalent to finding an
optimal policy. Our experimental results on MuJoCo-simulated environments and
vision-based reaching tasks with a robotic arm show that ERIL is more
sample-efficient than the baseline methods. We apply the method to human
behaviors that perform a pole-balancing task and describe how the estimated
reward functions show how every subject achieves her goal.",2022-11-13 16:04:09
XXX,journalArticle,2020,John Mark Bishop,Artificial Intelligence is stupid and causal reasoning won't fix it,,,,,http://arxiv.org/abs/2008.07371v1,"Artificial Neural Networks have reached Grandmaster and even super-human
performance across a variety of games: from those involving perfect-information
(such as Go) to those involving imperfect-information (such as Starcraft). Such
technological developments from AI-labs have ushered concomitant applications
across the world of business - where an AI brand tag is fast becoming
ubiquitous. A corollary of such widespread commercial deployment is that when
AI gets things wrong - an autonomous vehicle crashes; a chatbot exhibits racist
behaviour; automated credit scoring processes discriminate on gender etc. -
there are often significant financial, legal and brand consequences and the
incident becomes major news. As Judea Pearl sees it, the underlying reason for
such mistakes is that, 'all the impressive achievements of deep learning amount
to just curve fitting'. The key, Judea Pearl suggests, is to replace reasoning
by association with causal-reasoning - the ability to infer causes from
observed phenomena. It is a point that was echoed by Gary Marcus and Ernest
Davis in a recent piece for the New York Times: 'we need to stop building
computer systems that merely get better and better at detecting statistical
patterns in data sets - often using an approach known as Deep Learning - and
start building computer systems that from the moment of their assembly innately
grasp three basic concepts: time, space and causality'. In this paper,
foregrounding what in 1949 Gilbert Ryle termed a category mistake, I will offer
an alternative explanation for AI errors: it is not so much that AI machinery
cannot grasp causality, but that AI machinery - qua computation - cannot
understand anything at all.",2022-11-13 16:04:10
XXX,journalArticle,2020,"Peipei Xu, Wenjie Ruan, Xiaowei Huang",Towards the Quantification of Safety Risks in Deep Neural Networks,,,,,http://arxiv.org/abs/2009.06114v1,"Safety concerns on the deep neural networks (DNNs) have been raised when they
are applied to critical sectors. In this paper, we define safety risks by
requesting the alignment of the network's decision with human perception. To
enable a general methodology for quantifying safety risks, we define a generic
safety property and instantiate it to express various safety risks. For the
quantification of risks, we take the maximum radius of safe norm balls, in
which no safety risk exists. The computation of the maximum safe radius is
reduced to the computation of their respective Lipschitz metrics - the
quantities to be computed. In addition to the known adversarial example,
reachability example, and invariant example, in this paper we identify a new
class of risk - uncertainty example - on which humans can tell easily but the
network is unsure. We develop an algorithm, inspired by derivative-free
optimization techniques and accelerated by tensor-based parallelization on
GPUs, to support efficient computation of the metrics. We perform evaluations
on several benchmark neural networks, including ACSC-Xu, MNIST, CIFAR-10, and
ImageNet networks. The experiments show that, our method can achieve
competitive performance on safety quantification in terms of the tightness and
the efficiency of computation. Importantly, as a generic approach, our method
can work with a broad class of safety risks and without restrictions on the
structure of neural networks.",2022-11-13 16:04:10
XXX,journalArticle,2020,"Minhae Kwon, Saurabh Daptardar, Paul Schrater, Xaq Pitkow",Inverse Rational Control with Partially Observable Continuous Nonlinear Dynamics,,,,,http://arxiv.org/abs/2009.12576v2,"A fundamental question in neuroscience is how the brain creates an internal
model of the world to guide actions using sequences of ambiguous sensory
information. This is naturally formulated as a reinforcement learning problem
under partial observations, where an agent must estimate relevant latent
variables in the world from its evidence, anticipate possible future states,
and choose actions that optimize total expected reward. This problem can be
solved by control theory, which allows us to find the optimal actions for a
given system dynamics and objective function. However, animals often appear to
behave suboptimally. Why? We hypothesize that animals have their own flawed
internal model of the world, and choose actions with the highest expected
subjective reward according to that flawed model. We describe this behavior as
rational but not optimal. The problem of Inverse Rational Control (IRC) aims to
identify which internal model would best explain an agent's actions. Our
contribution here generalizes past work on Inverse Rational Control which
solved this problem for discrete control in partially observable Markov
decision processes. Here we accommodate continuous nonlinear dynamics and
continuous actions, and impute sensory observations corrupted by unknown noise
that is private to the animal. We first build an optimal Bayesian agent that
learns an optimal policy generalized over the entire model space of dynamics
and subjective rewards using deep reinforcement learning. Crucially, this
allows us to compute a likelihood over models for experimentally observable
action trajectories acquired from a suboptimal agent. We then find the model
parameters that maximize the likelihood using gradient ascent.",2022-11-13 16:04:11
XXX,journalArticle,2020,"Chintan Donda, Sayan Dasgupta, Soma S Dhavala, Keyur Faldu, Aditi Avasthi","A framework for predicting, interpreting, and improving Learning Outcomes",,,,,http://arxiv.org/abs/2010.02629v2,"It has long been recognized that academic success is a result of both
cognitive and non-cognitive dimensions acting together. Consequently, any
intelligent learning platform designed to improve learning outcomes (LOs) must
provide actionable inputs to the learner in these dimensions. However,
operationalizing such inputs in a production setting that is scalable is not
trivial. We develop an Embibe Score Quotient model (ESQ) to predict test scores
based on observed academic, behavioral and test-taking features of a student.
ESQ can be used to predict the future scoring potential of a student as well as
offer personalized learning nudges, both critical to improving LOs. Multiple
machine learning models are evaluated for the prediction task. In order to
provide meaningful feedback to the learner, individualized Shapley feature
attributions for each feature are computed. Prediction intervals are obtained
by applying non-parametric quantile regression, in an attempt to quantify the
uncertainty in the predictions. We apply the above modelling strategy on a
dataset consisting of more than a hundred million learner interactions on the
Embibe learning platform. We observe that the Median Absolute Error between the
observed and predicted scores is 4.58% across several user segments, and the
correlation between predicted and observed responses is 0.93. Game-like what-if
scenarios are played out to see the changes in LOs, on counterfactual examples.
We briefly discuss how a rational agent can then apply an optimal policy to
affect the learning outcomes by treating the above model like an Oracle.",2022-11-13 16:04:11
XXX,journalArticle,2020,"Santiago Miret, Somdeb Majumdar, Carroll Wainwright",Safety Aware Reinforcement Learning (SARL),,,,,http://arxiv.org/abs/2010.02846v1,"As reinforcement learning agents become increasingly integrated into complex,
real-world environments, designing for safety becomes a critical consideration.
We specifically focus on researching scenarios where agents can cause undesired
side effects while executing a policy on a primary task. Since one can define
multiple tasks for a given environment dynamics, there are two important
challenges. First, we need to abstract the concept of safety that applies
broadly to that environment independent of the specific task being executed.
Second, we need a mechanism for the abstracted notion of safety to modulate the
actions of agents executing different policies to minimize their side-effects.
In this work, we propose Safety Aware Reinforcement Learning (SARL) - a
framework where a virtual safe agent modulates the actions of a main
reward-based agent to minimize side effects. The safe agent learns a
task-independent notion of safety for a given environment. The main agent is
then trained with a regularization loss given by the distance between the
native action probabilities of the two agents. Since the safe agent effectively
abstracts a task-independent notion of safety via its action probabilities, it
can be ported to modulate multiple policies solving different tasks within the
given environment without further training. We contrast this with solutions
that rely on task-specific regularization metrics and test our framework on the
SafeLife Suite, based on Conway's Game of Life, comprising a number of complex
tasks in dynamic environments. We show that our solution is able to match the
performance of solutions that rely on task-specific side-effect penalties on
both the primary and safety objectives while additionally providing the benefit
of generalizability and portability.",2022-11-13 16:04:12
XXX,journalArticle,2020,"Heribert Wankerl, Maike L. Stern, Ali Mahdavi, Christoph Eichler, Elmar W. Lang",Parameterized Reinforcement Learning for Optical System Optimization,J. Phys. D: Appl. Phys. 54 305104 (2021),,,10.1088/1361-6463/abfddb,http://arxiv.org/abs/2010.05769v2,"Designing a multi-layer optical system with designated optical
characteristics is an inverse design problem in which the resulting design is
determined by several discrete and continuous parameters. In particular, we
consider three design parameters to describe a multi-layer stack: Each layer's
dielectric material and thickness as well as the total number of layers. Such a
combination of both, discrete and continuous parameters is a challenging
optimization problem that often requires a computationally expensive search for
an optimal system design. Hence, most methods merely determine the optimal
thicknesses of the system's layers. To incorporate layer material and the total
number of layers as well, we propose a method that considers the stacking of
consecutive layers as parameterized actions in a Markov decision process. We
propose an exponentially transformed reward signal that eases policy
optimization and adapt a recent variant of Q-learning for inverse design
optimization. We demonstrate that our method outperforms human experts and a
naive reinforcement learning algorithm concerning the achieved optical
characteristics. Moreover, the learned Q-values contain information about the
optical properties of multi-layer optical systems, thereby allowing physical
interpretation or what-if analysis.",2022-11-13 16:04:12
XXX,journalArticle,2020,"Victoria Krakovna, Laurent Orseau, Richard Ngo, Miljan Martic, Shane Legg",Avoiding Side Effects By Considering Future Tasks,,,,,http://arxiv.org/abs/2010.07877v1,"Designing reward functions is difficult: the designer has to specify what to
do (what it means to complete the task) as well as what not to do (side effects
that should be avoided while completing the task). To alleviate the burden on
the reward designer, we propose an algorithm to automatically generate an
auxiliary reward function that penalizes side effects. This auxiliary objective
rewards the ability to complete possible future tasks, which decreases if the
agent causes side effects during the current task. The future task reward can
also give the agent an incentive to interfere with events in the environment
that make future tasks less achievable, such as irreversible actions by other
agents. To avoid this interference incentive, we introduce a baseline policy
that represents a default course of action (such as doing nothing), and use it
to filter out future tasks that are not achievable by default. We formally
define interference incentives and show that the future task approach with a
baseline policy avoids these incentives in the deterministic case. Using
gridworld environments that test for side effects and interference, we show
that our method avoids interference and is more effective for avoiding side
effects than the common approach of penalizing irreversible actions.",2022-11-13 16:04:12
XXX,journalArticle,2020,"Avik Pal, Jonah Philion, Yuan-Hong Liao, Sanja Fidler",Emergent Road Rules In Multi-Agent Driving Environments,"International Conference on Learning Representations, 2021",,,,http://arxiv.org/abs/2011.10753v2,"For autonomous vehicles to safely share the road with human drivers,
autonomous vehicles must abide by specific ""road rules"" that human drivers have
agreed to follow. ""Road rules"" include rules that drivers are required to
follow by law -- such as the requirement that vehicles stop at red lights -- as
well as more subtle social rules -- such as the implicit designation of fast
lanes on the highway. In this paper, we provide empirical evidence that
suggests that -- instead of hard-coding road rules into self-driving algorithms
-- a scalable alternative may be to design multi-agent environments in which
road rules emerge as optimal solutions to the problem of maximizing traffic
flow. We analyze what ingredients in driving environments cause the emergence
of these road rules and find that two crucial factors are noisy perception and
agents' spatial density. We provide qualitative and quantitative evidence of
the emergence of seven social driving behaviors, ranging from obeying traffic
signals to following lanes, all of which emerge from training agents to drive
quickly to destinations without colliding. Our results add empirical support
for the social road rules that countries worldwide have agreed on for safe,
efficient driving.",2022-11-13 16:04:13
XXX,journalArticle,2020,"Suresh Venkatasubramanian, Nadya Bliss, Helen Nissenbaum, Melanie Moses",Interdisciplinary Approaches to Understanding Artificial Intelligence's Impact on Society,,,,,http://arxiv.org/abs/2012.06057v1,"Innovations in AI have focused primarily on the questions of ""what"" and
""how""-algorithms for finding patterns in web searches, for instance-without
adequate attention to the possible harms (such as privacy, bias, or
manipulation) and without adequate consideration of the societal context in
which these systems operate. In part, this is driven by incentives and forces
in the tech industry, where a more product-driven focus tends to drown out
broader reflective concerns about potential harms and misframings. But this
focus on what and how is largely a reflection of the engineering and
mathematics-focused training in computer science, which emphasizes the building
of tools and development of computational concepts.
  As a result of this tight technical focus, and the rapid, worldwide explosion
in its use, AI has come with a storm of unanticipated socio-technical problems,
ranging from algorithms that act in racially or gender-biased ways, get caught
in feedback loops that perpetuate inequalities, or enable unprecedented
behavioral monitoring surveillance that challenges the fundamental values of
free, democratic societies.
  Given that AI is no longer solely the domain of technologists but rather of
society as a whole, we need tighter coupling of computer science and those
disciplines that study society and societal values.",2022-11-13 16:04:14
XXX,journalArticle,2021,"Lu Cheng, Kush R. Varshney, Huan Liu","Socially Responsible AI Algorithms: Issues, Purposes, and Challenges",Journal of Artificial Intelligence Research 71 (2021) 1137-1181,,,,http://arxiv.org/abs/2101.02032v5,"In the current era, people and society have grown increasingly reliant on
artificial intelligence (AI) technologies. AI has the potential to drive us
towards a future in which all of humanity flourishes. It also comes with
substantial risks for oppression and calamity. Discussions about whether we
should (re)trust AI have repeatedly emerged in recent years and in many
quarters, including industry, academia, healthcare, services, and so on.
Technologists and AI researchers have a responsibility to develop trustworthy
AI systems. They have responded with great effort to design more responsible AI
algorithms. However, existing technical solutions are narrow in scope and have
been primarily directed towards algorithms for scoring or classification tasks,
with an emphasis on fairness and unwanted bias. To build long-lasting trust
between AI and human beings, we argue that the key is to think beyond
algorithmic fairness and connect major aspects of AI that potentially cause
AI's indifferent behavior. In this survey, we provide a systematic framework of
Socially Responsible AI Algorithms that aims to examine the subjects of AI
indifference and the need for socially responsible AI algorithms, define the
objectives, and introduce the means by which we may achieve these objectives.
We further discuss how to leverage this framework to improve societal
well-being through protection, information, and prevention/mitigation.",2022-11-13 16:04:14
XXX,journalArticle,2021,"Siyi Hu, Fengda Zhu, Xiaojun Chang, Xiaodan Liang",UPDeT: Universal Multi-agent Reinforcement Learning via Policy Decoupling with Transformers,,,,,http://arxiv.org/abs/2101.08001v3,"Recent advances in multi-agent reinforcement learning have been largely
limited in training one model from scratch for every new task. The limitation
is due to the restricted model architecture related to fixed input and output
dimensions. This hinders the experience accumulation and transfer of the
learned agent over tasks with diverse levels of difficulty (e.g. 3 vs 3 or 5 vs
6 multi-agent games). In this paper, we make the first attempt to explore a
universal multi-agent reinforcement learning pipeline, designing one single
architecture to fit tasks with the requirement of different observation and
action configurations. Unlike previous RNN-based models, we utilize a
transformer-based model to generate a flexible policy by decoupling the policy
distribution from the intertwined input observation with an importance weight
measured by the merits of the self-attention mechanism. Compared to a standard
transformer block, the proposed model, named as Universal Policy Decoupling
Transformer (UPDeT), further relaxes the action restriction and makes the
multi-agent task's decision process more explainable. UPDeT is general enough
to be plugged into any multi-agent reinforcement learning pipeline and equip
them with strong generalization abilities that enables the handling of multiple
tasks at a time. Extensive experiments on large-scale SMAC multi-agent
competitive games demonstrate that the proposed UPDeT-based multi-agent
reinforcement learning achieves significant results relative to
state-of-the-art approaches, demonstrating advantageous transfer capability in
terms of both performance and training speed (10 times faster).",2022-11-13 16:04:15
XXX,journalArticle,2021,Abhishek Gupta,Making Responsible AI the Norm rather than the Exception,,,,,http://arxiv.org/abs/2101.11832v2,"This report prepared by the Montreal AI Ethics Institute provides
recommendations in response to the National Security Commission on Artificial
Intelligence (NSCAI) Key Considerations for Responsible Development and
Fielding of Artificial Intelligence document. The report centres on the idea
that Responsible AI should be made the Norm rather than an Exception. It does
so by utilizing the guiding principles of: (1) alleviating friction in existing
workflows, (2) empowering stakeholders to get buy-in, and (3) conducting an
effective translation of abstract standards into actionable engineering
practices. After providing some overarching comments on the document from the
NSCAI, the report dives into the primary contribution of an actionable
framework to help operationalize the ideas presented in the document from the
NSCAI. The framework consists of: (1) a learning, knowledge, and information
exchange (LKIE), (2) the Three Ways of Responsible AI, (3) an
empirically-driven risk-prioritization matrix, and (4) achieving the right
level of complexity. All components reinforce each other to move from
principles to practice in service of making Responsible AI the norm rather than
the exception.",2022-11-13 16:04:15
XXX,journalArticle,2021,"Mingqi Yuan, Mao-on Pun",Exploring Beyond-Demonstrator via Meta Learning-Based Reward Extrapolation,,,,,http://arxiv.org/abs/2102.02454v12,"Extrapolating beyond-demonstrator (BD) performance through the imitation
learning (IL) algorithm aims to learn from and subsequently outperform the
demonstrator. To that end, a representative approach is to leverage inverse
reinforcement learning (IRL) to infer a reward function from demonstrations
before performing RL on the learned reward function. However, most existing
reward extrapolation methods require massive demonstrations, making it
difficult to be applied in tasks of limited training data. To address this
problem, one simple solution is to perform data augmentation to artificially
generate more training data, which may incur severe inductive bias and policy
performance loss. In this paper, we propose a novel meta learning-based reward
extrapolation (MLRE) algorithm, which can effectively approximate the
ground-truth rewards using limited demonstrations. More specifically, MLRE
first learns an initial reward function from a set of tasks that have abundant
training data. Then the learned reward function will be fine-tuned using data
of the target task. Extensive simulation results demonstrated that the proposed
MLRE can achieve impressive performance improvement as compared to other
similar BDIL algorithms.",2022-11-13 16:04:16
XXX,journalArticle,2021,"Markus Kneer, Michael T. Stuart",Playing the Blame Game with Robots,,,,10.1145/3434074.3447202,http://arxiv.org/abs/2102.04527v1,"Recent research shows -- somewhat astonishingly -- that people are willing to
ascribe moral blame to AI-driven systems when they cause harm [1]-[4]. In this
paper, we explore the moral-psychological underpinnings of these findings. Our
hypothesis was that the reason why people ascribe moral blame to AI systems is
that they consider them capable of entertaining inculpating mental states (what
is called mens rea in the law). To explore this hypothesis, we created a
scenario in which an AI system runs a risk of poisoning people by using a novel
type of fertilizer. Manipulating the computational (or quasi-cognitive)
abilities of the AI system in a between-subjects design, we tested whether
people's willingness to ascribe knowledge of a substantial risk of harm (i.e.,
recklessness) and blame to the AI system. Furthermore, we investigated whether
the ascription of recklessness and blame to the AI system would influence the
perceived blameworthiness of the system's user (or owner). In an experiment
with 347 participants, we found (i) that people are willing to ascribe blame to
AI systems in contexts of recklessness, (ii) that blame ascriptions depend
strongly on the willingness to attribute recklessness and (iii) that the
latter, in turn, depends on the perceived ""cognitive"" capacities of the system.
Furthermore, our results suggest (iv) that the higher the computational
sophistication of the AI system, the more blame is shifted from the human user
to the AI system.",2022-11-13 16:04:16
XXX,journalArticle,2021,Wenjing Chu,A Decentralized Approach towards Responsible AI in Social Ecosystems,,,,,http://arxiv.org/abs/2102.06362v3,"For AI technology to fulfill its full promises, we must have effective means
to ensure Responsible AI behavior and curtail potential irresponsible use,
e.g., in areas of privacy protection, human autonomy, robustness, and
prevention of biases and discrimination in automated decision making. Recent
literature in the field has identified serious shortcomings of narrow
technology focused and formalism-oriented research and has proposed an
interdisciplinary approach that brings the social context into the scope of
study. In this paper, we take a sociotechnical approach to propose a more
expansive framework of thinking about the Responsible AI challenges in both
technical and social context. Effective solutions need to bridge the gap
between a technical system with the social system that it will be deployed to.
To this end, we propose human agency and regulation as main mechanisms of
intervention and propose a decentralized computational infrastructure, or a set
of public utilities, as the computational means to bridge this gap. A
decentralized infrastructure is uniquely suited for meeting this challenge and
enable technical solutions and social institutions in a mutually reinforcing
dynamic to achieve Responsible AI goals. Our approach is novel in its
sociotechnical approach and its aim in tackling the structural issues that
cannot be solved within the narrow confines of AI technical research. We then
explore possible features of the proposed infrastructure and discuss how it may
help solve example problems recently studied in the field.",2022-11-13 16:04:16
XXX,journalArticle,2021,"Michiel A. Bakker, Richard Everett, Laura Weidinger, Iason Gabriel, William S. Isaac, Joel Z. Leibo, Edward Hughes",Modelling Cooperation in Network Games with Spatio-Temporal Complexity,,,,,http://arxiv.org/abs/2102.06911v1,"The real world is awash with multi-agent problems that require collective
action by self-interested agents, from the routing of packets across a computer
network to the management of irrigation systems. Such systems have local
incentives for individuals, whose behavior has an impact on the global outcome
for the group. Given appropriate mechanisms describing agent interaction,
groups may achieve socially beneficial outcomes, even in the face of short-term
selfish incentives. In many cases, collective action problems possess an
underlying graph structure, whose topology crucially determines the
relationship between local decisions and emergent global effects. Such
scenarios have received great attention through the lens of network games.
However, this abstraction typically collapses important dimensions, such as
geometry and time, relevant to the design of mechanisms promoting cooperation.
In parallel work, multi-agent deep reinforcement learning has shown great
promise in modelling the emergence of self-organized cooperation in complex
gridworld domains. Here we apply this paradigm in graph-structured collective
action problems. Using multi-agent deep reinforcement learning, we simulate an
agent society for a variety of plausible mechanisms, finding clear transitions
between different equilibria over time. We define analytic tools inspired by
related literatures to measure the social outcomes, and use these to draw
conclusions about the efficacy of different environmental interventions. Our
methods have implications for mechanism design in both human and artificial
agent systems.",2022-11-13 16:04:17
XXX,journalArticle,2021,"Tao Zhang, Quanyan Zhu",On the Equilibrium Elicitation of Markov Games Through Information Design,,,,,http://arxiv.org/abs/2102.07152v1,"This work considers a novel information design problem and studies how the
craft of payoff-relevant environmental signals solely can influence the
behaviors of intelligent agents. The agents' strategic interactions are
captured by an incomplete-information Markov game, in which each agent first
selects one environmental signal from multiple signal sources as additional
payoff-relevant information and then takes an action. There is a rational
information designer (designer) who possesses one signal source and aims to
control the equilibrium behaviors of the agents by designing the information
structure of her signals sent to the agents. An obedient principle is
established which states that it is without loss of generality to focus on the
direct information design when the information design incentivizes each agent
to select the signal sent by the designer, such that the design process avoids
the predictions of the agents' strategic selection behaviors. We then introduce
the design protocol given a goal of the designer referred to as obedient
implementability (OIL) and characterize the OIL in a class of obedient perfect
Bayesian Markov Nash equilibria (O-PBME). A new framework for information
design is proposed based on an approach of maximizing the optimal slack
variables. Finally, we formulate the designer's goal selection problem and
characterize it in terms of information design by establishing a relationship
between the O-PBME and the Bayesian Markov correlated equilibria, in which we
build upon the revelation principle in classic information design in economics.
The proposed approach can be applied to elicit desired behaviors of multi-agent
systems in competing as well as cooperating settings and be extended to
heterogeneous stochastic games in the complete- and the incomplete-information
environments.",2022-11-13 16:04:17
XXX,journalArticle,2021,"Benjamin Patrick Evans, Mikhail Prokopenko",A maximum entropy model of bounded rational decision-making with prior beliefs and market feedback,,,,10.3390/e23060669,http://arxiv.org/abs/2102.09180v3,"Bounded rationality is an important consideration stemming from the fact that
agents often have limits on their processing abilities, making the assumption
of perfect rationality inapplicable to many real tasks. We propose an
information-theoretic approach to the inference of agent decisions under
Smithian competition. The model explicitly captures the boundedness of agents
(limited in their information-processing capacity) as the cost of information
acquisition for expanding their prior beliefs. The expansion is measured as the
Kullblack-Leibler divergence between posterior decisions and prior beliefs.
When information acquisition is free, the homo economicus agent is recovered,
while in cases when information acquisition becomes costly, agents instead
revert to their prior beliefs. The maximum entropy principle is used to infer
least-biased decisions based upon the notion of Smithian competition formalised
within the Quantal Response Statistical Equilibrium framework. The
incorporation of prior beliefs into such a framework allowed us to
systematically explore the effects of prior beliefs on decision-making in the
presence of market feedback, as well as importantly adding a temporal
interpretation to the framework. We verified the proposed model using
Australian housing market data, showing how the incorporation of prior
knowledge alters the resulting agent decisions. Specifically, it allowed for
the separation of past beliefs and utility maximisation behaviour of the agent
as well as the analysis into the evolution of agent beliefs.",2022-11-13 16:04:18
XXX,journalArticle,2021,"Rui Yang, Jiafei Lyu, Yu Yang, Jiangpeng Yan, Feng Luo, Dijun Luo, Lanqing Li, Xiu Li",Bias-reduced Multi-step Hindsight Experience Replay for Efficient Multi-goal Reinforcement Learning,,,,,http://arxiv.org/abs/2102.12962v3,"Multi-goal reinforcement learning is widely applied in planning and robot
manipulation. Two main challenges in multi-goal reinforcement learning are
sparse rewards and sample inefficiency. Hindsight Experience Replay (HER) aims
to tackle the two challenges via goal relabeling. However, HER-related works
still need millions of samples and a huge computation. In this paper, we
propose Multi-step Hindsight Experience Replay (MHER), incorporating multi-step
relabeled returns based on $n$-step relabeling to improve sample efficiency.
Despite the advantages of $n$-step relabeling, we theoretically and
experimentally prove the off-policy $n$-step bias introduced by $n$-step
relabeling may lead to poor performance in many environments. To address the
above issue, two bias-reduced MHER algorithms, MHER($\lambda$) and Model-based
MHER (MMHER) are presented. MHER($\lambda$) exploits the $\lambda$ return while
MMHER benefits from model-based value expansions. Experimental results on
numerous multi-goal robotic tasks show that our solutions can successfully
alleviate off-policy $n$-step bias and achieve significantly higher sample
efficiency than HER and Curriculum-guided HER with little additional
computation beyond HER.",2022-11-13 16:04:18
XXX,journalArticle,2021,"Víctor Campos, Pablo Sprechmann, Steven Hansen, Andre Barreto, Steven Kapturowski, Alex Vitvitskyi, Adrià Puigdomènech Badia, Charles Blundell",Beyond Fine-Tuning: Transferring Behavior in Reinforcement Learning,,,,,http://arxiv.org/abs/2102.13515v3,"Designing agents that acquire knowledge autonomously and use it to solve new
tasks efficiently is an important challenge in reinforcement learning.
Knowledge acquired during an unsupervised pre-training phase is often
transferred by fine-tuning neural network weights once rewards are exposed, as
is common practice in supervised domains. Given the nature of the reinforcement
learning problem, we argue that standard fine-tuning strategies alone are not
enough for efficient transfer in challenging domains. We introduce Behavior
Transfer (BT), a technique that leverages pre-trained policies for exploration
and that is complementary to transferring neural network weights. Our
experiments show that, when combined with large-scale pre-training in the
absence of rewards, existing intrinsic motivation objectives can lead to the
emergence of complex behaviors. These pre-trained policies can then be
leveraged by BT to discover better solutions than without pre-training, and
combining BT with standard fine-tuning strategies results in additional
benefits. The largest gains are generally observed in domains requiring
structured exploration, including settings where the behavior of the
pre-trained policies is misaligned with the downstream task.",2022-11-13 16:04:19
XXX,journalArticle,2021,"Leandro Eichenberger, Michael Cochez, Benjamin Heitmann, Stefan Decker",Secure Evaluation of Knowledge Graph Merging Gain,,,,,http://arxiv.org/abs/2103.00082v1,"Finding out the differences and commonalities between the knowledge of two
parties is an important task. Such a comparison becomes necessary, when one
party wants to determine how much it is worth to acquire the knowledge of the
second party, or similarly when two parties try to determine, whether a
collaboration could be beneficial. When these two parties cannot trust each
other (for example, due to them being competitors) performing such a comparison
is challenging as neither of them would be willing to share any of their
assets. This paper addresses this problem for knowledge graphs, without a need
for non-disclosure agreements nor a third party during the protocol.
  During the protocol, the intersection between the two knowledge graphs is
determined in a privacy preserving fashion. This is followed by the computation
of various metrics, which give an indication of the potential gain from
obtaining the other parties knowledge graph, while still keeping the actual
knowledge graph contents secret. The protocol makes use of blind signatures and
(counting) Bloom filters to reduce the amount of leaked information. Finally,
the party who wants to obtain the other's knowledge graph can get a part of
such in a way that neither party is able to know beforehand which parts of the
graph are obtained (i.e., they cannot choose to only get or share the good
parts). After inspection of the quality of this part, the Buyer can decide to
proceed with the transaction.
  The analysis of the protocol indicates that the developed protocol is secure
against malicious participants. Further experimental analysis shows that the
resource consumption scales linear with the number of statements in the
knowledge graph.",2022-11-13 16:04:19
XXX,journalArticle,2021,"André Artelt, Valerie Vaquet, Riza Velioglu, Fabian Hinder, Johannes Brinkrolf, Malte Schilling, Barbara Hammer",Evaluating Robustness of Counterfactual Explanations,,,,,http://arxiv.org/abs/2103.02354v3,"Transparency is a fundamental requirement for decision making systems when
these should be deployed in the real world. It is usually achieved by providing
explanations of the system's behavior. A prominent and intuitive type of
explanations are counterfactual explanations. Counterfactual explanations
explain a behavior to the user by proposing actions -- as changes to the input
-- that would cause a different (specified) behavior of the system. However,
such explanation methods can be unstable with respect to small changes to the
input -- i.e. even a small change in the input can lead to huge or arbitrary
changes in the output and of the explanation. This could be problematic for
counterfactual explanations, as two similar individuals might get very
different explanations. Even worse, if the recommended actions differ
considerably in their complexity, one would consider such unstable
(counterfactual) explanations as individually unfair.
  In this work, we formally and empirically study the robustness of
counterfactual explanations in general, as well as under different models and
different kinds of perturbations. Furthermore, we propose that plausible
counterfactual explanations can be used instead of closest counterfactual
explanations to improve the robustness and consequently the individual fairness
of counterfactual explanations.",2022-11-13 16:04:20
XXX,journalArticle,2021,"Naoki Yokoyama, Sehoon Ha, Dhruv Batra",Success Weighted by Completion Time: A Dynamics-Aware Evaluation Criteria for Embodied Navigation,,,,,http://arxiv.org/abs/2103.08022v1,"We present Success weighted by Completion Time (SCT), a new metric for
evaluating navigation performance for mobile robots. Several related works on
navigation have used Success weighted by Path Length (SPL) as the primary
method of evaluating the path an agent makes to a goal location, but SPL is
limited in its ability to properly evaluate agents with complex dynamics. In
contrast, SCT explicitly takes the agent's dynamics model into consideration,
and aims to accurately capture how well the agent has approximated the fastest
navigation behavior afforded by its dynamics. While several embodied navigation
works use point-turn dynamics, we focus on unicycle-cart dynamics for our
agent, which better exemplifies the dynamics model of popular mobile robotics
platforms (e.g., LoCoBot, TurtleBot, Fetch, etc.). We also present
RRT*-Unicycle, an algorithm for unicycle dynamics that estimates the fastest
collision-free path and completion time from a starting pose to a goal location
in an environment containing obstacles. We experiment with deep reinforcement
learning and reward shaping to train and compare the navigation performance of
agents with different dynamics models. In evaluating these agents, we show that
in contrast to SPL, SCT is able to capture the advantages in navigation speed a
unicycle model has over a simpler point-turn model of dynamics. Lastly, we show
that we can successfully deploy our trained models and algorithms outside of
simulation in the real world. We embody our agents in an real robot to navigate
an apartment, and show that they can generalize in a zero-shot manner.",2022-11-13 16:04:20
XXX,journalArticle,2021,"Grace A. Lewis, Stephany Bellomo, Ipek Ozkaya",Characterizing and Detecting Mismatch in Machine-Learning-Enabled Systems,,,,,http://arxiv.org/abs/2103.14101v1,"Increasing availability of machine learning (ML) frameworks and tools, as
well as their promise to improve solutions to data-driven decision problems,
has resulted in popularity of using ML techniques in software systems. However,
end-to-end development of ML-enabled systems, as well as their seamless
deployment and operations, remain a challenge. One reason is that development
and deployment of ML-enabled systems involves three distinct workflows,
perspectives, and roles, which include data science, software engineering, and
operations. These three distinct perspectives, when misaligned due to incorrect
assumptions, cause ML mismatches which can result in failed systems. We
conducted an interview and survey study where we collected and validated common
types of mismatches that occur in end-to-end development of ML-enabled systems.
Our analysis shows that how each role prioritizes the importance of relevant
mismatches varies, potentially contributing to these mismatched assumptions. In
addition, the mismatch categories we identified can be specified as machine
readable descriptors contributing to improved ML-enabled system development. In
this paper, we report our findings and their implications for improving
end-to-end ML-enabled system development.",2022-11-13 16:04:21
XXX,journalArticle,2021,"David Leslie, Christopher Burr, Mhairi Aitken, Josh Cowls, Michael Katell, Morgan Briggs","Artificial intelligence, human rights, democracy, and the rule of law: a primer",,,,10.5281/zenodo.4639743,http://arxiv.org/abs/2104.04147v1,"In September 2019, the Council of Europe's Committee of Ministers adopted the
terms of reference for the Ad Hoc Committee on Artificial Intelligence (CAHAI).
The CAHAI is charged with examining the feasibility and potential elements of a
legal framework for the design, development, and deployment of AI systems that
accord with Council of Europe standards across the interrelated areas of human
rights, democracy, and the rule of law. As a first and necessary step in
carrying out this responsibility, the CAHAI's Feasibility Study, adopted by its
plenary in December 2020, has explored options for an international legal
response that fills existing gaps in legislation and tailors the use of binding
and non-binding legal instruments to the specific risks and opportunities
presented by AI systems. The Study examines how the fundamental rights and
freedoms that are already codified in international human rights law can be
used as the basis for such a legal framework. The purpose of this primer is to
introduce the main concepts and principles presented in the CAHAI's Feasibility
Study for a general, non-technical audience. It also aims to provide some
background information on the areas of AI innovation, human rights law,
technology policy, and compliance mechanisms covered therein. In keeping with
the Council of Europe's commitment to broad multi-stakeholder consultations,
outreach, and engagement, this primer has been designed to help facilitate the
meaningful and informed participation of an inclusive group of stakeholders as
the CAHAI seeks feedback and guidance regarding the essential issues raised by
the Feasibility Study.",2022-11-13 16:04:22
XXX,journalArticle,2021,"Ercument Ilhan, Jeremy Gow, Diego Perez-Liebana",Learning on a Budget via Teacher Imitation,,,,,http://arxiv.org/abs/2104.08440v3,"Deep Reinforcement Learning (RL) techniques can benefit greatly from
leveraging prior experience, which can be either self-generated or acquired
from other entities. Action advising is a framework that provides a flexible
way to transfer such knowledge in the form of actions between teacher-student
peers. However, due to the realistic concerns, the number of these interactions
is limited with a budget; therefore, it is crucial to perform these in the most
appropriate moments. There have been several promising studies recently that
address this problem setting especially from the student's perspective. Despite
their success, they have some shortcomings when it comes to the practical
applicability and integrity as an overall solution to the learning from advice
challenge. In this paper, we extend the idea of advice reusing via teacher
imitation to construct a unified approach that addresses both advice collection
and advice utilisation problems. We also propose a method to automatically tune
the relevant hyperparameters of these components on-the-fly to make it able to
adapt to any task with minimal human intervention. The experiments we performed
in 5 different Atari games verify that our algorithm either surpasses or
performs on-par with its top competitors while being far simpler to be
employed. Furthermore, its individual components are also found to be providing
significant advantages alone.",2022-11-13 16:04:22
XXX,journalArticle,2021,"Giulia Milan, Luca Vassio, Idilio Drago, Marco Mellia",RL-IoT: Reinforcement Learning to Interact with IoT Devices,,,,10.1109/COINS51742.2021.9524260,http://arxiv.org/abs/2105.00884v3,"Our life is getting filled by Internet of Things (IoT) devices. These devices
often rely on closed or poorly documented protocols, with unknown formats and
semantics. Learning how to interact with such devices in an autonomous manner
is the key for interoperability and automatic verification of their
capabilities. In this paper, we propose RL-IoT, a system that explores how to
automatically interact with possibly unknown IoT devices. We leverage
reinforcement learning (RL) to recover the semantics of protocol messages and
to take control of the device to reach a given goal, while minimizing the
number of interactions. We assume to know only a database of possible IoT
protocol messages, whose semantics are however unknown. RL-IoT exchanges
messages with the target IoT device, learning those commands that are useful to
reach the given goal. Our results show that RL-IoT is able to solve both simple
and complex tasks. With properly tuned parameters, RL-IoT learns how to perform
actions with the target device, a Yeelight smart bulb in our case study,
completing non-trivial patterns with as few as 400 interactions. RL-IoT paves
the road for automatic interactions with poorly documented IoT protocols, thus
enabling interoperable systems.",2022-11-13 16:04:23
XXX,journalArticle,2021,"Giorgio Angelotti, Nicolas Drougard, Caroline Ponzoni Carvalho Chanel",Exploitation vs Caution: Risk-sensitive Policies for Offline Learning,,,,,http://arxiv.org/abs/2105.13431v1,"Offline model learning for planning is a branch of machine learning that
trains agents to perform actions in an unknown environment using a fixed batch
of previously collected experiences. The limited size of the data set hinders
the estimate of the Value function of the relative Markov Decision Process
(MDP), bounding the performance of the obtained policy in the real world. In
this context, recent works showed that planning with a discount factor lower
than the one used during the evaluation phase yields more performing policies.
However, the optimal discount factor is finally chosen by cross-validation. Our
aim is to show that looking for a sub-optimal solution of a Bayesian MDP might
lead to better performances with respect to the current baselines that work in
the offline setting. Hence, we propose Exploitation vs Caution (EvC), an
algorithm that automatically selects the policy that solves a Risk-sensitive
Bayesian MDP in a set of policies obtained by solving several MDPs
characterized by different discount factors and transition dynamics. On one
hand, the Bayesian formalism elegantly includes model uncertainty and on
another hand the introduction of a risk-sensitive utility function guarantees
robustness. We evaluated the proposed approach in different discrete simple
environments offering a fair variety of MDP classes. We also compared the
obtained results with state-of-the-art offline learning for planning baselines
such as MOPO and MOReL. In the tested scenarios EvC is more robust than the
said approaches suggesting that sub-optimally solving an Offline Risk-sensitive
Bayesian MDP (ORBMDP) could define a sound framework for planning under model
uncertainty.",2022-11-13 16:04:24
XXX,journalArticle,2021,"Cassidy Laidlaw, Stuart Russell",Uncertain Decisions Facilitate Better Preference Learning,,,,,http://arxiv.org/abs/2106.10394v2,"Existing observational approaches for learning human preferences, such as
inverse reinforcement learning, usually make strong assumptions about the
observability of the human's environment. However, in reality, people make many
important decisions under uncertainty. To better understand preference learning
in these cases, we study the setting of inverse decision theory (IDT), a
previously proposed framework where a human is observed making non-sequential
binary decisions under uncertainty. In IDT, the human's preferences are
conveyed through their loss function, which expresses a tradeoff between
different types of mistakes. We give the first statistical analysis of IDT,
providing conditions necessary to identify these preferences and characterizing
the sample complexity -- the number of decisions that must be observed to learn
the tradeoff the human is making to a desired precision. Interestingly, we show
that it is actually easier to identify preferences when the decision problem is
more uncertain. Furthermore, uncertain decision problems allow us to relax the
unrealistic assumption that the human is an optimal decision maker but still
identify their exact preferences; we give sample complexities in this
suboptimal case as well. Our analysis contradicts the intuition that partial
observability should make preference learning more difficult. It also provides
a first step towards understanding and improving preference learning methods
for uncertain and suboptimal humans.",2022-11-13 16:04:25
XXX,journalArticle,2021,"Roel Dobbe, Thomas Krendl Gilbert, Yonatan Mintz",Hard Choices in Artificial Intelligence,,,,,http://arxiv.org/abs/2106.11022v1,"As AI systems are integrated into high stakes social domains, researchers now
examine how to design and operate them in a safe and ethical manner. However,
the criteria for identifying and diagnosing safety risks in complex social
contexts remain unclear and contested. In this paper, we examine the vagueness
in debates about the safety and ethical behavior of AI systems. We show how
this vagueness cannot be resolved through mathematical formalism alone, instead
requiring deliberation about the politics of development as well as the context
of deployment. Drawing from a new sociotechnical lexicon, we redefine vagueness
in terms of distinct design challenges at key stages in AI system development.
The resulting framework of Hard Choices in Artificial Intelligence (HCAI)
empowers developers by 1) identifying points of overlap between design
decisions and major sociotechnical challenges; 2) motivating the creation of
stakeholder feedback channels so that safety issues can be exhaustively
addressed. As such, HCAI contributes to a timely debate about the status of AI
development in democratic societies, arguing that deliberation should be the
goal of AI Safety, not just the procedure by which it is ensured.",2022-11-13 16:04:25
XXX,journalArticle,2021,"Carina Prunkl, Carolyn Ashurst, Markus Anderljung, Helena Webb, Jan Leike, Allan Dafoe",Institutionalising Ethics in AI through Broader Impact Requirements,Nature Machine Intelligence 3.2 (2021): 104-110,,,10.1038/s42256-021-00298-y,http://arxiv.org/abs/2106.11039v1,"Turning principles into practice is one of the most pressing challenges of
artificial intelligence (AI) governance. In this article, we reflect on a novel
governance initiative by one of the world's largest AI conferences. In 2020,
the Conference on Neural Information Processing Systems (NeurIPS) introduced a
requirement for submitting authors to include a statement on the broader
societal impacts of their research. Drawing insights from similar governance
initiatives, including institutional review boards (IRBs) and impact
requirements for funding applications, we investigate the risks, challenges and
potential benefits of such an initiative. Among the challenges, we list a lack
of recognised best practice and procedural transparency, researcher opportunity
costs, institutional and social pressures, cognitive biases, and the inherently
difficult nature of the task. The potential benefits, on the other hand,
include improved anticipation and identification of impacts, better
communication with policy and governance experts, and a general strengthening
of the norms around responsible research. To maximise the chance of success, we
recommend measures to increase transparency, improve guidance, create
incentives to engage earnestly with the process, and facilitate public
deliberation on the requirement's merits and future. Perhaps the most important
contribution from this analysis are the insights we can gain regarding
effective community-based governance and the role and responsibility of the AI
research community more broadly.",2022-11-13 16:04:26
XXX,journalArticle,2021,"Armin Moin, Andrei Mituca, Moharram Challenger, Atta Badii, Stephan Günnemann",ML-Quadrat & DriotData: A Model-Driven Engineering Tool and a Low-Code Platform for Smart IoT Services,,,,10.1109/ICSE-Companion55297.2022.9793752,http://arxiv.org/abs/2107.02692v4,"In this paper, we present ML-Quadrat, an open-source research prototype that
is based on the Eclipse Modeling Framework (EMF) and the state of the art in
the literature of Model-Driven Software Engineering (MDSE) for smart
Cyber-Physical Systems (CPS) and the Internet of Things (IoT). Its envisioned
users are mostly software developers who might not have deep knowledge and
skills in the heterogeneous IoT platforms and the diverse Artificial
Intelligence (AI) technologies, specifically regarding Machine Learning (ML).
ML-Quadrat is released under the terms of the Apache 2.0 license on Github.
Additionally, we demonstrate an early tool prototype of DriotData, a web-based
Low-Code platform targeting citizen data scientists and citizen/end-user
software developers. DriotData exploits and adopts ML-Quadrat in the industry
by offering an extended version of it as a subscription-based service to
companies, mainly Small- and Medium-Sized Enterprises (SME). The current
preliminary version of DriotData has three web-based model editors: text-based,
tree-/form-based and diagram-based. The latter is designed for domain experts
in the problem or use case domains (namely the IoT vertical domains) who might
not have knowledge and skills in the field of IT. Finally, a short video
demonstrating the tools is available on YouTube: https://youtu.be/VAuz25w0a5k",2022-11-13 16:04:27
XXX,journalArticle,2021,"Mohamed Abdelhack, Jiaming Zhang, Sandhya Tripathi, Bradley A Fritz, Daniel Felsky, Michael S Avidan, Yixin Chen, Christopher R King",A Modulation Layer to Increase Neural Network Robustness Against Data Quality Issues,,,,,http://arxiv.org/abs/2107.08574v2,"Data missingness and quality are common problems in machine learning,
especially for high-stakes applications such as healthcare. Developers often
train machine learning models on carefully curated datasets using only high
quality data; however, this reduces the utility of such models in production
environments. We propose a novel neural network modification to mitigate the
impacts of low quality and missing data which involves replacing the fixed
weights of a fully-connected layer with a function of an additional input. This
is inspired from neuromodulation in biological neural networks where the cortex
can up- and down-regulate inputs based on their reliability and the presence of
other data. In testing, with reliability scores as a modulating signal, models
with modulating layers were found to be more robust against degradation of data
quality, including additional missingness. These models are superior to
imputation as they save on training time by completely skipping the imputation
process and further allow the introduction of other data quality measures that
imputation cannot handle. Our results suggest that explicitly accounting for
reduced information quality with a modulating fully connected layer can enable
the deployment of artificial intelligence systems in real-time applications.",2022-11-13 16:04:27
XXX,journalArticle,2021,"Verena Praher, Katharina Prinz, Arthur Flexer, Gerhard Widmer","On the Veracity of Local, Model-agnostic Explanations in Audio Classification: Targeted Investigations with Adversarial Examples",,,,,http://arxiv.org/abs/2107.09045v2,"Local explanation methods such as LIME have become popular in MIR as tools
for generating post-hoc, model-agnostic explanations of a model's
classification decisions. The basic idea is to identify a small set of
human-understandable features of the classified example that are most
influential on the classifier's prediction. These are then presented as an
explanation. Evaluation of such explanations in publications often resorts to
accepting what matches the expectation of a human without actually being able
to verify if what the explanation shows is what really caused the model's
prediction. This paper reports on targeted investigations where we try to get
more insight into the actual veracity of LIME's explanations in an audio
classification task. We deliberately design adversarial examples for the
classifier, in a way that gives us knowledge about which parts of the input are
potentially responsible for the model's (wrong) prediction. Asking LIME to
explain the predictions for these adversaries permits us to study whether local
explanations do indeed detect these regions of interest. We also look at
whether LIME is more successful in finding perturbations that are more
prominent and easily noticeable for a human. Our results suggest that LIME does
not necessarily manage to identify the most relevant input features and hence
it remains unclear whether explanations are useful or even misleading.",2022-11-13 16:04:28
XXX,journalArticle,2021,"Elena Baninemeh, Siamak Farshidi, Slinger Jansen",A Decision Model for Decentralized Autonomous Organization Platform Selection: Three Industry Case Studies,,,,,http://arxiv.org/abs/2107.14093v1,"Decentralized autonomous organizations as a new form of online governance
arecollections of smart contracts deployed on a blockchain platform that
intercede groupsof people. A growing number of Decentralized Autonomous
Organization Platforms,such as Aragon and Colony, have been introduced in the
market to facilitate thedevelopment process of such organizations. Selecting
the best fitting platform ischallenging for the organizations, as a significant
number of decision criteria, such aspopularity, developer availability,
governance issues, and consistent documentation ofsuch platforms, should be
considered. Additionally, decision-makers at theorganizations are not experts
in every domain, so they must continuously acquirevolatile knowledge regarding
such platforms and keep themselves updated.Accordingly, a decision model is
required to analyze the decision criteria usingsystematic identification and
evaluation of potential alternative solutions for adevelopment project. We have
developed a theoretical framework to assist softwareengineers with a set of
Multi-Criteria Decision-Making problems in software production.This study
presents a decision model as a Multi-Criteria Decision-Making problem forthe
decentralized autonomous organization platform selection problem. Weconducted
three industry case studies in the context of three decentralizedautonomous
organizations to evaluate the effectiveness and efficiency of the decisionmodel
in assisting decision-makers.",2022-11-13 16:04:28
XXX,journalArticle,2021,"Ryan Hoque, Ashwin Balakrishna, Ellen Novoseller, Albert Wilcox, Daniel S. Brown, Ken Goldberg",ThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,,,,,http://arxiv.org/abs/2109.08273v1,"Effective robot learning often requires online human feedback and
interventions that can cost significant human time, giving rise to the central
challenge in interactive imitation learning: is it possible to control the
timing and length of interventions to both facilitate learning and limit burden
on the human supervisor? This paper presents ThriftyDAgger, an algorithm for
actively querying a human supervisor given a desired budget of human
interventions. ThriftyDAgger uses a learned switching policy to solicit
interventions only at states that are sufficiently (1) novel, where the robot
policy has no reference behavior to imitate, or (2) risky, where the robot has
low confidence in task completion. To detect the latter, we introduce a novel
metric for estimating risk under the current robot policy. Experiments in
simulation and on a physical cable routing experiment suggest that
ThriftyDAgger's intervention criteria balances task performance and supervisor
burden more effectively than prior algorithms. ThriftyDAgger can also be
applied at execution time, where it achieves a 100% success rate on both the
simulation and physical tasks. A user study (N=10) in which users control a
three-robot fleet while also performing a concentration task suggests that
ThriftyDAgger increases human and robot performance by 58% and 80% respectively
compared to the next best algorithm while reducing supervisor burden.",2022-11-13 16:04:29
XXX,journalArticle,2021,Vishal Rajput,Robustness of different loss functions and their impact on networks learning capability,,,,,http://arxiv.org/abs/2110.08322v2,"Recent developments in AI have made it ubiquitous, every industry is trying
to adopt some form of intelligent processing of their data. Despite so many
advances in the field, AIs full capability is yet to be exploited by the
industry. Industries that involve some risk factors still remain cautious about
the usage of AI due to the lack of trust in such autonomous systems.
Present-day AI might be very good in a lot of things but it is very bad in
reasoning and this behavior of AI can lead to catastrophic results. Autonomous
cars crashing into a person or a drone getting stuck in a tree are a few
examples where AI decisions lead to catastrophic results. To develop insight
and generate an explanation about the learning capability of AI, we will try to
analyze the working of loss functions. For our case, we will use two sets of
loss functions, generalized loss functions like Binary cross-entropy or BCE and
specialized loss functions like Dice loss or focal loss. Through a series of
experiments, we will establish whether combining different loss functions is
better than using a single loss function and if yes, then what is the reason
behind it. In order to establish the difference between generalized loss and
specialized losses, we will train several models using the above-mentioned
losses and then compare their robustness on adversarial examples. In
particular, we will look at how fast the accuracy of different models decreases
when we change the pixels corresponding to the most salient gradients.",2022-11-13 16:04:29
XXX,journalArticle,2021,"Tien-Hong Lo, Yao-Ting Sung, Berlin Chen",Improving End-To-End Modeling for Mispronunciation Detection with Effective Augmentation Mechanisms,,,,,http://arxiv.org/abs/2110.08731v1,"Recently, end-to-end (E2E) models, which allow to take spectral vector
sequences of L2 (second-language) learners' utterances as input and produce the
corresponding phone-level sequences as output, have attracted much research
attention in developing mispronunciation detection (MD) systems. However, due
to the lack of sufficient labeled speech data of L2 speakers for model
estimation, E2E MD models are prone to overfitting in relation to conventional
ones that are built on DNN-HMM acoustic models. To alleviate this critical
issue, we in this paper propose two modeling strategies to enhance the
discrimination capability of E2E MD models, each of which can implicitly
leverage the phonetic and phonological traits encoded in a pretrained acoustic
model and contained within reference transcripts of the training data,
respectively. The first one is input augmentation, which aims to distill
knowledge about phonetic discrimination from a DNN-HMM acoustic model. The
second one is label augmentation, which manages to capture more phonological
patterns from the transcripts of training data. A series of empirical
experiments conducted on the L2-ARCTIC English dataset seem to confirm the
efficacy of our E2E MD model when compared to some top-of-the-line E2E MD
models and a classic pronunciation-scoring based method built on a DNN-HMM
acoustic model.",2022-11-13 16:04:30
XXX,journalArticle,2021,"Mo Yu, Yang Zhang, Shiyu Chang, Tommi S. Jaakkola",Understanding Interlocking Dynamics of Cooperative Rationalization,,,,,http://arxiv.org/abs/2110.13880v1,"Selective rationalization explains the prediction of complex neural networks
by finding a small subset of the input that is sufficient to predict the neural
model output. The selection mechanism is commonly integrated into the model
itself by specifying a two-component cascaded system consisting of a rationale
generator, which makes a binary selection of the input features (which is the
rationale), and a predictor, which predicts the output based only on the
selected features. The components are trained jointly to optimize prediction
performance. In this paper, we reveal a major problem with such cooperative
rationalization paradigm -- model interlocking. Interlocking arises when the
predictor overfits to the features selected by the generator thus reinforcing
the generator's selection even if the selected rationales are sub-optimal. The
fundamental cause of the interlocking problem is that the rationalization
objective to be minimized is concave with respect to the generator's selection
policy. We propose a new rationalization framework, called A2R, which
introduces a third component into the architecture, a predictor driven by soft
attention as opposed to selection. The generator now realizes both soft and
hard attention over the features and these are fed into the two different
predictors. While the generator still seeks to support the original predictor
performance, it also minimizes a gap between the two predictors. As we will
show theoretically, since the attention-based predictor exhibits a better
convexity property, A2R can overcome the concavity barrier. Our experiments on
two synthetic benchmarks and two real datasets demonstrate that A2R can
significantly alleviate the interlock problem and find explanations that better
align with human judgments. We release our code at
https://github.com/Gorov/Understanding_Interlocking.",2022-11-13 16:04:31
XXX,journalArticle,2021,"Tejas Sudharshan Mathai, Sungwon Lee, Daniel C. Elton, Thomas C. Shen, Yifan Peng, Zhiyong Lu, Ronald M. Summers",Lymph Node Detection in T2 MRI with Transformers,,,,,http://arxiv.org/abs/2111.04885v1,"Identification of lymph nodes (LN) in T2 Magnetic Resonance Imaging (MRI) is
an important step performed by radiologists during the assessment of
lymphoproliferative diseases. The size of the nodes play a crucial role in
their staging, and radiologists sometimes use an additional contrast sequence
such as diffusion weighted imaging (DWI) for confirmation. However, lymph nodes
have diverse appearances in T2 MRI scans, making it tough to stage for
metastasis. Furthermore, radiologists often miss smaller metastatic lymph nodes
over the course of a busy day. To deal with these issues, we propose to use the
DEtection TRansformer (DETR) network to localize suspicious metastatic lymph
nodes for staging in challenging T2 MRI scans acquired by different scanners
and exam protocols. False positives (FP) were reduced through a bounding box
fusion technique, and a precision of 65.41\% and sensitivity of 91.66\% at 4 FP
per image was achieved. To the best of our knowledge, our results improve upon
the current state-of-the-art for lymph node detection in T2 MRI scans.",2022-11-13 16:04:31
XXX,journalArticle,2021,"Luca Pion-Tonachini, Kristofer Bouchard, Hector Garcia Martin, Sean Peisert, W. Bradley Holtz, Anil Aswani, Dipankar Dwivedi, Haruko Wainwright, Ghanshyam Pilania, Benjamin Nachman, Babetta L. Marrone, Nicola Falco, Prabhat, Daniel Arnold, Alejandro Wolf-Yadlin, Sarah Powers, Sharlee Climer, Quinn Jackson, Ty Carlson, Michael Sohn, Petrus Zwart, Neeraj Kumar, Amy Justice, Claire Tomlin, Daniel Jacobson, Gos Micklem, Georgios V. Gkoutos, Peter J. Bickel, Jean-Baptiste Cazier, Juliane Müller, Bobbie-Jo Webb-Robertson, Rick Stevens, Mark Anderson, Ken Kreutz-Delgado, Michael W. Mahoney, James B. Brown",Learning from learning machines: a new generation of AI technology to meet the needs of science,,,,,http://arxiv.org/abs/2111.13786v1,"We outline emerging opportunities and challenges to enhance the utility of AI
for scientific discovery. The distinct goals of AI for industry versus the
goals of AI for science create tension between identifying patterns in data
versus discovering patterns in the world from data. If we address the
fundamental challenges associated with ""bridging the gap"" between domain-driven
scientific models and data-driven AI learning machines, then we expect that
these AI models can transform hypothesis generation, scientific discovery, and
the scientific process itself.",2022-11-13 16:04:32
XXX,journalArticle,2021,"Michael Luo, Ashwin Balakrishna, Brijen Thananjeyan, Suraj Nair, Julian Ibarz, Jie Tan, Chelsea Finn, Ion Stoica, Ken Goldberg",MESA: Offline Meta-RL for Safe Adaptation and Fault Tolerance,"Workshop on Safe and Robust Control of Uncertain Systems at the
  35th Conference on Neural Information Processing Systems (NeurIPS 2021),
  Online",,,,http://arxiv.org/abs/2112.03575v1,"Safe exploration is critical for using reinforcement learning (RL) in
risk-sensitive environments. Recent work learns risk measures which measure the
probability of violating constraints, which can then be used to enable safety.
However, learning such risk measures requires significant interaction with the
environment, resulting in excessive constraint violations during learning.
Furthermore, these measures are not easily transferable to new environments. We
cast safe exploration as an offline meta-RL problem, where the objective is to
leverage examples of safe and unsafe behavior across a range of environments to
quickly adapt learned risk measures to a new environment with previously unseen
dynamics. We then propose MEta-learning for Safe Adaptation (MESA), an approach
for meta-learning a risk measure for safe RL. Simulation experiments across 5
continuous control domains suggest that MESA can leverage offline data from a
range of different environments to reduce constraint violations in unseen
environments by up to a factor of 2 while maintaining task performance. See
https://tinyurl.com/safe-meta-rl for code and supplementary material.",2022-11-13 16:04:32
XXX,journalArticle,2021,"Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, John Schulman",WebGPT: Browser-assisted question-answering with human feedback,,,,,http://arxiv.org/abs/2112.09332v3,"We fine-tune GPT-3 to answer long-form questions using a text-based
web-browsing environment, which allows the model to search and navigate the
web. By setting up the task so that it can be performed by humans, we are able
to train models on the task using imitation learning, and then optimize answer
quality with human feedback. To make human evaluation of factual accuracy
easier, models must collect references while browsing in support of their
answers. We train and evaluate our models on ELI5, a dataset of questions asked
by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior
cloning, and then performing rejection sampling against a reward model trained
to predict human preferences. This model's answers are preferred by humans 56%
of the time to those of our human demonstrators, and 69% of the time to the
highest-voted answer from Reddit.",2022-11-13 16:04:33
XXX,journalArticle,2022,"Tiago Gaspar Oliveira, Arlindo L. Oliveira","Assessing Policy, Loss and Planning Combinations in Reinforcement Learning using a New Modular Architecture",,,,,http://arxiv.org/abs/2201.02874v1,"The model-based reinforcement learning paradigm, which uses planning
algorithms and neural network models, has recently achieved unprecedented
results in diverse applications, leading to what is now known as deep
reinforcement learning. These agents are quite complex and involve multiple
components, factors that can create challenges for research. In this work, we
propose a new modular software architecture suited for these types of agents,
and a set of building blocks that can be easily reused and assembled to
construct new model-based reinforcement learning agents. These building blocks
include planning algorithms, policies, and loss functions.
  We illustrate the use of this architecture by combining several of these
building blocks to implement and test agents that are optimized to three
different test environments: Cartpole, Minigrid, and Tictactoe. One particular
planning algorithm, made available in our implementation and not previously
used in reinforcement learning, which we called averaged minimax, achieved good
results in the three tested environments.
  Experiments performed with this architecture have shown that the best
combination of planning algorithm, policy, and loss function is heavily problem
dependent. This result provides evidence that the proposed architecture, which
is modular and reusable, is useful for reinforcement learning researchers who
want to study new environments and techniques.",2022-11-13 16:04:33
XXX,journalArticle,2022,Erik Brynjolfsson,The Turing Trap: The Promise & Peril of Human-Like Artificial Intelligence,,,,,http://arxiv.org/abs/2201.04200v1,"In 1950, Alan Turing proposed an imitation game as the ultimate test of
whether a machine was intelligent: could a machine imitate a human so well that
its answers to questions indistinguishable from a human. Ever since, creating
intelligence that matches human intelligence has implicitly or explicitly been
the goal of thousands of researchers, engineers, and entrepreneurs. The
benefits of human-like artificial intelligence (HLAI) include soaring
productivity, increased leisure, and perhaps most profoundly, a better
understanding of our own minds.
  But not all types of AI are human-like. In fact, many of the most powerful
systems are very different from humans. So an excessive focus on developing and
deploying HLAI can lead us into a trap. As machines become better substitutes
for human labor, workers lose economic and political bargaining power and
become increasingly dependent on those who control the technology. In contrast,
when AI is focused on augmenting humans rather than mimicking them, then humans
retain the power to insist on a share of the value created. Furthermore,
augmentation creates new capabilities and new products and services, ultimately
generating far more value than merely human-like AI. While both types of AI can
be enormously beneficial, there are currently excess incentives for automation
rather than augmentation among technologists, business executives, and
policymakers.",2022-11-13 16:04:34
XXX,journalArticle,2022,"Mustafa O. Karabag, Cyrus Neary, Ufuk Topcu",Planning Not to Talk: Multiagent Systems that are Robust to Communication Loss,,,,,http://arxiv.org/abs/2201.06619v1,"In a cooperative multiagent system, a collection of agents executes a joint
policy in order to achieve some common objective. The successful deployment of
such systems hinges on the availability of reliable inter-agent communication.
However, many sources of potential disruption to communication exist in
practice, such as radio interference, hardware failure, and adversarial
attacks. In this work, we develop joint policies for cooperative multiagent
systems that are robust to potential losses in communication. More
specifically, we develop joint policies for cooperative Markov games with
reach-avoid objectives. First, we propose an algorithm for the decentralized
execution of joint policies during periods of communication loss. Next, we use
the total correlation of the state-action process induced by a joint policy as
a measure of the intrinsic dependencies between the agents. We then use this
measure to lower-bound the performance of a joint policy when communication is
lost. Finally, we present an algorithm that maximizes a proxy to this lower
bound in order to synthesize minimum-dependency joint policies that are robust
to communication loss. Numerical experiments show that the proposed
minimum-dependency policies require minimal coordination between the agents
while incurring little to no loss in performance; the total correlation value
of the synthesized policy is one fifth of the total correlation value of the
baseline policy which does not take potential communication losses into
account. As a result, the performance of the minimum-dependency policies
remains consistently high regardless of whether or not communication is
available. By contrast, the performance of the baseline policy decreases by
twenty percent when communication is lost.",2022-11-13 16:04:34
XXX,journalArticle,2022,"Pierre Carbonnelle, Simon Vandevelde, Joost Vennekens, Marc Denecker",IDP-Z3: a reasoning engine for FO(.),,,,,http://arxiv.org/abs/2202.00343v2,"An important sign of intelligence is the capacity to apply a body of
knowledge to a particular situation in order to not only derive new knowledge,
but also to determine relevant questions or provide explanations. Developing
interactive systems capable of performing such a variety of reasoning tasks for
the benefits of its users has proved difficult, notably for performance and/or
development cost reasons. Still, recently, a reasoning engine, called IDP3, has
been used to build such systems, but it lacked support for arithmetic
operations, seriously limiting its usefulness. We have developed a new
reasoning engine, IDP-Z3, that removes this limitation, and we put it to the
test in four knowledge-intensive industrial use cases.
  This paper describes FO(.) (aka FO-dot), the language used to represent
knowledge in the IDP3 and IDP-Z3 system. It then describes the generic
reasoning tasks that IDP-Z3 can perform, and how we used them to build a
generic user interface, called the Interactive Consultant. Finally, it reports
on the four use cases.
  In these four use cases, the interactive applications based on IDP-Z3 were
capable of intelligent behavior of value to users, while having a low
development cost (typically 10 days) and an acceptable response time (typically
below 3 seconds). Performance could be further improved, in particular for
problems on larger domains.",2022-11-13 16:04:35
XXX,journalArticle,2022,"Tong Mu, Stephan Zheng, Alexander Trott",Solving Dynamic Principal-Agent Problems with a Rationally Inattentive Principal,,,,,http://arxiv.org/abs/2202.01691v2,"Principal-Agent (PA) problems describe a broad class of economic
relationships characterized by misaligned incentives and asymmetric
information. The Principal's problem is to find optimal incentives given the
available information, e.g., a manager setting optimal wages for its employees.
Whereas the Principal is often assumed rational, comparatively little is known
about solutions when the Principal is boundedly rational, especially in the
sequential setting, with multiple Agents, and with multiple information
channels. Here, we develop RIRL, a deep reinforcement learning framework that
solves such complex PA problems with a rationally inattentive Principal. Such a
Principal incurs a cost for paying attention to information, which can model
forms of bounded rationality. We use RIRL to analyze rich economic phenomena in
manager-employee relationships. In the single-step setting, 1) RIRL yields
wages that are consistent with theoretical predictions; and 2) non-zero
attention costs lead to simpler but less profitable wage structures, and
increased Agent welfare. In a sequential setting with multiple Agents, RIRL
shows opposing consequences of the Principal's inattention to different
information channels: 1) inattention to Agents' outputs closes wage gaps based
on ability differences; and 2) inattention to Agents' efforts induces a social
dilemma dynamic in which Agents work harder, but essentially for free.
Moreover, RIRL reveals non-trivial relationships between the Principal's
inattention and Agent types, e.g., if Agents are prone to sub-optimal effort
choices, payment schedules are more sensitive to the Principal's attention
cost. As such, RIRL can reveal novel economic relationships and enables
progress towards understanding the effects of bounded rationality in dynamic
settings.",2022-11-13 16:04:36
XXX,journalArticle,2022,"Leonardo Lucio Custode, Giovanni Iacca",Interpretable pipelines with evolutionarily optimized modules for RL tasks with visual inputs,,,,10.1145/3520304.3528897,http://arxiv.org/abs/2202.04943v1,"The importance of explainability in AI has become a pressing concern, for
which several explainable AI (XAI) approaches have been recently proposed.
However, most of the available XAI techniques are post-hoc methods, which
however may be only partially reliable, as they do not reflect exactly the
state of the original models. Thus, a more direct way for achieving XAI is
through interpretable (also called glass-box) models. These models have been
shown to obtain comparable (and, in some cases, better) performance with
respect to black-boxes models in various tasks such as classification and
reinforcement learning. However, they struggle when working with raw data,
especially when the input dimensionality increases and the raw inputs alone do
not give valuable insights on the decision-making process. Here, we propose to
use end-to-end pipelines composed of multiple interpretable models co-optimized
by means of evolutionary algorithms, that allows us to decompose the
decision-making process into two parts: computing high-level features from raw
data, and reasoning on the extracted high-level features. We test our approach
in reinforcement learning environments from the Atari benchmark, where we
obtain comparable results (with respect to black-box approaches) in settings
without stochastic frame-skipping, while performance degrades in frame-skipping
settings.",2022-11-13 16:04:36
XXX,journalArticle,2022,Max W. Shen,"Trust in AI: Interpretability is not necessary or sufficient, while black-box interaction is necessary and sufficient",,,,,http://arxiv.org/abs/2202.05302v1,"The problem of human trust in artificial intelligence is one of the most
fundamental problems in applied machine learning. Our processes for evaluating
AI trustworthiness have substantial ramifications for ML's impact on science,
health, and humanity, yet confusion surrounds foundational concepts. What does
it mean to trust an AI, and how do humans assess AI trustworthiness? What are
the mechanisms for building trustworthy AI? And what is the role of
interpretable ML in trust? Here, we draw from statistical learning theory and
sociological lenses on human-automation trust to motivate an AI-as-tool
framework, which distinguishes human-AI trust from human-AI-human trust.
Evaluating an AI's contractual trustworthiness involves predicting future model
behavior using behavior certificates (BCs) that aggregate behavioral evidence
from diverse sources including empirical out-of-distribution and out-of-task
evaluation and theoretical proofs linking model architecture to behavior. We
clarify the role of interpretability in trust with a ladder of model access.
Interpretability (level 3) is not necessary or even sufficient for trust, while
the ability to run a black-box model at-will (level 2) is necessary and
sufficient. While interpretability can offer benefits for trust, it can also
incur costs. We clarify ways interpretability can contribute to trust, while
questioning the perceived centrality of interpretability to trust in popular
discourse. How can we empower people with tools to evaluate trust? Instead of
trying to understand how a model works, we argue for understanding how a model
behaves. Instead of opening up black boxes, we should create more behavior
certificates that are more correct, relevant, and understandable. We discuss
how to build trusted and trustworthy AI responsibly.",2022-11-13 16:04:37
XXX,journalArticle,2022,"Jan Balaguer, Raphael Koster, Ari Weinstein, Lucy Campbell-Gillingham, Christopher Summerfield, Matthew Botvinick, Andrea Tacchetti",HCMD-zero: Learning Value Aligned Mechanisms from Data,,,,,http://arxiv.org/abs/2202.10122v2,"Artificial learning agents are mediating a larger and larger number of
interactions among humans, firms, and organizations, and the intersection
between mechanism design and machine learning has been heavily investigated in
recent years. However, mechanism design methods often make strong assumptions
on how participants behave (e.g. rationality), on the kind of knowledge
designers have access to a priori (e.g. access to strong baseline mechanisms),
or on what the goal of the mechanism should be (e.g. total welfare). Here we
introduce HCMD-zero, a general purpose method to construct mechanisms making
none of these three assumptions. HCMD-zero learns to mediate interactions among
participants and adjusts the mechanism parameters to make itself more likely to
be preferred by participants. It does so by remaining engaged in an electoral
contest with copies of itself, thereby accessing direct feedback from
participants. We test our method on a stylized resource allocation game that
highlights the tension between productivity, equality and the temptation to
free ride. HCMD-zero produces a mechanism that is preferred by human
participants over a strong baseline, it does so automatically, without
requiring prior knowledge, and using human behavioral trajectories sparingly
and effectively. Our analysis shows HCMD-zero consistently makes the mechanism
policy more and more likely to be preferred by human participants over the
course of training, and that it results in a mechanism with an interpretable
and intuitive policy.",2022-11-13 16:04:37
XXX,journalArticle,2022,"Andi Peng, Besmira Nushi, Emre Kiciman, Kori Inkpen, Ece Kamar",Investigations of Performance and Bias in Human-AI Teamwork in Hiring,,,,,http://arxiv.org/abs/2202.11812v1,"In AI-assisted decision-making, effective hybrid (human-AI) teamwork is not
solely dependent on AI performance alone, but also on its impact on human
decision-making. While prior work studies the effects of model accuracy on
humans, we endeavour here to investigate the complex dynamics of how both a
model's predictive performance and bias may transfer to humans in a
recommendation-aided decision task. We consider the domain of ML-assisted
hiring, where humans -- operating in a constrained selection setting -- can
choose whether they wish to utilize a trained model's inferences to help select
candidates from written biographies. We conduct a large-scale user study
leveraging a re-created dataset of real bios from prior work, where humans
predict the ground truth occupation of given candidates with and without the
help of three different NLP classifiers (random, bag-of-words, and deep neural
network). Our results demonstrate that while high-performance models
significantly improve human performance in a hybrid setting, some models
mitigate hybrid bias while others accentuate it. We examine these findings
through the lens of decision conformity and observe that our model architecture
choices have an impact on human-AI conformity and bias, motivating the explicit
need to assess these complex dynamics prior to deployment.",2022-11-13 16:04:38
XXX,journalArticle,2022,"Michael S. Lee, Henny Admoni, Reid Simmons",Reasoning about Counterfactuals to Improve Human Inverse Reinforcement Learning,,,,,http://arxiv.org/abs/2203.01855v3,"To collaborate well with robots, we must be able to understand their decision
making. Humans naturally infer other agents' beliefs and desires by reasoning
about their observable behavior in a way that resembles inverse reinforcement
learning (IRL). Thus, robots can convey their beliefs and desires by providing
demonstrations that are informative for a human learner's IRL. An informative
demonstration is one that differs strongly from the learner's expectations of
what the robot will do given their current understanding of the robot's
decision making. However, standard IRL does not model the learner's existing
expectations, and thus cannot do this counterfactual reasoning. We propose to
incorporate the learner's current understanding of the robot's decision making
into our model of human IRL, so that a robot can select demonstrations that
maximize the human's understanding. We also propose a novel measure for
estimating the difficulty for a human to predict instances of a robot's
behavior in unseen environments. A user study finds that our test difficulty
measure correlates well with human performance and confidence. Interestingly,
considering human beliefs and counterfactuals when selecting demonstrations
decreases human performance on easy tests, but increases performance on
difficult tests, providing insight on how to best utilize such models.",2022-11-13 16:04:38
XXX,journalArticle,2022,"Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe",Training language models to follow instructions with human feedback,,,,,http://arxiv.org/abs/2203.02155v1,"Making language models bigger does not inherently make them better at
following a user's intent. For example, large language models can generate
outputs that are untruthful, toxic, or simply not helpful to the user. In other
words, these models are not aligned with their users. In this paper, we show an
avenue for aligning language models with user intent on a wide range of tasks
by fine-tuning with human feedback. Starting with a set of labeler-written
prompts and prompts submitted through the OpenAI API, we collect a dataset of
labeler demonstrations of the desired model behavior, which we use to fine-tune
GPT-3 using supervised learning. We then collect a dataset of rankings of model
outputs, which we use to further fine-tune this supervised model using
reinforcement learning from human feedback. We call the resulting models
InstructGPT. In human evaluations on our prompt distribution, outputs from the
1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3,
despite having 100x fewer parameters. Moreover, InstructGPT models show
improvements in truthfulness and reductions in toxic output generation while
having minimal performance regressions on public NLP datasets. Even though
InstructGPT still makes simple mistakes, our results show that fine-tuning with
human feedback is a promising direction for aligning language models with human
intent.",2022-11-13 16:04:39
XXX,journalArticle,2022,"Ingmar Kanitscheider, Harri Edwards",AutoDIME: Automatic Design of Interesting Multi-Agent Environments,,,,,http://arxiv.org/abs/2203.02481v1,"Designing a distribution of environments in which RL agents can learn
interesting and useful skills is a challenging and poorly understood task, for
multi-agent environments the difficulties are only exacerbated. One approach is
to train a second RL agent, called a teacher, who samples environments that are
conducive for the learning of student agents. However, most previous proposals
for teacher rewards do not generalize straightforwardly to the multi-agent
setting. We examine a set of intrinsic teacher rewards derived from prediction
problems that can be applied in multi-agent settings and evaluate them in
Mujoco tasks such as multi-agent Hide and Seek as well as a diagnostic
single-agent maze task. Of the intrinsic rewards considered we found value
disagreement to be most consistent across tasks, leading to faster and more
reliable emergence of advanced skills in Hide and Seek and the maze task.
Another candidate intrinsic reward considered, value prediction error, also
worked well in Hide and Seek but was susceptible to noisy-TV style distractions
in stochastic environments. Policy disagreement performed well in the maze task
but did not speed up learning in Hide and Seek. Our results suggest that
intrinsic teacher rewards, and in particular value disagreement, are a
promising approach for automating both single and multi-agent environment
design.",2022-11-13 16:04:40
XXX,journalArticle,2022,"Dayong Ye, Tianqing Zhu, Shuai Zhou, Bo Liu, Wanlei Zhou",Label-only Model Inversion Attack: The Attack that Requires the Least Information,,,,,http://arxiv.org/abs/2203.06555v1,"In a model inversion attack, an adversary attempts to reconstruct the data
records, used to train a target model, using only the model's output. In
launching a contemporary model inversion attack, the strategies discussed are
generally based on either predicted confidence score vectors, i.e., black-box
attacks, or the parameters of a target model, i.e., white-box attacks. However,
in the real world, model owners usually only give out the predicted labels; the
confidence score vectors and model parameters are hidden as a defense mechanism
to prevent such attacks. Unfortunately, we have found a model inversion method
that can reconstruct the input data records based only on the output labels. We
believe this is the attack that requires the least information to succeed and,
therefore, has the best applicability. The key idea is to exploit the error
rate of the target model to compute the median distance from a set of data
records to the decision boundary of the target model. The distance, then, is
used to generate confidence score vectors which are adopted to train an attack
model to reconstruct the data records. The experimental results show that
highly recognizable data records can be reconstructed with far less information
than existing methods.",2022-11-13 16:04:41
XXX,journalArticle,2022,Tong Owen Yang,Algebraic Learning: Towards Interpretable Information Modeling,,,,,http://arxiv.org/abs/2203.06690v1,"Along with the proliferation of digital data collected using sensor
technologies and a boost of computing power, Deep Learning (DL) based
approaches have drawn enormous attention in the past decade due to their
impressive performance in extracting complex relations from raw data and
representing valuable information. Meanwhile, though, rooted in its notorious
black-box nature, the appreciation of DL has been highly debated due to the
lack of interpretability. On the one hand, DL only utilizes statistical
features contained in raw data while ignoring human knowledge of the underlying
system, which results in both data inefficiency and trust issues; on the other
hand, a trained DL model does not provide to researchers any extra insight
about the underlying system beyond its output, which, however, is the essence
of most fields of science, e.g. physics and economics.
  This thesis addresses the issue of interpretability in general information
modeling and endeavors to ease the problem from two scopes. Firstly, a
problem-oriented perspective is applied to incorporate knowledge into modeling
practice, where interesting mathematical properties emerge naturally which cast
constraints on modeling. Secondly, given a trained model, various methods could
be applied to extract further insights about the underlying system. These two
pathways are termed as guided model design and secondary measurements.
Remarkably, a novel scheme emerges for the modeling practice in statistical
learning: Algebraic Learning (AgLr). Instead of being restricted to the
discussion of any specific model, AgLr starts from idiosyncrasies of a learning
task itself and studies the structure of a legitimate model class. This novel
scheme demonstrates the noteworthy value of abstract algebra for general AI,
which has been overlooked in recent progress, and could shed further light on
interpretable information modeling.",2022-11-13 16:04:41
XXX,journalArticle,2022,"Jens Heidrich, Andreas Jedlitschka, Adam Trendowicz, Anna Maria Vollmer",Building AI Innovation Labs together with Companies,,,,,http://arxiv.org/abs/2203.08465v1,"In the future, most companies will be confronted with the topic of Artificial
Intelligence (AI) and will have to decide on their strategy in this regards.
Currently, a lot of companies are thinking about whether and how AI and the
usage of data will impact their business model and what potential use cases
could look like. One of the biggest challenges lies in coming up with
innovative solution ideas with a clear business value. This requires business
competencies on the one hand and technical competencies in AI and data
analytics on the other hand. In this article, we present the concept of AI
innovation labs and demonstrate a comprehensive framework, from coming up with
the right ideas to incrementally implementing and evaluating them regarding
their business value and their feasibility based on a company's capabilities.
The concept is the result of nine years of working on data-driven innovations
with companies from various domains. Furthermore, we share some lessons learned
from its practical applications. Even though a lot of technical publications
can be found in the literature regarding the development of AI models and many
consultancy companies provide corresponding services for building AI
innovations, we found very few publications sharing details about what an
end-to-end framework could look like.",2022-11-13 16:04:42
XXX,journalArticle,2022,Brian Subirana,"An Artificial Intelligence Browser Architecture (AIBA) For Our Kind and Others: A Voice Name System Speech implementation with two warrants, Wake Neutrality and Value Preservation of Personally Identifiable Information",,,,,http://arxiv.org/abs/2203.16497v2,"Conversational commerce, first pioneered by Apple's Siri, is the first of may
applications based on always-on artificial intelligence systems that decide on
its own when to interact with the environment, potentially collecting 24x7
longitudinal training data that is often Personally Identifiable Information
(PII). A large body of scholarly papers, on the order of a million according to
a simple Google Scholar search, suggests that the treatment of many health
conditions, including COVID-19 and dementia, can be vastly improved by this
data if the dataset is large enough as it has happened in other domains (e.g.
GPT3). In contrast, current dominant systems are closed garden solutions
without wake neutrality and that can't fully exploit the PII data they have
because of IRB and Cohues-type constraints.
  We present a voice browser-and-server architecture that aims to address these
two limitations by offering wake neutrality and the possibility to handle PII
aiming to maximize its value. We have implemented this browser for the
collection of speech samples and have successfully demonstrated it can capture
over 200.000 samples of COVID-19 coughs. The architecture we propose is
designed so it can grow beyond our kind into other domains such as collecting
sound samples from vehicles, video images from nature, ingestible robotics,
multi-modal signals (EEG, EKG,...), or even interacting with other kinds such
as dogs and cats.",2022-11-13 16:04:42
XXX,journalArticle,2022,"Kamilia Mullakaeva, Luca Cosmo, Anees Kazi, Seyed-Ahmad Ahmadi, Nassir Navab, Michael M. Bronstein",Graph-in-Graph (GiG): Learning interpretable latent graphs in non-Euclidean domain for biological and healthcare applications,,,,,http://arxiv.org/abs/2204.00323v1,"Graphs are a powerful tool for representing and analyzing unstructured,
non-Euclidean data ubiquitous in the healthcare domain. Two prominent examples
are molecule property prediction and brain connectome analysis. Importantly,
recent works have shown that considering relationships between input data
samples have a positive regularizing effect for the downstream task in
healthcare applications. These relationships are naturally modeled by a
(possibly unknown) graph structure between input samples. In this work, we
propose Graph-in-Graph (GiG), a neural network architecture for protein
classification and brain imaging applications that exploits the graph
representation of the input data samples and their latent relation. We assume
an initially unknown latent-graph structure between graph-valued input data and
propose to learn end-to-end a parametric model for message passing within and
across input graph samples, along with the latent structure connecting the
input graphs. Further, we introduce a degree distribution loss that helps
regularize the predicted latent relationships structure. This regularization
can significantly improve the downstream task. Moreover, the obtained latent
graph can represent patient population models or networks of molecule clusters,
providing a level of interpretability and knowledge discovery in the input
domain of particular value in healthcare.",2022-11-13 16:04:43
XXX,journalArticle,2022,"Andrew Fuchs, Andrea Passarella, Marco Conti",A Cognitive Framework for Delegation Between Error-Prone AI and Human Agents,,,,,http://arxiv.org/abs/2204.02889v2,"With humans interacting with AI-based systems at an increasing rate, it is
necessary to ensure the artificial systems are acting in a manner which
reflects understanding of the human. In the case of humans and artificial AI
agents operating in the same environment, we note the significance of
comprehension and response to the actions or capabilities of a human from an
agent's perspective, as well as the possibility to delegate decisions either to
humans or to agents, depending on who is deemed more suitable at a certain
point in time. Such capabilities will ensure an improved responsiveness and
utility of the entire human-AI system. To that end, we investigate the use of
cognitively inspired models of behavior to predict the behavior of both human
and AI agents. The predicted behavior, and associated performance with respect
to a certain goal, is used to delegate control between humans and AI agents
through the use of an intermediary entity. As we demonstrate, this allows
overcoming potential shortcomings of either humans or agents in the pursuit of
a goal.",2022-11-13 16:04:43
XXX,journalArticle,2022,"Travis LaCroix, Alexandra Sasha Luccioni",Metaethical Perspectives on 'Benchmarking' AI Ethics,,,,,http://arxiv.org/abs/2204.05151v1,"Benchmarks are seen as the cornerstone for measuring technical progress in
Artificial Intelligence (AI) research and have been developed for a variety of
tasks ranging from question answering to facial recognition. An increasingly
prominent research area in AI is ethics, which currently has no set of
benchmarks nor commonly accepted way for measuring the 'ethicality' of an AI
system. In this paper, drawing upon research in moral philosophy and
metaethics, we argue that it is impossible to develop such a benchmark. As
such, alternative mechanisms are necessary for evaluating whether an AI system
is 'ethical'. This is especially pressing in light of the prevalence of
applied, industrial AI research. We argue that it makes more sense to talk
about 'values' (and 'value alignment') rather than 'ethics' when considering
the possible actions of present and future AI systems. We further highlight
that, because values are unambiguously relative, focusing on values forces us
to consider explicitly what the values are and whose values they are. Shifting
the emphasis from ethics to values therefore gives rise to several new ways of
understanding how researchers might advance research programmes for robustly
safe or beneficial AI. We conclude by highlighting a number of possible ways
forward for the field as a whole, and we advocate for different approaches
towards more value-aligned AI research.",2022-11-13 16:04:44
XXX,journalArticle,2022,"Samson Tan, Araz Taeihagh, Kathy Baxter",The Risks of Machine Learning Systems,,,,,http://arxiv.org/abs/2204.09852v1,"The speed and scale at which machine learning (ML) systems are deployed are
accelerating even as an increasing number of studies highlight their potential
for negative impact. There is a clear need for companies and regulators to
manage the risk from proposed ML systems before they harm people. To achieve
this, private and public sector actors first need to identify the risks posed
by a proposed ML system. A system's overall risk is influenced by its direct
and indirect effects. However, existing frameworks for ML risk/impact
assessment often address an abstract notion of risk or do not concretize this
dependence.
  We propose to address this gap with a context-sensitive framework for
identifying ML system risks comprising two components: a taxonomy of the first-
and second-order risks posed by ML systems, and their contributing factors.
First-order risks stem from aspects of the ML system, while second-order risks
stem from the consequences of first-order risks. These consequences are system
failures that result from design and development choices. We explore how
different risks may manifest in various types of ML systems, the factors that
affect each risk, and how first-order risks may lead to second-order effects
when the system interacts with the real world.
  Throughout the paper, we show how real events and prior research fit into our
Machine Learning System Risk framework (MLSR). MLSR operates on ML systems
rather than technologies or domains, recognizing that a system's design,
implementation, and use case all contribute to its risk. In doing so, it
unifies the risks that are commonly discussed in the ethical AI community
(e.g., ethical/human rights risks) with system-level risks (e.g., application,
design, control risks), paving the way for holistic risk assessments of ML
systems.",2022-11-13 16:04:44
XXX,journalArticle,2022,"Yi Wan, Ali Rahimi-Kalahroudi, Janarthanan Rajendran, Ida Momennejad, Sarath Chandar, Harm van Seijen",Towards Evaluating Adaptivity of Model-Based Reinforcement Learning Methods,,,,,http://arxiv.org/abs/2204.11464v2,"In recent years, a growing number of deep model-based reinforcement learning
(RL) methods have been introduced. The interest in deep model-based RL is not
surprising, given its many potential benefits, such as higher sample efficiency
and the potential for fast adaption to changes in the environment. However, we
demonstrate, using an improved version of the recently introduced Local Change
Adaptation (LoCA) setup, that well-known model-based methods such as PlaNet and
DreamerV2 perform poorly in their ability to adapt to local environmental
changes. Combined with prior work that made a similar observation about the
other popular model-based method, MuZero, a trend appears to emerge, suggesting
that current deep model-based methods have serious limitations. We dive deeper
into the causes of this poor performance, by identifying elements that hurt
adaptive behavior and linking these to underlying techniques frequently used in
deep model-based RL. We empirically validate these insights in the case of
linear function approximation by demonstrating that a modified version of
linear Dyna achieves effective adaptation to local changes. Furthermore, we
provide detailed insights into the challenges of building an adaptive nonlinear
model-based method, by experimenting with a nonlinear version of Dyna.",2022-11-13 16:04:45
XXX,journalArticle,2022,"Thulitha Senevirathna, Zujany Salazar, Vinh Hoa La, Samuel Marchal, Bartlomiej Siniarski, Madhusanka Liyanage, Shen Wang","A Survey on XAI for Beyond 5G Security: Technical Aspects, Use Cases, Challenges and Research Directions",,,,,http://arxiv.org/abs/2204.12822v1,"With the advent of 5G commercialization, the need for more reliable, faster,
and intelligent telecommunication systems are envisaged for the next generation
beyond 5G (B5G) radio access technologies. Artificial Intelligence (AI) and
Machine Learning (ML) are not just immensely popular in the service layer
applications but also have been proposed as essential enablers in many aspects
of B5G networks, from IoT devices and edge computing to cloud-based
infrastructures. However, most of the existing surveys in B5G security focus on
the performance of AI/ML models and their accuracy, but they often overlook the
accountability and trustworthiness of the models' decisions. Explainable AI
(XAI) methods are promising techniques that would allow system developers to
identify the internal workings of AI/ML black-box models. The goal of using XAI
in the security domain of B5G is to allow the decision-making processes of the
security of systems to be transparent and comprehensible to stakeholders making
the systems accountable for automated actions. In every facet of the
forthcoming B5G era, including B5G technologies such as RAN, zero-touch network
management, E2E slicing, this survey emphasizes the role of XAI in them and the
use cases that the general users would ultimately enjoy. Furthermore, we
presented the lessons learned from recent efforts and future research
directions on top of the currently conducted projects involving XAI.",2022-11-13 16:04:45
XXX,journalArticle,2022,"Tingting Zheng, Weixing chen, Shuqin Li, Hao Quan, Qun Bai, Tianhang Nan, Song Zheng, Xinghua Gao, Yue Zhao, Xiaoyu Cui",A Deep Reinforcement Learning Framework for Rapid Diagnosis of Whole Slide Pathological Images,,,,,http://arxiv.org/abs/2205.02850v1,"The deep neural network is a research hotspot for histopathological image
analysis, which can improve the efficiency and accuracy of diagnosis for
pathologists or be used for disease screening. The whole slide pathological
image can reach one gigapixel and contains abundant tissue feature information,
which needs to be divided into a lot of patches in the training and inference
stages. This will lead to a long convergence time and large memory consumption.
Furthermore, well-annotated data sets are also in short supply in the field of
digital pathology. Inspired by the pathologist's clinical diagnosis process, we
propose a weakly supervised deep reinforcement learning framework, which can
greatly reduce the time required for network inference. We use neural network
to construct the search model and decision model of reinforcement learning
agent respectively. The search model predicts the next action through the image
features of different magnifications in the current field of view, and the
decision model is used to return the predicted probability of the current field
of view image. In addition, an expert-guided model is constructed by
multi-instance learning, which not only provides rewards for search model, but
also guides decision model learning by the knowledge distillation method.
Experimental results show that our proposed method can achieve fast inference
and accurate prediction of whole slide images without any pixel-level
annotations.",2022-11-13 16:04:46
XXX,journalArticle,2022,"Maurice Jakesch, Zana Buçinca, Saleema Amershi, Alexandra Olteanu",How Different Groups Prioritize Ethical Values for Responsible AI,"2022 ACM Conference on Fairness, Accountability, and Transparency
  (FAccT '22), June 21-24, 2022, Seoul, Republic of Korea",,,10.1145/3531146.3533097,http://arxiv.org/abs/2205.07722v1,"Private companies, public sector organizations, and academic groups have
outlined ethical values they consider important for responsible artificial
intelligence technologies. While their recommendations converge on a set of
central values, little is known about the values a more representative public
would find important for the AI technologies they interact with and might be
affected by. We conducted a survey examining how individuals perceive and
prioritize responsible AI values across three groups: a representative sample
of the US population (N=743), a sample of crowdworkers (N=755), and a sample of
AI practitioners (N=175). Our results empirically confirm a common concern: AI
practitioners' value priorities differ from those of the general public.
Compared to the US-representative sample, AI practitioners appear to consider
responsible AI values as less important and emphasize a different set of
values. In contrast, self-identified women and black respondents found
responsible AI values more important than other groups. Surprisingly, more
liberal-leaning participants, rather than participants reporting experiences
with discrimination, were more likely to prioritize fairness than other groups.
Our findings highlight the importance of paying attention to who gets to define
responsible AI.",2022-11-13 16:04:46
XXX,journalArticle,2022,"Javier Del Ser, Alejandro Barredo-Arrieta, Natalia Díaz-Rodríguez, Francisco Herrera, Andreas Holzinger","Exploring the Trade-off between Plausibility, Change Intensity and Adversarial Power in Counterfactual Explanations using Multi-objective Optimization",,,,,http://arxiv.org/abs/2205.10232v1,"There is a broad consensus on the importance of deep learning models in tasks
involving complex data. Often, an adequate understanding of these models is
required when focusing on the transparency of decisions in human-critical
applications. Besides other explainability techniques, trustworthiness can be
achieved by using counterfactuals, like the way a human becomes familiar with
an unknown process: by understanding the hypothetical circumstances under which
the output changes. In this work we argue that automated counterfactual
generation should regard several aspects of the produced adversarial instances,
not only their adversarial capability. To this end, we present a novel
framework for the generation of counterfactual examples which formulates its
goal as a multi-objective optimization problem balancing three different
objectives: 1) plausibility, i.e., the likeliness of the counterfactual of
being possible as per the distribution of the input data; 2) intensity of the
changes to the original input; and 3) adversarial power, namely, the
variability of the model's output induced by the counterfactual. The framework
departs from a target model to be audited and uses a Generative Adversarial
Network to model the distribution of input data, together with a
multi-objective solver for the discovery of counterfactuals balancing among
these objectives. The utility of the framework is showcased over six
classification tasks comprising image and three-dimensional data. The
experiments verify that the framework unveils counterfactuals that comply with
intuition, increasing the trustworthiness of the user, and leading to further
insights, such as the detection of bias and data misrepresentation.",2022-11-13 16:04:47
