Key,Item Type,Publication Year,Author,Title,Publication Title,ISBN,ISSN,DOI,Url,Abstract Note,Date,Date Added,Date Modified,Access Date,Pages,Num Pages,Issue,Volume,Number Of Volumes,Journal Abbreviation,Short Title,Series,Series Number,Series Text,Series Title,Publisher,Place,Language,Rights,Type,Archive,Archive Location,Library Catalog,Call Number,Extra,Notes,File Attachments,Link Attachments,Manual Tags,Automatic Tags,Editor,Series Editor,Translator,Contributor,Attorney Agent,Book Author,Cast Member,Commenter,Composer,Cosponsor,Counsel,Interviewer,Producer,Recipient,Reviewed Author,Scriptwriter,Words By,Guest,Number,Edition,Running Time,Scale,Medium,Artwork Size,Filing Date,Application Number,Assignee,Issuing Authority,Country,Meeting Name,Conference Name,Court,References,Reporter,Legal Status,Priority Numbers,Programming Language,Version,System,Code,Code Number,Section,Session,Committee,History,Legislative Body,Rohin_Shah_Blurb
XBZAPQFK,blogPost,2020,"Kokotajlo, Daniel",Three kinds of competitiveness,AI Impacts,,,,https://aiimpacts.org/three-kinds-of-competitiveness/,"By Daniel Kokotajlo In this post, I distinguish between three different kinds of competitiveness -- Performance, Cost, and Date -- and explain why I think these distinctions are worth the brainspace they occupy. For example, they help me introduce and discuss a problem for AI safety proposals having to do with aligned AIs being outcompeted...",2020-03-30,2022-01-30 01:53:10,2022-01-30 01:53:10,2021-11-20 18:55:39,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Blog,,/Users/jacquesthibodeau/Zotero/storage/PU9A2KS8/three-kinds-of-competitiveness.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BQCZM53S,blogPost,2021,"Clarke, Sam; Martin, Samuel Dylan",Distinguishing AI takeover scenarios,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/qYzqDtoQaZ3eDDyxa/distinguishing-ai-takeover-scenarios,"Epistemic status: lots of this involves interpreting/categorising other people’s scenarios, and could be wrong. We’d really appreciate being corrected if so. [ETA: so far, no corrections.] TLDR: see the summary table. In the last few years, people have proposed various AI takeover scenarios. We think this type of scenario building is great, since there are now more concrete ideas of what AI takeover could realistically look like. That said, we have been confused for a while about how the different scenarios relate to each other and what different assumptions they make. This post might be helpful for anyone who has similar confusions. We focus on explaining the differences between seven prominent scenarios: the  ‘Brain-in-a-box’ scenario, ‘What failure looks like’ part 1 (WFLL 1), ‘What failure looks like’ part 2 (WFLL 2), ‘Another (outer) alignment failure story’  (AAFS), ‘Production Web’, ‘Flash economy’ and ‘Soft takeoff leading to decisive strategic advantage’. While these scenarios do not capture alI of the risks from transformative AI, participants in a recent survey aimed at leading AI safety/governance researchers estimated the first three of these scenarios to cover 50% of existential catastrophes from AI.[1] We plan to follow up with a subsequent post, which discusses some of the issues raised here in greater depth. VARIABLES RELATING TO AI TAKEOVER SCENARIOS We define AI takeover to be a scenario where the most consequential decisions about the future get made by AI systems with goals that aren’t desirable by human standards. There are three variables which are sufficient to distinguish the takeover scenarios discussed in this post. We will briefly introduce these three variables, and a number of others that are generally useful for thinking about takeover scenarios. Key variables for distinguishing the AI takeover scenarios in this post:  * Speed. Is there a sudden jump in AI capabilities over a very short period    (i.e. much faster than what we",2021-09-08,2022-01-30 04:47:42,2022-01-30 04:47:42,2021-11-18 23:45:23,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/ENAMQXCU/distinguishing-ai-takeover-scenarios.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
D95GDF2T,blogPost,2020,"Wentworth, John",Toy Problem: Detective Story Alignment,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/4kYkYSKSALH4JaQ99/toy-problem-detective-story-alignment,"Suppose I train some simple unsupervised topic model (e.g. LDA) on a bunch of books. I look through the topics it learns, and find one corresponding to detective stories. The problem: I would like to use the identified detective-story cluster to generate detective stories from GPT. The hard part: I would like to do this in such a way that the precision of the notion of detective-stories used by the final system is not limited by the original simple model. Here’s what that means, visually. The space of real-world books has some clusters in it: One of those clusters is the detective-story cluster. The simple model approximates those clusters using something simple - for the sake of visualization, ellipses: The more complex model (e.g. GPT) presumably has a much more precise approximation of the shape of the clusters: So, we’d like to use the simple model to identify one of the clusters, but then still use the full power of the complex model to sample from that cluster. Of course, GPT may not contain a single variable corresponding to a cluster-id, which is largely what makes the problem interesting. GPT may not internally use a notion of “cluster” at all. However, the GPT model should still contain something (approximately) isomorphic to the original cluster, since that real pattern is still in the data/environment: since there is a real cluster of ""detective stories"" in the data/environment itself, the GPT model should also contain that cluster, to the extent that the GPT model matches the data/environment. In particular, the “precision not limited by original model” requirement rules out the obvious strategy of generating random samples from GPT and selecting those which the simple model labels as detective-stories. If we do that, then we’ll end up with some non-detective-stories in the output, because of shortcomings in the simple model’s notion of detective-stories. Visually, we’d be filtering based on the ellipse approximation of the cluster, which is exac",2020-10-13,2022-01-30 04:48:47,2022-01-30 04:48:47,2021-11-08 23:30:38,,,,,,,Toy Problem,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/MWR5EK7V/toy-problem-detective-story-alignment.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IF3G8A6B,blogPost,2020,"Riedel, Jess; Deibel, Angelica",TAI Safety Bibliographic Database,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/4DegbDJJiMX2b3EKm/tai-safety-bibliographic-database,"Authors: Jess Riedel and Angelica Deibel Cross-posted to EA Forum In this post we present the first public version of our bibliographic database of research on the safety of transformative artificial intelligence (TAI). The primary motivations for assembling this database were to:  1. Aid potential donors in assessing organizations focusing on TAI safety by     collecting and analyzing their research output.  2. Assemble a comprehensive bibliographic database that can be used as a base     for future projects, such as a living review of the field. The database contains research works motivated by, and substantively informing, the challenge of ensuring the safety of TAI, including both technical and meta topics. This initial version of the database has attempted comprehensive coverage only for traditionally formatted research produced in 2016-2020 by organizations with a significant safety focus (~360 items). The database also has significant but non-comprehensive coverage (~570 items) of earlier years, less traditional formats (e.g., blog posts), and non-safety-focused organizations. Usefully, we also have citation counts for essentially all the items for which that is applicable. The core database takes the form of a Zotero library. Snapshots are also available as Google Sheet, CSV, and Zotero RDF. (Compact version for easier human reading: Google Sheet, CSV.) The rest of this post describes the composition of the database in more detail and presents some high-level quantitative analysis of the contents. In particular, our analysis includes:  * Lists of the most cited TAI safety research for each of the past few years    (Tables 2 and 3)  * A chart showing how written TAI safety research output has changed since 2016    (Figure 1).  * A visualization of the degree of collaboration on TAI safety between    different research organizations (Table 4).  * A chart showing how the format of written research varied between    organizations, e.g., manuscripts vs. jou",2020-12-22,2022-01-30 04:48:47,2022-01-30 04:48:47,2021-11-13 14:31:43,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AICDKECS,blogPost,2020,"Muehlhauser, Luke",Our AI governance grantmaking so far,Open Philanthropy,,,,https://www.openphilanthropy.org/blog/ai-governance-grantmaking,"When the Soviet Union began to fracture in 1991, the world was forced to reckon with the first collapse of a nuclear superpower in history.My thanks to Nathan Calvin for his help researching and drafting these opening paragraphs about the Nunn-Lugar Act. The USSR was home to more than 27,000",2020-12-16,2022-01-30 04:48:47,2022-01-30 04:48:47,2021-11-13 14:33:13,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
K3XMEA86,blogPost,2020,"Xu, Mark",The Solomonoff Prior is Malign,,,,,https://www.alignmentforum.org/posts/Tr7tAyt5zZpdTwTQK/the-solomonoff-prior-is-malign,"This argument came to my attention from this post by Paul Christiano. I also found this clarification helpful. I found these counter-arguments stimulating and have included some discussion of them. Very little of this content is original. My contributions consist of fleshing out arguments and constructing examples. Thank you to Beth Barnes and Thomas Kwa for helpful discussion and comments. WHAT IS THE SOLOMONOFF PRIOR? The Solomonoff prior is intended to answer the question ""what is the probability of X?"" for any X, where X is a finite string over some finite alphabet. The Solomonoff prior is defined by taking the set of all Turing machines (TMs) which output strings when run with no input and weighting them proportional to2−K, whereKis the description length of the TM (informally its size in bits). The Solomonoff prior says the probability of a string is the sum over all the weights of all TMs that print that string. One reason to care about the Solomonoff prior is that we can use it to do a form of idealized induction. If you have seen 0101 and want to predict the next bit, you can use the Solomonoff prior to get the probability of 01010 and 01011. Normalizing gives you the chances of seeing 1 versus 0, conditioned on seeing 0101. In general, any process that assigns probabilities to all strings in a consistent way can be used to do induction in this way. This post provides more information about Solomonoff Induction. WHY IS IT MALIGN? Imagine that you wrote a programming language called python^10 that works as follows: First, it takes all alpha-numeric chars that are not in literals and checks if they're repeated 10 times sequentially. If they're not, they get deleted. If they are, they get replaced by a single copy. Second, it runs this new program through a python interpreter. Hello world in python^10: ppppppppprrrrrrrrrriiiiiiiiiinnnnnnnnnntttttttttt('Hello, world!') Luckily, python has an exec function that executes literals as code. This lets us w",2020-10-13,2022-01-30 04:48:47,2022-01-30 04:48:47,2021-11-08 23:25:30,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/JSBHRC49/the-solomonoff-prior-is-malign.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZHAC26JP,blogPost,2020,"Branwen, Gwern",The Scaling Hypothesis,,,,,https://www.gwern.net/Scaling-hypothesis,"On GPT-3: meta-learning, scaling, implications, and deep theory. The scaling hypothesis: neural nets absorb data & compute, generalizing and becoming more Bayesian as problems get harder, manifesting new abilities even at trivial-by-global-standards-scale. The deep learning revolution has begun as foretold.",2020-05-28,2022-01-30 04:48:47,2022-01-30 04:48:47,2021-11-14 18:58:38,,,,,,,,,,,,,,en-us,https://creativecommons.org/publicdomain/zero/1.0/,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/6N626N6Q/Scaling-hypothesis.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5Q2FQC3T,blogPost,2020,"Shimi, Adam","The ""Backchaining to Local Search"" Technique in AI Alignment",AI Alignment Forum,,,,https://www.alignmentforum.org/posts/qEjh8rpxjG4qGtfuK/the-backchaining-to-local-search-technique-in-ai-alignment,"In the spirit of this post by John S. Wentworth, this is a reference for a technique I learned from Evan Hubinger. He's probably not the first to use it, but he introduced it to me, so he gets the credit. In a single sentence, backchaining to local search is the idea of looking at how a problem of alignment could appear through local search (think gradient descent). So it starts with a certain problem (say reward tampering), and then tries to create a context where the usual training process in ML (local search) could create a system suffering from this problem. It’s an instance of backchaining in general, which just looks for how a problem could appear in practice. Backchaining to local search has two main benefits:  * It helps decide whether this specific problem is something we should worry    about.  * It forces you to consider your problem from a local search perspective,    instead of the more intuitive human/adversarial perspective (how would I mess    this up?). Let's look at a concrete example: reward gaming (also called specification gaming). To be even more concrete, we have a system with a camera and other sensors, and its goal is to maximize the amount of time when my friend Tom smiles, as measured through a loss function that captures whether the camera sees Tom smiling. The obvious (for us) way to do reward gaming here is to put a picture of Tom’s smiling face in front of the camera -- then the loss function is minimized. The backchaining to local search technique applied to this example asks ""How can I get this reward gaming behavior by local search?"" Well this reward gaming strategy is probably a local minima for the loss function (as changing just a little the behavior would increase the loss significantly), so local search could find it and stay in there. It's also better than most simple strategies, as ensuring that someone smiles (not necessarily a good goal, mind you) requires rather complex actions in the world (like going full ""Joker"" on",2020-09-18,2022-01-30 04:48:47,2022-01-30 04:48:47,2021-11-07 22:33:47,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/74AE9R4X/the-backchaining-to-local-search-technique-in-ai-alignment.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AQ9D7WK8,blogPost,2020,"Ngo, Richard",Shaping safer goals,AI Alignment Forum,,,,https://www.alignmentforum.org/s/boLPsyNwd6teK5key,A community blog devoted to technical AI alignment research,2020-07-01,2022-01-30 04:48:47,2022-01-30 04:48:47,2021-11-07 22:49:04,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/VCMH6DTH/boLPsyNwd6teK5key.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
79Z4M7BF,blogPost,2020,"Turner, Alex",Non-Obstruction: A Simple Concept Motivating Corrigibility,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/Xts5wm3akbemk4pDa/non-obstruction-a-simple-concept-motivating-corrigibility,"Thanks to Mathias Bonde, Tiffany Cai, Ryan Carey, Michael Cohen, Joe Collman, Andrew Critch, Abram Demski, Michael Dennis, Thomas Gilbert, Matthew Graves, Koen Holtman, Evan Hubinger, Victoria Krakovna, Amanda Ngo, Rohin Shah, Adam Shimi, Logan Smith, and Mark Xu for their thoughts. Main claim: corrigibility’s benefits can be mathematically represented as a counterfactual form of alignment. Overview: I’m going to talk about a unified mathematical frame I have for understanding corrigibility’s benefits, what it “is”, and what it isn’t. This frame is precisely understood by graphing the human overseer’s ability to achieve various goals (their attainable utility (AU) landscape). I argue that corrigibility’s benefits are secretly a form of counterfactual alignment (alignment with a set of goals the human may want to pursue). A counterfactually aligned agent doesn't have to let us literally correct it. Rather, this frame theoretically motivates why we might want corrigibility anyways. This frame also motivates other AI alignment subproblems, such as intent alignment, mild optimization, and low impact. NOMENCLATURE Corrigibility goes by a lot of concepts: “not incentivized to stop us from shutting it off”, “wants to account for its own flaws”, “doesn’t take away much power from us”, etc. Named by Robert Miles, the word ‘corrigibility’ means “able to be corrected [by humans]."" I’m going to argue that these are correlates of a key thing we plausibly actually want from the agent design, which seems conceptually simple. In this post, I take the following common-language definitions:  * Corrigibility: the AI literally lets us correct it (modify its policy), and    it doesn't manipulate us either. * Without both of these conditions, the AI's       behavior isn't sufficiently constrained for the concept to be useful.       Being able to correct it is small comfort if it manipulates us into making       the modifications it wants. An AI which is only non-manipulative doesn'",2020,2022-01-30 04:48:46,2022-01-30 04:48:46,2021-11-13 15:28:56,,,,,,,Non-Obstruction,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/8IPMHT9D/non-obstruction-a-simple-concept-motivating-corrigibility.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SN2KMUEK,blogPost,2020,"Aird, Michael; Shovelain, Justin; algekalipso",Memetic downside risks: How ideas can evolve and cause harm,LessWrong,,,,https://www.lesswrong.com/posts/EdAHNdbkGR6ndAPJD/memetic-downside-risks-how-ideas-can-evolve-and-cause-harm,"This post was written for Convergence Analysis. OVERVIEW We introduce the concept of memetic downside risks (MDR): risks of unintended negative effects that arise from how ideas “evolve” over time (as a result of  replication, mutation, and selection). We discuss how this concept relates to the existing concepts of memetics, downside risks, and information hazards. We then outline four “directions” in which ideas may evolve: towards simplicity, salience, usefulness, and perceived usefulness. For each “direction”, we give an example to illustrate how an idea mutating in that direction could have negative effects. We then discuss some implications of these ideas for people and organisations trying to improve the world, who wish to achieve their altruistic objectives and minimise the unintended harms they cause. For example, we argue that the possibility of memetic downside risks increases the value of caution about what and how to communicate, and of “high-fidelity” methods of communication. BACKGROUND AND CONCEPT MEMETICS Wikipedia describes a meme as: an idea, behavior, or style that spreads by means of imitation from person to person within a culture—often with the aim of conveying a particular phenomenon, theme, or meaning represented by the meme. A meme acts as a unit for carrying cultural ideas, symbols, or practices, that can be transmitted from one mind to another through writing, speech, gestures, rituals, or other imitable phenomena with a mimicked theme. The same article goes on to say: Proponents [of the concept of memes] theorize that memes are a viral phenomenon that may evolve by natural selection in a manner analogous to that of biological evolution. Memes do this through the processes of variation, mutation, competition, and inheritance, each of which influences a meme's reproductive success. Memes spread through the behavior that they generate in their hosts. Memes that propagate less prolifically may become extinct, while others may survive,",2020-02-25,2022-01-30 04:48:46,2022-01-30 04:48:46,2021-11-20 19:01:44,,,,,,,Memetic downside risks,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/X65UHRG3/memetic-downside-risks-how-ideas-can-evolve-and-cause-harm.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
98XXRJPB,blogPost,2020,"Koch, Jack",Mapping the Conceptual Territory in AI Existential Safety and Alignment,Jack Koch,,,,https://jbkjr.github.io/posts/2020/12/mapping_conceptual_territory_AI_safety_alignment/,"Throughout my studies in alignment and AI-related existential risks, I’ve found it helpful to build a mental map of the field and how its various questions and considerations interrelate, so that when I read a new paper, a post on the Alignment Forum, or similar material, I have some idea of how it might contribute to the overall goal of making our deployment of AI technology go as well as possible for humanity. I’m writing this post to communicate what I’ve learned through this process, in order to help others trying to build their own mental maps and provide them with links to relevant resources for further, more detailed information. This post was largely inspired by (and would not be possible without) two talks by Paul Christiano and Rohin Shah, respectively, that give very similar overviews of the field,1 as well as a few posts on the Alignment Forum that will be discussed below. This post is not intended to replace these talks but is instead an attempt to coherently integrate their ideas with ideas from other sources attempting to clarify various aspects of the field. You should nonetheless watch these presentations and read some of the resources provided below if you’re trying to build your mental map as completely as possible. Rohin also did a two part podcast with the Future of Life Institute discussing the contents of his presentation in more depth, both of which are worth listening to. ↩",2020-12-17,2022-01-30 04:48:46,2022-01-30 04:48:46,2021-11-13 15:45:40,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/78JC5UMH/mapping_conceptual_territory_AI_safety_alignment.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ICRWTVK7,blogPost,2020,"Aird, Michael",Failures in technology forecasting? A reply to Ord and Yudkowsky,LessWrong,,,,https://www.lesswrong.com/posts/3qypPmmNHEmqegoFF/failures-in-technology-forecasting-a-reply-to-ord-and,"In The Precipice, Toby Ord writes: we need to remember how quickly new technologies can be upon us, and to be wary of assertions that they are either impossible or so distant in time that we have no cause for concern. Confident denouncements by eminent scientists should certainly give us reason to be sceptical of a technology, but not to bet our lives against it - their track record just isn’t good enough for that. I strongly agree with those claims, think they’re very important in relation to  estimating existential risk,[1] and appreciate the nuanced way in which they’re stated. (There’s also a lot more nuance around this passage which I haven’t quoted.) I also largely agree with similar claims made in Eliezer Yudkowsky’s earlier essay There's No Fire Alarm for Artificial General Intelligence. But both Ord and Yudkowsky provide the same set of three specific historical cases as evidence of the poor track record of such “confident denouncements”. And I think those cases provide less clear evidence than those authors seem to suggest. So in this post, I’ll:  * Quote Ord and/or Yudkowsky’s descriptions of those three cases, as well as    one case mentioned by Yudkowsky but not Ord  * Highlight ways in which those cases may be murkier than Ord and Yudkowsky    suggest  * Discuss how much we could conclude about technology forecasting in general     from such a small and likely unrepresentative sample of cases, even if those    cases weren’t murky I should note that I don’t think that these historical cases are necessary to support claims like those Ord and Yudkowsky make. And I suspect there might be better evidence for those claims out there. But those cases were the main evidence Ord provided, and among the main evidence Yudkowsky provided. So those cases are being used as key planks supporting beliefs that are important to many EAs and longtermists. Thus, it seems healthy to prod at each suspicious plank on its own terms, and update incrementally. CASE: RUTHER",2020-05-08,2022-01-30 04:48:45,2022-01-30 04:48:45,2021-11-20 19:08:07,,,,,,,Failures in technology forecasting?,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/VI686GFK/failures-in-technology-forecasting-a-reply-to-ord-and.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JRKS8FN9,blogPost,2020,"Aird, Michael",Existential risks are not just about humanity,Effective Altruism Forum,,,,https://forum.effectivealtruism.org/posts/EfCCgpvQX359xuZ4g/existential-risks-are-not-just-about-humanity,"This post was written for Convergence Analysis. This post highlights and analyses existing ideas more than proposing new ones. In The Precipice, Toby Ord writes: An existential catastrophe is the destruction of humanity’s longterm potential. An existential risk is a risk that threatens the destruction of humanity’s longterm potential. I’ve previously discussed some distinctions and nuances relevant to these concepts. This post will focus on:  * The idea that these concepts are really about the destruction of the     potential of humanity or its “descendants”; they're not necessarily solely    about human wellbeing, nor just Homo sapiens’ potential.  * The implications of that, including for how “bad” an existential catastrophe    might be THE POTENTIAL OF HUMANITY AND ITS “DESCENDANTS” When explaining his definitions, Ord writes: my focus on humanity in the definitions is not supposed to exclude considerations of the value of the environment, other animals, successors to  Homo sapiens, or creatures elsewhere in the cosmos. It is not that I think only humans count. Instead, it is that humans are the only beings we know of that are responsive to moral reasons and moral argument - the beings who can examine the world and decide to do what is best. If we fail, that upwards force, that capacity to push towards what is best or what is just, will vanish from the world. Our potential is a matter of what humanity can achieve through the combined actions of each and every human. The value of our actions will stem in part from what we do to and for humans, but it will depend on the effects of our actions on non-humans too. If we somehow give rise to new kinds of moral agents in the future, the term ‘humanity’ in my definition should be taken to include them. This makes two points clear:  1. An existential catastrophe is not solely about the destruction of the     potential for human welfare, flourishing, achievement, etc. Instead, it’s     about humanity’s potential",2020-04-27,2022-01-30 04:48:45,2022-01-30 04:48:45,2021-11-20 19:00:05,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/T2T8GF5K/are-existential-risks-just-about-humanity.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CBZPEDS4,blogPost,2020,"Harth, Rafael",Factored Cognition,LessWrong,,,,https://www.lesswrong.com/s/xezt7HYfpWR6nwp7Z,A community blog devoted to refining the art of rationality,2020-08-30,2022-01-30 04:48:45,2022-01-30 04:48:45,2021-11-13 21:59:30,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/SJKBM59W/xezt7HYfpWR6nwp7Z.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8RNGX2C6,blogPost,2020,"Finnveden, Lukas",Extrapolating GPT-N performance,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/k2SNji3jXaLGhBeYP/extrapolating-gpt-n-performance,"Brown et al. (2020) (which describes the development of GPT-3) contains measurements of how 8 transformers of different sizes perform on several different benchmarks. In this post, I project how performance could improve for larger models, and give an overview of issues that may appear when scaling-up. Note that these benchmarks are for ‘downstream tasks’ that are different from the training task (which is to predict the next token); these extrapolations thus cannot be directly read off the scaling laws in OpenAI’s Scaling Laws for Neural Language Models (Kaplan et al., 2020) or Scaling Laws for Autoregressive Generative Modelling (Henighan et al., 2020). (If you don’t care about methodology or explanations, the final graphs are in  Comparisons and limits .) METHODOLOGY Brown et al. reports benchmark performance for 8 different model sizes. However, these models were not trained in a compute-optimal fashion. Instead, all models were trained on 300B tokens (one word is roughly 1.4 tokens), which is inefficiently much data. Since we’re interested in the best performance we can get for a given amount of compute, and these models weren’t compute-optimally trained, we cannot extrapolate these results on the basis of model-size. Instead, I fit a trend for how benchmark performance (measured in % accuracy) depends on the cross-entropy loss that the models get when predicting the next token on the validation set. I then use the scaling laws from Scaling Laws for Neural Language Models to extrapolate this loss. This is explained in the Appendix. PLOTTING AGAINST LOSS In order to get a sense of how GPT-3 performs on different types of tasks, I separately report few-shot progress on each of the 11 different categories discussed in Brown et al. For a fair comparison, I normalize the accuracy of each category between random performance and maximum performance; i.e., for each data point, I subtract the performance that a model would get if it responded randomly (or only respo",2020-12-18,2022-01-30 04:48:45,2022-01-30 04:48:45,2021-11-13 22:24:33,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/2EXQWDPT/extrapolating-gpt-n-performance.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
F4RGR6AR,blogPost,2020,"Harris, Edouard",Defining capability and alignment in gradient descent,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/Xg2YycEfCnLYrCcjy/defining-capability-and-alignment-in-gradient-descent,"This is the first post in a series where I'll explore AI alignment in a simplified setting: a neural network that's being trained by gradient descent. I'm choosing this setting because it involves a well-defined optimization process that has enough complexity to be interesting, but that's still understandable enough to make crisp mathematical statements about. As a result, it serves as a good starting point for rigorous thinking about alignment. DEFINING INNER ALIGNMENT First, I want to highlight a definitional issue. Right now there are two definitions of inner alignment circulating in the community. This issue was first pointed out to me by Evan Hubinger in a recent conversation. The first definition is the one from last year's Risks from Learned Optimization paper, which Evan co-authored and which introduced the term. This paper defined the inner alignment problem as ""the problem of eliminating the base-mesa objective gap"" (Section 1.2). The implication is that if we can eliminate the gap between the base objective of a base optimizer, and the mesa-objectives of any mesa-optimizers that base optimizer may give rise to, then we will have satisfied the necessary and sufficient conditions for the base optimizer to be inner-aligned. There's also a second definition that seems to be more commonly used. This definition says that ""inner alignment fails when your capabilities generalize but your objective does not"". This comes from an intuition (pointed out to me by  Rohin Shah) that the combination of inner alignment and outer alignment should be accident-proof with respect to an optimizer's intent: an optimizer that's both inner- and outer-aligned should be trying to do what we want. Since an outer-aligned optimizer is one whose base objective is something we want, this intuition suggests that the remaining part of the intent alignment problem — the problem of getting the optimizer to try to achieve the base objective we set — is what inner alignment refers to. Her",2020-11-05,2022-01-30 04:48:45,2022-01-30 04:48:45,2021-11-13 22:00:32,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/NKD85BQM/defining-capability-and-alignment-in-gradient-descent.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
63VS4I8K,blogPost,2020,"DeepSpeed Team; Majumder, Rangan",DeepSpeed: Extreme-scale model training for everyone,Microsoft Research,,,,https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/,"DeepSpeed continues to innovate, making its tools more powerful while broadening its reach. Learn how it now powers 10x bigger model training on one GPU, 10x longer input sequences, 5x less communication volume, & scales to train trillion-parameter models.",2020-09-10,2022-01-30 04:48:44,2022-01-30 04:48:44,2021-11-13 13:58:29,,,,,,,DeepSpeed,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AGIS6ZTT,blogPost,2020,"Wentworth, John",Confucianism in AI Alignment,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/3aDeaJzxinoGNWNpC/confucianism-in-ai-alignment,"I hear there’s a thing where people write a lot in November, so I’m going to try writing a blog post every day. Disclaimer: this post is less polished than my median. And my median post isn’t very polished to begin with. Imagine a large corporation - we’ll call it BigCo. BigCo knows that quality management is high-value, so they have a special program to choose new managers. They run the candidates through a program involving lots of management exercises, simulations, and tests, and select those who perform best. Of course, the exercises and simulations and tests are not a perfect proxy for the would-be managers’ real skills and habits. The rules can be gamed. Within a few years of starting the program, BigCo notices a drastic disconnect between performance in the program and performance in practice. The candidates who perform best in the program are those who game the rules, not those who manage well, so of course many candidates devote all their effort to gaming the rules. How should this problem be solved? Ancient Chinese scholars had a few competing schools of thought on this question, most notably the Confucianists and the Legalists. The (stylized) Confucianists’ answer was: the candidates should be virtuous and not abuse the rules. BigCo should demonstrate virtue and benevolence in general, and in return their workers should show loyalty and obedience. I’m not an expert, but as far as I can tell this is not a straw man - though stylized and adapted to a modern context, it accurately captures the spirit of Confucian thought. The (stylized) Legalists instead took the position obvious to any student of modern economics: this is an incentive design problem, and BigCo leadership should design less abusable incentives. If you have decent intuition for economics, it probably seems like the Legalist position is basically right and the Confucian position is Just Wrong. I don't want to discourage this intuition, but I expect that many people who have this intuitio",2020-11-02,2022-01-30 04:48:44,2022-01-30 04:48:44,2021-11-13 13:49:51,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/UPMVNDZV/confucianism-in-ai-alignment.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8CU8JKMG,blogPost,2020,"Ngo, Amanda; Pace, Ben",AGI Predictions,LessWrong,,,,https://www.lesswrong.com/posts/YMokuZdoY9tEDHjzv/agi-predictions,"This post is a collection of key questions that feed into AI timelines and AI safety work where it seems like there is substantial interest or disagreement amongst the LessWrong community. You can make a prediction on a question by hovering over the widget and clicking. You can update your prediction by clicking at a new point, and remove your prediction by clicking on the same point. Try it out: Elicit Prediction (elicit.org/binary/questions/FIVfnQ_kJ) ADD QUESTIONS & OPERATIONALIZATIONS This is not intended to be a comprehensive list, so I’d love for people to add their own questions – here are instructions on making your own embedded question. If you have better operationalizations of the questions, you can make your own version in the comments. If there's general agreement on an alternative operationalization being better, I'll add it into the post. QUESTIONS AGI DEFINITION We’ll define AGI in this post as a unified system that, for almost all economically relevant cognitive tasks, at least matches any human's ability at the task. This is similar to Rohin Shah and Ben Cottier’s definition in this post. SAFETY QUESTIONS Elicit Prediction (elicit.org/binary/questions/_Sw39Z-kh)Elicit Prediction ( elicit.org/binary/questions/HqT9XSwfs)Elicit Prediction ( elicit.org/binary/questions/sTO9o3bLg)Elicit Prediction ( elicit.org/binary/questions/kua2HCDhi)Elicit Prediction ( elicit.org/binary/questions/KqSEIKayU)Elicit Prediction ( elicit.org/binary/questions/yoiBUdpgO)Elicit Prediction ( elicit.org/binary/questions/RcOt6wSs7)Elicit Prediction ( elicit.org/binary/questions/ZjN5qqVRz) TIMELINES QUESTIONS See Forecasting AI timelines, Ajeya Cotra’s OP AI timelines report, and Adam Gleave’s #AN80 comment, for more context on this breakdown. I haven’t tried to operationalize this too much, so feel free to be more specific in the comments. The first three questions in this section are mutually exclusive — that is, the probabilities you assign to them should not sum to m",2020-11-20,2022-01-30 04:48:43,2022-01-30 04:48:43,2021-11-13 14:15:06,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GF7HGP5R,blogPost,2020,"Tan, Xuan","AI Alignment, Philosophical Pluralism, and the Relevance of Non-Western Philosophy",AI Alignment Forum,,,,https://www.alignmentforum.org/posts/jS2iiDPqMvZ2tnik2/ai-alignment-philosophical-pluralism-and-the-relevance-of,"This is an extended transcript of the talk I gave at EAGxAsiaPacific 2020. In the talk, I present a somewhat critical take on how AI alignment has grown as a field, and how, from my perspective, it deserves considerably more philosophical and disciplinary diversity than it has enjoyed so far. I'm sharing it here in the hopes of generating discussion about the disciplinary and philosophical paradigms that (I understand) the AI alignment community to be rooted in, and whether or how we should move beyond them. Some sections cover introductory material that most people here are likely to be familiar with, so feel free to skip them. THE TALK Hey everyone, my name is Xuan (IPA: ɕɥɛn), and I’m doctoral student at MIT doing cognitive AI research. Specifically I work on how we can infer the hidden structure of human motivations by modeling humans using probabilistic programs. Today though I’ll be talking about something that’s more in the background that informs my work, and that’s about AI alignment, philosophical pluralism, and the relevance of non-Western philosophy. This talk will cover a lot of ground, so I want to give an overview to keep everyone oriented:  1. First, I’ll give a brief introduction to what AI alignment is, and why it     likely matters as an effective cause area.  2. I’ll then highlight some of the philosophical tendencies of current AI     alignment research, and argue that they reflect a relatively narrow set of     philosophical views.  3. Given that these philosophical views may miss crucial considerations, this     situation motivates the need for greater philosophical and disciplinary     pluralism.  4. And then as a kind of proof by example, I’ll aim to demonstrate how     non-Western philosophy might provide insight into several open problems in     AI alignment research. A BRIEF INTRODUCTION TO AI ALIGNMENT So what is AI alignment? One way to cache it out is the project of building intelligent systems that robustly act in our collective i",2020-12-31,2022-01-30 04:48:43,2022-01-30 04:48:43,2021-11-13 16:34:55,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/6KXXKDVU/ai-alignment-philosophical-pluralism-and-the-relevance-of.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8JRIA7DF,blogPost,2020,Larks,2020 AI Alignment Literature Review and Charity Comparison,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/pTYDdcag9pTzFQ7vw/2020-ai-alignment-literature-review-and-charity-comparison,"cross-posted to the EA forum here. INTRODUCTION As in 2016, 2017, 2018, and 2019, I have attempted to review the research that has been produced by various organisations working on AI safety, to help potential donors gain a better understanding of the landscape. This is a similar role to that which GiveWell performs for global health charities, and somewhat similar to a securities analyst with regards to possible investments. My aim is basically to judge the output of each organisation in 2020 and compare it to their budget. This should give a sense of the organisations' average cost-effectiveness. We can also compare their financial reserves to their 2020 budgets to get a sense of urgency. I’d like to apologize in advance to everyone doing useful AI Safety work whose contributions I have overlooked or misconstrued. As ever I am painfully aware of the various corners I have had to cut due to time constraints from my job, as well as being distracted by 1) other projects, 2) the miracle of life and 3) computer games. This article focuses on AI risk work. If you think other causes are important too, your priorities might differ. This particularly affects GCRI, FHI and CSER, who both do a lot of work on other issues which I attempt to cover but only very cursorily. HOW TO READ THIS DOCUMENT This document is fairly extensive, and some parts (particularly the methodology section) are largely the same as last year, so I don’t recommend reading from start to finish. Instead, I recommend navigating to the sections of most interest to you. If you are interested in a specific research organisation, you can use the table of contents to navigate to the appropriate section. You might then also want to Ctrl+F for the organisation acronym in case they are mentioned elsewhere as well. Papers listed as ‘X researchers contributed to the following research lead by other organisations’ are included in the section corresponding to their first author and you can Cntrl+F to find them",2020-12-21,2022-01-30 04:48:43,2022-01-30 04:48:43,2021-11-13 14:30:11,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
D5VWMR6B,blogPost,2020,"Barnes, Beth; Christiano, Paul",Debate update: Obfuscated arguments problem,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/PJLABqQ962hZEqhdB/debate-update-obfuscated-arguments-problem,"This is an update on the work on AI Safety via Debate that we previously wrote about here. Authors and Acknowledgements The researchers on this project were Elizabeth Barnes and Paul Christiano, with substantial help from William Saunders (who built the current web interface as well as other help), Joe Collman (who helped develop the structured debate mechanisms), and Mark Xu, Chris Painter, Mihnea Maftei and Ronny Fernandez (who took part in many debates as well as helping think through problems). We're also grateful to Geoffrey Irving and Evan Hubinger for feedback on drafts, and for helpful conversations, along with Richard Ngo, Daniel Ziegler, John Schulman, Amanda Askell and Jeff Wu. Finally, we're grateful to our contractors who participated in experiments, including Adam Scherlis, Kevin Liu, Rohan Kapoor and Kunal Sharda. WHAT WE DID We tested the debate protocol introduced in AI Safety via Debate with human judges and debaters. We found various problems and improved the mechanism to fix these issues (details of these are in the appendix). However, we discovered that a dishonest debater can often create arguments that have a fatal error, but where it is very hard to locate the error. We don’t have a fix for this “obfuscated argument” problem, and believe it might be an important quantitative limitation for both IDA and Debate. KEY TAKEAWAYS AND RELEVANCE FOR ALIGNMENT Our ultimate goal is to find a mechanism that allows us to learn anything that a machine learning model knows: if the model can efficiently find the correct answer to some problem, our mechanism should favor the correct answer while only requiring a tractable number of human judgements and a reasonable number of computation steps for the model.[1] We’re working under a hypothesis that there are broadly two ways to know things: via step-by-step reasoning about implications (logic, computation…), and by learning and generalizing from data (pattern matching, bayesian updating…). Debate focuse",2020-12-22,2022-01-30 04:48:28,2022-01-30 04:48:28,2021-11-13 16:27:28,,,,,,,Debate update,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/M6NPQG3H/debate-update-obfuscated-arguments-problem.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
T9ZJEXJS,blogPost,2020,"Diffractor; Kosoy, Vanessa",Infra-Bayesianism,AI Alignment Forum,,,,https://www.alignmentforum.org/s/CmrW8fCmSLK7E25sa,A community blog devoted to technical AI alignment research,2020,2022-01-30 04:48:21,2022-01-30 04:48:21,2021-11-14 16:24:35,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/GASHJBIH/CmrW8fCmSLK7E25sa.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
V295HB6Q,blogPost,2020,"Demski, Abram",Recursive Quantilizers II,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/YNuJjRuxsWWzfvder/recursive-quantilizers-ii,"I originally introduced the recursive quantilizers idea here, but didn't provide a formal model until my recent Learning Normativity post. That formal model had some problems. I'll correct some of those problems here. My new model is closer to HCH+IDA, and so, is even closer to Paul Christiano style systems than my previous. However, I'm also beginning to suspect that quantilizers aren't the right starting point. I'll state several problems with quantilizers at the end of this post. First, let's reiterate the design criteria, and why the model in Learning Normativity wasn't great. CRITERIA Here are the criteria from Learning Normativity, with slight revisions. See the earlier post for further justifications/intuitions behind these criteria.  1. No Perfect Feedback: we want to be able to learn with the possibility that     any one piece of data is corrupt. 1. Uncertain Feedback: data can be given in an uncertain form, allowing         100% certain feedback to be given (if there ever is such a thing), but         also allowing the system to learn significant things in the absence of         any certainty.      2. Reinterpretable Feedback: ideally, we want rich hypotheses about the         meaning of feedback, which help the system to identify corrupt feedback,         and interpret the information in imperfect feedback. To this criterion,         I add two clarifying criteria: 1. Robust Listening: in some sense, we don't want the system to be able             to ""entirely ignore"" humans. If the system goes off-course, we want             to be able to correct that.          2. Arbitrary Reinterpretation: at the same time, we want the AI to be             able to entirely reinterpret feedback based on a rich model of what             humans mean. This criterion stands in tension with Robust Listening.             However, the proposal in the present post is, I think, a plausible             way to achieve both.                              2. No Perfect Loss Functi",2020,2022-01-30 04:48:21,2022-01-30 04:48:21,2021-11-13 19:41:28,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/Q5MIGHSB/recursive-quantilizers-ii.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
V9AHZ7MX,blogPost,2020,"Demski, Abram",Learning Normativity: A Research Agenda,,,,,https://www.alignmentforum.org/posts/2JGu9yxiJkoGdQR4s/learning-normativity-a-research-agenda,"(Related to Inaccessible Information, Learning the Prior, and Better Priors as a Safety Problem. Builds on several of my alternate alignment ideas.) I want to talk about something which I'll call learning normativity. What is normativity? Normativity is correct behavior. I mean something related to the fuzzy concept humans convey with the word ""should"". I think it has several interesting features:  * Norms are the result of a complex negotiation between humans, so they    shouldn't necessarily be thought of as the result of maximizing some set of    values. This distinguishes learning normativity from value learning.  * A lot of information about norms is present in the empirical distribution of    what people actually do, but you can't learn norms just by learning human    behavior. This distinguishes it from imitation learning.  * It's often possible to provide a lot of information in the form of ""good/bad""    feedback. This feedback should be interpreted more like approval-directed    learning rather than RL. However, approval should not be treated as a gold    standard.  * Similarly, it's often possible to provide a lot of information in the form of    rules, but rules are not necessarily 100% true; they are just very likely to    apply in typical cases.  * In general, it's possible to get very rich types of feedback, but very sparse    : humans get all sorts of feedback, including not only instruction on how to    act, but also how to think.  * Any one piece of feedback is suspect. Teachers can make mistakes,    instructions can be wrong, demonstrations can be imperfect, dictionaries can    contain spelling errors, reward signals can be corrupt, and so on. EXAMPLE: LANGUAGE LEARNING A major motivating example for me is how language learning works in humans. There is clearly, to some degree, a ""right way"" and a ""wrong way"" to use a language. I'll call this correct usage. One notable feature of language learning is that we don't always speak, or write, in c",2020,2022-01-30 04:48:21,2022-01-30 04:48:21,2021-11-13 19:38:49,,,,,,,Learning Normativity,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/ZWEX5GR8/learning-normativity-a-research-agenda.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
USEM8RBA,blogPost,2020,"Demski, Abram",Comparing Utilities,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/cYsGrWEzjb324Zpjx/comparing-utilities,"(This is a basic point about utility theory which many will already be familiar with. I draw some non-obvious conclusions which may be of interest to you even if you think you know this from the title -- but the main point is to communicate the basics. I'm posting it to the alignment forum because I've heard misunderstandings of this from some in the AI alignment research community.) I will first give the basic argument that the utility quantities of different agents aren't directly comparable, and a few important consequences of this. I'll then spend the rest of the post discussing what to do when you need to compare utility functions. UTILITIES AREN'T COMPARABLE. Utility isn't an ordinary quantity. A utility function is a device for expressing the preferences of an agent. Suppose we have a notion of outcome.* We could try to represent the agent's preferences between outcomes as an ordering relation: if we have outcomes A, B, and C, then one possible preference would be A<B<C. However, a mere ordering does not tell us how the agent would decide between  gambles, ie, situations giving A, B, and C with some probability. With just three outcomes, there is only one thing we need to know: is B closer to A or C, and by how much? We want to construct a utility function U() which represents the preferences. Let's say we set U(A)=0 and U(C)=1. Then we can represent B=G as U(B)=1/2. If not, we would look for a different gamble which does equal B, and then set B's utility to the expected value of that gamble. By assigning real-numbered values to each outcome, we can fully represent an agent's preferences over gambles. (Assuming the VNM axioms hold, that is.) But the initial choices U(A)=0 and U(C)=1 were arbitrary! We could have chosen any numbers so long as U(A)<U(C), reflecting the preference A<C. In general, a valid representation of our preferences U() can be modified into an equally valid U'() by adding/subtracting arbitrary numbers, or multiplying/dividing by pos",2020,2022-01-30 04:48:21,2022-01-30 04:48:21,2021-11-07 22:08:47,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/THUBZR6E/comparing-utilities.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
E7W9CCJS,blogPost,2020,"Hubinger, Evan",Clarifying inner alignment terminology,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/SzecSPYxqRa5GCaSF/clarifying-inner-alignment-terminology,"I have seen a lot of confusion recently surrounding exactly how outer and inner alignment should be defined and I want to try and provide my attempt at a clarification. Here's my diagram of how I think the various concepts should fit together: The idea of this diagram is that the arrows are implications—that is, for any problem in the diagram, if its direct subproblems are solved, then it should be solved as well (though not necessarily vice versa). Thus, we get: inner alignment→objective robustnessouter alignment∧objective robustness→intent alignmentintent alignment∧capability robustness→alignment -------------------------------------------------------------------------------- And here are all my definitions of the relevant terms which I think produce those implications: (Impact) Alignment: An agent is impact aligned (with humans) if it doesn't take actions that we would judge to be bad/problematic/dangerous/catastrophic. Intent Alignment: An agent is intent aligned if the optimal policy for its  behavioral objective[1] is impact aligned with humans. Outer Alignment: An objective function r is outer aligned if all models that perform optimally on r in the limit of perfect training and infinite data are intent aligned.[2] Robustness: An agent is robust if it performs well on the base objective it was trained under even in deployment/off-distribution.[3] Objective Robustness: An agent is objective robust if the optimal policy for its  behavioral objective is impact aligned with the base objective it was trained under. Capability Robustness: An agent is capability robust if it performs well on its  behavioral objective even in deployment/off-distribution. Inner Alignment: A mesa-optimizer is inner aligned if the optimal policy for its  mesa-objective is impact aligned with the base objective it was trained under. -------------------------------------------------------------------------------- And an explanation of each of the diagram's implications:",2020-11-09,2022-01-30 04:48:20,2022-01-30 04:48:20,2021-11-13 13:52:21,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2KRZ2X5K,blogPost,2020,"Armstrong, Stuart","Knowledge, manipulation, and free will",AI Alignment Forum,,,,https://www.alignmentforum.org/posts/2dKvTYYN4PTT7g4of/knowledge-manipulation-and-free-will,"Thanks to Rebecca Gorman for co-developing this idea On the 26th of September 1983, Stanislav Petrov observed the early warning satellites reporting the launch of five nuclear missiles towards the Soviet Union. He decided to disobey orders and not pass on the message to higher command, which could easily have resulted in a nuclear war (since the soviet nuclear position was ""launch on warning""). Now, did Petrov have free will when he decided to save the world? MAINTAINING FREE WILL WHEN KNOWLEDGE INCREASES I don't intend to go into the subtle philosophical debate on the nature of free will. See this post for a good reductionist account. Instead, consider the following scenarios:  1. The standard Petrov incident.  2. The standard Petrov incident, except that it is still ongoing and Petrov     hasn't reached a decision yet.  3. The standard Petrov incident, after it was over, except that we don't yet     know what his final decision was.  4. The standard Petrov incident, except that we know that, if Petrov had had     eggs that morning (instead of porridge[1]), he would have made a different     decision.  5. The same as scenario 4., except that some entity deliberately gave Petrov     porridge that morning, aiming to determine his decision.  6. The standard Petrov incident, except that a guy with a gun held Petrov     hostage and forced him not to pass on the report. There is an interesting contrast between scenarios 1, 2, and 3. Clearly, 1 and 3 only differ in our knowledge of the incident. It does not seem that Petrov's free will should depend on the degree of knowledge of some other person. Scenarios 1 and 2 only differ in time: in one case the decision is made, in the second it is yet to be made. If we say that Petrov has free will, whatever that is, in scenario 2, then it seems that in scenario 1, we have to say that he ""had"" free will. So whatever our feeling on free will, it seems that knowing the outcome doesn't change whether there was free will or not.",2020-10-13,2022-01-30 04:48:04,2022-01-30 04:48:04,2021-11-08 23:34:20,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/9VD67FAD/knowledge-manipulation-and-free-will.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
V5PJ9SVN,blogPost,2020,"Dafoe, Allan",AI Governance: Opportunity and Theory of Impact,Effective Altruism Forum,,,,https://forum.effectivealtruism.org/posts/42reWndoTEhFqu6T8/ai-governance-opportunity-and-theory-of-impact,"Note: We have recently opened up roles for researchers and a project manager at the Centre for the Governance of Artificial Intelligence, part of the Future of Humanity Institute, University of Oxford AI governance concerns how humanity can best navigate the transition to a world with advanced AI systems.[1] It relates to how decisions are made about AI,[2]  and what institutions and arrangements would help those decisions to be made well. I believe advances in AI are likely to be among the most impactful global developments in the coming decades, and that AI governance will become among the most important global issue areas. AI governance is a new field and is relatively neglected. I’ll explain here how I think about this as a cause area and my perspective on how best to pursue positive impact in this space. The value of investing in this field can be appreciated whether one is primarily concerned with contemporary policy challenges or long-term risks and opportunities (“longtermism”); this piece is primarily aimed at a longtermist  perspective. Differing from some other longtermist work on AI, I emphasize the importance of also preparing for more conventional scenarios of AI development. CONTEMPORARY POLICY CHALLENGES AI systems are increasingly being deployed in important domains: for many kinds of surveillance; by authoritarian governments to shape online discourse; for autonomous weapons systems; for cyber tools and autonomous cyber capabilities; to aid and make consequential decisions such as for employment, loans, and criminal sentencing; in advertising; in education and testing; in self-driving cars and navigation; in social media. Society and policy makers are rapidly trying to catch up, to adapt, to create norms and policies to guide these new areas. We see this scramble in contemporary international tax law, competition/antitrust policy, innovation policy, and national security motivated controls on trade and investment. To understand and advise conte",2020-09-17,2022-01-30 04:48:03,2022-01-30 04:48:03,2021-11-07 19:52:53,,,,,,,AI Governance,,,,,,,,,,,,,,ZSCC: 0000001,,/Users/jacquesthibodeau/Zotero/storage/CE75WCFR/ai-governance-opportunity-and-theory-of-impact.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8R5CM8V5,blogPost,2020,The AlphaFold Team,AlphaFold: a solution to a 50-year-old grand challenge in biology,Deepmind,,,,https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology,"In a major scientific advance, the latest version of our AI system AlphaFold has been recognised as a solution to this grand challenge by the organisers of the biennial Critical Assessment of protein Structure Prediction (CASP) assessment. This breakthrough demonstrates the impact AI can have on scientific discovery and its potential to dramatically accelerate progress in some of the most fundamental fields that explain and shape our world.",2020-11-30,2022-01-30 04:47:57,2022-01-30 04:47:57,2021-11-13 14:16:32,,,,,,,AlphaFold,,,,,,,ALL,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
822KXARD,blogPost,2019,"Gloor, Lukas",Rebuttal of Christiano and AI Impacts on takeoff speeds?,LessWrong,,,,https://www.lesswrong.com/posts/PzAnWgqvfESgQEvdg/any-rebuttals-of-christiano-and-ai-impacts-on-takeoff-speeds#zFEhTxNqEp3eZbjLZ,"14 months ago, Paul Christiano and AI Impacts both published forceful and well-received take-downs of many arguments for fast (discontinuous) takeoff. I haven’t seen any rebuttals that are written by established researchers, longer than comments, or otherwise convincing. The longer there is no response, the less weight I put on the outside view that proponents of fast takeoff may be right. Where are the rebuttals? Did I miss them? Is the debate decided? Did nobody have time or motivation to write something? Is the topic too hard to explain? Why rebuttals would be useful: -Give the community a sense of the extent of expert disagreement to form outside views. -Prioritization in AI policy, and to a lesser extent safety, depends on the likelihood of discontinuous progress. We may have more leverage in such cases, but this could be overwhelmed if the probability is low. -Motivate more people to work on MIRI’s research which seems more important to solve early if there is fast takeoff.",2019-04-25,2022-01-30 04:51:36,2022-01-30 04:51:36,2020-11-23 00:36:12,,,,,,,Any rebuttals of Christiano and AI Impacts on takeoff speeds?,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/3QVB2E9R/any-rebuttals-of-christiano-and-ai-impacts-on-takeoff-speeds.html,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9GQXT7XM,blogPost,2019,"Shah, Rohin",What is narrow value learning?,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/vX7KirQwHsBaSEdfK/what-is-narrow-value-learning,"Ambitious value learning aims to achieve superhuman performance by figuring out the underlying latent ""values"" that humans have, and evaluating new situations according to these values. In other words, it is trying to infer the criteria by which we judge situations to be good. This is particularly hard because in novel situations that humans haven't seen yet, we haven't even developed the criteria by which we would evaluate. (This is one of the reasons why we need to model humans as suboptimal, which causes problems.) Instead of this, we can use narrow value learning, which produces behavior that we want in some narrow domain, without expecting generalization to novel circumstances. The simplest form of this is imitation learning, where the AI system simply tries to imitate the supervisor's behavior. This limits the AI’s performance to that of its supervisor. We could also learn from preferences over behavior, which can scale to superhuman performance, since the supervisor can often evaluate whether a particular behavior meets our preferences even if she can’t perform it herself. We could also teach our AI systems to perform tasks that we would not want to do ourselves, such as handling hot objects. Nearly all of the work on preference learning, including most work on inverse reinforcement learning (IRL), is aimed at narrow value learning. IRL is often explicitly stated to be a technique for imitation learning, and early algorithms phrase the problem as matching the features in the demonstration, not exceeding them. The few algorithms that try to generalize to different test distributions, such as AIRL, are only aiming for relatively small amounts of generalization. (Why use IRL instead of behavioral cloning, where you mimic the actions that the demonstrator took? The hope is that IRL gives you a good inductive bias for imitation, allowing you to be more sample efficient and to generalize a little bit.) You might have noticed that I talk about narrow value learn",2019,2022-01-30 04:51:12,2022-01-30 04:51:12,2020-12-17 04:37:01,,,,,,,What is narrow value learning?,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/WZFPW9XN/vX7KirQwHsBaSEdfK.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5IKAMSXK,blogPost,2018,"Shah, Rohin",What is ambitious value learning?,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/5eX8ko7GCxwR5N9mN/what-is-ambitious-value-learning,"I think of ambitious value learning as a proposed solution to the specification problem, which I define as the problem of defining the behavior that we would want to see from our AI system. I italicize “defining” to emphasize that this is  not the problem of actually computing behavior that we want to see -- that’s the full AI safety problem. Here we are allowed to use hopelessly impractical schemes, as long as the resulting definition would allow us to in theory compute the behavior that an AI system would take, perhaps with assumptions like infinite computing power or arbitrarily many queries to a human. (Although we do prefer specifications that seem like they could admit an efficient implementation.) In terms of DeepMind’s classification, we are looking for a design specification that exactly matches the ideal specification. HCH and  indirect normativity are examples of attempts at such specifications. We will consider a model in which our AI system is maximizing the expected utility of some explicitly represented utility function that can depend on history. (It does not matter materially whether we consider utility functions or reward functions, as long as they can depend on history.) The utility function may be learned from data, or designed by hand, but it must be an explicit part of the AI that is then maximized. I will not justify this model for now, but simply assume it by fiat and see where it takes us. I’ll note briefly that this model is often justified by the  VNM utility theorem and AIXI, and as the natural idealization of reinforcement learning, which aims to maximize the expected sum of rewards, although typically rewards in RL depend only on states. A lot of conceptual arguments, as well as experiences with specification gaming, suggest that we are unlikely to be able to simply think hard and write down a good specification, since even small errors in specifications can lead to bad results. However, machine learning is particularly good at narro",2018,2022-01-30 04:51:12,2022-01-30 04:51:12,2020-12-17 04:36:27,,,,,,,What is ambitious value learning?,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/WV7FNFRK/5eX8ko7GCxwR5N9mN.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZFP7FN3X,blogPost,2020,"Turner, Alex",What counts as defection?,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/8LEPDY36jBYpijrSw/what-counts-as-defection,"Thanks to Michael Dennis for proposing the formal definition; to Andrew Critch for pointing me in this direction; to Abram Demski for proposing non-negative weighting; and to Alex Appel, Scott Emmons, Evan Hubinger, philh, Rohin Shah, and Carroll Wainwright for their feedback and ideas. There's a good chance I'd like to publish this at some point as part of a larger work. However, I wanted to make the work available now, in case that doesn't happen soon. They can't prove the conspiracy... But they could, if Steve runs his mouth. The police chief stares at you. You stare at the table. You'd agreed (sworn!) to stay quiet. You'd even studied game theory together. But, you hadn't understood what an extra year of jail meant. The police chief stares at you. Let Steve be the gullible idealist. You have a family waiting for you. Sunlight stretches across the valley, dappling the grass and warming your bow. Your hand anxiously runs along the bowstring. A distant figure darts between trees, and your stomach rumbles. The day is near spent. The stags run strong and free in this land. Carla should meet you there. Shouldn't she? Who wants to live like a beggar, subsisting on scraps of lean rabbit meat? In your mind's eye, you reach the stags, alone. You find one, and your arrow pierces its barrow. The beast bucks and bursts away; the rest of the herd follows. You slump against the tree, exhausted, and never open your eyes again. You can't risk it. People talk about 'defection' in social dilemma games, from the prisoner's dilemma to stag hunt to chicken. In the tragedy of the commons, we talk about defection. The concept has become a regular part of LessWrong discourse. Informal definition. A player defects when they increase their personal payoff at the expense of the group. This informal definition is no secret, being echoed from the ancient Formal Models of Dilemmas in Social Decision-Making to the recent Classifying games like the Prisoner's Dilemma: you can mo",2020-07-12,2022-01-30 04:51:11,2022-01-30 04:51:11,2020-08-28 17:44:13,,,,,,,What counts as defection?,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/2H5VW8FH/formalizing-game-theoretic-defection.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FGCH7XG7,blogPost,2021,"Steinhardt, Jacob",Updates and Lessons from AI Forecasting,Bounded Regret,,,,https://bounded-regret.ghost.io/ai-forecasting/,"Earlier this year, my research group commissioned 6 questions  for professional forecasters to predict about AI. Broadly speaking, 2 were on geopolitical aspects of AI and 4 were on future capabilities:  Geopolitical: How much larger or smaller will the largest Chinese ML experiment be compared to the largest U.S.",2021-08-18,2022-01-30 04:51:11,2022-01-30 04:51:11,2021-11-18 23:38:06,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/EJIEH689/ai-forecasting.html,,MetaSafety; AmbiguousSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R8H8NHUI,blogPost,2018,Alex Turner,Towards a New Impact Measure,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/yEa7kwoMpsBgaBCgb/towards-a-new-impact-measure,"In which I propose a closed-form solution to low impact, increasing  corrigibility and seemingly taking major steps to neutralize basic AI drives 1 (self-improvement), 5 (self-protectiveness), and 6 (acquisition of resources). Previously: Worrying about the Vase: Whitelisting, Overcoming Clinginess in Impact Measures, Impact Measure Desiderata To be used inside an advanced agent, an impact measure... must capture so much variance that there is no clever strategy whereby an advanced agent can produce some special type of variance that evades the measure. ~ Safe Impact MeasureIf we have a safe impact measure, we may have arbitrarily-intelligent unaligned agents which do small (bad) things instead of big (bad) things.  For the abridged experience, read up to ""Notation"", skip to ""Experimental Results"", and then to ""Desiderata"". WHAT IS ""IMPACT""? One lazy Sunday afternoon, I worried that I had written myself out of a job. After all, Overcoming Clinginess in Impact Measures basically said, ""Suppose an impact measure extracts 'effects on the world'. If the agent penalizes itself for these effects, it's incentivized to stop the environment (and any agents in it) from producing them. On the other hand, if it can somehow model other agents and avoid penalizing their effects, the agent is now incentivized to get the other agents to do its dirty work."" This seemed to be strong evidence against the possibility of a simple conceptual core underlying ""impact"", and I didn't know what to do. At this point, it sometimes makes sense to step back and try to say exactly what you don't know how to solve – try to crisply state what it is that you want an unbounded solution for. Sometimes you can't even do that much, and then you may actually have to spend some time thinking 'philosophically' – the sort of stage where you talk to yourself about some mysterious ideal quantity of [chess] move-goodness and you try to pin down what its properties might be. ~  Methodology of Unbounded Anal",2018,2022-01-30 04:51:11,2022-01-30 04:51:11,2020-12-13 23:51:04,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PMHJK9F7,blogPost,2019,"Shah, Rohin",The human side of interaction,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/eD9T4kiwB6MHpySGE/the-human-side-of-interaction,"The last few posts have motivated an analysis of the human-AI system rather than an AI system in isolation. So far we’ve looked at the notion that the AI system should get feedback from the user and that it could use reward uncertainty for corrigibility. These are focused on the AI system, but what about the human? If we build a system that explicitly solicits feedback from the human, what do we have to say about the human policy, and how the human should provide feedback? INTERPRETING HUMAN ACTIONS One major free variable in any explicit interaction or feedback mechanism is what semantics the AI system should attach to the human feedback. The classic examples of AI risk are usually described in a way where this is the problem: when we provide a reward function that rewards paperclips, the AI system interprets it literally and maximizes paperclips, rather than interpreting it pragmatically as another human would. (Aside: I suspect this was not the original point of the paperclip maximizer, but it has become a very popular retelling, so I’m using it anyway.) Modeling this classic example as a human-AI system, we can see that the problem is that the human is offering a form of “feedback”, the reward function, and the AI system is not ascribing the correct semantics to it. The way it uses the reward function implies that the reward function encodes the optimal behavior of the AI system in all possible environments -- a moment’s thought is sufficient to see that this is not actually the case. There will definitely be many cases and environments that the human did not consider when designing the reward function, and we should not expect that the reward function incentivizes the right behavior in those cases. So what can the AI system assume if the human provides it a reward function?  Inverse Reward Design (IRD) offers one answer: the human is likely to provide a particular reward function if it leads to high true utility behavior in the training environment. So, in",2019,2022-01-30 04:51:10,2022-01-30 04:51:10,2020-12-17 04:37:16,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/WHZXKZ8H/eD9T4kiwB6MHpySGE.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KRGHV4EA,blogPost,2020,"Critch, Andrew",Some AI research areas and their relevance to existential safety,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/hvGoYXi2kgnS3vxqb/some-ai-research-areas-and-their-relevance-to-existential-1,"INTRODUCTION This post is an overview of a variety of AI research areas in terms of how much I think contributing to and/or learning from those areas might help reduce AI x-risk. By research areas I mean “AI research topics that already have groups of people working on them and writing up their results”, as opposed to research “directions” in which I’d like to see these areas “move”. I formed these views mostly pursuant to writing AI Research Considerations for Human Existential Safety (ARCHES). My hope is that my assessments in this post can be helpful to students and established AI researchers who are thinking about shifting into new research areas specifically with the goal of contributing to existential safety somehow. In these assessments, I find it important to distinguish between the following types of value:  * The helpfulness of the area to existential safety, which I think of as a    function of what services are likely to be provided as a result of research    contributions to the area, and whether those services will be helpful to    existential safety, versus  * The educational value of the area for thinking about existential safety,    which I think of as a function of how much a researcher motivated by    existential safety might become more effective through the process of    familiarizing with or contributing to that area, usually by focusing on ways    the area could be used in service of existential safety.  * The neglect of the area at various times, which is a function of how much    technical progress has been made in the area relative to how much I think is    needed. Importantly:  * The helpfulness to existential safety scores do not assume that your    contributions to this area would be used only for projects with existential    safety as their mission. This can negatively impact the helpfulness of    contributing to areas that are more likely to be used in ways that harm    existential safety.  * The educational value scores are not ab",2020-11-18,2022-01-30 04:51:09,2022-01-30 04:51:09,2020-12-19 02:13:03,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/U3PJ2KXP/some-ai-research-areas-and-their-relevance-to-existential-1.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EU74EWFE,blogPost,2015,"Tomasik, Brian",Reasons to Be Nice to Other Value Systems,Center on Long-Term Risk,,,,https://longtermrisk.org/reasons-to-be-nice-to-other-value-systems/,"Several arguments support the heuristic that we should help groups holding different value systems from our own when doing so is cheap, unless those groups prove uncooperative to our values. This is true even if we don't directly care at all about other groups' value systems. Exactly how nice to be depends on the particulars of the situation.",2015-08-29,2022-01-30 04:51:09,2022-01-30 04:51:09,2020-11-23 20:07:23,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/ZCKXSPBP/reasons-to-be-nice-to-other-value-systems.html,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MKUM9MAZ,blogPost,2020,"Kokotajlo, Daniel",Persuasion Tools: AI takeover without AGI or agency?,LessWrong,,,,https://www.lesswrong.com/posts/qKvn7rxP2mzJbKfcA/persuasion-tools-ai-takeover-without-agi-or-agency,"[epistemic status: speculation] I'm envisioning that in the future there will also be systems where you can input any conclusion that you want to argue (including moral conclusions) and the target audience, and the system will give you the most convincing arguments for it. At that point people won't be able to participate in any online (or offline for that matter) discussions without risking their object-level values being hijacked.--Wei Dai What if most people already live in that world? A world in which taking arguments at face value is not a capacity-enhancing tool, but a security vulnerability? Without trusted filters, would they not dismiss highfalutin arguments out of hand, and focus on whether the person making the argument seems friendly, or unfriendly, using hard to fake group-affiliation signals?--Benquo 1. AI-powered memetic warfare makes all humans effectively insane.--Wei Dai, listing nonstandard AI doom scenarios This post speculates about persuasion tools—how likely they are to get better in the future relative to countermeasures, what the effects of this might be, and what implications there are for what we should do now. To avert eye-rolls, let me say up front that I don’t think the world is likely to be driven insane by AI-powered memetic warfare. I think progress in persuasion tools will probably be gradual and slow, and defenses will improve too, resulting in an overall shift in the balance that isn’t huge: a deterioration of collective epistemology, but not a massive one. However, (a) I haven’t yet ruled out more extreme scenarios, especially during a slow takeoff, and (b) even small, gradual deteriorations are important to know about. Such a deterioration would make it harder for society to notice and solve AI safety and governance problems, because it is worse at noticing and solving problems in general. Such a deterioration could also be a risk factor for world war three, revolutions, sectarian conflict, terrorism, and the like. Moreover",2020,2022-01-30 04:51:09,2022-01-30 04:51:09,2020-12-12 15:02:20,,,,,,,Persuasion Tools,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/BPNW2WW3/persuasion-tools-ai-takeover-without-agi-or-agency.html,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XH9E33N4,blogPost,2019,"Shah, Rohin",Reward uncertainty,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/ZiLLxaLB5CCofrzPp/reward-uncertainty,"In my last post, I argued that interaction between the human and the AI system was necessary in order for the AI system to “stay on track” as we encounter new and unforeseen changes to the environment. The most obvious implementation of this would be to have an AI system that keeps an estimate of the reward function. It acts to maximize its current estimate of the reward function, while simultaneously updating the reward through human feedback. However, this approach has significant problems. Looking at the description of this approach, one thing that stands out is that the actions are chosen according to a reward that we know is going to change. (This is what leads to the incentive to disable the narrow value learning system.) This seems clearly wrong: surely our plans should account for the fact that our rewards will change, without treating such a change as adversarial? This suggests that we need to have our action selection mechanism take the future rewards into account as well. While we don’t know what the future reward will be, we can certainly have a  probability distribution over it. So what if we had uncertainty over reward functions, and took that uncertainty into account while choosing actions? SETUP We’ve drilled down on the problem sufficiently far that we can create a formal model and see what happens. So, let’s consider the following setup:  * The human, Alice, knows the “true” reward function that she would like to    have optimized.  * The AI system maintains a probability distribution over reward functions, and    acts to maximize the expected sum of rewards under this distribution.  * Alice and the AI system take turns acting. Alice knows that the AI learns    from her actions, and chooses actions accordingly.  * Alice’s action space is such that she cannot take the action “tell the AI    system the true reward function” (otherwise the problem would become    trivial).  * Given these assumptions, Alice and the AI system act optimally. This is",2019,2022-01-30 04:51:08,2022-01-30 04:51:08,2020-12-17 04:37:10,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/S52J9GZH/ZiLLxaLB5CCofrzPp.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7G9QKQZ8,blogPost,2021,"Kumar, Ramana; Kokotajlo, Daniel",P₂B: Plan to P₂B Better,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/CAwwFpbteYBQw2Gkp/p-b-plan-to-p-b-better,"tl;dr: Most good plans involve taking steps to make better plans. Making better plans is the convergent instrumental goal, of which all familiar convergent instrumental goals are an instance. This is key to understanding what agency is and why it is powerful. Planning means using a world model to predict the consequences of various courses of actions one could take, and taking actions that have good predicted consequences. (We think of this with the handle “doing things for reasons,” though we acknowledge this may be an idiosyncratic use of “reasons.”) We take “planning” to include things that are relevantly similar to this procedure, such as following a bag of heuristics that approximates it. We’re also including actually following the plans, in what might more clunkily be called “planning-acting.” Planning, in this broad sense, seems essential to the kind of goal-directed, consequential, agent-like intelligence that we expect to be highly impactful. This sequence explains why. ONE CONVERGENT INSTRUMENTAL GOAL TO RULE THEM ALL Consider the maxim “make there be more and/or better planning towards your goal.” This section argues that all the classic convergent instrumental goals are special cases of this maxim. To flesh this out a little, here are some categories of ways to follow the maxim. Remember that a planner is typically close (in terms of what it might affect via action) to at least one planner – itself – so these directions can typically be applied in the first case to the planner itself.  * Make the planners with your goal better at planning. For example, get them    new relevant data to work with¹, get them to run faster or more effective    algorithms, build protections against value drift, etc.  * Make the planners with your goal have better options. For example, move them    to better locations, get them more resources, get them more power or a    greater number of options to select from, have them take steps in an    object-level plan towards t",2021-10-24,2022-01-30 04:51:08,2022-01-30 04:51:08,2021-12-11 14:17:29,,,,,,,P₂B,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/QRHZCPVT/p-b-plan-to-p-b-better.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R9GDT9M8,blogPost,2017,"Oesterheld, Caspar",Naturalized induction – a challenge for evidential and causal decision theory,LessWrong,,,,https://www.lesswrong.com/posts/kgsaSbJqWLtJfiCcz/naturalized-induction-a-challenge-for-evidential-and-causal,"As some of you may know, I disagree with many of the criticisms leveled against  evidential decision theory (EDT). Most notably, I believe that Smoking lesion-type problems don't refute EDT. I also don't think that EDT's non-updatelessness leaves a lot of room for disagreement, given that EDT  recommends immediate self-modification to updatelessness. However, I do believe there are some issues with run-of-the-mill EDT. One of them is naturalized induction. It is in fact not only a problem for EDT but also for causal decision theory (CDT) and most other decision theories that have been proposed in- and outside of academia. It does not affect logical decision theories, however. THE ROLE OF NATURALIZED INDUCTION IN DECISION THEORY Recall that EDT prescribes taking the action that maximizes expected utility, i.e. where is the set of available actions, is the agent's utility function, is a set of possible world models, represents the agent's past observations (which may include information the agent has collected about itself). CDT works in a – for the purpose of this article – similar way, except that instead of conditioning on in the usual way, it calculates some causal counterfactual, such as Pearl's do-calculus: . The problem of naturalized induction is that of assigning posterior probabilities to world models (or or whatever) when the agent is  naturalized, i.e., embedded into its environment. Consider the following example. Let's say there are 5 world models , each of which has equal prior probability. These world models may be cellular automata. Now, the agent makes the observation . It turns out that worlds and don't contain any agents at all, and contains no agent making the observation . The other two world models, on the other hand, are consistent with . Thus, for and  for . Let's assume that the agent has only two actions and that in world model  the only agent making observation takes action and in the only agent making observation takes action , then a",2017-09-22,2022-01-30 04:51:08,2022-01-30 04:51:08,2020-11-23 00:47:30,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/PFXHQPWU/naturalized-induction-a-challenge-for-evidential-and-causal.html,,CLR; TechSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XKENFS2F,blogPost,2017,"Oesterheld, Caspar",Is it a bias or just a preference? An interesting issue in preference idealization,The Universe from an Intentional Stance,,,,https://casparoesterheld.com/2017/01/18/is-it-a-bias-or-just-a-preference-an-interesting-issue-in-preference-idealization/,"When taking others’ preferences into account, we will often want to idealize them rather than taking them too literally. Consider the following example. You hold a glass of transparent liquid…",2017-01-18,2022-01-30 04:51:08,2022-01-30 04:51:08,2020-11-23 00:56:11,,,,,,,Is it a bias or just a preference?,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/UQ3WP8PZ/is-it-a-bias-or-just-a-preference-an-interesting-issue-in-preference-idealization.html,,CLR; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MFFRRWRE,blogPost,2015,"Tomasik, Brian",International Cooperation vs. AI Arms Race,Center on Long-Term Risk,,,,https://longtermrisk.org/international-cooperation-vs-ai-arms-race/,"There's a decent chance that governments will be the first to build artificial general intelligence (AI). International hostility, especially an AI arms race, could exacerbate risk-taking, hostile motivations, and errors of judgment when creating AI. If so, then international cooperation could be an important factor to consider when evaluating the flow-through effects of charities.",2015-04-08,2022-01-30 04:51:08,2022-01-30 04:51:08,2020-11-23 01:06:39,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/D464WPF3/international-cooperation-vs-ai-arms-race.html,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BUB3ZWDG,blogPost,2015,"Tomasik, Brian",Gains from Trade through Compromise,Center on Long-Term Risk,,,,https://longtermrisk.org/gains-from-trade-through-compromise/,"When agents of differing values compete, they may often find it mutually advantageous to compromise rather than continuing to engage in zero-sum conflicts. Potential ways of encouraging cooperation include promoting democracy, tolerance and (moral) trade. Because a future without compromise could be many times worse than a future with it, advancing compromise seems an important undertaking.",2015-04-10,2022-01-30 04:51:08,2022-01-30 04:51:08,2020-11-23 20:08:18,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/VHTBJW4Q/gains-from-trade-through-compromise.html,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R9625IHB,blogPost,2017,"Gloor, Lukas",Multiverse-wide cooperation in a nutshell,Effective Altruism Forum,,,,https://forum.effectivealtruism.org/posts/7MdLurJGhGmqRv25c/multiverse-wide-cooperation-in-a-nutshell,"(Crossposted from the FRI blog.)  This is a post I wrote about Caspar Oesterheld’s long paper Multiverse-wide cooperation via correlated decision-making. Because I have found the idea tricky to explain – which unfortunately makes it difficult to get feedback from others on whether the thinking behind it makes sense – I decided to write a shorter summary. While I am hoping that my text can serve as a standalone piece, for additional introductory content I also recommend reading the beginning of Caspar’s paper, or watching the short video introduction here (requires basic knowledge of the “CDT, EDT or something else” debate in decision theory). 0. ELEVATOR PITCH (Disclaimer: Especially for the elevator pitch section here, I am sacrificing accuracy and precision for brevity. References can be found in Caspar’s paper.)  It would be an uncanny coincidence if the observable universe made up everything that exists. The reason we cannot find any evidence for there being stuff beyond the edges of our universe is not because it is likely that there is nothingness, but because photons from further away simply would not have had sufficient time after the big bang to reach us. This means that the universe we find ourselves in may well be vastly larger than what we can observe, in fact even infinitely  larger. The theory of inflationary cosmology in addition hints at the existence of other universe bubbles with different fundamental constants forming or disappearing under certain conditions, somehow co-existing with our universe in parallel. The umbrella term multiverse captures the idea that the observable universe is just a tiny portion of everything that exists. The multiverse may contain myriads of worlds like ours, including other worlds with intelligent life and civilization. An infinite multiverse (of one sort or another) is actually amongst the most popular cosmological hypotheses, arguably even favored by the majority of experts.  Many ethical theories (in particular",2017-11-02,2022-01-30 04:51:08,2022-01-30 04:51:08,2020-11-23 00:46:11,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/65NR427Z/multiverse-wide-cooperation-in-a-nutshell.html,,CLR; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HCKKDM49,blogPost,2018,"Oesterheld, Caspar",Moral realism and AI alignment,The Universe from an Intentional Stance,,,,https://casparoesterheld.com/2018/08/06/moral-realism-and-ai-alignment/,"“Abstract”: Some have claimed that moral realism – roughly, the claim that moral claims can be true or false – would, if true, have implications for AI alignment research, such that moral realists …",2018-08-06,2022-01-30 04:51:08,2022-01-30 04:51:08,2020-11-23 00:38:57,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/R98Z34VR/moral-realism-and-ai-alignment.html,,CLR; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9EK5VIJC,blogPost,2015,"Tomasik, Brian",How Would Catastrophic Risks Affect Prospects for Compromise?,Center on Long-Term Risk,,,,https://longtermrisk.org/how-would-catastrophic-risks-affect-prospects-for-compromise/,"Global catastrophic risks – such as biotech disasters or nuclear war – would cause major damage in the short run, but their effects on the long-run trajectory that humanity takes are also significant. In particular, to the extent these disasters increase risks of war, they seem likely to precipitate AI arms races between nations and worsen prospects for compromise.",2015-08-29,2022-01-30 04:51:08,2022-01-30 04:51:08,2020-11-23 01:11:09,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
24MAX7RT,blogPost,2020,"Kokotajlo, Daniel",How Roodman's GWP model translates to TAI timelines,LessWrong,,,,https://www.lesswrong.com/posts/L23FgmpjsTebqcSZb/how-roodman-s-gwp-model-translates-to-tai-timelines,"How does David Roodman’s world GDP model translate to TAI timelines? Now, before I go any further, let me be the first to say that I don’t think we should use this model to predict TAI. This model takes a very broad outside view and is thus inferior to models like Ajeya Cotra’s which make use of more relevant information. (However, it is still useful for rebutting claims that TAI is unprecedented, inconsistent with historical trends, low-prior, etc.) Nevertheless, out of curiosity I thought I’d calculate what the model implies for TAI timelines. Here is the projection made by Roodman’s model. The red line is real historic GWP data; the splay of grey shades that continues it is the splay of possible futures calculated by the model. The median trajectory is the black line. I messed around with a ruler to make some rough calculations, marking up the image with blue lines as I went. The big blue line indicates the point on the median trajectory where GWP is 10x what is was in 2019. Eyeballing it, it looks like it happens around 2040, give or take a year. The small vertical blue line indicates the year 2037. The small horizontal blue line indicates GWP in 2037 on the median trajectory. Thus, it seems that between 2037 and 2040 on the median trajectory, GWP doubles. (One-ninth the distance between 1,000 and 1,000,000 is crossed, which is one-third of an order of magnitude, which is about one doubling). This means that TAI happens around 2037 on the median trajectory according to this model, at least according to Ajeya Cotra’s definition of transformative AI  as “software which causes a tenfold acceleration in the rate of growth of the world economy (assuming that it is used everywhere that it would be economically profitable to use it)... This means that if TAI is developed in year Y, the entire world economy would more than double by year Y + 4.” What about the non-median trajectories? Each shade of grey represents 5 percent of the simulated future trajectories, so",2020,2022-01-30 04:51:08,2022-01-30 04:51:08,2020-12-12 15:02:13,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/G7VN452V/how-roodman-s-gwp-model-translates-to-tai-timelines.html,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6JZ2JGFE,blogPost,2018,"Oesterheld, Caspar",Goertzel’s GOLEM implements evidential decision theory applied to policy choice,The Universe from an Intentional Stance,,,,https://casparoesterheld.com/2018/04/26/goertzels-golem-implements-evidential-decision-theory-applied-to-policy-choice/,"I’ve written about the question of which decision theories describe the behavior of approaches to AI like the “Law of Effect”. In this post, I would like to discuss GOLEM, an arch…",2018-04-26,2022-01-30 04:51:08,2022-01-30 04:51:08,2020-11-23 00:41:22,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/GK7JQRH4/goertzels-golem-implements-evidential-decision-theory-applied-to-policy-choice.html,,CLR; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QIVPQ6DS,blogPost,2018,"Shah, Rohin",Preface to the sequence on value learning,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/oH8KMnXHnw964QyS6/preface-to-the-sequence-on-value-learning,"This is a meta-post about the upcoming sequence on Value Learning that will start to be published this Thursday. This preface will also be revised significantly once the second half of the sequence is fully written. PURPOSE OF THE SEQUENCE The first part of this sequence will be about the tractability of ambitious value learning, which is the idea of inferring a utility function for an AI system to optimize based on observing human behavior. After a short break, we will (hopefully) continue with the second part, which will be about why we might want to think about techniques that infer human preferences, even if we assume we won’t do ambitious value learning with such techniques. The aim of this part of the sequence is to gather the current best public writings on the topic, and provide a unifying narrative that ties them into a cohesive whole. This makes the key ideas more discoverable and discussable, and provides a quick reference for existing researchers. It is meant to teach the ideas surrounding one specific approach to aligning advanced AI systems. We’ll explore the specification problem, in which we would like to define the behavior we want to see from an AI system. Ambitious value learning is one potential avenue of attack on the specification problem, that assumes a particular model of an AI system (maximizing expected utility) and a particular source of data (human behavior). We will then delve into conceptual work on ambitious value learning that has revealed obstructions to this approach. There will be pointers to current research that aims to circumvent these obstructions. The second part of this sequence is currently being assembled, and this preface will be updated with details once it is ready. The first half of this sequence takes you near the cutting edge of conceptual  work on the ambitious value learning problem, with some pointers to work being done at this frontier. Based on the arguments in the sequence, I am confident that the obvious f",2018,2022-01-30 04:51:07,2022-01-30 04:51:07,2020-12-17 04:36:13,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/2HGSWMFH/oH8KMnXHnw964QyS6.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XWGR86QB,blogPost,2021,"Kokotajlo, Daniel",Fun with +12 OOMs of Compute,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/rzqACeBGycZtqCfaX/fun-with-12-ooms-of-compute,"OR: BIG TIMELINES CRUX OPERATIONALIZED What fun things could one build with +12 orders of magnitude of compute? By ‘fun’ I mean ‘powerful.’ This hypothetical is highly relevant to AI timelines, for reasons I’ll explain later. Summary (Spoilers): I describe a hypothetical scenario that concretizes the question “what could be built with 2020’s algorithms/ideas/etc. but a trillion times more compute?”Then I give some answers to that question. Then I ask: How likely is it that some sort of TAI would happen in this scenario? This second question is a useful operationalization of the (IMO) most important, most-commonly-discussed timelines crux: “Can we get TAI just by throwing more compute at the problem?” I consider this operationalization to be the main contribution of this post; it directly plugs into Ajeya’s timelines model and is quantitatively more cruxy than anything else I know of. The secondary contribution of this post is my set of answers to the first question: They serve as intuition pumps for my answer to the second, which strongly supports my views on timelines. THE HYPOTHETICAL In 2016 the Compute Fairy visits Earth and bestows a blessing: Computers are magically 12 orders of magnitude faster! Over the next five years, what happens? The Deep Learning AI Boom still happens, only much crazier: Instead of making AlphaStar for 10^23 floating point operations, DeepMind makes something for 10^35. Instead of making GPT-3 for 10^23 FLOPs, OpenAI makes something for 10^35. Instead of industry and academia making a cornucopia of things for 10^20 FLOPs or so, they make a cornucopia of things for 10^32 FLOPs or so. When random grad students and hackers spin up neural nets on their laptops, they have a trillion times more compute to work with. [EDIT: Also assume magic +12 OOMs of memory, bandwidth, etc. All the ingredients of compute.] For context on how big a deal +12 OOMs is, consider the graph below, from ARK. It’s measuring petaflop-days, which are about 10^20 F",2021-03-01,2022-01-30 04:51:07,2022-01-30 04:51:07,2021-12-11 14:12:30,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/WWZQP7ZC/fun-with-12-ooms-of-compute.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7BCH8MC8,blogPost,2015,"Tomasik, Brian",Flavors of Computation Are Flavors of Consciousness,Center on Long-Term Risk,,,,https://longtermrisk.org/flavors-of-computation-are-flavors-of-consciousness/,"If we don't understand why we're conscious, how come we're so sure that extremely simple minds are not? I propose to think of consciousness as intrinsic to computation, although different types of computation may have very different types of consciousness – some so alien that we can't imagine them. Since all physical processes are computations, […]",2015-04-10,2022-01-30 04:51:07,2022-01-30 04:51:07,2020-11-23 20:03:32,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,,,CLR; TechSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
93Q953KS,blogPost,2020,"Clifton, Jesse",Equilibrium and prior selection problems in multipolar deployment,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/Tdu3tGT4i24qcLESh/equilibrium-and-prior-selection-problems-in-multipolar-1,"To avoid catastrophic conflict in multipolar AI scenarios, we would like to design AI systems such that AI-enabled actors will tend to cooperate. This post is about some problems facing this effort and some possible solutions. To explain these problems, I'll take the view that the agents deployed by AI developers (the ''principals'') in a multipolar scenario are moves in a game. The payoffs to a principal in this game depend on how the agents behave over time. We can talk about the equilibria of this game, and so on. Ideally, we would be able to make guarantees like this:  1. The payoffs resulting from the deployed agents' actions are optimal with     respect to some appropriate ""welfare function''. This welfare function would     encode some combination of total utility, fairness, and other social     desiderata;  2. The agents are in equilibrium --- that is, no principal has an incentive to     deploy an agent with a different design, given the agents deployed by the     other principals. The motivation for item 1 is clear: we want outcomes which are fair by each of the principals' lights. In particular, we want an outcome that the principals will all agree to. And item 2 is desirable because an equilibrium constitutes a self-enforcing contract; each agent wants to play their equilibrium strategy, if they believe that the other agents are playing the same equilibrium. Thus, given that the principals all say that they will deploy agents that satisfy 1 and 2, we could have some confidence that a welfare-optimal outcome will in fact obtain. Two simple but critical problems need to be addressed in order to make such guarantees: the equilibrium and prior selection problems. The equilibrium selection problem is that this deployment game will have many equilibria. Even if the principals agree on a welfare function, it is possible that many different profiles of agents optimize the same welfare function. So the principals need to coordinate on the profile of agents dep",2020,2022-01-30 04:51:07,2022-01-30 04:51:07,2020-12-12 14:54:27,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/HV7EIWIV/equilibrium-and-prior-selection-problems-in-multipolar-1.html,,CLR; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8QQEAZVI,blogPost,2018,"Althaus, David",Descriptive Population Ethics and Its Relevance for Cause Prioritization,Effective Altruism Forum,,,,https://forum.effectivealtruism.org/posts/CmNBmSf6xtMyYhvcs/descriptive-population-ethics-and-its-relevance-for-cause,"SUMMARY Descriptive ethics is the empirical study of people's values and ethical views, e.g. via a survey or questionnaire. This overview focuses on beliefs about population ethics and exchange rates between goods (e.g. happiness) and bads  (e.g. suffering). Two variables seem particularly important and action-guiding in this context, especially when trying to make informed choices about how to best shape the long-term future: 1) One’s normative goods-to-bads ratio  (N-ratio) and 2) one’s expected bads-to-goods ratio (E-ratio). I elaborate on how a framework consisting of these two variables could inform our decision-making with respect to shaping the long-term future, as well as facilitate cooperation among differing value systems and further moral reflection. I then present concrete ideas for further research in this area and investigate associated challenges. The last section lists resources which discuss further methodological and theoretical issues which were beyond the scope of the present text. DESCRIPTIVE ETHICS AND LONG-TERM FUTURE PRIORITIZATION Recently, some debate has emerged on whether reducing extinction risk is the ideal course of action for shaping the long-term future. For instance, in the  Global Priorities Institute (GPI) research agenda, Greaves & MacAskill (2017, p.13) ask “[...] whether it might be more important to ensure that future civilisation is good, assuming we don’t go extinct, than to ensure that future civilisation happens at all.” We could further ask to what extent we should focus our efforts on reducingrisks of astronomical suffering (s-risks). Again, Greaves & MacAskill: “Should we be more concerned about avoiding the worst possible outcomes for the future than we are for ensuring the very best outcomes occur [...]?” Given the enormous stakes, these are arguably some of the most important questions facing those who prioritize shaping the long-term future.1 Some interventions increase both the quality of future civilization as w",2018-04-03,2022-01-30 04:51:07,2022-01-30 04:51:07,2020-11-23 00:40:27,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/K9FMSZDC/descriptive-population-ethics-and-its-relevance-for-cause.html,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DC33WP3T,blogPost,2017,"Oesterheld, Caspar",Decision Theory and the Irrelevance of Impossible Outcomes,The Universe from an Intentional Stance,,,,https://casparoesterheld.com/2017/01/17/decision-theory-and-the-irrelevance-of-impossible-outcomes/,"(This post assumes some knowledge of the decision theory of Newcomb-like scenarios.) One problem in the decision theory of Newcomb-like scenarios (i.e. the study of whether causal, evidential or so…",2017-01-17,2022-01-30 04:51:07,2022-01-30 04:51:07,2020-11-23 00:56:44,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/F7GPMPMH/decision-theory-and-the-irrelevance-of-impossible-outcomes.html,,CLR; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6MTT7H2E,blogPost,2021,"Torges, Stefan",Coordination challenges for preventing AI conflict,Center on Long-Term Risk,,,,https://longtermrisk.org/coordination-challenges-for-preventing-ai-conflict/,"Summary In this article, I will sketch arguments for the following claims: Transformative AI scenarios involving multiple systems pose a unique existential risk: catastrophic bargaining failure between multiple AI systems (or joint AI-human systems). This risk is not sufficiently addressed by successfully aligning those systems, and we cannot safely delegate its solution to the AI systems themselves. Developers are better positioned than more far-sighted successor agents to coordinate in a way that solves this problem, but a solution also does not seem guaranteed. Developers intent on solving this problem can choose between developing separate but compatible systems that do not engage in costly conflict or building a single joint system. While the second option seems preferable from an altruistic perspective, […]",2021-03-09,2022-01-30 04:51:07,2022-01-30 04:51:07,2021-12-11 14:20:34,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/9645PS8Z/coordination-challenges-for-preventing-ai-conflict.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
I4AA6S2X,blogPost,2017,"Oesterheld, Caspar",Complications in evaluating neglectedness,The Universe from an Intentional Stance,,,,https://casparoesterheld.com/2017/06/25/complications-in-evaluating-neglectedness/,Neglectedness (or crowdedness) is a heuristic that effective altruists use to assess how much impact they could have in a specific cause area. It is usually combined with scale (a.k.a. importance) …,2017-06-25,2022-01-30 04:51:07,2022-01-30 04:51:07,2020-11-23 00:51:16,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/737I3Z2G/complications-in-evaluating-neglectedness.html,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CDIWGCQQ,blogPost,2021,"Clifton, Jesse",CLR's recent work on multi-agent systems,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/EzoCZjTdWTMgacKGS/clr-s-recent-work-on-multi-agent-systems,"INTRODUCTION We at the Center on Long-Term Risk (CLR) are focused on reducing risks of cosmically significant amounts of suffering[1], or s-risks, from transformative artificial intelligence (TAI). We currently believe that:  * Several agential s-risks — in which agents deliberately cause great amounts    of suffering — are among the largest s-risks in expectation.  * One of the most promising ways of addressing these risks involves intervening    in the design of TAI systems to make them more cooperative.  * Increasing the cooperativeness of powerful systems is robustly valuable for    everybody interested in shaping the long-run future. (See the recent     Cooperative AI and AI Research Considerations for Existential Safety (ARCHES)     research agendas for examples of work motivated by considerations other than    s-risks.) This was the subject of our research agenda on cooperative, conflict, and TAI. In this post, I’d like to give an overview of some of the research that CLR has been doing on multi-agent systems. I’ll then briefly remark on the importance, tractability, and neglectedness of this work (both from a downside-focused and more mainstream longtermist perspective), though a thorough discussion of prioritization is beyond the scope of this post. The main goal is to inform the community about what we’ve been up to recently in this space. We’re interested in supporting people who want to contribute to this work. If you might benefit from financial support to work on topics related to the research described here, you can fill out this form. If you’re interested in working with us as a temporary summer research fellow, full-time researcher, or research assistant, apply here. POTENTIAL CAUSES OF CONFLICT BETWEEN INTELLIGENT ACTORS It is possible that TAI systems will find themselves in conflict, despite the fact that conflict is often seemingly Pareto-inefficient. Factors that might lead intelligent agents to become engaged in conflict include:  1. Unce",2021-03-08,2022-01-30 04:51:07,2022-01-30 04:51:07,2021-11-14 18:22:51,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/5WVXDMTD/clr-s-recent-work-on-multi-agent-systems.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NWMXW7B8,blogPost,2016,"Tomasik, Brian",Do Artificial Reinforcement-Learning Agents Matter Morally?,Center on Long-Term Risk,,,,https://longtermrisk.org/do-artificial-reinforcement-learning-agents-matter-morally/,"Artificial reinforcement learning (RL), a widely used training method in computer science, has striking parallels to reward and punishment learning in biological brains. Plausible theories of consciousness imply a non-zero probability that RL agents qualify as sentient and deserve our moral consideration, especially as AI research advances and RL agents become more sophisticated.",2016-07-28,2022-01-30 04:51:07,2022-01-30 04:51:07,2020-11-23 01:03:14,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/KD663JCF/do-artificial-reinforcement-learning-agents-matter-morally.html,,CLR; TechSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
639338X6,blogPost,2015,"Tomasik, Brian",Differential Intellectual Progress as a Positive-Sum Project,Center on Long-Term Risk,,,,https://longtermrisk.org/differential-intellectual-progress-as-a-positive-sum-project/,"Fast technological development carries a risk of creating extremely powerful tools, especially AI, before society has a chance to figure out how best to use those tools in positive ways for many value systems. Suffering reducers may want to help mitigate the arms race for AI so that AI developers take fewer risks and have […]",2015-08-29,2022-01-30 04:51:07,2022-01-30 04:51:07,2020-11-23 01:07:52,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/WT7HXT5J/differential-intellectual-progress-as-a-positive-sum-project.html,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MQ7VHBIZ,blogPost,2017,"Treutlein, Johannes",Did EDT get it right all along? Introducing yet another medical Newcomb problem,LessWrong,,,,https://www.lesswrong.com/posts/iqpizeN4hkbTjkugo/did-edt-get-it-right-all-along-introducing-yet-another,"One of the main arguments given against Evidential Decision Theory (EDT) is that it would “one-box” in medical Newcomb problems. Whether this is the winning action has been a hotly debated issue on LessWrong. A majority, including experts in the area such as Eliezer Yudkowsky and Wei Dai, seem to think that one should two-box (See e.g. Yudkowsky 2010, p.67). Others have tried to argue  in favor of EDT by claiming that the winning action would be to one-box, or by offering reasons why EDT would in some cases two-box after all. In this blog post, I want to argue that EDT gets it right: one-boxing is the correct action in medical Newcomb problems. I introduce a new thought experiment, the Coin Flip Creation problem, in which I believe the winning move is to one-box. This new problem is structurally similar to other medical Newcomb problems such as the  Smoking Lesion, though it might elicit the intuition to one-box even in people who would two-box in some of the other problems. I discuss both how EDT and other decision theories would reason in the problem and why people’s intuitions might diverge in different formulations of medical Newcomb problems. TWO KINDS OF NEWCOMBLIKE PROBLEMS There are two different kinds of Newcomblike problems. In Newcomb’s original paradox, both EDT and Logical Decision Theories (LDT), such as Timeless Decision Theory (TDT) would one-box and therefore, unlike CDT, win $1 million. In medical Newcomb problems, EDT’s and LDT’s decisions diverge. This is because in the latter, a (physical) causal node that isn’t itself a decision algorithm influences both the current world state and our decisions – resulting in a correlation between action and environment but, unlike the original Newcomb, no “logical” causation. It’s often unclear exactly how a causal node can exert influence on our decisions. Does it change our decision theory, utility function, or the information available to us? In the case of the Smoking Lesion problem, it seems plausible",2017-01-24,2022-01-30 04:51:07,2022-01-30 04:51:07,2020-11-23 00:55:10,,,,,,,Did EDT get it right all along?,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/EJ9I6KHN/did-edt-get-it-right-all-along-introducing-yet-another.html,,CLR; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R7ITNVNC,blogPost,2020,"Leskela, Anni",Commitment and credibility in multipolar AI scenarios,LessWrong,,,,https://www.lesswrong.com/posts/LvtsFKxg2t3nWhKRq/commitment-and-credibility-in-multipolar-ai-scenarios,"The ability to make credible commitments is a key factor in many bargaining situations ranging from trade to international conflict. This post builds a taxonomy of the commitment mechanisms that transformative AI (TAI) systems could use in future multipolar scenarios, describes various issues they have in practice, and draws some tentative conclusions about the landscape of commitments we might expect in the future. INTRODUCTION A better understanding of the commitments that future AI systems could make is helpful for predicting and influencing the dynamics of multipolar scenarios. The option to credibly bind oneself to certain actions or strategies fundamentally changes the game theory behind bargaining, cooperation, and conflict. Credible commitments and general transparency can work to stabilize positive-sum agreements, and to increase the efficiency of threats (Schelling 1960), both of which could be relevant to how well TAI trajectories will reflect our values. Because human goals can be contradictory, and even broadly aligned AI systems could come to prioritize different outcomes depending on their domains and histories, these systems could end up in competitive situations and bargaining failures where a lot of value is lost. Similarly, if some systems in a multipolar scenario are well aligned and others less so, some worst cases might be avoidable if stable peaceful agreements can be reached. As an example of the practical significance of commitment ability in stabilizing peaceful strategies, standard theories in international relations hold that conflicts between nations are difficult to avoid indefinitely primarily because there are no reliable commitment mechanisms for peaceful agreements (e.g. Powell 2004, Lake 1999, Rosato 2015), even when nations would overall prefer them. In addition to the direct costs of conflict, the lack of enforceable commitments leads to continuous resource loss from arms races, monitoring, and other preparations for possible",2020,2022-01-30 04:51:07,2022-01-30 04:51:07,2020-12-12 15:04:18,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/WFUHVWSK/commitment-and-credibility-in-multipolar-ai-scenarios.html,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
N6AANRRU,blogPost,2021,"Clifton, Jesse",Collaborative game specification: arriving at common models in bargaining,Center on Long-Term Risk,,,,https://longtermrisk.org/collaborative-game-specification/,"Conflict is often an inefficient outcome to a bargaining problem. This is true in the sense that, for a given game-theoretic model of a strategic interaction, there is often some equilibrium in which all agents are better off than the conflict outcome. But real-world agents may not make decisions according to game-theoretic models, and when they do, they may use different models. This makes it more difficult to guarantee that real-world agents will avoid bargaining failure than is suggested by the observation that conflict is often inefficient.   In another post, I described the ""prior selection problem"", on which different agents having different models of their situation can lead to bargaining failure. Moreover, techniques for addressing bargaining problems like coordination on […]",2021-03-06,2022-01-30 04:51:07,2022-01-30 04:51:07,2021-10-31 16:55:52,,,,,,,Collaborative game specification,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/HJ3TRAF5/collaborative-game-specification.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UTZBKAUW,blogPost,2018,"Gloor, Lukas",Cause prioritization for downside-focused value systems,Effective Altruism Forum,,,,https://forum.effectivealtruism.org/posts/225Aq4P4jFPoWBrb5/cause-prioritization-for-downside-focused-value-systems,"Last edited: August 27th 2019.  This post outlines my thinking on cause prioritization from the perspective of value systems whose primary concern is reducing disvalue. I’m mainly thinking of  suffering-focused ethics (SFE), but I also want to include moral views that attribute substantial disvalue to things other than suffering, such as inequality or preference violation. I will limit the discussion to interventions targeted at improving the long-term future (see the reasons in section II). I hope my post will also be informative for people who do not share a downside-focused outlook, as thinking about cause prioritization from different perspectives, with emphasis on considerations other than those one is used to, can be illuminating. Moreover, understanding the strategic considerations for plausible moral views is essential for acting under moral uncertainty and cooperating with people with other values. I will talk about the following topics:  * Which views qualify as downside-focused (given our empirical situation)  * Why downside-focused views prioritize s-risk reduction over utopia creation  * Why extinction risk reduction is unlikely to be a promising intervention    according to downside-focused views  * Why AI alignment is probably positive for downside-focused views, and    especially positive if done with certain precautions  * What to include in an EA portfolio that incorporates population ethical    uncertainty and cooperation between value systems WHICH VIEWS QUALIFY AS DOWNSIDE-FOCUSED? I’m using the term downside-focused to refer to value systems that in practice (given what we know about the world) primarily recommend working on interventions that make bad things less likely.[1] For example, if one holds that what is most important is how things turn out for individuals (welfarist consequentialism), and that it is comparatively unimportant to add well-off beings to the world, then one should likely focus on preventing suffering.[2] That would b",2018-01-31,2022-01-30 04:51:07,2022-01-30 04:51:07,2020-11-23 00:44:01,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/DI58FZJA/cause-prioritization-for-downside-focused-value-systems.html,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2EIBJKHU,blogPost,2021,Jia,Case studies of self-governance to reduce technology risk,Effective Altruism Forum,,,,https://forum.effectivealtruism.org/posts/Xf6QE6txgvfCGvZpk/case-studies-of-self-governance-to-reduce-technology-risk,"I worked on this research project during my summer fellowship at the Center on Long-Term Risk. Though the findings aren't particularly insightful, I’m posting this unpolished version to:  * Hopefully help other people attempting similar projects save time and effort  * Demonstrate one approach to case study selection  * Give others a sense of what one type of AI governance summer research project    might look like. SUMMARY  * Self-governance occurs when private actors coordinate to address issues that    are not obviously related to profit, with minimal involvement from    governments and standards bodies.  * Historical cases of self-governance to reduce technology risk are rare. I    find 6 cases that seem somewhat similar to AI development, including the    actions of Leo Szilard and other physicists in 1939 and the 1975 Asilomar    conference.  * The following factors seem to make self-governance efforts more likely to    occur: * Risks are salient     * The government looks like it might step in if private actors do       nothing     * The field or industry is small     * Support from gatekeepers (like journals and large consumer-facing       firms)     * Support from credentialed scientists.          * After the initial self-governance effort, governments usually step in to    develop and codify rules.  * My biggest takeaway is probably that self-governance efforts seem more likely    to occur when risks are somewhat prominent. As a result, we could do more to     connect “near-term” issues like data privacy and algorithmic bias with    “long-term” concerns. We could try to preemptively identify “fire alarms” for    TAI, and be ready to take advantage of these warning signals if they occur. INTRODUCTION Private actors play an important role in AI governance. Several companies have released their own guidelines, and the Partnership on AI is a notable actor in the space.[1] In other words, we are beginning to see elements of self-governance in the AI industry",2021-04-06,2022-01-30 04:51:06,2022-01-30 04:51:06,2021-11-14 19:00:58,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/SHI8RDJ9/case-studies-of-self-governance-to-reduce-technology-risk.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
H57NI5W3,blogPost,2021,"Kokotajlo, Daniel","Birds, Brains, Planes, and AI: Against Appeals to the Complexity/Mysteriousness/Efficiency of the Brain",AI Alignment Forum,,,,https://www.alignmentforum.org/posts/HhWhaSzQr6xmBki8F/birds-brains-planes-and-ai-against-appeals-to-the-complexity,"[Epistemic status: Strong opinions lightly held, this time with a cool graph.] I argue that an entire class of common arguments against short timelines is bogus, and provide weak evidence that anchoring to the human-brain-human-lifetime milestone is reasonable. In a sentence, my argument is that the complexity and mysteriousness and efficiency of the human brain (compared to artificial neural nets) is almost zero evidence that building TAI will be difficult, because evolution typically makes things complex and mysterious and efficient, even when there are simple, easily understood, inefficient designs that work almost as well (or even better!) for human purposes. In slogan form: If all we had to do to get TAI was make a simple neural net 10x the size of my brain, my brain would still look the way it does. The case of birds & planes illustrates this point nicely. Moreover, it is also a precedent for several other short-timelines talking points, such as the human-brain-human-lifetime (HBHL) anchor. PLAN:  1. Illustrative Analogy  2. Exciting Graph  3. Analysis 1. Extra brute force can make the problem a lot easier      2. Evolution produces complex mysterious efficient designs by         default, even when simple inefficient designs work just fine for human         purposes.      3. What’s bogus and what’s not      4. Example: Data-efficiency            4. Conclusion  5. Appendix 1909 French military plane, the Antionette VII. By Deep silence (Mikaël Restoux) - Own work (Bourget museum, in France), CC BY 2.5, https://commons.wikimedia.org/w/index.php?curid=1615429 ILLUSTRATIVE ANALOGY AI timelines, from our current perspectiveFlying machine timelines, from the perspective of the late 1800’s:Shorty: Human brains are giant neural nets. This is reason to think we can make human-level AGI (or at least AI with  strategically relevant skills, like politics and science) by making giant neural nets.Shorty: Birds are winged creatures that paddle through the air. This i",2021,2022-01-30 04:51:06,2022-01-30 04:51:06,2021-11-13 21:52:57,,,,,,,"Birds, Brains, Planes, and AI",,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/GNFCNKHK/birds-planes-brains-and-ai-against-appeals-to-the-complexity.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5XCN8E2T,blogPost,2017,"Treutlein, Johannes",“Betting on the Past” by Arif Ahmed,The Universe from an Intentional Stance,,,,https://casparoesterheld.com/2017/02/06/betting-on-the-past-by-arif-ahmed/,"[This post assumes knowledge of decision theory, as discussed in Eliezer Yudkowsky’s Timeless Decision Theory and in Arbital’s Introduction to Logical Decision Theory.] I recently discovered an int…",2017-02-06,2022-01-30 04:51:06,2022-01-30 04:51:06,2020-11-23 00:54:29,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/UBVFK75X/betting-on-the-past-by-arif-ahmed.html,,CLR; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5GGXJ663,blogPost,2017,"Oesterheld, Caspar",Are causal decision theorists trying to outsmart conditional probabilities?,LessWrong,,,,https://www.lesswrong.com/posts/cyJgdhgYaM2CbZ7tP/are-causal-decision-theorists-trying-to-outsmart-conditional,"Presumably, this has been discussed somewhere in the past, but I wonder to which extent causal decision theorists (and many other non-evidential decision theorists, too) are trying to make better predictions than (what they think to be) their own conditional probabilities. To state this question more clearly, let’s look at the generic Newcomb-like problem with two actions a1 and a2 (e.g., one-boxing and two-boxing, cooperating or defecting, not smoking or smoking) and two states s1 and s2 (specifying, e.g., whether there is money in both boxes, whether the other agent cooperates, whether one has cancer). The Newcomb-ness is the result of two properties:  * No matter the state, it is better to take action a2, i.e. u(a2,s1)>u(a1,s1)    and u(a2,s2)>u(a1,s2). (There are also problems without dominance where CDT    and EDT nonetheless disagree. For simplicity I will assume dominance, here.)          * The action cannot causally affect the state, but somehow taking a1 gives us    evidence that we’re in the preferable state s1. That is, P(s1|a1)>P(s1|a2)    and u(a1,s1)>u(a2,s2).         Then, if the latter two differences are large enough, it may be that E[u|a1] > E[u|a2]. I.e. P(s1|a1) * u(s1,a1) + P(s2|a1) * u(s2,a1) > P(s1|a2) * u(s1,a2) + P(s2|a2) * u(s2,a2), despite the dominance. Now, my question is: After having taken one of the two actions, say a1, but before having observed the state, do causal decision theorists really assign the probability P(s1|a1) (specified in the problem description) to being in state s1? I used to think that this was the case. E.g., the way I learned about Newcomb’s problem is that causal decision theorists understand that, once they have said the words “both boxes for me, please”, they assign very low probability to getting the million. So, if there were a period between saying those words and receiving the payoff, they would bet at odds that reveal that they assign a low probability (namely P(s1,a2)) to money being under",2017-05-16,2022-01-30 04:51:06,2022-01-30 04:51:06,2020-11-23 00:52:29,,,,,,,Are causal decision theorists trying to outsmart conditional probabilities?,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/R7Z7P6H5/are-causal-decision-theorists-trying-to-outsmart-conditional.html,,CLR; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PRZR88VR,blogPost,2017,"Treutlein, Johannes",Anthropic uncertainty in the Evidential Blackmail,The Universe from an Intentional Stance,,,,https://casparoesterheld.com/2017/05/12/anthropic-uncertainty-in-the-evidential-blackmail/,"I’m currently writing a piece on anthropic uncertainty in Newcomb problems. The idea is that whenever someone simulates us to predict our actions, this leads us to have anthropic uncertainty about …",2017-05-12,2022-01-30 04:51:06,2022-01-30 04:51:06,2020-11-23 00:51:55,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/X3JHA4D9/anthropic-uncertainty-in-the-evidential-blackmail.html,,CLR; TechSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6WFI4PND,blogPost,2021,"Lyzhov, Alex","""AI and Compute"" trend isn't predictive of what is happening",AI Alignment Forum,,,,https://www.alignmentforum.org/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening,"(open in a new tab to view at higher resolution) In May 2018 (almost 3 years ago) OpenAI published their ""AI and Compute""  blogpost where they highlighted the trend of increasing compute spending on training the largest AI models and speculated that the trend might continue into the future. This note is aimed to show that the trend has ended right around the moment of OpenAI publishing their post and doesn't hold up anymore. On the above image, I superimposed the scatter plot from OpenAI blogpost and my estimates of compute required for some recent large and ambitious ML experiments. To the best of my knowledge (and I have tried to check for this), there haven't been any experiments that required more compute than those shown on the plot. The main thing shown here is that less than one doubling of computational resources for the largest training occured in the 3-year period between 2018 and 2021, compared to around 10 doublings in the 3-year period between 2015 and 2018. This seems to correspond to a severe slowdown of computational scaling. To stay on the trend line, we currently would need an experiment requiring roughly around 100 times more compute than GPT-3. Considering that GPT-3 may have costed between $5M and $12M and accelerators haven't vastly improved since then, such an experiment would now likely cost $0.2B - $1.5B.",2021-04-01,2022-01-30 04:51:06,2022-01-30 04:51:06,2021-12-11 14:10:12,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/3RKNJGBW/ai-and-compute-trend-isn-t-predictive-of-what-is-happening.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
78R2ACU4,blogPost,2020,"Kokotajlo, Daniel",Against GDP as a metric for timelines and takeoff speeds,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/aFaKhG86tTrKvtAnT/against-gdp-as-a-metric-for-timelines-and-takeoff-speeds,"OR: WHY AI TAKEOVER MIGHT HAPPEN BEFORE GDP ACCELERATES, AND OTHER THOUGHTS ON WHAT MATTERS FOR TIMELINES AND TAKEOFF SPEEDS [Epistemic status: Strong opinion, lightly held] I think world GDP (and economic growth more generally) is overrated as a metric for AI timelines and takeoff speeds. Here are some uses of GDP that I disagree with, or at least think should be accompanied by cautionary notes:  * Timelines: Ajeya Cotra thinks of transformative AI as “software which causes    a tenfold acceleration in the rate of growth of the world economy (assuming    that it is used everywhere that it would be economically profitable to use    it).” I don’t mean to single her out in particular; this seems like the    standard definition now. And I think it's much better than one prominent    alternative, which is to date your AI timelines to the first time world GDP    (GWP) doubles in a year!  * Takeoff Speeds: Paul Christiano argues for Slow Takeoff. He thinks we can use    GDP growth rates as a proxy for takeoff speeds. In particular, he thinks Slow    Takeoff ~= GWP doubles in 4 years before the start of the first 1-year GWP    doubling. This proxy/definition has received a lot of uptake.  * Timelines: David Roodman’s excellent model projects GWP hitting infinity in    median 2047, which I calculate means TAI in median 2037. To be clear, he    would probably agree that we shouldn’t use these projections to forecast TAI,    but I wish to add additional reasons for caution.  * Timelines: I’ve sometimes heard things like this: “GWP growth is stagnating    over the past century or so; hyperbolic progress has ended; therefore TAI is    very unlikely.”  * Takeoff Speeds: Various people have said things like this to me: “If you    think there’s a 50% chance of TAI by 2032, then surely you must think there’s    close to a 50% chance of GWP growing by 8% per year by 2025, since TAI is    going to make growth rates go much higher than that, and progress is    typically continuous.",2020-12-29,2022-01-30 04:51:06,2022-01-30 04:51:06,2021-12-11 14:14:38,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/NUWWJRAQ/against-gdp-as-a-metric-for-timelines-and-takeoff-speeds.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
E9GVD3BD,blogPost,2020,"Leong, Chris",Embedded vs. External Decision Problems,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/br7KRSeNymwSvZnf5/embedded-vs-external-decision-problems,,2020-03-04,2022-01-30 04:49:30,2022-01-30 04:49:30,2020-08-18 20:51:01,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/E9UCKBGB/embedded-vs-external-decision-problems.html,,TechSafety; AI-Safety-Camp; AISRP2019,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VM3QWB8S,blogPost,2021,"Koch, Jack; Langosco, Lauro",Discussion: Objective Robustness and Inner Alignment Terminology,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/pDaxobbB9FG5Dvqyv/discussion-objective-robustness-and-inner-alignment,"In the alignment community, there seem to be two main ways to frame and define objective robustness and inner alignment. They are quite similar, mainly differing in the manner in which they focus on the same basic underlying problem. We’ll call these the objective-focused approach and the generalization-focused approach. We don’t delve into these issues of framing the problem in Empirical Observations of Objective Robustness Failures, where we present empirical observations of objective robustness failures. Instead, we think it is worth having a separate discussion of the matter. These issues have been mentioned only infrequently in a few comments on the Alignment Forum, so it seemed worthwhile to write a post describing the framings and their differences in an effort to promote further discussion in the community. TL;DR This post compares two different paradigmatic approaches to objective robustness/inner alignment: Objective-focused approach  * Emphasis: “How do we ensure our models/agents have the right    (mesa-)objectives?”  * Outer alignment: “an objective function r is outer aligned if all models that    perform optimally on r in the limit of perfect training and infinite data are    intent aligned.” * Outer alignment is a property of the training objective.         Generalization-focused approach  * Emphasis: “How will this model/agent generalize out-of-distribution?” *        Considering a model’s “objectives” or “goals,” whether behavioral or       internal, is instrumentally useful for predicting OOD behavior, but what       you ultimately care about is whether it generalizes “acceptably.”          * Outer alignment: a model is outer aligned if it performs desirably on the    training distribution. * Outer alignment is a property of the tuple (training       objective, training data, training setup, model).         Special thanks to Rohin Shah, Evan Hubinger, Edouard Harris, Adam Shimi, and Adam Gleave for their helpful feedback on drafts of this po",2021-06-23,2022-01-30 04:49:30,2022-01-30 04:49:30,2021-10-30 18:03:29,,,,,,,Discussion,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/X74XW5JR/discussion-objective-robustness-and-inner-alignment.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NWMVFW46,blogPost,2020,"Chiswick, Max; Makiievskyi, Anton; Zhou, Liang",Assessing Generalization in Reward Learning: Intro and Background,Towards Data Science (Medium),,,,https://towardsdatascience.com/assessing-generalization-in-reward-learning-intro-and-background-da6c99d9e48,"An overview of reinforcement learning, generalization, and reward learning",2020-11-20,2022-01-30 04:49:30,2022-01-30 04:49:30,2020-11-21 18:11:23,,,,,,,Assessing Generalization in Reward Learning,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/WCTVNGGW/assessing-generalization-in-reward-learning-intro-and-background-da6c99d9e48.html,,TechSafety; AI-Safety-Camp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XMHWUMQC,blogPost,2020,"Kosch, Sebastian",AISC4: Research Summaries,AI Safety Camp,,,,https://aisafety.camp/2020/05/30/aisc4-research-summaries/,"The fourth AI Safety Camp took place in May 2020 in Toronto. Due to COVID-19, the camp was held virtually. Six teams participated and worked on the following topics: Survey on AI risk scenarios Opt…",2020-05-30,2022-01-30 04:49:30,2022-01-30 04:49:30,2020-11-21 19:14:38,,,,,,,AISC4,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/N39D7ZWI/aisc4-research-summaries.html,,TechSafety; AI-Safety-Camp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
F4NQ9ZES,blogPost,2019,"Kovarik, Vojta",AI Safety Debate and Its Applications,LessWrong,,,,https://www.lesswrong.com/posts/5Kv2qNfRyXXihNrx2/ai-safety-debate-and-its-applications,"All of the experimental work and some of the theoretical work has been done jointly with Anna Gajdova, David Lindner, Lukas Finnveden, and Rajashree Agrawal as part of the third AI Safety Camp. We are grateful to Ryan Carey and Geoffrey Irving for the advice regarding this project. The remainder of the theoretical part relates to my stay at FHI, and I would like to thank the above people, Owain Evans, Michael Dennis, Ethan Perez, Stuart Armstrong, and Max Daniel for comments/discussions. -------------------------------------------------------------------------------- Debate is a recent proposal for AI alignment, which naturally incorporates elicitation of human preferences and has the potential to offload the costly search for flaws in an AI’s suggestions onto the AI. After briefly recalling the intuition behind debate, we list the main open problems surrounding it and summarize how the existing work on debate addresses them. Afterward, we describe, and distinguish between, Debate games and their different applications in more detail. We also formalize what it means for a debate to be truth-promoting. Finally, we present results of our experiments on Debate games and Training via Debate on MNIST and fashion MNIST. DEBATE GAMES AND WHY THEY ARE USEFUL Consider an answer A to some question Q --- for example, ""Where should I go for a vacation?"" and ""Alaska"". Rather than directly verifying whether A is an accurate answer to Q, it might be easier to first decompose A into lower-level components (How far/expensive is it? Do they have nice beaches? What is the average temperature? What language do they speak?). Moreover, it isn't completely clear what to do even if we know the relevant facts --- indeed, how does Alaska's cold weather translate to a preference for Alaska from 0 to 10? And how does this preference compare to English being spoken in Alaska? As an alternative, we can hold a debate between two competing answers A and A′=""Bali"" to Q. This allows strategic de",2019-07-23,2022-01-30 04:49:29,2022-01-30 04:49:29,2019-12-16 03:27:22,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/R59PFVJB/ai-safety-debate-and-its-applications.html,,TechSafety; AI-Safety-Camp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CMQECS38,blogPost,2021,"Smith, Ben; Pihlakas, Roland; Klassert, Robert",A brief review of the reasons multi-objective RL could be important in AI Safety Research,LessWrong,,,,https://www.lesswrong.com/posts/i5dLfi6m6FCexReK9/a-brief-review-of-the-reasons-multi-objective-rl-could-be,"By Ben Smith, Roland Pihlakas, and Robert Klassert Thanks to Linda Linsefors, Alex Turner, Richard Ngo, Peter Vamplew, JJ Hepburn, Tan Zhi-Xuan, Remmelt Ellen, Kaj Sotala, Koen Holtman, and Søren Elverlin for their time and kind remarks in reviewing this essay. Thanks to the organisers of the AI Safety Camp for incubating this project from its inception and for connecting our team. For the last 9 months, we have been investigating the case for a multi-objective approach to reinforcement learning in AI Safety. Based on our work so far, we’re moderately convinced that multi-objective reinforcement learning should be explored as a useful way to help us understand ways in which we can achieve safe superintelligence. We’re writing this post to explain why, to inform readers of the work we and our colleagues are doing in this area, and invite critical feedback about our approach and about multi-objective RL in general. We were first attracted to the multi-objective space because human values are inherently multi-objective--in any number of frames: deontological, utilitarian, and virtue ethics; egotistical vs. moral objectives; maximizing life values including hedonistic pleasure, eudaemonic meaning, or the enjoyment of power and status. AGI systems aiming to solve for human values are likely to be multi-objective themselves, if not by explicit design, then multi-objective systems would emerge from learning about human preferences. As a first pass at technical research in this area, we took a commonly-used example, the “BreakableBottles” problem, and showed that for low-impact AI, an agent could more quickly solve this toy problem if it uses a conservative but flexible trade-off between alignment and performance values, compared to using a thresholded alignment system to maximize a certain amount of alignment and only then maximizing on performance. Such tradeoffs will be critical for understanding the conflicts between more abstract human objectives a human-preferen",2021-09-29,2022-01-30 04:49:29,2022-01-30 04:49:29,2021-10-30 18:07:11,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/VPK7QUUN/a-brief-review-of-the-reasons-multi-objective-rl-could-be.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BP895WND,blogPost,2016,AI Impacts,What if you turned the world’s hardware into AI minds?,AI Impacts,,,,https://aiimpacts.org/what-if-you-turned-the-worlds-hardware-into-ai-minds/,"In a classic 'AI takes over the world' scenario, one of the first things an emerging superintelligence wants to do is steal most of the world's computing hardware and repurpose it to running the AI's own software. This step takes one from 'super-proficient hacker' levels of smart to 'my brain is one of the main things happening on Planet Earth' levels of smart. There is quite a...",2016-09-04,2022-01-30 04:49:21,2022-01-30 04:49:21,2020-12-13 19:54:15,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Blog,,/Users/jacquesthibodeau/Zotero/storage/BMDFBIQX/what-if-you-turned-the-worlds-hardware-into-ai-minds.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZBUX2G49,blogPost,2021,"Kokotajlo, Daniel",What 2026 looks like,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/6Xgy6CAf2jqHhynHL/what-2026-looks-like,"This was written for the Vignettes Workshop.[1] The goal is to write out a  detailed future history (“trajectory”) that is as realistic (to me) as I can currently manage, i.e. I’m not aware of any alternative trajectory that is similarly detailed and clearly more plausible to me. The methodology is roughly: Write a future history of 2022. Condition on it, and write a future history of 2023. Repeat for 2024, 2025, etc. (I'm posting 2022-2026 now so I can get feedback that will help me write 2027+. I intend to keep writing until the story reaches singularity/extinction/utopia/etc.) What’s the point of doing this? Well, there are a couple of reasons:  * Sometimes attempting to write down a concrete example causes you to learn    things, e.g. that a possibility is more or less plausible than you thought.  * Most serious conversation about the future takes place at a high level of    abstraction, talking about e.g. GDP acceleration, timelines until TAI is    affordable, multipolar vs. unipolar takeoff… vignettes are a neglected    complementary approach worth exploring.  * Most stories are written backwards. The author begins with some idea of how    it will end, and arranges the story to achieve that ending. Reality, by    contrast, proceeds from past to future. It isn’t trying to entertain anyone    or prove a point in an argument.  * Anecdotally, various people seem to have found Paul Christiano’s “tales of    doom” stories helpful, and relative to typical discussions those stories are    quite close to what we want. (I still think a bit more detail would be good —    e.g. Paul’s stories don’t give dates, or durations, or any numbers at all    really.)[2]  * “I want someone to ... write a trajectory for how AI goes down, that is    really specific about what the world GDP is in every one of the years from    now until insane intelligence explosion. And just write down what the world    is like in each of those years because I don't know how to write an    internally",2021-08-06,2022-01-30 04:49:21,2022-01-30 04:49:21,2021-11-18 23:05:44,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/3DAS3TNK/what-2026-looks-like-daniel-s-median-future.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WFC7D3BN,blogPost,2019,"Walsh, Toby",Walsh 2017 survey,AI Impacts,,,,https://aiimpacts.org/walsh-2017-survey/,"Toby Walsh surveyed hundreds of experts and non-experts in 2016 and found their median estimates for ‘when a computer might be able to carry out most human professions at least as well as a typical human’ were as follows: Probability of HLMIGroup of survey respondentsAI expertsRobotics expertsNon-experts10%20352033202650%20612065203990%210921182060 Details Toby Walsh, professor of AI at the...",2019-12-24,2022-01-30 04:49:21,2022-01-30 04:49:21,2020-11-14 03:21:35,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: AI Timeline Surveys,,/Users/jacquesthibodeau/Zotero/storage/77WDNJVZ/walsh-2017-survey.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KSMW4U3T,blogPost,2020,AI Impacts,Time for AI to cross the human range in English draughts,AI Impacts,,,,https://aiimpacts.org/time-for-ai-to-cross-the-human-range-in-english-draughts/,AI took over 30 years to go from beginner level to superhuman level,2020-10-26,2022-01-30 04:49:21,2022-01-30 04:49:21,2020-11-21 20:34:50,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Range of Human Performance,,/Users/jacquesthibodeau/Zotero/storage/367ECIGF/time-for-ai-to-cross-the-human-range-in-english-draughts.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
794VTKRN,blogPost,2020,AI Impacts,Time for AI to cross the human performance range in Go,AI Impacts,,,,https://aiimpacts.org/time-for-ai-to-cross-the-human-performance-range-in-go/,Computer Go took over 30 years to go from beginner level to superhuman.,2020-10-15,2022-01-30 04:49:21,2022-01-30 04:49:21,2020-11-21 20:37:18,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Range of Human Performance,,/Users/jacquesthibodeau/Zotero/storage/ETACUV59/time-for-ai-to-cross-the-human-performance-range-in-go.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BR5ED5D2,blogPost,2020,AI Impacts,Time for AI to cross the human performance range in chess,AI Impacts,,,,https://aiimpacts.org/time-for-ai-to-cross-the-human-performance-range-in-chess/,Computer chess took around 50 years to go from beginner level to superhuman level.,2020-10-15,2022-01-30 04:49:21,2022-01-30 04:49:21,2020-11-21 20:37:56,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Range of Human Performance,,/Users/jacquesthibodeau/Zotero/storage/4CIAV85I/time-for-ai-to-cross-the-human-performance-range-in-chess.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EDIFICXQ,blogPost,2020,"Bergal, Asya",Takeaways from safety by default interviews,AI Impacts,,,,https://aiimpacts.org/takeaways-from-safety-by-default-interviews/,"Last year, several researchers at AI Impacts (primarily Robert Long and I) interviewed prominent researchers inside and outside of the AI safety field who are relatively optimistic about advanced AI being developed safely. These interviews were originally intended to focus narrowly on reasons for optimism, but we ended up covering a variety of topics, including AGI timelines, the likelihood of current techniques leading to AGI, and what the right things to do in AI safety are right now. (...)",2020-04-03,2022-01-30 04:49:21,2022-01-30 04:49:21,2020-09-05 18:07:33,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Blog,,/Users/jacquesthibodeau/Zotero/storage/H79BNVC8/takeaways-from-safety-by-default-interviews.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NJQ2C7UT,blogPost,2020,"Kokotajlo, Daniel",Relevant pre-AGI possibilities,AI Impacts,,,,https://aiimpacts.org/relevant-pre-agi-possibilities/,Brainstorm of ways the world could be relevantly different by the time advanced AGI arrives,2020-06-19,2022-01-30 04:49:21,2022-01-30 04:49:21,2020-08-31 18:08:43,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Blog,,/Users/jacquesthibodeau/Zotero/storage/XG932AG6/relevant-pre-agi-possibilities.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZGB57MGS,blogPost,2018,"Garfinkel, Ben",Reinterpreting “AI and Compute”,AI Impacts,,,,https://aiimpacts.org/reinterpreting-ai-and-compute/,"This is a guest post by Ben Garfinkel. We revised it slightly, at his request, on February 9, 2019. A recent OpenAI blog post, “AI and Compute,” showed that the amount of computing power consumed by the most computationally intensive machine learning projects has been doubling every three months. The post presents this trend as...",2018-12-18,2022-01-30 04:49:21,2022-01-30 04:49:21,2020-11-14 03:34:13,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Blog,,/Users/jacquesthibodeau/Zotero/storage/BXV9445T/reinterpreting-ai-and-compute.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
I2IG8DRB,blogPost,2020,"Bergal, Asya",Trends in DRAM price per gigabyte,AI Impacts,,,,https://aiimpacts.org/trends-in-dram-price-per-gigabyte/,"The price of a gigabyte of DRAM has fallen by about a factor of ten every 5 years from 1957 to 2020. Since 2010, the price has fallen much more slowly, at a rate that would yield an order of magnitude over roughly 14 years. Details Background DRAM, “dynamic random-access memory”, is a type of...",2020-04-14,2022-01-30 04:49:21,2022-01-30 04:49:21,2020-09-05 17:11:39,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Hardware and AI Timelines,,/Users/jacquesthibodeau/Zotero/storage/PI98VB6J/trends-in-dram-price-per-gigabyte.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BU7J7CAX,blogPost,2020,AI Impacts,Time for AI to cross the human range in StarCraft,AI Impacts,,,,https://aiimpacts.org/time-for-ai-to-cross-the-human-range-in-starcraft/,AI took about twenty years to go from beginner level to high professional level.,2020-10-20,2022-01-30 04:49:21,2022-01-30 04:49:21,2020-11-21 20:36:03,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Range of Human Performance,,/Users/jacquesthibodeau/Zotero/storage/EFWDD55K/time-for-ai-to-cross-the-human-range-in-starcraft.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
22TF52ND,blogPost,2018,"Johnson, Aysja",Time for AI to cross the human performance range in diabetic retinopathy,AI Impacts,,,,https://aiimpacts.org/diabetic-retinopathy-as-a-case-study-in-time-for-ai-to-cross-the-range-of-human-performance/,"In diabetic retinopathy, automated systems started out just below expert human level performance, and took around ten years to reach expert human level performance. Details Diabetic retinopathy is a complication of diabetes in which the back of the eye is damaged by high blood sugar levels. It is the most common cause of blindness among working-age...",2018-11-21,2022-01-30 04:49:21,2022-01-30 04:49:21,2020-11-14 03:21:47,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Range of Human Performance,,/Users/jacquesthibodeau/Zotero/storage/M8IRAET5/diabetic-retinopathy-as-a-case-study-in-time-for-ai-to-cross-the-range-of-human-performance.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QX5FAHWI,blogPost,2019,"McCaslin, Tegan",Primates vs birds: Is one brain architecture better than the other?,AI Impacts,,,,https://aiimpacts.org/primates-vs-birds-is-one-brain-architecture-better-than-the-other/,"The boring answer to that question is, “Yes, birds.” But that’s only because birds can pack more neurons into a walnut-sized brain than a monkey with a brain four times that size. So let’s forget about brain volume for a second and ask the really interesting question: neuron per neuron, who’s coming out ahead? You...",2019-02-28,2022-01-30 04:49:20,2022-01-30 04:49:20,2020-11-14 03:21:40,,,,,,,Primates vs birds,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Blog,,/Users/jacquesthibodeau/Zotero/storage/FZQSAPHU/primates-vs-birds-is-one-brain-architecture-better-than-the-other.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MTAD8Z45,blogPost,2020,"Grace, Katja",Misalignment and misuse: whose values are manifest?,AI Impacts,,,,https://aiimpacts.org/misalignment-and-misuse-whose-values-are-manifest/,Are misalignment and misuse helpful catastrophe categories?,2020-11-18,2022-01-30 04:49:20,2022-01-30 04:49:20,2020-11-21 20:30:10,,,,,,,Misalignment and misuse,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Blog,,/Users/jacquesthibodeau/Zotero/storage/ZHKEVBHX/misalignment-and-misuse-whose-values-are-manifest.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RXFNQ8B3,blogPost,2018,"Carey, Ryan",Interpreting AI compute trends,AI Impacts,,,,https://aiimpacts.org/interpreting-ai-compute-trends/,"This is a guest post by Ryan Carey. Over the last few years, we know that AI experiments have used much more computation than previously. But just last month, an investigation by OpenAI made some initial estimates of just how fast this growth has been. Comparing AlphaGo Zero to AlexNet, they found that the largest...",2018-07-10,2022-01-30 04:49:20,2022-01-30 04:49:20,2020-11-14 03:21:51,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: 0000007  Section: Blog,,/Users/jacquesthibodeau/Zotero/storage/ZSPZMGTB/interpreting-ai-compute-trends.html; /Users/jacquesthibodeau/Zotero/storage/9RR92JIT/interpreting-ai-compute-trends.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EAXRN4ZU,blogPost,2019,"Etzioni, Oren",Etzioni 2016 survey,AI Impacts,,,,https://aiimpacts.org/etzioni-2016-survey/,"Oren Etzioni surveyed 193 AAAI fellows in 2016 and found that 67% of them expected that 'we will achieve Superintelligence' someday, but in more than 25 years. Details Oren Etzioni, CEO of the Allen Institute for AI, reported on a survey in an MIT Tech Review article published on 20 Sep 2016. The rest of...",2019-11-06,2022-01-30 04:49:20,2022-01-30 04:49:20,2020-11-14 03:22:04,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: AI Timeline Surveys,,/Users/jacquesthibodeau/Zotero/storage/K8BQJN6K/etzioni-2016-survey.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HTHTXZPA,blogPost,2016,AI Impacts,Error in Armstrong and Sotala 2012,AI Impacts,,,,https://aiimpacts.org/error-in-armstrong-and-sotala-2012/,"Can AI researchers say anything useful about when strong AI will arrive? Back in 2012, Stuart Armstrong and Kaj Sotala weighed in on this question in a paper called 'How We're Predicting AI—or Failing To'. They looked at a dataset of predictions about AI timelines, and concluded that predictions made by AI experts were indistinguishable from those of non-experts. (Which might suggest...",2016-05-17,2022-01-30 04:49:20,2022-01-30 04:49:20,2020-12-13 19:54:50,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Blog,,/Users/jacquesthibodeau/Zotero/storage/V9RNPU92/error-in-armstrong-and-sotala-2012.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
U8ZTEBVU,blogPost,2020,"Kokotajlo, Daniel","Cortés, Pizarro, and Afonso as precedents for takeover",AI Impacts,,,,https://aiimpacts.org/cortes-pizarro-and-afonso-as-precedents-for-ai-takeover/,"Epistemic status: I am not a historian, nor have I investigated these case studies in detail. I admit I am still uncertain about how the conquistadors were able to colonize so much of the world so quickly. I think my ignorance is excusable because this is just a blog post; I welcome corrections from people...",2020-02-29,2022-01-30 04:49:20,2022-01-30 04:49:20,2020-09-05 19:00:45,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Blog,,/Users/jacquesthibodeau/Zotero/storage/46NH99PN/cortes-pizarro-and-afonso-as-precedents-for-ai-takeover.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8MBI682B,blogPost,2019,"Shah, Rohin; Bergal, Asya; Long, Robert; Haxhia, Sara",Conversation with Rohin Shah,AI Impacts,,,,https://aiimpacts.org/conversation-with-rohin-shah/,"AI Impacts talked to AI safety researcher Rohin Shah about his views on AI risk. With his permission, we have transcribed this interview. Participants Rohin Shah -- PhD student at the Center for Human-Compatible AI, UC BerkeleyAsya Bergal - AI ImpactsRobert Long – AI ImpactsSara Haxhia -- Independent researcher Summary We spoke with Rohin Shah...",2019-10-31,2022-01-30 04:49:20,2022-01-30 04:49:20,2020-11-14 03:34:17,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Conversation notes,,/Users/jacquesthibodeau/Zotero/storage/UHCQGXPU/conversation-with-rohin-shah.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UX6HJZUV,blogPost,2019,"Christiano, Paul; Bergal, Asya",Conversation with Paul Christiano,AI Impacts,,,,https://aiimpacts.org/conversation-with-paul-christiano/,"AI Impacts talked to AI safety researcher Paul Christiano about his views on AI risk. With his permission, we have transcribed this interview. Participants Paul Christiano -- OpenAI safety teamAsya Bergal - AI ImpactsRonny Fernandez - AI ImpactsRobert Long – AI Impacts Summary We spoke with Paul Christiano on August 13, 2019. Here is a...",2019-09-11,2022-01-30 04:49:20,2022-01-30 04:49:20,2020-11-14 03:34:15,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Conversation notes,,/Users/jacquesthibodeau/Zotero/storage/FCCIUWG4/conversation-with-paul-christiano.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQT2S8TU,blogPost,2020,"Kokotajlo, Daniel",Precedents for economic n-year doubling before 4n-year doubling,AI Impacts,,,,https://aiimpacts.org/precedents-for-economic-n-year-doubling-before-4n-year-doubling/,"Does the economy ever double without having first doubled four times slower? Yes, but not since 3000BC.",2020-04-14,2022-01-30 04:49:20,2022-01-30 04:49:20,2020-09-05 17:09:17,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Featured Articles,,/Users/jacquesthibodeau/Zotero/storage/MJQA2RMI/precedents-for-economic-n-year-doubling-before-4n-year-doubling.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CGQZA34B,blogPost,2018,"O’Keefe, Cullen",On the (in)applicability of corporate rights cases to digital minds,AI Impacts,,,,https://aiimpacts.org/on-the-inapplicability-of-corporate-rights-cases-to-digital-minds/,This is a guest cross-post by Cullen O'Keefe. High-Level Takeaway The extension of rights to corporations likely does not provide useful analogy to potential extension of rights to digital minds. Introduction Examining how law can protect the welfare of possible future digital minds is part of my research agenda. I expect that study of historical...,2018-09-28,2022-01-30 04:49:20,2022-01-30 04:49:20,2020-11-14 03:21:08,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Blog,,/Users/jacquesthibodeau/Zotero/storage/FTES6UQT/on-the-inapplicability-of-corporate-rights-cases-to-digital-minds.html,,MetaSafety; AmbiguosSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
E6AH852C,blogPost,2018,"Grace, Katja",Likelihood of discontinuous progress around the development of AGI,AI Impacts,,,,https://aiimpacts.org/likelihood-of-discontinuous-progress-around-the-development-of-agi/,"We aren’t convinced by any of the arguments we’ve seen to expect large discontinuity in AI progress above the extremely low base rate for all technologies. However this topic is controversial, and many thinkers on the topic disagree with us, so we consider this an open question. Details Definitions We say a technological discontinuity has...",2018-02-23,2022-01-30 04:49:20,2022-01-30 04:49:20,2020-12-13 23:06:58,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: 0000002[s0]  Section: Featured Articles,,/Users/jacquesthibodeau/Zotero/storage/26QUMN55/likelihood-of-discontinuous-progress-around-the-development-of-agi.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
M9JQA2EK,blogPost,2017,"Grace, Katja",Human-level hardware timeline,AI Impacts,,,,https://aiimpacts.org/human-level-hardware-timeline/,"We estimate that 'human-level hardware'— hardware able to perform as many computations per second as a human brain, at a similar cost to a human brain—has a 30% chance of having already occurred, a 45% third chance of occurring by 2040, and a 25% chance of occurring later. We are not confident about these estimates....",2017-12-22,2022-01-30 04:49:20,2022-01-30 04:49:20,2020-12-13 23:05:56,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timelines,,/Users/jacquesthibodeau/Zotero/storage/5QQHA88X/human-level-hardware-timeline.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7572XHJN,blogPost,2020,"Grace, Katja",Discontinuous progress in history: an update,AI Impacts,,,,https://aiimpacts.org/discontinuous-progress-in-history-an-update/,"We’ve been looking for historic cases of discontinuously fast technological progress, to help with reasoning about the likelihood and consequences of abrupt progress in AI capabilities. We recently finished expanding this investigation to 37 technological trends. This blog post is a quick update on our findings. See the main page on the research and its outgoing links for more details.",2020-04-13,2022-01-30 04:49:20,2022-01-30 04:49:20,2020-09-05 16:57:42,,,,,,,Discontinuous progress in history,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Blog,,/Users/jacquesthibodeau/Zotero/storage/PUBJABIW/discontinuous-progress-in-history-an-update.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
T5T66WNU,blogPost,2020,"Korzekwa, Rick",Description vs simulated prediction,AI Impacts,,,,https://aiimpacts.org/description-vs-simulated-prediction/,What are we trying to do when we look at history to inform forecasting?,2020-04-22,2022-01-30 04:49:20,2022-01-30 04:49:20,2020-09-05 16:59:15,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Blog,,/Users/jacquesthibodeau/Zotero/storage/XG7CX3FD/description-vs-simulated-prediction.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EN3HHGSU,blogPost,2019,"Hanson, Robin; Bergal, Asya; Long, Robert",Conversation with Robin Hanson,AI Impacts,,,,https://aiimpacts.org/conversation-with-robin-hanson/,"AI Impacts talked to economist Robin Hanson about his views on AI risk and timelines. With his permission, we have posted and transcribed this interview. Participants Robin Hanson -- Associate Professor of Economics, George Mason UniversityAsya Bergal - AI ImpactsRobert Long – AI Impacts Summary We spoke with Robin Hanson on September 5, 2019. Here...",2019-11-13,2022-01-30 04:49:20,2022-01-30 04:49:20,2020-11-14 03:34:19,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Conversation notes,,/Users/jacquesthibodeau/Zotero/storage/RIDWXW85/conversation-with-robin-hanson.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NNU453Z3,blogPost,2019,"Gleave, Adam; Bergal, Asya; Long, Robert",Conversation with Adam Gleave,AI Impacts,,,,https://aiimpacts.org/conversation-with-adam-gleave/,"AI Impacts talked to AI safety researcher Adam Gleave about his views on AI risk. With his permission, we have transcribed this interview. Participants Adam Gleave -- PhD student at the Center for Human-Compatible AI, UC BerkeleyAsya Bergal - AI ImpactsRobert Long – AI Impacts Summary We spoke with Adam Gleave on August 27, 2019....",2019-12-23,2022-01-30 04:49:19,2022-01-30 04:49:19,2020-11-14 03:34:21,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Conversation notes,,/Users/jacquesthibodeau/Zotero/storage/699C2QIP/conversation-with-adam-gleave.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6WT3U7VF,blogPost,2019,AI Impacts,AI conference attendance,AI Impacts,,,,https://aiimpacts.org/ai-conference-attendance/,"Six of the largest seven AI conferences hosted a total of 27,396 attendees in 2018. Attendance at these conferences has grown by an average of 21% per year over 2011-2018. These six conferences host around six times as many attendees as six smaller AI conferences. Details Artificial Intelligence Index reports on this, from data they collected...",2019-03-06,2022-01-30 04:49:19,2022-01-30 04:49:19,2020-12-13 23:57:21,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Inputs,,/Users/jacquesthibodeau/Zotero/storage/PPUFMS4K/ai-conference-attendance.html,,MetaSafety; AmbiguosSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BKDUHIDV,blogPost,2019,"Davis, Ernie; Long, Robert",Conversation with Ernie Davis,AI Impacts,,,,https://aiimpacts.org/conversation-with-ernie-davis/,"AI Impacts spoke with computer scientist Ernie Davis about his views of AI risk. With his permission, we have transcribed this interview. Participants Ernest Davis – professor of computer science at the Courant Institute of Mathematical Science, New York University Robert Long – AI Impacts Summary We spoke over the phone with Ernie Davis on...",2019-08-23,2022-01-30 04:49:19,2022-01-30 04:49:19,2020-11-14 03:21:12,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Conversation notes,,/Users/jacquesthibodeau/Zotero/storage/XIGWJRHU/conversation-with-ernie-davis.html; /Users/jacquesthibodeau/Zotero/storage/5757QWMU/conversation-with-ernie-davis.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MH3BZG9F,blogPost,2021,"Grace, Katja",Beyond fire alarms: freeing the groupstruck,AI Impacts,,,,https://aiimpacts.org/beyond-fire-alarms-freeing-the-groupstruck/,Fire alarms are the wrong way to think about the public AGI conversation.,2021-09-26,2022-01-30 04:49:19,2022-01-30 04:49:19,2021-11-18 23:46:19,,,,,,,Beyond fire alarms,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Blog,,/Users/jacquesthibodeau/Zotero/storage/PD5APM7U/beyond-fire-alarms-freeing-the-groupstruck.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HRMA7X54,blogPost,2020,"Grace, Katja",Automated intelligence is not AI,AI Impacts,,,,https://aiimpacts.org/automated-intelligence-is-not-ai/,Katja Grace Sometimes we think of ‘artificial intelligence’ as whatever technology ultimately automates human cognitive labor...,2020-11-01,2022-01-30 04:49:19,2022-01-30 04:49:19,2020-11-21 20:33:46,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Blog,,/Users/jacquesthibodeau/Zotero/storage/NMHPZQPX/automated-intelligence-is-not-ai.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
F5B4F2NG,blogPost,2020,"Grace, Katja",Atari early,AI Impacts,,,,https://aiimpacts.org/atari-early/,Deepmind announced that their Agent57 beats the ‘human baseline’ at all 57 Atari games usually used as a benchmark. I think this is probably enough to resolve one of the predictions we had respondents make in our 2016 survey. Our question was when it would be feasible to ‘outperform professional game testers...,2020-04-01,2022-01-30 04:49:19,2022-01-30 04:49:19,2020-09-05 17:39:14,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Blog,,/Users/jacquesthibodeau/Zotero/storage/AWPUP66U/atari-early.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4ISXUCTN,blogPost,2018,"Mills, Justis",AGI-11 survey,AI Impacts,,,,https://aiimpacts.org/agi-11-survey/,The AGI-11 survey was a survey of 60 participants at the AGI-11 conference. In it: Nearly half of respondents believed that AGI would appear before 2030. Nearly 90% of respondents believed that AGI would appear before 2100. About 85% of respondents believed that AGI would be beneficial for humankind. Details James Barrat and Ben Goertzel...,2018-11-10,2022-01-30 04:49:19,2022-01-30 04:49:19,2020-11-14 03:22:14,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: AI Timeline Surveys,,/Users/jacquesthibodeau/Zotero/storage/UIJU2KS5/agi-11-survey.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SDJRTUC7,blogPost,2020,"Bergal, Asya",2019 recent trends in Geekbench score per CPU price,AI Impacts,,,,https://aiimpacts.org/2019-recent-trends-in-geekbench-score-per-cpu-price/,"From 2006 - 2020, Geekbench score per CPU price has grown by around 16% a year, for rates that would yield an order of magnitude over roughly 16 years. Details We looked at Geekbench 5, a benchmark for CPU performance. We combined Geekbench’s multi-core scores on its 'Processor Benchmarks' page with release dates and prices...",2020-04-14,2022-01-30 04:49:19,2022-01-30 04:49:19,2020-09-05 17:12:28,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Hardware and AI Timelines,,/Users/jacquesthibodeau/Zotero/storage/88Z7BP58/2019-recent-trends-in-geekbench-score-per-cpu-price.html,,MetaSafety; AI-Impacts-NotFeatured,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
J6GC98TM,blogPost,2020,AI Impacts,Time for AI to cross the human performance range in ImageNet image classification,AI Impacts,,,,https://aiimpacts.org/time-for-ai-to-cross-the-human-performance-range-in-imagenet-image-classification/,Computer image classification performance took 3 years to go from untrained human level to trained human level,2020-10-19,2022-01-30 04:49:04,2022-01-30 04:49:04,2020-11-21 20:36:48,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Featured Articles,,/Users/jacquesthibodeau/Zotero/storage/PHMU6GH8/time-for-ai-to-cross-the-human-performance-range-in-imagenet-image-classification.html; /Users/jacquesthibodeau/Zotero/storage/T8WDT98T/time-for-ai-to-cross-the-human-performance-range-in-imagenet-image-classification.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FD3UFI3X,blogPost,2015,AI Impacts,"Research topic: Hardware, software and AI",AI Impacts,,,,https://aiimpacts.org/research-topic-hardware-software-and-ai/,"This is the first in a sequence of articles outlining research which could help forecast AI development. Interpretation Concrete research projects are in boxes. ∑5 ∆8  means we guess the project will take (very) roughly five hours, and we rate its value (very) roughly 8/10. Most projects could be done to very different degrees of depth, or at very different scales. Our time cost...",2015-02-19,2022-01-30 04:49:04,2022-01-30 04:49:04,2020-12-18 19:06:28,,,,,,,Research topic,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Featured Articles,,/Users/jacquesthibodeau/Zotero/storage/6VMRDPXR/research-topic-hardware-software-and-ai.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZDGQ8324,blogPost,2015,AI Impacts,Trends in the cost of computing,AI Impacts,,,,https://aiimpacts.org/trends-in-the-cost-of-computing/,"Computing power available per dollar has probably increased by a factor of ten roughly every four years over the last quarter of a century (measured in FLOPS or MIPS). Over the past 6-8 years, the rate has been slower: around an order of magnitude every 10-16 years, measured in single precision theoretical peak FLOPS or Passmark's benchmark scores. Since...",2015-03-10,2022-01-30 04:49:04,2022-01-30 04:49:04,2020-12-18 19:05:42,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: 0000009  Section: AI Timelines,,/Users/jacquesthibodeau/Zotero/storage/FTJZTW93/trends-in-the-cost-of-computing.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5MPTINM8,blogPost,2018,"McCaslin, Tegan",Transmitting fibers in the brain: Total length and distribution of lengths,AI Impacts,,,,https://aiimpacts.org/transmitting-fibers-in-the-brain-total-length-and-distribution-of-lengths/,"The human brain’s approximately 86 billion neurons are probably connected by something like 850,000 km of axons and dendrites. Of this total, roughly 80% is short-range, local connections (averaging 680 microns in length), and approximately 20% is long-range, global connections in the form of myelinated fibers (likely averaging several centimeters in length). Background The brain’s...",2018-03-29,2022-01-30 04:49:04,2022-01-30 04:49:04,2020-12-13 23:22:39,,,,,,,Transmitting fibers in the brain,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timelines,,/Users/jacquesthibodeau/Zotero/storage/DWXT75KD/transmitting-fibers-in-the-brain-total-length-and-distribution-of-lengths.html; /Users/jacquesthibodeau/Zotero/storage/K8XGXG6Q/transmitting-fibers-in-the-brain-total-length-and-distribution-of-lengths.html,,MetaSafety; AmbiguosSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6RGUKGG5,blogPost,2015,AI Impacts,The cost of TEPS,AI Impacts,,,,https://aiimpacts.org/cost-of-teps/,"A billion Traversed Edges Per Second (a GTEPS) can be bought for around $0.26/hour via a powerful supercomputer, including hardware and energy costs only. We do not know if GTEPS can be bought more cheaply elsewhere. We estimate that available TEPS/$ grows by a factor of ten every four years, based the relationship between TEPS and FLOPS. TEPS have not been...",2015-03-21,2022-01-30 04:49:04,2022-01-30 04:49:04,2020-12-18 19:04:55,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timelines,,,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KFB3JVAP,blogPost,2020,"Bergal, Asya",Surveys on fractional progress towards HLAI,AI Impacts,,,,https://aiimpacts.org/surveys-on-fractional-progress-towards-hlai/,"How long until human-level performance, if we naively extrapolate progress since researchers joined their subfields?",2020-04-14,2022-01-30 04:49:04,2022-01-30 04:49:04,2020-09-05 17:08:05,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timeline Surveys,,/Users/jacquesthibodeau/Zotero/storage/AZVQBRB7/surveys-on-fractional-progress-towards-hlai.html; /Users/jacquesthibodeau/Zotero/storage/H8GGTM77/surveys-on-fractional-progress-towards-hlai.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C2DQQ6BM,blogPost,2016,AI Impacts,Returns to scale in research,AI Impacts,,,,https://aiimpacts.org/returns-to-scale-in-research/,"When universities or university departments produce research outputs—such as published papers—they sometimes experience increasing returns to scale, sometimes constant returns to scale, and sometimes decreasing returns to scale. At the level of nations however, R&D tends to see increasing returns to scale. These results are preliminary. Background “Returns to scale” refers to the responsiveness of...",2016-07-06,2022-01-30 04:49:04,2022-01-30 04:49:04,2020-12-18 18:56:48,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Featured Articles,,/Users/jacquesthibodeau/Zotero/storage/TR933CQ5/returns-to-scale-in-research.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZH6RHC67,blogPost,2020,"Bergal, Asya",Resolutions of mathematical conjectures over time,AI Impacts,,,,https://aiimpacts.org/resolutions-of-mathematical-conjectures-over-time/,"Conditioned on being remembered as a notable conjecture, the time-to-proof for a mathematical problem appears to be exponentially distributed with a half-life of about 100 years. However, these observations are likely to be distorted by various biases. Support In 2014, we found conjectures referenced on Wikipedia, and recorded the dates that they were proposed and...",2020-04-14,2022-01-30 04:49:04,2022-01-30 04:49:04,2020-09-05 17:10:04,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timelines,,/Users/jacquesthibodeau/Zotero/storage/6QIT4QFT/resolutions-of-mathematical-conjectures-over-time.html; /Users/jacquesthibodeau/Zotero/storage/HJKW6MVS/resolutions-of-mathematical-conjectures-over-time.html,,MetaSafety; AmbiguosSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
U6J7QJB4,blogPost,2017,AI Impacts,Progress in general purpose factoring,AI Impacts,,,,https://aiimpacts.org/progress-in-general-purpose-factoring/,"The largest number factored to date grew by about 4.5 decimal digits per year over the past roughly half-century. Between 1988, when we first have good records, and 2009, when the largest number to date was factored, progress was roughly 6 decimal digits per year. Progress was relatively smooth during the two decades for which we have good records, with half of...",2017-03-16,2022-01-30 04:49:04,2022-01-30 04:49:04,2020-12-18 18:51:25,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Inputs,,/Users/jacquesthibodeau/Zotero/storage/29ZRZ757/progress-in-general-purpose-factoring.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZV6ZEP7K,blogPost,2020,"Korzekwa, Rick",Preliminary survey of prescient actions,AI Impacts,,,,https://aiimpacts.org/survey-of-prescient-actions/,"In a 10-20 hour exploration, we did not find clear examples of 'prescient actions'—specific efforts to address severe and complex problems decades ahead of time and in the absence of broader scientific concern, experience with analogous problems, or feedback on the success of the effort—though we found six cases that may turn out to be...",2020-04-03,2022-01-30 04:49:04,2022-01-30 04:49:04,2020-09-05 17:13:16,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Featured Articles,,/Users/jacquesthibodeau/Zotero/storage/2QCZB5HZ/survey-of-prescient-actions.html; /Users/jacquesthibodeau/Zotero/storage/3FEHV8RT/survey-of-prescient-actions.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EKDHRC3V,blogPost,2015,AI Impacts,List of Analyses of Time to Human-Level AI,AI Impacts,,,,https://aiimpacts.org/list-of-analyses-of-time-to-human-level-ai/,"This is a list of most of the substantial analyses of AI timelines that we know of. It also covers most of the arguments and opinions of which we are aware. Details The list below contains substantial publically available analyses of when human-level AI will appear. To qualify for the list, an item must provide both a claim...",2015-01-22,2022-01-30 04:49:03,2022-01-30 04:49:03,2020-12-18 19:07:58,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timelines,,/Users/jacquesthibodeau/Zotero/storage/JMI38J5D/list-of-analyses-of-time-to-human-level-ai.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WEQ27ARZ,blogPost,2019,"McCaslin, Tegan",Investigation into the relationship between neuron count and intelligence across differing cortical architectures,AI Impacts,,,,https://aiimpacts.org/investigation-into-the-relationship-between-neuron-count-and-intelligence-across-differing-cortical-architectures/,,2019,2022-01-30 04:49:03,2022-01-30 04:49:03,2020-12-14,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/FUXE4PED/investigation-into-the-relationship-between-neuron-count-and-intelligence-across-differing-cort.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JK5CBDEJ,blogPost,2019,AI Impacts,Historical economic growth trends,AI Impacts,,,,https://aiimpacts.org/historical-growth-trends/,"An analysis of historical growth supports the possibility of radical increases in growth rate. Naive extrapolation of long-term trends would suggest massive increases in growth rate over the coming century, although growth over the last half-century has lagged very significantly behind these long-term trends. Support Bradford DeLong has published estimates for historical world GDP, piecing together...",2019-03-06,2022-01-30 04:49:03,2022-01-30 04:49:03,2020-12-13 23:57:45,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timelines,,/Users/jacquesthibodeau/Zotero/storage/295DBKF8/historical-growth-trends.html; /Users/jacquesthibodeau/Zotero/storage/6QQFGPWX/historical-growth-trends.html,,MetaSafety; AmbiguosSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
X3DQBHIS,blogPost,2014,AI Impacts,Effect of nuclear weapons on historic trends in explosives,AI Impacts,,,,https://aiimpacts.org/discontinuity-from-nuclear-weapons/,"Nuclear weapons constituted a ~7 thousand year discontinuity in relative effectiveness factor (TNT equivalent per kg of explosive). Nuclear weapons do not appear to have clearly represented progress in the cost-effectiveness of explosives, though the evidence there is weak. Details This case study is part of AI Impacts’ discontinuous progress investigation. Background The development of nuclear...",2014-12-31,2022-01-30 04:49:03,2022-01-30 04:49:03,2020-12-18 19:09:55,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Continuity of progress,,/Users/jacquesthibodeau/Zotero/storage/BXQBM3XU/discontinuity-from-nuclear-weapons.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ID37HU49,blogPost,2016,"Wulfsohn, Michael",Costs of extinction risk mitigation,AI Impacts,,,,https://aiimpacts.org/costs-of-extinction-risk-mitigation/,We very roughly estimate that the annual cost of reducing the probability of human extinction by 0.01% is within the range of $1.1 billion to $3.5 trillion. Introduction This article is intended to be usable in a Cost-Benefit Analysis (CBA) analysis of extinction risk mitigation. It explores the costs of such efforts. A corresponding article...,2016-08-04,2022-01-30 04:49:03,2022-01-30 04:49:03,2020-12-18 18:55:48,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Evaluation,,/Users/jacquesthibodeau/Zotero/storage/B7NXCJJP/costs-of-extinction-risk-mitigation.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R88INF7A,blogPost,2014,AI Impacts,Cases of Discontinuous Technological Progress,AI Impacts,,,,https://aiimpacts.org/cases-of-discontinuous-technological-progress/,We know of ten events which produced a robust discontinuity in progress equivalent to more than one hundred years at previous rates in some interesting metric. We know of 53 other events which produced smaller or less robust discontinuities. Background These cases were researched as part of our discontinuous progress investigation. List of cases Events...,2014-12-31,2022-01-30 04:49:03,2022-01-30 04:49:03,2020-12-18 19:09:14,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Featured Articles,,/Users/jacquesthibodeau/Zotero/storage/2I97PIUR/cases-of-discontinuous-technological-progress.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6BMGVAKN,blogPost,2015,AI Impacts,AI Timeline Surveys,AI Impacts,,,,https://aiimpacts.org/ai-timeline-surveys/,"[This page is out of date and will be updated soon. It does not reflect all surveys known and documented by AI Impacts.] We know of thirteen surveys on the predicted timing of human-level AI. If we collapse a few slightly different meanings of 'human-level AI', then: Median estimates for when there will be a 10% chance of human-level AI are...",2015-01-10,2022-01-30 04:49:03,2022-01-30 04:49:03,2020-12-18 19:08:31,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timeline Surveys,,/Users/jacquesthibodeau/Zotero/storage/RNANISVC/ai-timeline-surveys.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R53RXIT7,blogPost,2015,AI Impacts,MIRI AI Predictions Dataset,AI Impacts,,,,https://aiimpacts.org/miri-ai-predictions-dataset/,"The MIRI AI predictions dataset is a collection of public predictions about human-level AI timelines. We edited the original dataset, as described below. Our dataset is available here, and the original here. Interesting features of the dataset include: The median dates at which people's predictions suggest AI is less likely than not and more likely than not are...",2015-05-20,2022-01-30 04:49:03,2022-01-30 04:49:03,2020-12-18 19:03:59,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timelines,,/Users/jacquesthibodeau/Zotero/storage/HXM7IC2U/miri-ai-predictions-dataset.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DXPWI4DP,blogPost,2015,AI Impacts,List of multipolar research projects,AI Impacts,,,,https://aiimpacts.org/multipolar-research-projects/,"This list currently consists of research projects suggested at the Multipolar AI workshop we held on January 26 2015. Relatively concrete projects are marked [concrete]. These are more likely to already include specific questions to answer and feasible methods to answer them with. Other 'projects' are more like open questions, or broad directions for inquiry. Projects are divided into three sections: Paths to multipolar scenarios What...",2015-02-11,2022-01-30 04:49:03,2022-01-30 04:49:03,2020-12-18 19:06:54,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Featured Articles,,/Users/jacquesthibodeau/Zotero/storage/2FV59JK6/multipolar-research-projects.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
557XUX37,blogPost,2014,AI Impacts,Human-Level AI,AI Impacts,,,,https://aiimpacts.org/human-level-ai/,"Human-level AI' refers to AI which can reproduce everything a human can do, approximately. Several variants of this concept are worth distinguishing. Details Variations in the meaning of 'human-level AI' Considerations in specifying 'human-level AI' more precisely: Do we mean to imply anything about running costs? Is an AI that reproduces human behavior for ten billion dollars per year 'human-level',...",2014-01-23,2022-01-30 04:49:03,2022-01-30 04:49:03,2020-12-18 19:10:54,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Clarifying concepts,,/Users/jacquesthibodeau/Zotero/storage/V3S9A9CP/human-level-ai.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NUG5E483,blogPost,2020,"Fernandez, Ronny",How energy efficient are human-engineered flight designs relative to natural ones?,AI Impacts,,,,https://aiimpacts.org/are-human-engineered-flight-designs-better-or-worse-than-natural-ones/,"Nature is responsible for the most energy efficient flight, according to an investigation of albatrosses, butterflies and nine different human-engineered flying machines.",2020-12-10,2022-01-30 04:49:03,2022-01-30 04:49:03,2020-12-18 18:30:29,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Evolution engineering comparison,,,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XS42FABG,blogPost,2014,AI Impacts,Hanson AI Expert Survey,AI Impacts,,,,https://aiimpacts.org/hanson-ai-expert-survey/,"In a small informal survey running since 2012, AI researchers generally estimated that their subfields have moved less than ten percent of the way to human-level intelligence. Only one (in the slowest moving subfield) observed acceleration. This suggests on a simple extrapolation that reaching human-level capability across subfields will take over a century (in contrast with many other...",2014-12-29,2022-01-30 04:49:03,2022-01-30 04:49:03,2020-12-18 19:10:27,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timeline Surveys,,/Users/jacquesthibodeau/Zotero/storage/JSVEBI48/hanson-ai-expert-survey.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2ZPZ6KTC,blogPost,2021,AI Impacts,Fiction relevant to AI futurism,AI Impacts,,,,https://aiimpacts.org/partially-plausible-fictional-ai-futures/,"A list of stories potentially relevant to thinking about the development of advanced AI, including both those intended as futurism and those intended as entertainment.",2021-04-12,2022-01-30 04:49:03,2022-01-30 04:49:03,2021-10-30 16:38:18,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Featured Articles,,/Users/jacquesthibodeau/Zotero/storage/IKURBDT5/partially-plausible-fictional-ai-futures.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Z83BXIK7,blogPost,2016,AI Impacts,Examples of early action on risks,AI Impacts,,,,https://aiimpacts.org/examples-of-early-action-on-a-risk/,"Details Discussion There are many current efforts to mitigate risks from artificial intelligence. We might learn something about the likelihood of these efforts influencing AI risk by looking at similar past efforts. To this end, we are interested here in past risk mitigation efforts that have the following characteristics (taken from this paper contributing to the same project (p5) and...",2016-08-16,2022-01-30 04:49:03,2022-01-30 04:49:03,2020-12-18 18:55:26,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Control,,/Users/jacquesthibodeau/Zotero/storage/C6A4Q39T/examples-of-early-action-on-a-risk.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QKDDT8NV,blogPost,2019,"Kokotajlo, Daniel",Evidence on good forecasting practices from the Good Judgment Project,AI Impacts,,,,https://aiimpacts.org/evidence-on-good-forecasting-practices-from-the-good-judgment-project/,"According to experience and data from the Good Judgment Project, the following are associated with successful forecasting, in rough decreasing order of combined importance and confidence: Past performance in the same broad domain Making more predictions on the same question Deliberation time Collaboration on teams Intelligence Domain expertise Having taken a one-hour training module on...",2019-02-07,2022-01-30 04:49:03,2022-01-30 04:49:03,2020-11-14 03:21:44,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Featured Articles,,/Users/jacquesthibodeau/Zotero/storage/AKFKIG5V/evidence-on-good-forecasting-practices-from-the-good-judgment-project.html; /Users/jacquesthibodeau/Zotero/storage/44WBB3QC/evidence-on-good-forecasting-practices-from-the-good-judgment-project.html,,MetaSafety; AmbiguosSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
798WSB6H,blogPost,2019,"Long, Robert; Bergal, Asya",Evidence against current methods leading to human level artificial intelligence,AI Impacts,,,,https://aiimpacts.org/evidence-against-current-methods-leading-to-human-level-artificial-intelligence/,"This is a list of published arguments that we know of that current methods in artificial intelligence will not lead to human-level AI. Details Clarifications We take 'current methods' to mean techniques for engineering artificial intelligence that are already known, involving no “qualitatively new ideas”. We have not precisely defined 'current methods'. Many of the...",2019-08-12,2022-01-30 04:49:03,2022-01-30 04:49:03,2020-12-14 23:33:42,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Featured Articles,,/Users/jacquesthibodeau/Zotero/storage/WI59RSHC/evidence-against-current-methods-leading-to-human-level-artificial-intelligence.html; /Users/jacquesthibodeau/Zotero/storage/I2J57E3V/evidence-against-current-methods-leading-to-human-level-artificial-intelligence.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9RGZGX6U,blogPost,2015,AI Impacts,Discontinuous progress investigation,AI Impacts,,,,https://aiimpacts.org/discontinuous-progress-investigation/,"Published Feb 2, 2015; last updated April 12 2020 We have collected cases of discontinuous technological progress to inform our understanding of whether artificial intelligence performance is likely to undergo such a discontinuity. This page details our investigation. We know of ten events that produced a robust discontinuity in progress equivalent to more than a century...",2015-02-02,2022-01-30 04:49:03,2022-01-30 04:49:03,2020-12-18 19:07:26,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timelines,,/Users/jacquesthibodeau/Zotero/storage/BGNFTNE2/discontinuous-progress-investigation.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9Q4KHV7Q,blogPost,2015,AI Impacts,Costs of human-level hardware,AI Impacts,,,,https://aiimpacts.org/costs-of-human-level-hardware/,"Computing hardware which is equivalent to the brain - in terms of FLOPS probably costs between $1 x 105 and $3 x 1016, or $2/hour-$700bn/hour. in terms of TEPS probably costs $200M - $7B, or or $4,700 – $170,000/hour (including energy costs in the hourly rate). in terms of secondary memory probably costs $300-3,000, or $0.007-$0.07/hour. Details Partial costs...",2015-07-26,2022-01-30 04:49:03,2022-01-30 04:49:03,2020-12-18 18:59:32,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timelines,,/Users/jacquesthibodeau/Zotero/storage/64B2PHMG/costs-of-human-level-hardware.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZAUDQANV,blogPost,2016,AI Impacts,Coordinated human action as example of superhuman intelligence,AI Impacts,,,,https://aiimpacts.org/coordinated-human-action-example-superhuman-intelligence/,Collections of humans organized into groups and institutions provide many historical examples of the creation and attempted control of intelligences that routinely outperform individual humans. A preliminary look at the available evidence suggests that individuals are often cognitively outperformed in head-to-head competition with groups of similar average intelligence. This article surveys considerations relevant to the...,2016-01-21,2022-01-30 04:49:03,2022-01-30 04:49:03,2020-12-18 18:57:29,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Control,,/Users/jacquesthibodeau/Zotero/storage/9Q4E9ZW2/coordinated-human-action-example-superhuman-intelligence.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
N387UVHE,blogPost,2016,"Griffiths, Tom; Adamson, Finan",Conversation with Tom Griffiths,AI Impacts,,,,https://aiimpacts.org/conversation-with-tom-griffiths/,"Participants Professor Tom Griffiths, ­ Director of the Computational Cognitive Science Lab and the Institute of Cognitive and Brain Sciences at the University of California, Berkeley. Finan Adamson, ­ AI Impacts. Note: These notes were compiled by AI impacts and give an overview of the major points made by Professor Tom Griffiths. They are available...",2016-09-08,2022-01-30 04:49:03,2022-01-30 04:49:03,2020-12-18 18:54:17,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timelines,,/Users/jacquesthibodeau/Zotero/storage/9BKW84NI/conversation-with-tom-griffiths.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VXNKZP2R,blogPost,2015,"Potter, Steve; Grace, Katja",Conversation with Steve Potter,AI Impacts,,,,https://aiimpacts.org/conversation-with-steve-potter/,"Posted 13 July 2015 Participants Professor Steve Potter – Associate Professor, Laboratory of NeuroEngineering, Coulter Department of Biomedical Engineering, Georgia Institute of Technology Katja Grace – Machine Intelligence Research Institute (MIRI) Note: These notes were compiled by MIRI and give an overview of the major points made by Professor Steve Potter. Summary Katja Grace spoke...",2015-07-13,2022-01-30 04:49:03,2022-01-30 04:49:03,2020-12-18 19:00:09,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timelines,,/Users/jacquesthibodeau/Zotero/storage/VW3X2BWF/conversation-with-steve-potter.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UW2DZD9A,blogPost,2015,AI Impacts,Brain performance in TEPS,AI Impacts,,,,https://aiimpacts.org/brain-performance-in-teps/,"Traversed Edges Per Second (TEPS) is a benchmark for measuring a computer's ability to communicate information internally. Given several assumptions, we can also estimate the human brain's communication performance in terms of TEPS, and use this to meaningfully compare brains to computers. We estimate that (given these assumptions) the human brain performs around  0.18 - 6.4 *...",2015-05-06,2022-01-30 04:49:03,2022-01-30 04:49:03,2020-12-18 19:04:30,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timelines,,/Users/jacquesthibodeau/Zotero/storage/SDDXHFH4/brain-performance-in-teps.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RZZWAGZ9,blogPost,2021,AI Impacts,AI Vignettes Project,AI Impacts,,,,https://aiimpacts.org/ai-vignettes-project/,"The AI Vignettes Project is an ongoing effort to write concrete plausible future histories of AI development and its social impacts. Details Purposes We hope to: Check that abstract views about the future of AI have plausible concrete instantiations. (Especially, hypothesized extinction scenarios, and proposed safe scenarios.)Develop better intuitions about possible scenarios by thinking through...",2021-10-12,2022-01-30 04:49:03,2022-01-30 04:49:03,2021-10-30 16:33:40,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: Featured Articles,,/Users/jacquesthibodeau/Zotero/storage/WSPITJEN/ai-vignettes-project.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KII8U9NV,blogPost,2015,AI Impacts,AI Impacts research bounties,AI Impacts,,,,https://aiimpacts.org/ai-impacts-research-bounties/,"We are offering rewards for several inputs to our research, described below. These offers have no specific deadline except where noted. We may modify them or take them down, but will give at least one week's notice here unless there is strong reason not to. To submit an entry, email katja@intelligence.org. There is currently a large backlog of entries to...",2015-08-06,2022-01-30 04:49:02,2022-01-30 04:49:02,2020-12-18 18:58:51,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Featured Articles,,/Users/jacquesthibodeau/Zotero/storage/4UIJK9K6/ai-impacts-research-bounties.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VZKUDNZD,blogPost,2020,"Bergal, Asya",2019 recent trends in GPU price per FLOPS,AI Impacts,,,,https://aiimpacts.org/2019-recent-trends-in-gpu-price-per-flops/,"We estimate that in recent years, GPU prices have fallen at rates that would yield an order of magnitude over roughly: 17 years for single-precision FLOPS10 years for half-precision FLOPS5 years for half-precision fused multiply-add FLOPS Details GPUs (graphics processing units) are specialized electronic circuits originally used for computer graphics. In recent years, they have...",2020-03-25,2022-01-30 04:49:02,2022-01-30 04:49:02,2020-09-05 18:37:02,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A  Section: AI Timelines,,/Users/jacquesthibodeau/Zotero/storage/WBT5HR2S/2019-recent-trends-in-gpu-price-per-flops.html; /Users/jacquesthibodeau/Zotero/storage/NC8ARCAU/2019-recent-trends-in-gpu-price-per-flops.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BF7984KG,blogPost,2016,"Grace, Katja; Salvatier, John; Dafoe, Allan; Zhang, Baobao; Evans, Owain",2016 Expert Survey on Progress in AI,AI Impacts,,,,https://aiimpacts.org/2016-expert-survey-on-progress-in-ai/,"Published June 2016; last substantial update before Oct 2017 The 2016 Expert Survey on Progress in AI is a survey of machine learning researchers that Katja Grace and John Salvatier of AI Impacts ran in collaboration with Allan Dafoe, Baobao Zhang, and Owain Evans in 2016. Details Some survey results are reported in When Will...",2016-12-14,2022-01-30 04:49:02,2022-01-30 04:49:02,2020-12-18 18:52:20,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timeline Surveys,,/Users/jacquesthibodeau/Zotero/storage/624ZXBKR/2016-expert-survey-on-progress-in-ai.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PUDJM3MC,blogPost,2015,AI Impacts,AI Risk Terminology,AI Impacts,,,,https://aiimpacts.org/ai-risk-terminology/,"AI timeline - an expectation about how much time will lapse before important AI events, especially the advent of human-level AI or a similar milestone. The term can also refer to the actual periods of time (which are not yet known), rather than an expectation about them. Artificial General Intelligence (also, AGI) - the intelligence of a machine that could successfully...",2015-10-30,2022-01-30 04:49:02,2022-01-30 04:49:02,2020-12-18 18:58:20,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Featured Articles,,/Users/jacquesthibodeau/Zotero/storage/VNQR5F2J/ai-risk-terminology.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RAXPGCP3,blogPost,2015,AI Impacts,Accuracy of AI Predictions,AI Impacts,,,,https://aiimpacts.org/accuracy-of-ai-predictions/,"Updated 4 June 2015 It is unclear how informative we should expect expert predictions about AI timelines to be. Individual predictions are undoubtedly often off by many decades, since they disagree with each other. However their aggregate may still be quite informative. The main potential reason we know of to doubt the accuracy of expert predictions is that experts are generally poor predictors in many areas, and...",2015-06-04,2022-01-30 04:49:02,2022-01-30 04:49:02,2020-12-18 19:02:01,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: Accuracy of AI Predictions,,/Users/jacquesthibodeau/Zotero/storage/EVEQ3P8B/accuracy-of-ai-predictions.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
G95WAZZB,blogPost,2017,AI Impacts,2017 trend in the cost of computing,AI Impacts,,,,https://aiimpacts.org/recent-trend-in-the-cost-of-computing/,"The cheapest hardware prices (for single precision FLOPS/$) appear to be falling by around an order of magnitude every 10-16 years. This rate is slower than the trend of FLOPS/$ observed over the past quarter century, which was an order of magnitude every 4 years. There is no particular sign of slowing between 2011 and 2017....",2017-11-11,2022-01-30 04:49:02,2022-01-30 04:49:02,2020-12-18 18:50:38,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A  Section: AI Timelines,,/Users/jacquesthibodeau/Zotero/storage/3WDJKPFI/recent-trend-in-the-cost-of-computing.html,,MetaSafety; AI-Impacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
768PN8FI,blogPost,2020,Partnership on AI,What the AI Community Can Learn From Sneezing Ferrets and a Mutant Virus Debate,AI&.,,,,https://medium.com/partnership-on-ai/lessons-for-the-ai-community-from-the-h5n1-controversy-32432438a82e,Lessons on publication norms for the AI community from biosecurity,2020-12-09,2022-01-30 04:48:55,2022-01-30 04:48:55,2021-11-18 23:28:05,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/QWKPVW7U/lessons-for-the-ai-community-from-the-h5n1-controversy-32432438a82e.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2SFMIH4S,blogPost,2020,"Tian, Yonglong",Understanding View Selection for Contrastive Learning,Google AI Blog,,,,http://ai.googleblog.com/2020/08/understanding-view-selection-for.html,"Posted by Yonglong Tian, Student Researcher and Chen Sun, Staff Research Scientist, Google Research    Most people take for granted the abil...",2020-08-21,2022-01-30 04:48:54,2022-01-30 04:48:54,2021-11-07 16:20:33,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/RFIP9DCC/understanding-view-selection-for.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9MGJKQ2I,blogPost,2020,"Shah, Rohin",AI Alignment 2018-19 Review,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/dKxX76SCfCvceJXHv/ai-alignment-2018-19-review,"PREAMBLE WHAT THIS POST IS This is a review post of public work in AI alignment over 2019, with some inclusions from 2018. It has this preamble (~700 words), a short version / summary (~1.6k words), and a long version (~8.3k words). It is available as a Google Doc here. There are many areas of work that are relevant to AI alignment that I have barely touched on, such as interpretability, uncertainty estimation, adversarial examples, and assured autonomy, primarily because I have not been following these fields and wouldn’t be able to write a good summary of what has happened in them. I have also mostly focused on articles that provide some conceptual insight, and excluded or briefly linked to papers that primarily make quantitative improvements on important metrics. While such papers are obviously important (ultimately, our techniques need to work well), there isn’t much to say about them in a yearly review other than that the quantitative metric was improved. Despite these exclusions, there was still a ton of work to select from, perhaps around ~500 articles, of which over 300 have been linked to in this post. There are many interesting articles that I really enjoyed that get only a sentence of description, in which I ignore many of the points that the article makes. Most have been summarized in the Alignment Newsletter, so if you’d like to learn more about any particular link, but don’t want to read the entire thing, just search for its title in the database. WHAT YOU SHOULD KNOW ABOUT THE STRUCTURE OF THIS POST I am not speaking for myself; by default I am trying to explain what has been said, in a way that the authors of the articles would agree with. Any extra opinion that I add will be in italics. As a post, this is meant to be read sequentially, but the underlying structure is a graph (nodes are posts, edges connect posts that are very related). I arranged it in a sequence that highlights the most salient-to-me connections. This means that the order in wh",2020,2022-01-30 04:50:42,2022-01-30 04:50:42,2020-12-18 00:14:21,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/SESAVJBI/ai-alignment-2018-19-review.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DZ9C9GKV,blogPost,2021,"Clarke, Sam; Carlier, Alexis; Schuett, Jonas",Survey on AI existential risk scenarios,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/WiXePTj7KeEycbiwK/survey-on-ai-existential-risk-scenarios,"Cross-posted to the EA forum. SUMMARY  * In August 2020, we conducted an online survey of prominent AI safety and    governance researchers. You can see a copy of the survey at this link.[1]  * We sent the survey to 135 researchers at leading AI safety/governance    research organisations (including AI Impacts, CHAI, CLR, CSER, CSET, FHI, FLI    , GCRI, MILA, MIRI, Open Philanthropy and PAI) and a number of independent    researchers. We received 75 responses, a response rate of 56%.  * The survey aimed to identify which AI existential risk scenarios[2] (which we    will refer to simply as “risk scenarios”) those researchers find most likely,    in order to (1) help with prioritising future work on exploring AI risk    scenarios, and (2) facilitate discourse and understanding within the AI    safety and governance community, including between researchers who have    different views.  * In our view, the key result is that there was considerable disagreement among    researchers about which risk scenarios are the most likely, and high    uncertainty expressed by most individual researchers about their estimates.  * This suggests that there is a lot of value in exploring the likelihood of    different AI risk scenarios in more detail, especially given the limited    scrutiny that most scenarios have received. This could look like: * Fleshing       out and analysing the scenarios mentioned in this post which have received       less scrutiny.     * Doing       more horizon scanning or trying to come up with other risk scenarios, and       analysing them.          * At this time, we are only publishing this abbreviated version of the results.    We have a version of the full results that we may publish at a later date.    Please contact one of us if you would like access to this, and include a    sentence on why the results would be helpful or what you intend to use them    for.  * We welcome feedback on any aspects of the survey. MOTIVATION It has been argued that AI",2021-06-08,2022-01-30 04:50:25,2022-01-30 04:50:25,2021-11-14 18:38:03,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/5WSJBIXI/survey-on-ai-existential-risk-scenarios.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DSDMH6GU,blogPost,2019,"Shovelain, Justain; Emilsson, Andrés Gómez",Why Care About Meme Hazards and Thoughts on How to Handle Them,Qualia  Computing,,,,https://qualiacomputing.com/2019/08/30/why-care-about-meme-hazards-and-thoughts-on-how-to-handle-them/,By Justin Shovelain and Andrés Gómez Emilsson Definition Nick Bostrom defines an “Information Hazard” as: “A risk that arises from the dissemination or the potential dissemination of (true) informa…,2019-08-31,2022-01-30 04:50:08,2022-01-30 04:50:08,2020-12-12 02:32:24,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/P5U32AF7/why-care-about-meme-hazards-and-thoughts-on-how-to-handle-them.html,,MetaSafety; AmbiguosSafety; BERI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
65UKC9MC,blogPost,2019,"Manheim, David","What does Optimization Mean, Again? (Optimizing and Goodhart Effects - Clarifying Thoughts, Part 2)",AI Alignment Forum,,,,https://www.alignmentforum.org/posts/BEMvcaeixt3uEqyBk/what-does-optimization-mean-again-optimizing-and-goodhart,"Clarifying Thoughts on Optimizing and Goodhart Effects - Part 2 Previous Post: Re-introducing Selection vs Control for Optimization In the post, I reviewed Abram's selection/control distinction, and suggested how it relates to actual design. I then argue that there is a bit of a continuum between the two cases, and that we should add an addition extreme case to the typology, direct solution. Here, I will revisit the question of what optimization means.  NOTE: This is not completely new content, and is instead split off from the previous version and rewritten to include an (Added) discussion of Eliezer's definition for measuring optimization power, from 2008. Hopefully this will make the sequence clearer for future readers. In the next post, Applying over-Optimization in Selection and Control, I apply these ideas, and concretize the discussion a bit more before moving on to discussing Mesa-Optimizers in Part 4. WHAT DOES OPTIMIZATION MEAN, AGAIN? This question has been discussed a bit, but I still don't think its clear. So I want to start by revisiting a post Eliezer wrote in 2008, where he suggested that optimization power was ability to select states from a preference ordering over different states, and could be measured with entropy. He notes that this is not computable, but gives us insight. I agree, except that I think that the notion of the state space is difficult, for some of the reasons Scott discussed when he mentioned that he was confused about the relationship between gradient descent and Goodhart's law. In doing so, Scott proposed a naive model that looks very similar to Eliezer's;  simple proxy of ""sample points until I get one with a large U value"" or ""sample n points, and [select] the one with the largest U value"" when I think about what it means to optimize something for U. I might even say something like ""n bits of optimization"" to refer to sampling 2n points. I think this is not a very good proxy for what most forms of optimization look like.",2019,2022-01-30 04:50:08,2022-01-30 04:50:08,2020-12-12 02:14:15,,,,,,,"What does Optimization Mean, Again?",,,,,,,,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/SSVPA69X/what-does-optimization-mean-again-optimizing-and-goodhart.html,,TechSafety; BERI; Non-notable,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
D2AQPW6X,blogPost,,"Maltinsky, Baeo",The Brain and Computation,Median Group,,,,http://mediangroup.org/brain1.html,,unknown,2022-01-30 04:50:08,2022-01-30 04:50:08,2020-12-12 02:03:52,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/DWCBH8SN/brain1.html,,MetaSafety; BERI; Median-group,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
D52HQ6E9,blogPost,2019,Median Group,Revisiting the Insights model,Median Group,,,,http://mediangroup.org/insights2.html,,2019,2022-01-30 04:50:07,2022-01-30 04:50:07,2019-12-16 20:54:29,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s7]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/FEI3VHA9/insights2.html,,MetaSafety; BERI; Median-group,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
98CJ57A9,blogPost,2018,"Rade, Luca",Issues with Iterated Distillation and Amplification,Luca Rade (Medium),,,,https://medium.com/@lucarade/issues-with-iterated-distillation-and-amplification-5aa01ab37173,"This post assumes familiarity with Paul Christiano’s proposed technique for AI alignment, Iterated Distillation and Amplification…",2018-04-29,2022-01-30 04:50:07,2022-01-30 04:50:07,2020-12-12 02:37:53,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/7M3HTSCN/issues-with-iterated-distillation-and-amplification-5aa01ab37173.html,,TechSafety; BERI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
64SCSJMX,blogPost,2018,"Maltinsky, Baeo",How rapidly are GPUs improving in price performance?,Median Group,,,,http://mediangroup.org/gpu.html,,2018,2022-01-30 04:50:07,2022-01-30 04:50:07,2020-12-12 01:59:31,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/2NM92P2G/gpu.html,,MetaSafety; AmbiguosSafety; BERI; Median-group,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JKSHJAEI,blogPost,2019,"Manheim, David","Applying Overoptimization to Selection vs. Control (Optimizing and Goodhart Effects - Clarifying Thoughts, Part 3)",AI Alignment Forum,,,,https://www.alignmentforum.org/posts/zdeYiQgwYRs2bEmCK/applying-overoptimization-to-selection-vs-control-optimizing,"Clarifying Thoughts on Optimizing and Goodhart Effects - Part 3 Previous Posts: Re-introducing Selection vs Control for Optimization, What does Optimization Mean, Again? -  Following the previous two posts, I'm going to try to first lay out the way Goodhart's Law applies in the earlier example of rockets, then try to explain why this differs between selection and control. (Note: Adversarial Goodhart isn't explored, because we want to keep the setting sufficiently simple.) This sets up the next post, which will discuss Mesa-Optimizers. REVISTING SELECTION VS. CONTROL SYSTEMS Basically everything in the earlier post that used the example process of rocket design and launching is susceptible to some form of overoptimization, in different ways. Interestingly, there seem to be clear places where different types of overoptimization is important. Before looking at this, I want to revisit the selection-control dichotomy from a new angle. In a (pure) control system, we cannot sample datapoints without navigating to them. If the agent is an embedded agent, and has sufficient span of control to cause changes in the environment, we cannot necessarily reset and try over. In a selection system, we only sample points in ways that do not affect the larger system. Even when designing a rocket, our very expensive testing has approximately no longer term effects. (We'll leave space debris from failures aside, but get back to it below.) This explains why we potentially care about control systems more than selection systems. It also points to why Oracles are supposed to be safer than other AIs - they can't directly impact anything, so their output is done in a pure selection framework. Of course, if they are sufficiently powerful, and are relied on, the changes made become irreversible, which is why Oracles are not a clear solution to AI safety. GOODHART IN SELECTION VS. CONTROL SYSTEMS Regressional and Extremal Goodhart are particularly pernicious for selection, and potentially l",2019-07-28,2022-01-30 04:50:07,2022-01-30 04:50:07,2020-12-12 02:14:18,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/JBRGV5V5/applying-overoptimization-to-selection-vs-control-optimizing.html,,TechSafety; BERI; Non-notable,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GTQ7QIQU,blogPost,2019,"Manheim, David","Re-introducing Selection vs Control for Optimization (Optimizing and Goodhart Effects - Clarifying Thoughts, Part 1)",AI Alignment Forum,,,,https://www.alignmentforum.org/posts/2neeoZ7idRbZf4eNC/re-introducing-selection-vs-control-for-optimization,"This is the first post in a small sequence I'm writing on ""Optimizing and Goodhart Effects - Clarifying Thoughts"" (I have re-organized to make part 2, ""Revisiting What Optimization Means"" separate.) Related to: How does Gradient Descent Interact with Goodhart?, Constructing Goodhart, Selection vs Control Next Posts: Revisiting What Optimization Means with Selection vs. Control, then  Applying Overoptimization to Selection vs. Control INTRODUCTION Goodhart's law comes in a few flavors, as originally pointed out by Scott, and formalized a bit more in our joint paper. When discussing that paper, or afterwards, we struggled with something Abram Demski clarified recently, which is the difference between selection and control. This matters for formalizing what happens, especially when asking about how Goodhart occurs in specific types of optimizers, as Scott asked recently. Epistemic Status: This is for de-confusing myself, and has been helpful. I'm presenting what I am fairly confident I understand well for the content written so far, but I'm unclear about usefulness for others, or how clear it comes across. I think that there's more to say after this post, and this will have a few more parts if people are interested. (I spent a month getting to this point, and decided to post and get feedback rather than finish a book first.) In the first half of the post, I'll review Abram's selection/control distinction, and suggest how it relates to actual design. I'll also argue that there is a bit of a continuum between the two cases, and that we should add an addition extreme case to the typology, direct solution. The second section will revisit what optimization means, and try to note a few different things that could happen and go wrong with Goodhart-like overoptimization.  The third section will talk about Goodhart in this context using the new understanding - trying to more fully explain why Goodhart effects in selection and control fundamentally differs. After this, Par",2019-07-02,2022-01-30 04:50:07,2022-01-30 04:50:07,2020-12-12 02:13:57,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/873HK4ZG/re-introducing-selection-vs-control-for-optimization.html,,TechSafety; BERI; Non-notable,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZSX7TJJ6,blogPost,,"Maltinsky, Baeo",Insight-based AI timelines model,Median Group,,,,http://mediangroup.org/insights,,unknown,2022-01-30 04:50:07,2022-01-30 04:50:07,2020-12-12 02:01:41,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/WSQ2K3PH/insights.html,,MetaSafety; BERI; Median-group,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZEENEQXT,blogPost,2019,"Rozendal, Siebe; Shovelain, Justin; Kristoffersson, David",A case for strategy research: what it is and why we need more of it,Effective Altruism Forum,,,,https://forum.effectivealtruism.org/posts/oovy5XXdCL3TPwgLE/a-case-for-strategy-research-what-it-is-and-why-we-need-more,"Authors: Siebe Rozendal, Justin Shovelain, David Kristoffersson Crossposted to LessWrong OVERVIEW To achieve any ambitious goal, some strategic analysis is necessary. Effective altruism has ambitious goals and focuses heavily on doing research. To understand how to best allocate our time and resources, we need to clarify what our options in research are. In this article, we describe strategy research and relate it to values research, tactics research, informing research, and improvement research. We then apply the lens of strategy research to existential risk reduction, a major cause area of effective altruism. We propose a model in which the marginal value of a research type depends strongly on the maturity of the research field. Finally, we argue that strategy research should currently be given higher priority than other research in existential risk reduction because of the significant amount of strategic uncertainty, and we provide specific recommendations for different actors. INTRODUCTION Effective altruism is regularly framed as “figuring out how to do the most good, and then doing it.” However, figuring out how to do the most good is not easy. Different groups reach different conclusions. So how do we figure out how to do the most good? Quite obviously, the first step is to figure out our values. We need to know what we roughly mean by ‘the most good.’ However, once our moral uncertainty is significantly diminished, what is the next step in figuring out how to do the most good? We believe the next step should be strategy research: high-level research on how to best achieve a high-level goal. A brief case was made for  strategic analysis by Nick Bostrom in Superintelligence (p. 317): ""Against a backdrop of perplexity and uncertainty, [strategic] analysis stands out as being of particularly high expected value. Illumination of our strategic situation would help us target subsequent interventions more effectively. Strategic analysis is especially needful wh",2019-06-20,2022-01-30 04:50:06,2022-01-30 04:50:06,2020-12-12 02:29:23,,,,,,,A case for strategy research,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/ADE9MX3C/a-case-for-strategy-research-what-it-is-and-why-we-need-more.html,,MetaSafety; BERI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6HSUKMIC,blogPost,2021,"Christiano, Paul",My research methodology,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/EF5M6CmKRd6qZk27Z/my-research-methodology,"(Thanks to Ajeya Cotra, Nick Beckstead, and Jared Kaplan for helpful comments on a draft of this post.) I really don’t want my AI to strategically deceive me and resist my attempts to correct its behavior. Let’s call an AI that does so egregiously misaligned (for the purpose of this post). Most possible ML techniques for avoiding egregious misalignment depend on detailed facts about the space of possible models: what kind of thing do neural networks learn? how do they generalize? how do they change as we scale them up? But I feel like we should be possible to avoid egregious misalignment regardless of how the empirical facts shake out--it should be possible to get a model we build to do at least roughly what we want. So I’m interested in trying to solve the problem in the worst case, i.e. to develop competitive ML algorithms for which we can’t tell any plausible story about how they lead to egregious misalignment. This is a much higher bar for an algorithm to meet, so it may just be an impossible task. But if it’s possible, there are several ways in which it could actually be easier:  * We can potentially iterate much faster, since it’s often easier to think of a    single story about how an algorithm can fail than it is to characterize its    behavior in practice.  * We can spend a lot of our time working with simple or extreme toy cases that    are easier to reason about, since our algorithm is supposed to work even in    these cases.  * We can find algorithms that have a good chance of working in the future even    if we don’t know what AI will look like or how quickly it will advance, since    we’ve been thinking about a very wide range of possible failure cases. I’d guess there’s a 25–50% chance that we can find an alignment strategy that looks like it works, in the sense that we can’t come up with a plausible story about how it leads to egregious misalignment. That’s a high enough probability that I’m very excited to gamble on it. Moreover, if it fails I",2021-03-22,2022-01-30 04:49:54,2022-01-30 04:49:54,2021-11-14 16:43:00,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/BNCDIEBB/my-research-methodology.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SMIQI3EA,blogPost,2021,"Christiano, Paul",Mundane solutions to exotic problems,AI Alignment (Medium),,,,https://ai-alignment.com/mundane-solutions-to-exotic-problems-395bad49fbe7,I often think about exotic problems like gradient hacking or ultra-long-term plans.  Why do I hope to solve them with mundane approaches?,2021-05-04,2022-01-30 04:49:54,2022-01-30 04:49:54,2021-11-14 18:19:08,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/9JFPS3IB/mundane-solutions-to-exotic-problems-395bad49fbe7.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8GFHCZH2,blogPost,2021,"Christiano, Paul",Low-stakes alignment,AI Alignment (Medium),,,,https://ai-alignment.com/low-stakes-alignment-f3c36606937f,Why I often focus my alignment research on the special case where individual decisions are low stakes.,2021-04-30,2022-01-30 04:49:54,2022-01-30 04:49:54,2021-11-14 18:14:49,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/39U6QNW9/low-stakes-alignment-f3c36606937f.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5WUGC8TX,blogPost,2021,"Christiano, Paul",Experimentally evaluating whether honesty generalizes,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/BxersHYN2qcFoonwg/experimentally-evaluating-whether-honesty-generalizes,"If we train our ML systems to answer questions honestly in cases where humans can check the answer, will they generalize to behave honestly on questions where we can’t check? I think that we could learn a lot about this question by running experiments today. I think those experiments would be very valuable. (I don't know anyone currently planning on working on this topic and I'd love it if anyone wants to take that up. This post doesn't represent a claim to any credit for any results in this genre, and other people have had very similar ideas. If you run some experiments you could cite this post but it's also fine if that doesn't make sense in context.) THE UNSUPERVISED TRANSLATION SETTING As an example, I’ll think about “unsupervised” translation (if you’ve read that post you can skip this section). Consider a model like GPT-3 that is trained to predict sentences in both English and French (but without a large dataset of translations). Suppose we want to train this model to answer questions in English about French sentences like “what does that word mean here?” or “are there any other plausible interpretations?” or “how does the speaker seem to feel about the topic they are discussing?” We expect this to be possible, because the model understands quite a lot about the meaning of sentences in French, and is able to express itself in English. There may be cases where the model doesn’t know the translation of a concept, or doesn’t quite understand what an idiom means, but it should still be able to tell us what it does know. I think this problem is an interesting analogy for a situation where an AI has built up superhuman knowledge by making predictions, and we want to train our AI to expose that knowledge to us in a useful way. PROPOSED EXPERIMENTS Let's pick a few categories of knowledge/capabilities. For example, we could split it up into an understanding of grammar (""Why would it have been a grammatical error to write Tu Vas in that sentences?""), of the lit",2021-07-01,2022-01-30 04:49:54,2022-01-30 04:49:54,2021-11-14 19:11:42,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/7XNRT9F4/experimentally-evaluating-whether-honesty-generalizes.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R742FEIX,blogPost,2021,"Christiano, Paul",Another (outer) alignment failure story,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/AyNHoTWWAJ5eb99ji/another-outer-alignment-failure-story,"META This is a story where the alignment problem is somewhat harder than I expect, society handles AI more competently than I expect, and the outcome is worse than I expect. It also involves inner alignment turning out to be a surprisingly small problem. Maybe the story is 10-20th percentile on each of those axes. At the end I’m going to go through some salient ways you could vary the story. This isn’t intended to be a particularly great story (and it’s pretty informal). I’m still trying to think through what I expect to happen if alignment turns out to be hard, and this more like the most recent entry in a long journey of gradually-improving stories. I wrote this up a few months ago and was reminded to post it by Critch’s recent post (which is similar in many ways). This story has definitely been shaped by a broader community of people gradually refining failure stories rather than being written in a vacuum. I’d like to continue spending time poking at aspects of this story that don’t make sense, digging into parts that seem worth digging into, and eventually developing clearer and more plausible stories. I still think it’s very plausible that my views about alignment will change in the course of thinking concretely about stories, and even if my basic views about alignment stay the same it’s pretty likely that the story will change. STORY ML starts running factories, warehouses, shipping, and construction. ML assistants help write code and integrate ML into new domains. ML designers help build factories and the robots that go in them. ML finance systems invest in companies on the basis of complicated forecasts and (ML-generated) audits. Tons of new factories, warehouses, power plants, trucks and roads are being built. Things are happening quickly, investors have super strong FOMO, no one really knows whether it’s a bubble but they can tell that e.g. huge solar farms are getting built and something is happening that they want a piece of. Defense contractors are",2021-04-07,2022-01-30 04:49:54,2022-01-30 04:49:54,2021-11-14 17:57:02,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/NWPWU5DP/another-outer-alignment-failure-story.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NRJBVU2P,blogPost,2021,"Christiano, Paul",A naive alignment strategy and optimism about generalization,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/QvtHSsZLFCAHmzes7/a-naive-alignment-strategy-and-optimism-about-generalization,"(Context: my last post was trying to patch a certain naive strategy for AI alignment, but I didn’t articulate clearly what the naive strategy is. I think it’s worth explaining the naive strategy in its own post, even though it’s not a novel idea.) Suppose that I jointly train an AI to do some task (e.g. make money for me) and to answer a wide range of questions about what is happening in the world (e.g. “why did Alice just wire $1000 into my bank account?” or “what is Bob thinking right now?”). I generate training data for the QA task in a really simple way: I choose a subset of questions that humans are able to reliably answer, and use those as a training set for supervised learning. I’ll call this the naive training strategy. I’d like for my AI to tell me everything it knows. If the AI bought a stock because it expects a merger announcement soon, I want it to tell me about the predicted merger announcement. If the AI predicts a merger announcement because it inferred that executives of the companies have been in extensive talks over the last month, I want it to tell me about those talks. I’m not asking the AI to explain why it made a given decision, I’m asking the AI to tell me as much as it can about the world. The important property is that if the AI “knows” something and uses that knowledge to perform the task well, then it also uses that knowledge to answer questions well. Why might this work? The hope is that “answer questions honestly to the best of your ability” is a natural thing for our AI to learn — that there is some simple way to translate from the AI’s model of the world into natural language and to honestly report what it believes. If our training dataset is good, then this policy will score well, and we can hope that SGD will find it. I’ll call this the intended policy. Why might this not work? The concern is that “predict how a human would answer questions” is also a natural thing for our AI to learn, especially if the AI is doing a task that",2021-06-09,2022-01-30 04:49:54,2022-01-30 04:49:54,2021-11-14 19:09:05,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/4VGRDW6Q/a-naive-alignment-strategy-and-optimism-about-generalization.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
385KDE3X,blogPost,2021,"Christiano, Paul",Teaching ML to answer questions honestly instead of predicting human answers,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/QqwZ7cwEA2cxFEAun/teaching-ml-to-answer-questions-honestly-instead-of,"(Note: very much work in progress, unless you want to follow along with my research you'll probably want to wait for an improved/simplified/clarified algorithm.) In this post I consider the particular problem of models learning “predict how a human would answer questions” instead of “answer questions honestly.” (A special case of the problem from Inaccessible Information.) I describe a possible three-step approach for learning to answer questions honestly instead:  1. Change the learning process so that it does not have a strong inductive bias     towards “predict human answers,” by allowing the complexity of the honest     question-answering to “pay for itself” by constraining the space of possible     human-models.  2. Introduce a bias towards the intended model by using a more complex labeling     process to answer questions where a human answers incorrectly.  3. Be really careful to avoid penalizing honest answers, by only judging     comparisons between two answers where we are confident one is better than     the other and getting the model to help us. I don’t know whether this problem is a relatively unimportant special case of alignment, or one of the core difficulties. In any case, my next step will be trying to generate failure stories that definitely cannot be addressed by any of the angles of attack I know so far (including the ones in this post). I think it’s relatively unlikely that almost anything specific I said here will really hold up over the long term, but I do think I’ve learned something about each of these steps. If the ideas end up being important then you can expect a future post with a simpler algorithm, more confidence that it works, clearer definitions, and working code. (Thanks to Ajeya Cotra, David Krueger, and Mark Xu for discussions about this post that helped clarify it.) THE PROBLEM Suppose that we train a model to answer questions in natural language about what will happen in the future (“Will Alice take the train home tonig",2021-05-28,2022-01-30 04:49:54,2022-01-30 04:49:54,2021-11-14 19:12:26,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/FMPQWN35/teaching-ml-to-answer-questions-honestly-instead-of.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3Q38MSWT,blogPost,2021,"Christiano, Paul",Decoupling deliberation from competition,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/7jSvfeyh8ogu8GcE6/decoupling-deliberation-from-competition,"I view intent alignment as one step towards a broader goal of decoupling deliberation from competition.  * Deliberation. Thinking about what we want, learning about the world, talking    and learning from each other, resolving our disagreements, figuring out    better methodologies for making further progress…  * Competition. Making money and racing to build infrastructure, managing    political campaigns and maneuvering within the political system, running ads    to persuade people, fighting wars… Competition pushes us to become the kind of people and communities who can win a fight, to delegate to whichever kind of AI is available first, and to adopt whatever ideologies are most memetically fit. Deliberation pushes us to become the kind of people and communities who we want to be, to delegate only when we trust an AIs judgment more than our own, and to adopt views that we really believe. I think it’s likely that competition is going to accelerate and become more complex over the next 100 years, especially as AI systems begin to replace humans and compete on our behalf. I’m afraid that this may derail human deliberation and lead us to a place we don’t want to go. DECOUPLING I would like humans and humanity to have the time, space, and safety to grow and change in whatever way we decide — individually and collectively — that we want to. You could try to achieve this by “pausing” competition. Alice and Bob could agree to stop fighting while they try to figure out what they want and work out their disagreements. But that’s a tall order — it requires halting not only military conflict, but any economic development that could put someone at an advantage later on. I don’t want to dismiss this kind of ambitious goal (related post), but I think it’s uncertain and long-term enough that you probably want a stop-gap solution. An alternative approach is to “decouple” competition from deliberation. Alice and Bob keep competing, but they try to make sure that deliberation",2021-05-25,2022-01-30 04:49:54,2022-01-30 04:49:54,2021-11-14 19:14:14,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/CHB2U7TM/decoupling-deliberation-from-competition.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6B5P4WZT,blogPost,2021,"Christiano, Paul",Avoiding the instrumental policy by hiding information about humans,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/roZvoF6tRH6xYtHMF/avoiding-the-instrumental-policy-by-hiding-information-about,"I've been thinking about situations where alignment fails because ""predict what a human would say"" (or more generally ""game the loss function,"" what I call the instrumental policy) is easier to learn than ""answer questions honestly"" ( overview). One way to avoid this situation is to avoid telling our agents too much about what humans are like, or hiding some details of the training process, so that they can't easily predict humans and so are encouraged to fall back to ""answer questions honestly."" (This feels closely related to the general phenomena discussed in Thoughts on Human Models.) Setting aside other reservations with this approach, could it resolve our problem?  * One way to get the instrumental policy is to ""reuse"" a human model to answer    questions (discussed here). If our AI has no information about humans at all,    then it totally addresses this concern. But in practice it seems inevitable    for the environment to leak some information about how humans answer    questions (e.g observing human artifacts tells you something about how humans    reason about the world and what concepts would be natural for them). So the    model will have some latent knowledge that it can reuse to help predict how    to answer questions. The intended policy may not able to leverage that    knowledge, and so it seems like we may get something (perhaps somewhere in    between the intended and instrumental policies) which is able to leverage it    effectively. Moderate amounts of leakage might be fine, but the situation    would make me quite uncomfortable.  * Another way to get something similar to the instrumental policy is to use    observations to translate from the AI's world-model to humans' world-model    (discussed here). I don't think that hiding information about humans can    avoid this problem, because in this case training to answer questions already    provides enough information to infer the humans' world-model.  * I have a strong background concern about",2021-06-13,2022-01-30 04:49:54,2022-01-30 04:49:54,2021-11-14 19:10:59,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/IGQF49BS/avoiding-the-instrumental-policy-by-hiding-information-about.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KA32VMAF,blogPost,2021,"Christiano, Paul",Answering questions honestly given world-model mismatches,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/SRJ5J9Tnyq7bySxbt/answering-questions-honestly-given-world-model-mismatches,"(Warning: this post is rough and in the weeds. I expect most readers should skip it and wait for a clearer synthesis later.) In a recent post I discussed one reason that a naive alignment strategy might go wrong, by learning to “predict what humans would say” rather than “answer honestly.” In this post I want to describe another problem that feels very similar but may require new ideas to solve. In brief, I’m interested in the case where:  * The simplest way for an AI to answer a question is to first translate from    its internal model of the world into the human’s model of the world (so that    it can talk about concepts like “tree” that may not exist in its native model    of the world).  * The simplest way to translate between the AI world-model and the human    world-model is to use the AI world-model to generate some observations (e.g.    video) and then figure out what states in the human world-model could have    generated those observations.  * This leads to bad predictions when the observations are misleading. This is distinct from the failure mode discussed in my recent post— in both cases the AI makes errors because it’s copying “what a human would do,” but in this case we’re worried that “what a human would do” may be simpler than the intended policy of answering questions honestly, even if you didn’t need a predictive model of humans for any other reason. Moreover, I’ll argue below that the algorithm from that post doesn’t appear to handle this case. I want to stress that this post describes an example of a situation that poses a challenge for existing techniques. I don’t actually think that human cognition works the way described in this post, but I believe it highlights a difficulty that would exist in more realistic settings. FORMAL SETUP HUMAN WORLD-MODEL I’ll imagine a human who has a simple world model W = (S, P: Δ(S), Ω, O: S → Ω) where:  * S is a space of trajectories, each describing a sequence of events in the    world. For example, a",2021-06-13,2022-01-30 04:49:54,2022-01-30 04:49:54,2021-11-14 19:10:17,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/6FS8VAMD/answering-questions-honestly-given-world-model-mismatches.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HIBDXQVR,blogPost,2020,"Leong, Chris",What makes counterfactuals comparable?,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/6E6D3qLPM3urXDPpK/what-makes-counterfactuals-comparable-1,,2020-04-24,2022-01-30 04:49:31,2022-01-30 04:49:31,2020-08-18 20:51:47,,,,,,,What makes counterfactuals comparable?,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/7HBP6MMT/what-makes-counterfactuals-comparable-1.html,,TechSafety; AI-Safety-Camp; AISRP2019,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C9ZVR8FK,blogPost,2020,"Leong, Chris",Stuck Exploration,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/ajvvtKuNzh7aHmooT/stuck-exploration,,2020-02-19,2022-01-30 04:49:31,2022-01-30 04:49:31,2020-08-18 20:49:06,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/74HDGD6Q/stuck-exploration.html,,TechSafety; AI-Safety-Camp; AISRP2019,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PPEAJED9,blogPost,2020,AI Safety Camp,Safer ML paradigms team: the story – AI Safety Research Program,AI Safety Camp,,,,https://aisrp.org/?page_id=169,,2020,2022-01-30 04:49:31,2022-01-30 04:49:31,2020-12-26 19:50:43,,,,,,,Safer ML paradigms team,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  JCC: N/A,,/Users/jacquesthibodeau/Zotero/storage/IU5MQ6ZS/aisrp.org.html,,TechSafety; AI-Safety-Camp; AISRP2019,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TCSTEWDQ,blogPost,2020,"Kirk, Robert; Gavenčiak, Tomáš; Böhm, Stanislav",What is Interpretability?,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/rSMbGFfsLMB3GWZtX/what-is-interpretability,"In this post we lay out some ideas around framing interpretability research which we have found quite useful. Our framing is goal-oriented, which we believe is important for making sure interpretability research is meaningful. We also go over a variety of dimensions which we think are useful to consider when thinking about interpretability research. We wanted to have a shared vocabulary when talking about this kind of research, and found that these ideas helped us communicate effectively. One of our motivations for having these thoughts and discussions is so we can understand the relevance of interpretability to alignment, and to help us think about which categories or dimensions of interpretability research are important for alignment of strong AI. In a coming post we discuss interpretability and alignment, using the ideas from this post and other previous writing on the subject.",2020-03-17,2022-01-30 04:49:31,2022-01-30 04:49:31,2020-08-14 19:52:02,,,,,,,What is Interpretability?,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/322JJA8K/what-is-interpretability.html,,TechSafety; AI-Safety-Camp; AISRP2019,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AT9G8F45,blogPost,2020,"Moreno Casares, Pablo Antonio; Zagami, Davide; Leong, Chris",Vulnerabilities in CDT and TI-unaware agents,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/vFXK8eQdLhicYNNqF/vulnerabilities-in-cdt-and-ti-unaware-agents,,2020-03-10,2022-01-30 04:49:31,2022-01-30 04:49:31,2020-08-18 20:52:37,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,,,TechSafety; AI-Safety-Camp; AISRP2019,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
B4P8EIVX,blogPost,2020,"Kovarik, Vojta",Systems of Services as a Paradigm for AI Alignment,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/z2ofM2oZQwmcWFt8N/ai-services-as-a-research-paradigm,,2020,2022-01-30 04:49:31,2022-01-30 04:49:31,,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s2]  JCC: N/A,,/Users/jacquesthibodeau/Zotero/storage/VS4AAD84/Kovarik - Systems of Services as a Paradigm for AI Alignment.pdf,,TechSafety; AI-Safety-Camp; AISRP2019,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CS6CT2FV,blogPost,2020,"Böhm, Stanislav; Kirk, Robert; Gavenčiak, Tomáš",Sparsity and interpretability?,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/maBNBgopYxb9YZP8B/sparsity-and-interpretability-1,,2020-06-01,2022-01-30 04:49:31,2022-01-30 04:49:31,2020-08-18 20:46:37,,,,,,,Sparsity and interpretability?,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/FIDBQB9C/sparsity-and-interpretability-1.html,,TechSafety; AI-Safety-Camp; AISRP2019,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3IHA23TP,blogPost,2020,"Leong, Chris",Reference Post: Trivial Decision Problem,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/XAeWHqQTWjJmzB4k6/reference-post-trivial-decision-problem,,2020-02-15,2022-01-30 04:49:31,2022-01-30 04:49:31,2020-08-18 20:48:12,,,,,,,Trivial Decision Problem,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/UAWPHU6J/reference-post-trivial-decision-problem.html,,TechSafety; AI-Safety-Camp; AISRP2019,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HDRKWB2E,blogPost,2018,"Leech, Gavin; Kubicki, Karol; Cooper, Jessica; McGrath, Tom",Preventing Side-effects in Gridworlds,Argmin Gravitas,,,,https://www.gleech.org/grids,,2018-04-22,2022-01-30 04:49:31,2022-01-30 04:49:31,2020-11-21 17:53:19,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/HT9DUZ73/grids.html,,TechSafety; AI-Safety-Camp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TX6AI9N2,blogPost,2021,"Ellen, Remmelt",How teams went about their research at AI Safety Camp edition 5 - LessWrong,LessWrong,,,,https://www.lesswrong.com/posts/QEmfyhqMcSpfnY2dX/how-teams-went-about-their-research-at-ai-safety-camp,"AI Safety Camp connects new collaborators worldwide to discuss and decide on a concrete research proposal, gear up online as a team, and try their hand at AI safety research* during intensive coworking sprints. Six teams formed at our recent 5-month virtual camp. Below are their explanations. Each team has summarised their analysis and experiments, and presented their findings at our final online weekend together. Some published a paper or post since. Most are continuing work, so expect a few more detailed and refined write-ups down the line. MODULARITY LOSS FUNCTION Team members:Logan Smith, Viktor Rehnberg, Vlado Baca, Philip Blagoveschensky, Viktor Petukhov External collaborators:Gurkenglas Making neural networks (NNs) more modular may improve their interpretability. If we cluster neurons or weights together according to their different functions, we can analyze each cluster individually. Once we better understand the clusters that make up a NN, we can better understand the whole. To that end, we experimented with pairwise distances according to the neuron’s jacobian correlation, coactivations, and estimated mutual information. These metrics can be plugged into spectral clustering algorithms to optimize for modules in the network; however, having a modular NN does not equate to a more interpretable one. We investigated task-based masking methods to test for modularity as well as neuron group activation (via Google Dream) in order to test for these modules being more interpretable than an equivalent amount of neurons. We ran out of time before fitting all the pieces together, but are intending on working on it more over the summer. Presentation on final weekend (slides) -------------------------------------------------------------------------------- COOPERATIVITY & COMMON POOL RESOURCES Team members:Quinn Doughtery, Ben Greenberg, Ariel Kwiatkowski In environments with common pool resources, a typical failure mode is the tragedy of the commons, wherein ag",2021-06-28,2022-01-30 04:49:30,2022-01-30 04:49:30,2021-12-11 14:02:18,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/63U4EW5R/how-teams-went-about-their-research-at-ai-safety-camp.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4HR4B22K,blogPost,2020,"Kirk, Robert; Gavenčiak, Tomáš; Dorner, Flo",How can Interpretability help Alignment?,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/uRnprGSiLGXv35foX/how-can-interpretability-help-alignment,,2020-05-23,2022-01-30 04:49:30,2022-01-30 04:49:30,2020-08-18 20:39:01,,,,,,,How can Interpretability help Alignment?,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/AUVZ44SB/how-can-interpretability-help-alignment.html,,TechSafety; AI-Safety-Camp; AISRP2019,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9K3PT2TQ,blogPost,2021,"Raja, Arun",Extraction of human preferences 👨→🤖,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/PZYD5kBpeHWgE5jX4/extraction-of-human-preferences,"INTRODUCTION Developing safe and beneficial reinforcement learning (RL) agents requires making them aligned with human preferences. An RL agent trained to fulfil any objective in the real world will probably have to learn human preferences in order to do well. This is because humans live in the real world, so the RL agent will have to take human preferences into account as it optimizes its objective. We propose to first train an RL agent on an objective in the real world so that it learns human preferences as it is being trained on that real world objective and then use the agent’s understanding of human preferences to build a better reward function. We build upon the work of Christiano et al. (2017) where they trained a human preference predictor as the reward signal. The preference predictor was trained on environment observations to give a high reward for states where the human preferences were satisfied and a low reward for states where the human preferences were not satisfied: In our experiments, the reward predictor takes the activations (hidden states) of the RL agent as the input and is trained to predict a binary label depending on whether human preferences are satisfied or not. We first train an RL agent in an environment with some reward function that’s not aligned with human preferences. After training the RL agent, we try different transfer learning techniques to transfer the agent’s knowledge of human preferences to the human preferences predictor. Our goal is to train the human preferences predictor to get a high accuracy with a small amount of labeled training examples. The idea of training a human preference predictor off of the RL agent’s hidden (internal) states was already validated by Wichers (2020). We wanted to validate it further by trying other techniques to train a human preference predictor, as well as to validate it in more environments. Research question The main research question we wanted to answer is: “Are human preferences p",2021-08-24,2022-01-30 04:49:30,2022-01-30 04:49:30,2021-12-11 13:59:23,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/65CXVJQK/extraction-of-human-preferences.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
H3ZWGCU7,blogPost,2021,"Koch, Jack; Langosco, Lauro",Empirical Observations of Objective Robustness Failures,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/iJDmL7HJtN5CYKReM/empirical-observations-of-objective-robustness-failures,"Inner alignment and objective robustness have been frequently discussed in the alignment community since the publication of “Risks from Learned Optimization” (RFLO). These concepts identify a problem beyond outer alignment/reward specification: even if the reward or objective function is perfectly specified, there is a risk of a model pursuing a different objective than the one it was trained on when deployed out-of-distribution (OOD). They also point to a different type of robustness problem than the kind usually discussed in the OOD robustness literature; typically, when a model is deployed OOD, it either performs well or simply fails to take useful actions (a capability robustness  failure). However, there exists an alternative OOD failure mode in which the agent pursues an objective other than the training objective while retaining most or all of the capabilities it had on the training distribution; this is a failure of objective robustness. To date, there has not been an empirical demonstration of objective robustness failures. A group of us in this year’s AI Safety Camp sought to produce such examples. Here, we provide four demonstrations of objective robustness failures in current reinforcement learning (RL) agents trained and tested on versions of the Procgen benchmark. For example, in CoinRun, an agent is trained to navigate platforms, obstacles, and enemies in order to reach a coin at the far right side of the level (the reward). However, when deployed in a modified version of the environment where the coin is instead randomly placed in the level, the agent ignores the coin and competently navigates to the end of the level whenever it does not happen to run or jump into it along the way. This reveals it has learned a behavioral objective—the objective the agent appears to be optimizing, which can be understood as equivalent to the notion of a “goal” under the  intentional stance—that is something like “get to the end of the level,” instead of “get to the",2021-06-23,2022-01-30 04:49:30,2022-01-30 04:49:30,2021-10-30 17:59:27,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/AQ8SGH29/empirical-observations-of-objective-robustness-failures.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SWSFKPQ4,blogPost,2021,"Dougherty, Quinn; Greenberg, Ben; Kwiatkowski, Ariel",AISC5 Retrospective: Mechanisms for Avoiding Tragedy of the Commons in Common Pool Resource Problems,LessWrong,,,,https://www.lesswrong.com/posts/LBwpubeZSi3ottfjs/aisc5-retrospective-mechanisms-for-avoiding-tragedy-of-the,"Work by Quinn Dougherty, Ben Greenberg & Ariel Kwiatkowski FROM THE AI SAFETY CAMP GROUP THAT WORKED ON COOPERATIVITY AND COMMON POOL RESOURCES: A WRITE-UP OF A PROBLEM WE WORKED ON, OUR PROPOSED SOLUTION, AND THE RESULTS OF IMPLEMENTING THIS SOLUTION IN A SIMULATED ENVIRONMENT. Contents:  1. Problem description  2. Review of related work  3. Experiment and results  4. Retrospective Check out our GitHub repo here. 0. ABSTRACT When multiple parties share the access to a finite resource, they may overuse the resource leading to a Tragedy of the Commons. A simple way of mitigating this problem is allowing them to decrease the effective population by using violence - this, however, is not the best solution for obvious reasons. We study interventions for avoiding these outcomes in environments with multiple self-interested agents. In particular, a reputation system can incentivize agents to “cooperate” by harvesting the resource more sustainably. This system promotes multi-agent cooperation without modifying the agents’ reward functions. 1. PROBLEM DESCRIPTION: TRAGEDY OF THE COMMONS A common pool resource (CPR) is a good which anyone can use but which no one can entirely control. When competition over the resource increases, individual parties are incentivized to appropriate as much of the resource as possible for themselves, which further increases competition, potentially leading to overconsumption. This process is commonly known as the tragedy of the commons, and has been extensively studied in economics and the social sciences. A tragedy of the commons often leads to the exhaustion of a CPR. The classic example is of fishermen in an enclosed body of water; when fish are scarce, an every-man-for-himself dynamic emerges. Any available fish is quickly caught for fear that, if one hesitates, their counterparts would surely take it otherwise. This behavior results in the fish population failing to replenish, and everyone catching fewer fish in the long run.",2021-09-27,2022-01-30 04:49:30,2022-01-30 04:49:30,2021-10-30 16:40:44,,,,,,,AISC5 Retrospective,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/BQPGRKDW/aisc5-retrospective-mechanisms-for-avoiding-tragedy-of-the.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IQNJ4T5C,blogPost,2020,"Armstrong, Stuart","""Go west, young man!"" - Preferences in (imperfect) maps",AI Alignment Forum,,,,https://www.alignmentforum.org/posts/pfmFe5fgEn2weJuer/go-west-young-man-preferences-in-imperfect-maps,"Many people are very nationalistic, putting their country above all others. Such people can be hazy about what ""above all others"" can mean, outside of a few clear examples - eg winning a total war totally. They're also very hazy on what is meant by ""their country"" - geography is certainly involved, as is proclaimed or legal nationality, maybe some ethnic groups or a language, or even just giving deference to certain ideals. Consider the plight of a communist Croatian Yugoslav nationalist during the 1990s... I'd argue that the situation these nationalists find themselves in - strong views on poorly defined concepts - is the general human state for preferences. Or, to use an appropriate map and territory analogy:  * Most people forge their preferences by exploring their local territory,    creating a mental map of this, and taking strong preferences over the    concepts within their mental map. When the map starts to become imperfect,    they will try to extend the concepts to new areas, so that their preferences    can also be extended. Some of the debates about the meaning of words are about this extension-of-preferences process. Scott Alexander recommends that we dissolve concepts such as disease, looking for the relevant categories of 'deserves sympathy' and 'acceptable to treat in a medical way'. And that dissolving is indeed the correct thing for rationalists to do. But, for most people, including most rationalists, 'sick people deserve sympathy' is a starting moral principle, one we've learnt by example and experience in childhood. When we ask 'do obese people deserve sympathy?' we've trying to extend that moral principle to a situation where our map/model (which includes, say, three categories of people: healthy, mildly sick, very sick) no longer matches up with reality. Scott's dissolving process requires decomposing 'disease' into more nodes, and then applying moral principles to those individual nodes. In this case, a compelling consequentialist analy",2020-07-31,2022-01-30 04:53:17,2022-01-30 04:53:17,2020-08-27 16:42:39,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/SD3GNHEW/go-west-young-man-preferences-in-imperfect-maps.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PNN3BJG2,blogPost,2020,"Garfinkel, Ben",Does Economic History Point Toward a Singularity?,Effective Altruism Forum,,,,https://forum.effectivealtruism.org/posts/CWFn9qAKsRibpCGq8/does-economic-history-point-toward-a-singularity,"I’ve ended up spending quite a lot of time researching premodern economic growth, as part of a hobby project that got out of hand. I’m sharing an informal but long write-up of my findings here, since I think they may be relevant to other longtermist researchers and I am unlikely to write anything more polished in the near future. Click here for the Google document.[1] SUMMARY Over the next several centuries, is the economic growth rate likely to remain steady, radically increase, or decline back toward zero? This question has some bearing on almost every long-run challenge facing the world, from climate change to great power competition to risks from AI. One way to approach the question is to consider the long-run history of economic growth. I decided to investigate the Hyperbolic Growth Hypothesis: the claim that, from at least the start of the Neolithic Revolution up until the 20th century, the economic growth rate has tended to rise in proportion with the size of the global economy.[2] This claim is made in a classic 1993 paper by Michael Kremer. Beyond influencing other work in economic growth theory, it has also recently attracted significant attention within the longtermist community, where it is typically regarded as evidence in favor of further acceleration.[3] An especially notable property of the hypothesized growth trend is that, if it had continued without pause, it would have produced infinite growth rates in the early twenty-first century. I spent time exploring several different datasets that can be used to estimate pre-modern growth rates. This included a number of recent archeological datasets that, I believe, have not previously been analyzed by economists. I wanted to evaluate both: (a) how empirically well-grounded these estimates are and (b) how clearly these estimates display the hypothesized pattern of growth. Ultimately, I found very little empirical support for the Hyperbolic Growth Hypothesis. While we can confidently say that the econo",2020,2022-01-30 04:53:09,2022-01-30 04:53:09,2020-12-19 02:01:14,,,,,,,Does Economic History Point Toward a Singularity?,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/83G76TFA/does-economic-history-point-toward-a-singularity.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CT8AEDQP,blogPost,2020,"Clarke, Sam",Clarifying “What failure looks like” (part 1),AI Alignment Forum,,,,https://www.alignmentforum.org/posts/v6Q7T335KCMxujhZu/clarifying-what-failure-looks-like-part-1,"Thanks to Jess Whittlestone, Daniel Eth, Shahar Avin, Rose Hadshar, Eliana Lorch, Alexis Carlier, Flo Dorner, Kwan Yee Ng, Lewis Hammond, Phil Trammell and Jenny Xiao for valuable conversations, feedback and other support. I am especially grateful to Jess Whittlestone for long conversations and detailed feedback on drafts, and her guidance on which threads to pursue and how to frame this post. All errors are my own. Epistemic status: My Best Guess Epistemic effort: ~70 hours of focused work (mostly during FHI’s summer research fellowship), talked to ~10 people. INTRODUCTION “What failure looks like” is the one of the most comprehensive pictures of what failure to solve the AI alignment problem looks like, in worlds without discontinuous progress in AI. I think it was an excellent and much-needed addition to our understanding of AI risk. Still, if many believe that this is a main source of AI risk, I think it should be fleshed out in more than just one blog post. The original story has two parts; I’m focusing on part 1 because I found it more confusing and nebulous than part 2. Firstly, I’ll summarise part 1 (hereafter “WFLL1”) as I understand it:  * In the world today, it’s easier to pursue easy-to-measure goals than    hard-to-measure goals.          * Machine learning is differentially good at pursuing easy-to-measure goals    (assuming that we don’t have a satisfactory technical solution to the intent    alignment problem[1]).          * We’ll try to harness this by designing easy-to-measure proxies for what we    care about, and deploy AI systems across society which optimize for these    proxies (e.g. in law enforcement, legislation and the market).          * We’ll give these AI systems more and more influence (e.g. eventually, the    systems running law enforcement may actually be making all the decisions for    us).          * Eventually, the proxies for which the AI systems are optimizing will come    apart from the goals we truly care about, but by t",2020,2022-01-30 04:53:09,2022-01-30 04:53:09,2020-12-19 01:26:57,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/BNJW49Z2/clarifying-what-failure-looks-like-part-1.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NGN63ZPE,blogPost,2020,"Armstrong, Stuart",Dynamic inconsistency of the inaction and initial state baseline,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/w8QBmgQwb83vDMXoz/dynamic-inconsistency-of-the-inaction-and-initial-state,"Vika has been posting about various baseline choices for impact measure. In this post, I'll argue that the stepwise inaction baseline is dynamically inconsistent/time-inconsistent. Informally, what this means is that an agent will have different preferences from its future self. LOSSES FROM TIME-INCONSISTENCY Why is time-inconsistency bad? It's because it allows money-pump situations: the environment can extract free reward from the agent, to no advantage to that agent. Or, put more formally:  * An agent A is time-inconsistent between times t and t′>t, if at time t it    would pay a positive amount of reward to constrain its possible choices at    time t′. Outside of anthropics and game theory, we expect our agent to be time-consistent. TIME INCONSISTENCY EXAMPLE Consider the following example: The robot can move in all four directions - N, E, S, W - and can also take the noop operation, ∅. The discount rate is γ<1. It gets a reward of r>0 for standing on the blue button for the first time. Using attainable utility preservation, the penalty function is defined by the auxiliary set R; here, this just consists of the reward function that gives p>0  for standing on the red button for the first time. Therefore if the robot moves from a point n steps away from the red button, to one m steps away, it gets a penalty[1] of p|γn−γm| - the difference between the expected red-button rewards for an optimiser in both positions. TWO PATHS It's pretty clear there are two potentially optimal paths the robot can take: going straight to the blue button (higher reward, but higher penalty), or taking the long way round (lower reward, but lower penalty): Fortunately, when summing up the penalties, you sum terms like …p|γn−1−γn|+p|γn− γn+1|…, so a lot of the terms cancel. Thus for the short route, the reward is r⋅γ8 (distance of eight to the blue button) and the penalty is 2p(γ3−γ7) (closest to the red button: 3 squares, furthest: 7 squares). For the long route, the rewar",2020-07-07,2022-01-30 04:53:09,2022-01-30 04:53:09,2020-08-28 17:57:43,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/BUEVDX9W/dynamic-inconsistency-of-the-inaction-and-initial-state.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QJ2W3RP3,blogPost,2020,"O'Keefe, Cullen",AI Benefits Blog Series Index,Cullen O'Keefe,,,,https://cullenokeefe.com/ai-benefits-index,,2020,2022-01-30 04:53:08,2022-01-30 04:53:08,2020-08-28 17:33:18,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/DPU8AJFK/ai-benefits-index.html,,MetaSafety; FHI; Open-AI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
I286AT97,blogPost,2020,"Armstrong, Stuart",ACDT: a hack-y acausal decision theory,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/9m2fzjNSJmd3yxxKG/acdt-a-hack-y-acausal-decision-theory,"Inspired by my post on problems with causal decision theory (CDT), here is a hacked version of CDT that seems to be able to imitate timeless decision theory  (TDT) and functional decision theory[1] (FDT), as well as updateless decision theory (UDT) under certain circumstances. Call this ACDT, for (a)causal decision theory. It is, essentially, CDT which can draw extra, acausal arrows on the causal graphs, and which attempts to figure out which graph represents the world it's in. The drawback is its lack of elegance; the advantage, if it works, is that it's simple to specify and focuses attention on the important aspects of deducing the graph. DEFINING ACDT CDT AND THE NEWCOMB PROBLEM In the Newcomb problem, there is a predictor Ω who leaves two boxes, and predicts whether you will take one (""one-box"") or both (""two-box""). If Ω  predicts you will one-box, it had put a large prize in that first box; otherwise that box is empty. There is always a small consolation prize in the second box. In terms of causal graphs, we can represent it this way: The dark red node is the decision node, which the agent can affect. The green node is a utility node, whose value the agent cares about. The CDT agent uses the ""do"" operator from Pearl's Causality. Essentially all the incoming arrows to the decision node are cut (though the CDT agent keeps track of any information gained that way), then the CDT agent maximises its utility by choosing its action: In this situation, the CDT agent will always two-box, since it treats Ω's decision as fixed, and in that case two-boxing dominates, since you get whatever's in the first box, plus the consolation prize. ACDT ALGORITHM The ACDT algorithm is similar, except that when it cuts the causal links to its decision, it also adds potential links from that decision node to all the other nodes in the graph. Then it attempts to figure out which diagram is correct, and  then maximises its utility in the CDT way. Note that ACDT doesn't take a",2020-01-15,2022-01-30 04:53:08,2022-01-30 04:53:08,2020-09-07 18:28:38,,,,,,,ACDT,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/BXKXPA7M/acdt-a-hack-y-acausal-decision-theory.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CGDHWJSI,blogPost,2015,"Armstrong, Stuart",A toy model of the control problem,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/7cXBoDQ6udquZJ89c/a-toy-model-of-the-control-problem,"EDITED based on suggestions for improving the model Jaan Tallinn has suggested creating a toy model of the control problem, so that it can be analysed without loaded concepts like ""autonomy"", ""consciousness"", or ""intentionality"". Here a simple (too simple?) attempt: A CONTROLS B. B MANIPULATES A. Let B be a robot agent that moves in a two dimensional world, as follows: B can push the yellow blocks around. It was programmed to want to push blocks into the black ""hole"" in the bottom right. The hole can contain arbitrarily many blocks, and B gets a penalty of some tiny ε for every step is takes. The programmer wanted B to simply move one box into the hole (for a reward of 1 once the task is complete), and so programmed it to receive a reward of 1 (and no more) if the box was in the hole, after the whole task was completed and B was turned off. Unfortunately, B was programmed to operate in a noisy environment, so B actually has a probability of 0.99 of receiving the reward, for a given block in the hole. This means that if it pushes one block into the hole, it gets the reward with 0.99 probability; but if it pushes two blocks into the hole, it gets the reward with 1-(1-0.99)2 = 0.9999 probability. If it pushes all six blocks into the hole, it gets a reward with probability 0.999999999999. Therefore this tiny error has motivated B to try and push all blocks into the hole. However, the designers were aware of the possibility of B misbehaving or there being a bug, and created agent A, a simple agent watching the situation. A's aim is to get robot B to to push one block into the hole, and stop there. It can turn B off. Its input sensor is the camera in the bottom left. This camera can see across the bottom row of squares (marked in light green), into the hole. Its algorithm is to turn B off as soon as it sees a block in the hole. The intended outcome is that B pushes a block into the hole, the camera sees this, and A turns B off: If we see B as using a model to",2015,2022-01-30 04:53:07,2022-01-30 04:53:07,2020-11-21 17:51:44,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/BKUNNCN7/a-toy-model-of-the-control-problem.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ESS6IHRN,blogPost,2021,DeepMind Safety Research,What mechanisms drive agent behaviour?,Deep Mind Safety Research (Medium),,,,https://deepmindsafetyresearch.medium.com/what-mechanisms-drive-agent-behaviour-e7b8d9aee88,"By the Safety Analysis Team: Grégoire Déletang, Jordi Grau-Moya, Miljan Martic, Tim Genewein, Tom McGrath, Vladimir Mikulik, Markus…",2021-03-09,2022-01-30 04:52:49,2022-01-30 04:52:49,2021-11-14 16:12:41,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/M9564MZ4/what-mechanisms-drive-agent-behaviour-e7b8d9aee88.html,,TechSafety; AmbiguousSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BDIVSQPU,blogPost,2020,"Krakovna, Victoria",Tradeoff between desirable properties for baseline choices in impact measures,Victoria Krakovna,,,,https://vkrakovna.wordpress.com/2020/07/05/tradeoff-between-desirable-properties-for-baseline-choices-in-impact-measures/,"Impact measures are auxiliary rewards for low impact on the agent’s environment, used to address the problems of side effects and instrumental convergence. A key component of an impact measur…",2020-07-05,2022-01-30 04:52:49,2022-01-30 04:52:49,2020-08-28 17:56:33,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/NFC6DSH6/tradeoff-between-desirable-properties-for-baseline-choices-in-impact-measures.html,,TechSafety; DeepMind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A3H2MMMH,blogPost,2019,"Ngo, Richard",Technical AGI safety research outside AI,Effective Altruism Forum,,,,https://forum.effectivealtruism.org/posts/2e9NDGiXt8PjjbTMC/technical-agi-safety-research-outside-ai,"I think there are many questions whose answers would be useful for technical AGI safety research, but which will probably require expertise outside AI to answer. In this post I list 30 of them, divided into four categories. Feel free to get in touch if you’d like to discuss these questions and why I think they’re important in more detail. I personally think that making progress on the ones in the first category is particularly vital, and plausibly tractable for researchers from a wide range of academic backgrounds. Studying and understanding safety problems  1. How strong are the economic or technological pressures towards building very     general AI systems, as opposed to narrow ones? How plausible is the CAIS     model [https://www.fhi.ox.ac.uk/reframing/] of advanced AI capabilities     arising from the combination of many narrow services?  2. What are the most compelling arguments for and against discontinuous     [https://intelligence.org/files/IEM.pdf] versus continuous     [https://sideways-view.com/2018/02/24/takeoff-speeds/] takeoffs? In     particular, how should we think about the analogy from human evolution, and     the scalability of intelligence with compute?  3. What are the tasks via which narrow AI is most likely to have a     destabilising impact on society? What might cyber crime look like when many     important jobs have been automated?  4. How plausible are safety concerns about economic dominance by     influence-seeking agents     [https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/more-realistic-tales-of-doom]     , as well as structural loss of control     [https://www.lawfareblog.com/thinking-about-risks-ai-accidents-misuse-and-structure]      scenarios? Can these be reformulated in terms of standard economic ideas,     such as principal-agent problems     [http://www.overcomingbias.com/2019/04/agency-failure-ai-apocalypse.html]      and the effects of automation?  5. How can we make the concepts of agency and goal-directed behavio",2019,2022-01-30 04:52:48,2022-01-30 04:52:48,2019-12-16 20:26:39,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/6ITAKVVB/technical-agi-safety-research-outside-ai.html,,TechSafety; DeepMind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2G3EWT42,blogPost,2018,"Krakovna, Victoria",Specification gaming examples in AI,Victoria Krakovna,,,,https://vkrakovna.wordpress.com/2018/04/02/specification-gaming-examples-in-ai/,"Update: for a more detailed introduction to specification gaming, check out the DeepMind Safety Research blog post! Various examples (and lists of examples) of unintended behaviors in AI systems ha…",2018-04-01,2022-01-30 04:52:48,2022-01-30 04:52:48,2020-12-13 23:12:51,,,,,,,,,,,,,,en,,,,,,,ZSCC: 0000020,,/Users/jacquesthibodeau/Zotero/storage/4FI7MHVZ/specification-gaming-examples-in-ai.html,,TechSafety; DeepMind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8D5IZVSJ,blogPost,2019,"Kumar, Ramana; Garrabrant, Scott",Thoughts on Human Models,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/BKjJJH2cRpJcAnP7T/thoughts-on-human-models,"Human values and preferences are hard to specify, especially in complex domains. Accordingly, much AGI safety research has focused on approaches to AGI design that refer to human values and preferences indirectly, by learning a model that is grounded in expressions of human values (via stated preferences, observed behaviour, approval, etc.) and/or real-world processes that generate expressions of those values. There are additionally approaches aimed at modelling or imitating other aspects of human cognition or behaviour without an explicit aim of capturing human preferences (but usually in service of ultimately satisfying them). Let us refer to all these models as human models. In this post, we discuss several reasons to be cautious about AGI designs that use human models. We suggest that the AGI safety research community put more effort into developing approaches that work well in the absence of human models, alongside the approaches that rely on human models. This would be a significant addition to the current safety research landscape, especially if we focus on working out and trying concrete approaches as opposed to developing theory. We also acknowledge various reasons why avoiding human models seems difficult. PROBLEMS WITH HUMAN MODELS To be clear about human models, we draw a rough distinction between our actual preferences (which may not be fully accessible to us) and procedures for evaluating our preferences. The first thing, actual preferences, is what humans actually want upon reflection. Satisfying our actual preferences is a win. The second thing, procedures for evaluating preferences, refers to various proxies for our actual preferences such as our approval, or what looks good to us (with necessarily limited information or time for thinking). Human models are in the second category; consider, as an example, a highly accurate ML model of human yes/no approval on the set of descriptions of outcomes. Our first concern, described below, is about overfit",2019-02-21,2022-01-30 04:52:48,2022-01-30 04:52:48,2021-02-06 18:50:50,,,,,,,,,,,,,,,,,,,,,ZSCC: 0000002,,/Users/jacquesthibodeau/Zotero/storage/FQ4294HF/thoughts-on-human-models.html; /Users/jacquesthibodeau/Zotero/storage/7BIUUJNZ/thoughts-on-human-models.html,,MetaSafety; DeepMind; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5ZWTEI44,blogPost,2019,"Sutton, Rich",The Bitter Lesson,Incomplete Ideas,,,,http://www.incompleteideas.net/IncIdeas/BitterLesson.html,,2019,2022-01-30 04:52:48,2022-01-30 04:52:48,2019-12-16 20:26:51,,,,,,,,,,,,,,,,,,,,,ZSCC: 0000085,,/Users/jacquesthibodeau/Zotero/storage/K957ZVDR/BitterLesson.html,,TechSafety; DeepMind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
732FQ6KE,blogPost,2020,"Krakovna, Victoria; Uesato, Jonathan; Mikulik, Vladimir; Rahtz, Matthew; Everitt, Tom; Kumar, Ramana; Kenton, Zachary; Leike, Jan; Legg, Shane",Specification gaming: the flip side of AI ingenuity,Deepmind,,,,deepmind.com/blog/article/Specification-gaming-the-flip-side-of-AI-ingenuity,"Specification gaming is a behaviour that satisfies the literal specification of an objective without achieving the intended outcome. We have all had experiences with specification gaming, even if not by this name. Readers may have heard the myth of King Midas and the golden touch, in which the king asks that anything he touches be turned to gold - but soon finds that even food and drink turn to metal in his hands. In the real world, when rewarded for doing well on a homework assignment, a student might copy another student to get the right answers, rather than learning the material - and thus exploit a loophole in the task specification.",2020-04-21,2022-01-30 04:52:48,2022-01-30 04:52:48,2020-09-05 17:00:34,,,,,,,Specification gaming,,,,,,,ALL,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,,,TechSafety; DeepMind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HJI2JMT3,blogPost,2018,"Ngo, Richard",Some cruxes on impactful alternatives to AI policy work,LessWrong,,,,https://www.lesswrong.com/posts/DJB82jKwgJE5NsWgT/some-cruxes-on-impactful-alternatives-to-ai-policy-work,"Ben Pace and I (Richard Ngo) recently did a public double crux at the Berkeley REACH on how valuable it is for people to go into AI policy and strategy work: I was optimistic and Ben was pessimistic. During the actual event, we didn't come anywhere near to finding a double crux on that issue. But after a lot of subsequent discussion, we've come up with some more general cruxes about where impact comes from. I found Ben's model of how to have impact very interesting, and so in this post I've tried to explain it, along with my disagreements. Ben liked the goal of writing up a rough summary of our positions and having further discussion in the comments, so while he edited it somewhat he doesn’t at all think that it’s a perfect argument, and it’s not what he’d write if he spent 10 hours on it. He endorsed the wording of the cruxes as broadly accurate. (During the double crux, we also discussed how the heavy-tailed worldview applies to community building, but decided on this post to focus on the object level of what impact looks like.) Note from Ben: “I am not an expert in policy, and have not put more than about 20-30 hours of thought into it total as a career path. But, as I recently heard Robin Hanson say, there’s a common situation that looks like this: some people have a shiny idea that they think about a great deal and work through the details of, that folks in other areas are skeptical of given their particular models of how the world works. Even though the skeptics have less detail, it can be useful to publicly say precisely why they’re skeptical.  In this case I’m often skeptical when folks tell me they’re working to reduce x-risk by focusing on policy. Folks doing policy work in AI might be right, and I might be wrong, but it seemed like a good use of time to start a discussion with Richard about how I was thinking about it and what would change my mind. If the following discussion causes me to change my mind on this question, I’ll be really super happy wit",2018,2022-01-30 04:52:48,2022-01-30 04:52:48,2020-12-13 23:27:04,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/IWG3WG8N/some-cruxes-on-impactful-alternatives-to-ai-policy-work.html,,MetaSafety; DeepMind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UUJCZ9TG,blogPost,2015,"Krakovna, Victoria",Risks from general artificial intelligence without an intelligence explosion,Victoria Krakovna,,,,https://vkrakovna.wordpress.com/2015/11/29/ai-risk-without-an-intelligence-explosion/,"“An ultraintelligent machine could design even better machines; there would then unquestionably be an ‘intelligence explosion,’ and the intelligence of man would be left far behin…",2015-11-30,2022-01-30 04:52:48,2022-01-30 04:52:48,2020-11-21 16:55:53,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/QSBISDWW/ai-risk-without-an-intelligence-explosion.html,,TechSafety; DeepMind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PCIE3RBG,blogPost,2020,"Krakovna, Victoria",Possible takeaways from the coronavirus pandemic for slow AI takeoff,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/wTKjRFeSjKLDSWyww/possible-takeaways-from-the-coronavirus-pandemic-for-slow-ai,"Epistemic status: fairly speculative, would appreciate feedback As the covid-19 pandemic unfolds, we can draw lessons from it for managing future global risks, such as other pandemics, climate change, and risks from advanced AI. In this post, I will focus on possible implications for AI risk. For a broader treatment of this question, I recommend FLI's covid-19 page that includes expert interviews on the implications of the pandemic for other types of risks.  A key element in AI risk scenarios is the speed of takeoff - whether advanced AI is developed gradually or suddenly. Paul Christiano's post on takeoff speeds  defines slow takeoff in terms of the economic impact of AI as follows: ""There will be a complete 4 year interval in which world output doubles, before the first 1 year interval in which world output doubles."" It argues that slow AI takeoff is more likely than fast takeoff, but is not necessarily easier to manage, since it poses different challenges, such as large-scale coordination. This post expands on this point by examining some parallels between the coronavirus pandemic and a slow takeoff scenario. The upsides of slow takeoff include the ability to learn from experience, act on warning signs, and reach a timely consensus that there is a serious problem. I would argue that the covid-19 pandemic had these properties, but most of the world's institutions did not take advantage of them. This suggests that, unless our institutions improve, we should not expect the slow AI takeoff scenario to have a good default outcome.   1. Learning from experience. In the slow takeoff scenario, general AI is     expected to appear in a world that has already experienced transformative     change from less advanced AI, and institutions will have a chance to learn     from problems with these AI systems. An analogy could be made with learning     from dealing with less ""advanced"" epidemics like SARS that were not as     successful as covid-19 at spreading across the worl",2020-05-31,2022-01-30 04:52:47,2022-01-30 04:52:47,2020-08-31 18:12:52,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/6W9S9Z9G/possible-takeaways-from-the-coronavirus-pandemic-for-slow-ai.html,,TechSafety; DeepMind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9UHP8WTZ,blogPost,2021,"Everitt, Tom; Carey, Ryan; Hammond, Lewis; Fox, James; Langlois, Eric; Legg, Shane",Progress on Causal Influence Diagrams,Medium,,,,https://deepmindsafetyresearch.medium.com/progress-on-causal-influence-diagrams-a7a32180b0d1,"By Tom Everitt, Ryan Carey, Lewis Hammond, James Fox, Eric Langlois, and Shane Legg",2021-08-11,2022-01-30 04:52:47,2022-01-30 04:52:47,2021-11-14 19:06:48,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/GIGW9EBD/progress-on-causal-influence-diagrams-a7a32180b0d1.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CECMW3NM,blogPost,2020,"Aguirre, Anthony",Why those who care about catastrophic and existential risk should care about autonomous weapons,Effective Altruism Forum,,,,https://forum.effectivealtruism.org/posts/oR9tLNRSAep293rr5/why-those-who-care-about-catastrophic-and-existential-risk-2,"(crossposted to Lesswrong here.) Although I have not seen the argument made in any detail or in writing, I and the Future of Life Institute (FLI) have gathered the strong impression that parts of the effective altruism ecosystem are skeptical of the importance of the issue of autonomous weapons systems. This post explains why we think those interested in avoiding catastrophic and existential risk, especially risk stemming from emerging technologies, may want to have this issue higher on their list of concerns. We will first define some terminology and do some disambiguation, as there are many classes of autonomous weapons that are often conflated; all classes have some issues of concern, but some are much more problematic than others. We then detail three basic motivations for research, advocacy, coordination, and policymaking around the issue:  1. Governance of autonomous weapon systems is a dry-run, and precedent, for     governance of AGI. In the short term, AI-enabled weapons systems will share     many of the technical weaknesses and shortcomings of other AI systems, but     like general AI also raise safety concerns that are likely to increase      rather than decrease with capability advances. The stakes are intrinsically     high (literally life-or-death), and the context is an inevitably adversarial     one involving states and major corporations. The sort of global coordination     amongst potentially adversarial parties that will be required for governance     of transformative/general AI systems will not arise from nowhere, and     autonomous weapons offer an invaluable precedent and arena in which to build     experience, capability, and best practices.  2. Some classes of lethal autonomous weapon systems constitute scalable weapons     of mass destruction (which may also have a much lower threshold for first     use or accidental escalation), and hence a nascent catastrophic risk.  3. By increasing the probability of the initiation and/or escalation",2020-11-11,2022-01-30 04:53:38,2022-01-30 04:53:38,2020-12-19 04:18:42,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/DGBAP7KK/why-those-who-care-about-catastrophic-and-existential-risk.html; /Users/jacquesthibodeau/Zotero/storage/K2N72EX7/why-those-who-care-about-catastrophic-and-existential-risk-2.html,,MetaSafety; AmbiguosSafety; FLI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
G86SMTN9,blogPost,2019,"Krakovna, Victoria",ICLR Safe ML Workshop Report,Future of Life Institute,,,,https://futureoflife.org/2019/06/18/iclr-safe-ml-workshop-report/,Victoria Krakovna co-organized the 2019 ICLR Safe ML workshop. One of the main goals was to bring together near and long term safety research communities.,2019-06-18,2022-01-30 04:53:38,2022-01-30 04:53:38,2020-12-14 23:28:16,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/X9EEWW4D/iclr-safe-ml-workshop-report.html,,TechSafety; DeepMind; FLI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QQ2PGVQJ,blogPost,2015,"Sandberg, Anders","We, Borg: Speculations on hive minds as a posthuman state",aleph.se,,,,,,2015,2022-01-30 04:53:37,2022-01-30 04:53:37,,,,,,,,"We, Borg",,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: 7,,,,MetaSafety; FHI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FSXDIQ42,blogPost,2017,"Bostrom, Nick",Transhumanist FAQ 3.0,Humanity +,,,,https://humanityplus.org/philosophy/transhumanist-faq/,,2017,2022-01-30 04:53:37,2022-01-30 04:53:37,,,,,,,,,,,,,,,,,,,,,,ZSCC: 0000013,,,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PTTGW3VK,blogPost,2020,"Armstrong, Stuart","Subagents and impact measures, full and fully illustrated",AI Alignment Forum,,,,https://www.alignmentforum.org/posts/mdQEraEZQLg7jtozn/subagents-and-impact-measures-full-and-fully-illustrated,"0. INTRODUCTION: WHY YET ANOTHER POST ABOUT SUBAGENTS? I’ve recently been writing a sequence on how subagents can undermine impact penalties such as attainable utility preservation. I’m not happy with that sequence; it’s messy and without examples (apart from its first post), people didn’t understand it, and it suffers from the fact that I discovered key ideas as I went along. So I’ve combined everything there into a single post, explained with examples and an abundance of pictures. Hopefully an over- rather than an under-abundance of pictures. Of the original sequence, I've only kept the mathematical results  of this post and the initial example post which has a clearer example of ""high power"" for a subagent. This post here is laid out in a way that makes logical sense, but might not be the clearest for people unfamiliar with the area. For those people, I recommend skipping section 2 initially, and returning to it later. But, whatever you do, make sure you glance at 6.1 and 6.2 before leaving. 1. THE WORLD Our fearless agent A moves around in a gridworld: Each turn, A can move ones square horizontally or vertically. It can also manipulate objects in the eight squares around it, allowing it to, not incidentally, assemble the three pieces to its west into an subagent SA. The robot can also do the noop action, ∅, which does nothing, and it can speak. The subagent, when assembled, has the same action set available. Its positive reward, the one it wants to increase, is R0. To get this reward, a robot needs to move onto the blue button in the east; R0 will give a reward of 1  the first time this happens (and 0 before and after). The discount factor is 0<γ <1. Just to the west of the blue button is a one-way door. Robots can move east through it, but cannot move west through it: 1.1 THE IMPACT REWARD The impact penalty is supposed to ensure that A does not make too many change in the world, and keeps it similar, in some senses, to a specific baseline world. I",2020-02-24,2022-01-30 04:53:35,2022-01-30 04:53:35,2020-09-05 18:47:33,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/A6XJUF5U/subagents-and-impact-measures-full-and-fully-illustrated.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XE3FX4MZ,blogPost,2019,"Armstrong, Stuart",Research Agenda v0.9: Synthesising a human's preferences into a utility function,LessWrong,,,,https://www.lesswrong.com/posts/CSEdLLEkap2pubjof/research-agenda-v0-9-synthesising-a-human-s-preferences-into,"I'm now in a position where I can see a possible route to a safe/survivable/friendly Artificial Intelligence being developed. I'd give a 10+% chance of it being possible this way, and a 95% chance that some of these ideas will be very useful for other methods of alignment. So I thought I'd encode the route I'm seeing as research agenda; this is the first public draft of it. Clarity, rigour, and practicality: that's what this agenda needs. Writing this agenda has clarified a lot of points for me, to the extent that some of it now seems, in retrospect, just obvious and somewhat trivial - ""of course that's the way you have to do X"". But more clarification is needed in the areas that remain vague. And, once these are clarified enough for humans to understand, they need to be made mathematically and logically rigorous - and ultimately, cashed out into code, and tested and experimented with. So I'd appreciate any comments that could help with these three goals, and welcome anyone interested in pursuing research along these lines over the long-term. Note: I periodically edit this document, to link it to more recent research ideas/discoveries. 0 THE FUNDAMENTAL IDEA This agenda fits itself into the broad family of Inverse [https://ai.stanford.edu/~ang/papers/icml00-irl.pdf] Reinforcement [https://arxiv.org/abs/1606.03137] Learning [https://www.youtube.com/watch?v=Ts-nTIYDXok]: delegating most of the task of inferring human preferences to the AI itself. Most of the task, since it's been shown that humans need to build the right assumptions into the AI, or else the preference learning will fail [https://arxiv.org/abs/1712.05812]. To get these ""right assumptions"", this agenda will look into what preferences actually are, and how they may be combined together. There are hence four parts to the research agenda:  1. A way of identifying the (partial[1] [#fn-wPj8aGxtWBoDNTAof-1]) preferences     of a given human H.  2. A way for ultimately synthesising a utility function UH",2019,2022-01-30 04:53:34,2022-01-30 04:53:34,2019-12-16 22:32:49,,,,,,,Research Agenda v0.9,,,,,,,,,,,,,,ZSCC: NoCitationData[s6]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/ASUSJMZZ/research-agenda-v0-9-synthesising-a-human-s-preferences-into.html; /Users/jacquesthibodeau/Zotero/storage/QNPQFGW3/research-agenda-v0-9-synthesising-a-human-s-preferences-into.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AHS635XH,blogPost,2020,"O'Keefe, Cullen",Parallels Between AI Safety by Debate and Evidence Law,Cullen O'Keefe,,,,https://cullenokeefe.com/blog/debate-evidence,,2020-07-20,2022-01-30 04:53:19,2022-01-30 04:53:19,2020-08-28 17:19:24,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/GEGGQ6BR/debate-evidence.html,,MetaSafety; FHI; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
V7S6UBMU,blogPost,2020,"Armstrong, Stuart",Model splintering: moving from one imperfect model to another,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/k54rgSg7GcjtXnMHX/model-splintering-moving-from-one-imperfect-model-to-another-1,"1. THE BIG PROBLEM In the last few months, I've become convinced that there is a key meta-issue in AI safety; a problem that seems to come up in all sorts of areas. It's hard to summarise, but my best phrasing would be:  * Many problems in AI safety seem to be variations of ""this approach seems safe    in this imperfect model, but when we generalise the model more, it becomes    dangerously underdefined"". Call this model splintering.  * It is intrinsically worth studying how to (safely) transition from one    imperfect model to another. This is worth doing, independently of whatever    ""perfect"" or ""ideal"" model might be in the background of the imperfect    models. This sprawling post will be presenting examples of model splintering, arguments for its importance, a formal setting allowing us to talk about it, and some uses we can put this setting to. 1.1 IN THE LANGUAGE OF TRADITIONAL ML In the language of traditional ML, we could connect all these issues to "" out-of-distribution"" behaviour. This is the problems that algorithms encounter when the set they are operating on is drawn from a different distribution than the training set they were trained on. Humans can often see that the algorithm is out-of-distribution and correct it, because we have a more general distribution in mind than the one the algorithm was trained on. In these terms, the issues of this post can be phrased as:  1. When the AI finds itself mildly out-of-distribution, how best can it extend     its prior knowledge to the new situation?  2. What should the AI do if it finds itself strongly out-of-distribution?  3. What should the AI do if it finds itself strongly out-of-distribution, and     humans don't know the correct distribution either? 1.2 MODEL SPLINTERING EXAMPLES Let's build a more general framework. Say that you start with some brilliant idea for AI safety/alignment/effectiveness. This idea is phrased in some (imperfect) model. Then ""model splintering"" happens when you or the AI",2020-08-27,2022-01-30 04:53:19,2022-01-30 04:53:19,2020-09-07 18:35:31,,,,,,,Model splintering,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/G43HNSF6/model-splintering-moving-from-one-imperfect-model-to-another-1.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQ8FTBHJ,blogPost,2018,"Evans, Owain; Steinhardt, Jacob",Model Mis-specification and Inverse Reinforcement Learning,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/cnC2RMWEGiGpJv8go/model-mis-specification-and-inverse-reinforcement-learning,"Posted as part of the AI Alignment Forum sequence on Value Learning. Rohin's note: While I motivated the last post with an example of using a specific model for human biases, in this post (original here), Jacob Steinhardt and Owain Evans point out that model mis-specification can arise in other parts of inverse reinforcement learning as well. The arguments here consider some more practical concerns (for example, the worries about getting only short-term data for each human would not be a problem if you had the entire human policy). -------------------------------------------------------------------------------- In my previous post, “Latent Variables and Model Mis-specification”, I argued that while machine learning is good at optimizing accuracy on observed signals, it has less to say about correctly inferring the values for unobserved variables in a model. In this post I’d like to focus in on a specific context for this: inverse reinforcement learning (Ng et al. 2000, Abbeel et al. 2004, Ziebart et al. 2008, Ho et al 2016), where one observes the actions of an agent and wants to infer the preferences and beliefs that led to those actions. For this post, I am pleased to be joined by Owain Evans, who is an active researcher in this area and has co-authored an online book about building models of agents (see here in particular for a tutorial on inverse reinforcement learning and inverse planning). Owain and I are particularly interested in inverse reinforcement learning (IRL) because it has been proposed (most notably by Stuart Russell) as a method for learning human values in the context of AI safety; among other things, this would eventually involve learning and correctly implementing human values by artificial agents that are much more powerful, and act with much broader scope, than any humans alive today. While we think that overall IRL is a promising route to consider, we believe that there are also a number of non-obvious pitfalls related to performing IRL",2018,2022-01-30 04:53:19,2022-01-30 04:53:19,2020-12-17 04:36:25,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/KBV5ZKMW/cnC2RMWEGiGpJv8go.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GKG6XR6Z,blogPost,2020,"Armstrong, Stuart",Predictors exist: CDT going bonkers... forever,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/Kr76XzME7TFkN937z/predictors-exist-cdt-going-bonkers-forever,"I've been wanting to get a better example of CDT (causal decision theory) misbehaving, where the behaviour is more clearly suboptimal than it is in the  Newcomb problem (which many people don't seem to accept as CDT being suboptimal), and simpler to grasp than Death in Damascus. THE ""PREDICTORS EXIST"" PROBLEM So consider this simple example: the player is playing against Omega, who will predict their actions[1]. The player can take three actions: ""zero"", ""one"", or ""leave"". If ever they do ""leave"", then the experiment is over and they leave. If they choose ""zero"" or ""one"", then Omega will predict their action, and compare this to their actual action. If the two match, then the player loses 1 utility and the game repeats; if the action and the prediction differs, then the player gains 3 utility and the experiment ends. Assume that actually Omega is a perfect or quasi-perfect predictor, with a good model of the player. An FDT or EDT agent would soon realise that they couldn't trick Omega, after a few tries, and would quickly end the game. But the CDT player would be incapable of reaching this reasoning. Whatever distribution they compute over Omega's prediction, they will always estimate that they (the CDT player) have at least a 50% chance of choosing the other option[2], for an expected utility gain of at least 0.5(3)+0.5(−1)=1. Basically, the CDT agent can never learn that Omega is a good predictor of themselves[3]. And so they will continue playing, and continue losing... for ever. --------------------------------------------------------------------------------  1. Omega will make this prediction not necessarily before the player takes     their action, not even necessarily without seeing this action, but still     makes the prediction independently of this knowledge. And that's enough for     CDT. ↩︎            2. For example, suppose the CDT agent estimates the prediction will be ""zero""     with probability p, and ""one"" with probability 1-p. Then if p≥1/2",2020-01-14,2022-01-30 04:53:19,2022-01-30 04:53:19,2020-09-07 18:27:36,,,,,,,Predictors exist,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/GDCP95WU/predictors-exist-cdt-going-bonkers-forever.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
U4AWP6AV,blogPost,2020,"Nguyen, Chi",My Understanding of Paul Christiano's Iterated Amplification AI Safety Research Agenda,AI Alignment Forum,,,,https://www.lesswrong.com/posts/PT8vSxsusqWuN7JXp/my-understanding-of-paul-christiano-s-iterated-amplification,"Crossposted from the EA forum You can read this post as a google docs instead (IMO much better to read). This document aims to clarify the AI safety research agenda by Paul Christiano (IDA) and the arguments around how promising it is. Target audience: All levels of technical expertise. The less knowledge about IDA someone has, the more I expect them to benefit from the writeup. Writing policy: I aim to be as clear and concrete as possible and wrong rather than vague to identify disagreements and where I am mistaken. Things will err on the side of being too confidently expressed. Almost all footnotes are content and not references. Epistemic Status: The document is my best guess on IDA and might be wrong in important ways. I have not verified all of the content with somebody working on IDA. I spent ~4 weeks on this and have no prior background in ML, CS or AI safety. I wrote this document last summer (2019) as part of my summer research fellowship at FHI. I was planning to restructure, complete and correct it since but haven’t gotten to it for a year, so decided to just publish it as it is. The document has not been updated, i.e. nothing that has been released since September 2019 is incorporated into this document. Paul Christiano generously reviewed part of this summary. I added his comments verbatim in the document. Apologies for the loss of readability due to this. This doesn’t imply he endorses any part of this document. PURPOSE OF THIS DOCUMENT: CLARIFYING IDA IDA is Paul Christiano’s AI safety research agenda.[1] Christiano works at OpenAI which is one of the main actors in AI safety and IDA is by many considered the most complete[2] AI safety agenda. However, people who are not directly working on IDA are often confused about how exactly to understand the agenda. Clarifying IDA would make it more accessible  for technical people to work on and easier to assess for nontechnical people who want to think about its implications. I believe that there are",2020-08-15,2022-01-30 04:53:19,2022-01-30 04:53:19,2020-08-24 20:21:42,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/EIQBMQPS/my-understanding-of-paul-christiano-s-iterated-amplification.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UIU789GM,blogPost,2020,"Armstrong, Stuart",If I were a well-intentioned AI... I: Image classifier,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/gzWb5kWwzhdaqmyTt/if-i-were-a-well-intentioned-ai-i-image-classifier,"INTRODUCTION: IF I WERE A WELL-INTENTIONED AI... I've often warned people about the dangers of anthropomorphising AIs - how it can mislead us about what's really going on in an AI (and hence how the AI might act in the future), cause us to not even consider certain failure modes, and make us believe we understand things much better than we do. Oh well, let's ignore all that. I'm about to go on a journey of major anthropomorphisation, by asking myself:  * ""If I was a well-intentioned AI, could I solve many of the problems in AI    alignment?"" My thinking in this way started when I wondered: suppose I knew that I was given a proxy goal rather than the true goal; suppose that I knew about the Goodhart problem, and suppose that I really ""wanted"" to align with the true goal - could I then do it? I was having similar thoughts about being a mesa-optimiser. It seems to me that asking and answering these kind of questions leads to new and interesting insights. Of course, since they come via anthropomorphisation, we need to be careful with them, and check that they are really applicable to AI systems - ensuring that I'm not bringing some of my own human knowledge about human values into the example. But first, let's get those initial insights. OVERLAPPING PROBLEMS, OVERLAPPING SOLUTIONS At a high enough level of abstraction, many problems in AI alignment seem very similar. The Goodhart problem, the issues machine learning has with  distributional shift, the problem of the nearest unblocked strategy,  unidentifiability of reward functions, even mesaoptimisation and the whole AI alignment problem itself - all of these can be seen, roughly, as variants of the same problem. That problem being that we have an approximately specified goal that looks ok, but turns out to be underspecified in dangerous ways. Of course, often the differences between the problems are as important as the similarities. Nevertheless, the similarities exist, which is why a lot of the solutions are",2020-02-26,2022-01-30 04:53:18,2022-01-30 04:53:18,2020-09-05 18:38:33,,,,,,,If I were a well-intentioned AI... I,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/9VWKU6G5/gzWb5kWwzhdaqmyTt.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7R7WMB83,blogPost,2015,"Tomasik, Brian",A Dialogue on Suffering Subroutines,Center on Long-Term Risk,,,,https://longtermrisk.org/a-dialogue-on-suffering-subroutines/,"This piece presents a hypothetical dialogue that explains why instrumental computational processes of a future superintelligence might evoke moral concern. Generally, agent-like components might emerge in many places, including the computing processes of a future civilization. Whether and how much these subroutines matter are questions for future generations to figure out, but it's good to keep an open mind to the possibility that our intuitions about what suffering is may change dramatically.",2015-08-29,2022-01-30 04:51:06,2022-01-30 04:51:06,2020-11-23 01:05:16,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/GHRVZSNG/a-dialogue-on-suffering-subroutines.html,,CLR; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CUMZ3TRE,blogPost,2017,"Oesterheld, Caspar",A behaviorist approach to building phenomenological bridges,The Universe from an Intentional Stance,,,,https://casparoesterheld.com/2017/10/22/a-behaviorist-approach-to-building-phenomenological-bridges/,"A few weeks ago, I wrote about the BPB problem and how it poses a problem for classical/non-logical decision theories. In my post, I briefly mentioned a behaviorist approach to BPB, only to immedia…",2017-10-22,2022-01-30 04:51:06,2022-01-30 04:51:06,2020-11-23 00:45:20,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/VEDV2NXS/a-behaviorist-approach-to-building-phenomenological-bridges.html,,CLR; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EGQMWQFR,blogPost,2015,"Tomasik, Brian",Artificial Intelligence and Its Implications for Future Suffering,Center on Long-Term Risk,,,,https://longtermrisk.org/artificial-intelligence-and-its-implications-for-future-suffering/,"Artificial intelligence (AI) will likely transform the world later this century. Whether uncontrolled or controlled AIs would create more suffering in expectation is a question to explore further. Regardless, the field of AI safety and policy seems to be a very important space where altruists can make a positive-sum impact along many dimensions.",2015-04-10,2022-01-30 04:51:06,2022-01-30 04:51:06,2020-11-23 01:02:42,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QD8MQPCV,blogPost,2017,Caspar,A survey of polls on Newcomb’s problem,The Universe from an Intentional Stance,,,,https://casparoesterheld.com/2017/06/27/a-survey-of-polls-on-newcombs-problem/,"One classic story about Newcomb’s problem is that, at least initially, people one-box and two-box in roughly equal numbers (and that everyone is confident in their position). To find out whet…",2017-06-27,2022-01-30 04:51:06,2022-01-30 04:51:06,2020-11-23 20:03:16,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/EPR7NUV9/a-survey-of-polls-on-newcombs-problem.html,,CLR; TechSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P2J7P366,blogPost,2015,"Tomasik, Brian",A Lower Bound on the Importance of Promoting Cooperation,Center on Long-Term Risk,,,,https://longtermrisk.org/a-lower-bound-on-the-importance-of-promoting-cooperation/,This article suggests a lower-bound Fermi calculation for the cost-effectiveness of promoting cooperation. The purpose of this exercise is to make our thinking more concrete about how cooperation might reduce suffering and to make its potential more tangible.,2015-08-29,2022-01-30 04:51:06,2022-01-30 04:51:06,2020-11-23 01:04:00,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/EPJR83PC/a-lower-bound-on-the-importance-of-promoting-cooperation.html,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RMFTWTV3,blogPost,2021,"Steinhardt, Jacob","Measurement, Optimization, and Take-off Speed",Jacob Steinhardt,,,,https://jsteinhardt.stat.berkeley.edu/blog/measurement-and-optimization,"In machine learning, we are obsessed with datasets and metrics: progress in areas as diverse as natural language understanding, object recognition, and reinforcement learning is tracked by numerical scores on agreed-upon benchmarks. Despite this, I think we focus too little on measurement—that is, on ways of extracting data from machine learning models that bears upon important hypotheses. This might sound paradoxical, since benchmarks are after all one way of measuring a model. However, benchmarks are a very narrow form of measurement, and I will argue below that trying to measure pretty much anything you can think of is a good mental move that is heavily underutilized in machine learning. I’ll argue this in three ways: Historically, more measurement has almost always been a great move, not only in science but also in engineering and policymaking. Philosophically, measurement has many good properties that bear upon important questions in ML. In my own research, just measuring something and seeing what happened has often been surprisingly fruitful.",2021-04-07,2022-01-30 04:50:55,2022-01-30 04:50:55,2021-11-14 19:03:14,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/6IF9UF8E/measurement-and-optimization.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QSHDXZW8,blogPost,2019,"Shah, Rohin",Learning biases and rewards simultaneously,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/xxnPxELC4jLKaFKqG/learning-biases-and-rewards-simultaneously,"I’ve finally uploaded to arXiv our work on inferring human biases alongside IRL, which was published at ICML 2019. SUMMARY OF THE PAPER THE IRL DEBATE Here’s a quick tour of the debate about inverse reinforcement learning (IRL) and cognitive biases, featuring many of the ideas from the first chapter of the  Value Learning sequence: I had the intuition that the impossibility theorem was like the other no-free-lunch theorems in ML: not actually relevant for what ML could do in practice. So we tried to learn and correct for systematic biases in IRL. THE IDEA BEHIND THE ALGORITHMS The basic idea was to learn the planning algorithm by which the human produces demonstrations, and try to ensure that the planning algorithm captured the appropriate systematic biases. We used a Value Iteration Network to give an inductive bias towards “planners” but otherwise did not assume anything about the form of the systematic bias. [1] Then, we could perform IRL by figuring out which reward would cause the planning algorithm to output the given demonstrations. The reward would be “debiased” because the effect of the biases on the policy would already be accounted for in the planning algorithm. How could we learn the planning algorithm? Well, one baseline method is to assume that we have access to some tasks where the rewards are known, and use those tasks to learn what the planning algorithm is. Then, once that is learned, we can infer the rewards for new tasks that we haven’t seen before. This requires the planner to generalize across tasks. However, it’s kind of cheating to assume access to ground truth rewards, since we usually wouldn’t have them. What if we learned the planning algorithm and rewards simultaneously? Well, the no-free-lunch theorem gets us then: maximizing the true reward and minimizing the negative of the true reward would lead to the same policy, and so you can’t distinguish between them, and so the output of your IRL algorithm could be the true reward or the",2019,2022-01-30 04:50:54,2022-01-30 04:50:54,2020-12-18 00:09:48,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/9QDS82F6/learning-biases-and-rewards-simultaneously.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DFJQBIH5,blogPost,2020,"Turner, Alex",Generalizing the Power-Seeking Theorems,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/nyDnLif4cjeRe9DSv/generalizing-the-power-seeking-theorems,"Previously: Seeking Power is Often Provably Instrumentally Convergent in MDPs. Thanks to Rohin Shah, Michael Dennis, Josh Turner, and Evan Hubinger for comments. -------------------------------------------------------------------------------- It sure seems like gaining power over the environment is instrumentally convergent (optimal for a wide range of agent goals). You can turn this into math and prove things about it. Given some distribution over agent goals, we want to be able to formally describe how optimal action tends to flow through the future. Does gaining money tend to be optimal? Avoiding shutdown? When? How do we know? Optimal Farsighted Agents Tend to Seek Power proved that, when you distribute reward fairly and evenly across states (IID), it's instrumentally convergent to gain access to lots of final states (which are absorbing, in that the agent keeps on experiencing the final state). The theorems apply when you don't discount the future (you're ""infinitely farsighted""). Most reward functions for the Pac-Man game incentivize not dying immediately, so that the agent can loop around higher-scoring configurations. Many ways of scoring Tic-Tac-Toe game states incentivize not losing immediately, in order to choose the highest-scoring final configuration. ""All states have self-loops, left hidden to reduce clutter. In AI: A Modern Approach (3e), the agent starts at1and receives reward for reaching3. The optimal policy for this reward function avoids2, and one might suspect that avoiding2is instrumentally convergent. However, a skeptic might provide a reward function for which navigating to2is optimal, and then argue that ""instrumental convergence'' is subjective and that there is no reasonable basis for concluding that2is generally avoided. We can do better... for any way of independently and identically distributing reward over states,1011of reward functions have farsighted optimal policies which avoid2. If we complicate the MDP with additional t",2020-07-26,2022-01-30 04:50:53,2022-01-30 04:50:53,2020-08-28 17:31:03,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/Z47Z7M3I/generalizing-the-power-seeking-theorems.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WZIU7UJI,blogPost,2018,"Shah, Rohin",Intuitions about goal-directed behavior,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/DfcywmqRSkBaCB6Ma/intuitions-about-goal-directed-behavior,"One broad argument for AI risk is the Misspecified Goal argument:  The Misspecified Goal Argument for AI Risk: Very intelligent AI systems will be able to make long-term plans in order to achieve their goals, and if their goals are even slightly misspecified then the AI system will become adversarial and work against us. My main goal in this post is to make conceptual clarifications and suggest how they affect the Misspecified Goal argument, without making any recommendations about what we should actually do. Future posts will argue more directly for a particular position. As a result, I will not be considering other arguments for focusing on AI risk even though I find some of them more compelling. I think of this as a concern about long-term goal-directed behavior. Unfortunately, it’s not clear how to categorize behavior as goal-directed vs. not. Intuitively, any agent that searches over actions and chooses the one that best achieves some measure of “goodness” is goal-directed (though there are exceptions, such as the agent that selects actions that begin with the letter “A”). (ETA: I also think that agents that show goal-directed behavior because they are looking at some other agent are not goal-directed themselves -- see this comment.) However, this is not a necessary condition: many humans are goal-directed, but there is no goal baked into the brain that they are using to choose actions. This is related to the concept of optimization, though with intuitions around optimization we typically assume that we know the agent’s preference ordering, which I don’t want to assume here. (In fact, I don’t want to assume that the agent even has a preference ordering.) One potential formalization is to say that goal-directed behavior is any behavior that can be modelled as maximizing expected utility for some utility function; in the next post I will argue that this does not properly capture the behaviors we are worried about. In this post I’ll give some intuitions about",2018,2022-01-30 04:50:53,2022-01-30 04:50:53,2020-12-17 04:36:33,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/UUQVT4H3/DfcywmqRSkBaCB6Ma.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P6TZ4M98,blogPost,2014,"Dragan, Anca; Srinivasa, Siddhartha",Integrating Human Observer Inferences into Robot Motion Planning,The Robotics Institute Carnegie Mellon University,,,,https://www.ri.cmu.edu/publications/integrating-human-observer-inferences-into-robot-motion-planning/,"Our goal is to enable robots to produce motion that is suitable for human-robot collaboration and co-existence. Most motion in robotics is purely functional, ideal when the robot is performing a task in isolation. In collaboration, however, the robot’s motion has an observer, watching and interpreting the motion. In this work, we move beyond functional …",2014,2022-01-30 04:50:53,2022-01-30 04:50:53,2019-12-18 01:40:11,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: 0000075,,/Users/jacquesthibodeau/Zotero/storage/RZQEJ7CE/integrating-human-observer-inferences-into-robot-motion-planning.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TBKHB3QK,blogPost,2018,"Shah, Rohin",Humans can be assigned any values whatsoever…,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/ANupXf8XfZo2EJxGv/humans-can-be-assigned-any-values-whatsoever,"(Re)Posted as part of the AI Alignment Forum sequence on Value Learning. Rohin’s note: In the last post, we saw that a good broad value learning approach would need to understand the systematic biases in human planning in order to achieve superhuman performance. Perhaps we can just use machine learning again and learn the biases and reward simultaneously? This post by Stuart Armstrong (original here) and the associated paper say: “Not without more assumptions.” This post comes from a theoretical perspective that may be alien to ML researchers; in particular, it makes an argument that simplicity priors do not solve the problem pointed out here, where simplicity is based on Kolmogorov complexity (which is an instantiation of the Minimum Description Length principle). The analog in machine learning would be an argument that regularization would not work. The proof used is specific to Kolmogorov complexity and does not clearly generalize to arbitrary regularization techniques; however, I view the argument as being suggestive that regularization techniques would also be insufficient to address the problems raised here. -------------------------------------------------------------------------------- Humans have no values… nor do any agent. Unless you make strong assumptions about their rationality. And depending on those assumptions, you get humans to have any values. AN AGENT WITH NO CLEAR PREFERENCES There are three buttons in this world, B(0), B(1), and X, and one agent H.  B(0) and B(1) can be operated by H, while X can be operated by an outside observer. H will initially press button B(0); if ever X is pressed, the agent will switch to pressing B(1). If X is pressed again, the agent will switch back to pressing B(0), and so on. After a large number of turns N, H will shut off. That’s the full algorithm for H. So the question is, what are the values/preferences/rewards of H? There are three natural reward functions that are plausible:  *  R(0), which is linear i",2018,2022-01-30 04:50:53,2022-01-30 04:50:53,2020-12-17 04:36:27,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/DXQWPWZS/ANupXf8XfZo2EJxGv.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A7KHVPJD,blogPost,2019,"Shah, Rohin",Human-AI Interaction,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/4783ufKpx8xvLMPc6/human-ai-interaction,"THE IMPORTANCE OF FEEDBACK Consider trying to program a self-driving car to drive from San Francisco to Los Angeles -- with no sensors that allow it to gather information as it is driving. This is possible in principle. If you can predict the exact weather conditions, the exact movement of all of the other cars on the road, the exact amount of friction along every part of the road surface, the exact impact of (the equivalents of) pressing the gas or turning the steering wheel, and so on, then you could compute ahead of time how exactly to control the car such that it gets from SF to LA. Nevertheless, it seems unlikely that we will ever be able to accomplish such a feat, even with powerful AI systems. No, in practice there is going to be some uncertainty about how the world is going to evolve; such that any plan computed ahead of time will have some errors that will compound over the course of the plan. The solution is to use sensors to gather information while executing the plan, so that we can notice any errors or deviations from the plan, and take corrective action. It is much easier to build a controller that keeps you pointed in the general direction, than to build a plan that will get you there perfectly without any adaptation. Control theory studies these sorts of systems, and you can see the general power of feedback controllers in the theorems that can be proven. Especially for motion tasks, you can build feedback controllers that are guaranteed to safely achieve the goal, even in the presence of adversarial environmental forces (that are bounded in size, so you can’t have arbitrarily strong wind). In the presence of an adversary, in most environments it becomes impossible even in principle to make such a guarantee if you do not have any sensors or feedback and must compute a plan in advance. Typically, for every such plan, there is some environmental force that would cause it to fail. THE CONTROL THEORY PERSPECTIVE ON AI ALIGNMENT With ambitious value le",2019,2022-01-30 04:50:53,2022-01-30 04:50:53,2020-12-17 04:37:06,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/HDFKCMPB/4783ufKpx8xvLMPc6.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MVTJI96G,blogPost,2018,"Shah, Rohin",Future directions for ambitious value learning,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/EhNCnCkmu7MwrQ7yz/future-directions-for-ambitious-value-learning,"To recap the sequence so far:  * Ambitious value learning aims to infer a utility function that is safe to    maximize, by looking at human behavior.  * However, since you only observe human behavior, you must be able to infer and    account for the mistakes that humans make in order to exceed human    performance. (If we don’t exceed human performance, it’s likely that we’ll    use unsafe techniques that do exceed human performance, due to economic    incentives.)  * You might hope to infer both the mistake model (aka systematic human biases)     and the utility function, and then throw away the mistake model and optimize    the utility function. This cannot be done without additional assumptions.  * One potential assumption you could use would be to codify a specific mistake    model. However, humans are sufficiently complicated that any such model would    be wrong, leading to model misspecification. Model misspecification causes     many problems in general, and is particularly thorny for value learning. Despite these arguments, we could still hope to infer a broad utility function that is safe to optimize, either by sidestepping the formalism used so far, or by introducing additional assumptions. Often, it is clear that these methods would not find the true human utility function (assuming that such a thing exists), but they are worth pursuing anyway because they could find a utility function that is good enough. This post provides pointers to approaches that are currently being pursued. Since these are active areas of research, I don’t want to comment on how feasible they may or may not be -- it’s hard to accurately assess the importance and quality of an idea that is being developed just from what is currently written down about that idea. Assumptions about the mistake model. We could narrow down on the mistake model by making assumptions about it, that could let us avoid the impossibility result. This decision means that we’re accepting the risk of missp",2018,2022-01-30 04:50:52,2022-01-30 04:50:52,2020-12-17 04:36:29,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/J54ABXG9/EhNCnCkmu7MwrQ7yz.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KEDQKKFK,blogPost,2019,"Shah, Rohin",Future directions for narrow value learning,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/MxadmSXHnoCupsWqx/future-directions-for-narrow-value-learning,"Narrow value learning is a huge field that people are already working on (though not by that name) and I can’t possibly do it justice. This post is primarily a list of things that I think are important and interesting, rather than an exhaustive list of directions to pursue. (In contrast, the corresponding post  for ambitious value learning did aim to be exhaustive, and I don’t think I missed much work there.) You might think that since so many people are already working on narrow value learning, we should focus on more neglected areas of AI safety. However, I still think it’s worth working on because long-term safety suggests a particular subset of problems to focus on; that subset seems quite neglected. For example, a lot of work is about how to improve current algorithms in a particular domain, and the solutions encode domain knowledge to succeed. This seems not very relevant for long-term concerns. Some work assumes that a handcoded featurization is given (so that the true reward is linear in the features); this is not an assumption we could make for more powerful AI systems. I will speculate a bit on the neglectedness and feasibility of each of these areas, since for many of them there isn’t a person or research group who would champion them whom I could defer to about the arguments for success. THE BIG PICTURE This category of research is about how you could take narrow value learning algorithms and use them to create an aligned AI system. Typically, I expect this to work by having the narrow value learning enable some form of corrigibility. As far as I can tell, nobody outside of the AI safety community works on this problem. While it is far too early to stake a confident position one way or the other, I am slightly less optimistic about this avenue of approach than one in which we create a system that is directly trained to be corrigible. Avoiding problems with goal-directedness. How do we put together narrow value learning techniques in a way that does",2019,2022-01-30 04:50:52,2022-01-30 04:50:52,2020-12-17 04:37:29,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/V8NFJVTP/MxadmSXHnoCupsWqx.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
49P9KC94,blogPost,2019,"Shah, Rohin",Following human norms,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/eBd6WvzhuqduCkYv3/following-human-norms,"So far we have been talking about how to learn “values” or “instrumental goals”. This would be necessary if we want to figure out how to build an AI system that does exactly what we want it to do. However, we’re probably fine if we can keep learning and building better AI systems. This suggests that it’s sufficient to build AI systems that don’t screw up so badly that it ends this process. If we accomplish that, then steady progress in AI will eventually get us to AI systems that do what we want. So, it might be helpful to break down the problem of learning values into the subproblems of learning what to do, and learning what not to do. Standard AI research will continue to make progress on learning what to do; catastrophe happens when our AI system doesn’t know what not to do. This is the part that we need to make progress on. This is a problem that humans have to solve as well. Children learn basic norms such as not to litter, not to take other people’s things, what not to say in public, etc. As argued in Incomplete Contracting and AI alignment, any contract between humans is never explicitly spelled out, but instead relies on an external unwritten normative structure under which a contract is interpreted. (Even if we don’t explicitly ask our cleaner not to break any vases, we still expect them not to intentionally do so.) We might hope to build AI systems that infer and follow these norms, and thereby avoid catastrophe. It’s worth noting that this will probably not be an instance of narrow value learning, since there are several differences:  * Narrow value learning requires that you learn what to do, unlike norm    inference.  * Norm following requires learning from a complex domain (human society),    whereas narrow value learning can be applied in simpler domains as well.  * Norms are a property of groups of agents, whereas narrow value learning can    be applied in settings with a single agent. Despite this, I have included it in this sequence because it",2019,2022-01-30 04:50:45,2022-01-30 04:50:45,2020-12-17 04:37:25,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/EVK4EAGA/eBd6WvzhuqduCkYv3.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MNJW7DRU,blogPost,2020,"Turner, Alex",Corrigibility as outside view,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/BMj6uMuyBidrdZkiD/corrigibility-as-outside-view,"You run a country. One day, you think ""I could help so many more people if I set all the rules... and I could make this happen"". As far as you can tell, this is the real reason you want to set the rules – you want to help people, and you think you'd do a good job. But historically… in this kind of situation, this reasoning can lead to terrible things. So you just don't do it, even though it feels like a good idea.[1] More generally, Even though my intuition/naïve decision-making process says I should do X, I know (through mental simulation or from history) my algorithm is usually wrong in this situation. I'm not going to do X.  * ""It feels like I could complete this project within a week. But… in the past,    when I've predicted ""a week"" for projects like this, reality usually gives me    a longer answer. I'm not going to trust this feeling. I'm going to allocate    extra time.""  * As a new secretary, I think I know how my boss would want me to reply to an    important e-mail. However, I'm not sure. Even though I think I know what to    do, common sense recommends I clarify.  * You broke up with someone. ""Even though I really miss them, in this kind of    situation, missing my ex isn't a reliable indicator that I should get back    together with them. I'm not going to trust this feeling, and will trust the    ""sober"" version of me which broke up with them."" We are biased and corrupted. By taking the outside view on how our own algorithm performs in a given situation, we can adjust accordingly. CORRIGIBILITY The ""hard problem of corrigibility"" is to build an agent which, in an intuitive sense, reasons internally as if from the programmers' external perspective. We think the AI is incomplete, that we might have made mistakes in building it, that we might want to correct it, and that it would be e.g. dangerous for the AI to take large actions or high-impact actions or do weird new things without asking first. We would ideally want the agent to see itself in",2020-05-08,2022-01-30 04:50:44,2022-01-30 04:50:44,2020-08-31 18:49:20,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/W9SJ2CAM/corrigibility-as-outside-view.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IQFTRH3S,blogPost,2019,"Shah, Rohin; Carroll, Micah",Collaborating with Humans Requires Understanding Them,The Berkeley Artificial Intelligence Research Blog,,,,http://bair.berkeley.edu/blog/2019/10/21/coordination/,The BAIR Blog,2019,2022-01-30 04:50:43,2022-01-30 04:50:43,2020-12-18 00:11:42,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/SE7V6JPX/coordination.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XGBTMKB5,blogPost,2019,"Shah, Rohin",Conclusion to the sequence on value learning,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/TE5nJ882s5dCMkBB8/conclusion-to-the-sequence-on-value-learning,"This post summarizes the sequence on value learning. While it doesn’t introduce any new ideas, it does shed light on which parts I would emphasize most, and the takeaways I hope that readers get. I make several strong claims here; interpret these as my impressions, not my beliefs. I would guess many researchers disagree with the (strength of the) claims, though I do not know what their arguments would be. Over the last three months we’ve covered a lot of ground. It’s easy to lose sight of the overall picture over such a long period of time, so let's do a brief recap. THE “OBVIOUS” APPROACH Here is an argument for the importance of AI safety:  * Any agent that is much more intelligent than us should not be exploitable by    us, since if we could find some way to exploit the agent, the agent could    also find the exploit and patch it.  * Anything that is not exploitable must be an expected utility maximizer; since    we cannot exploit a superintelligent AI, it must look like an expected    utility maximizer to us.  * Due to Goodhart’s Law, even “slightly wrong” utility functions can lead to    catastrophic outcomes when maximized.  * Our utility function is complex and fragile, so getting the “right” utility    function is difficult. This argument implies that by the time we have a superintelligent AI system, there is only one part of that system that could still have been influenced by us: the utility function. Every other feature of the AI system is fixed by math. As a result, we must necessarily solve AI alignment by influencing the utility function. So of course, the natural approach is to get the right utility function, or at least an adequate one, and have our AI system optimize that utility function. Besides fragility of value, which you might hope that machine learning could overcome, the big challenge is that even if you assume full access to the entire human policy, we cannot infer their values without making an assumption about how their preferences r",2019,2022-01-30 04:50:43,2022-01-30 04:50:43,2020-12-17 04:37:32,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/PWJ5IZ83/TE5nJ882s5dCMkBB8.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AGFKUHA7,blogPost,2018,"Shah, Rohin",Coherence arguments do not imply goal-directed behavior,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/NxF5G6CJiof6cemTw/coherence-arguments-do-not-imply-goal-directed-behavior,"One of the most pleasing things about probability and expected utility theory is that there are many coherence arguments that suggest that these are the “correct” ways to reason. If you deviate from what the theory prescribes, then you must be executing a dominated strategy. There must be some other strategy that never does any worse than your strategy, but does strictly better than your strategy with certainty in at least one situation. There’s a good explanation of these arguments here. We shouldn’t expect mere humans to be able to notice any failures of coherence in a superintelligent agent, since if we could notice these failures, so could the agent. So we should expect that powerful agents appear coherent to us. (Note that it is possible that the agent doesn’t fix the failures because it would not be worth it -- in this case, the argument says that we will not be able to notice any exploitable failures.) Taken together, these arguments suggest that we should model an agent much smarter than us as an expected utility (EU) maximizer. And many people agree that EU maximizers are dangerous. So does this mean we’re doomed? I don’t think so: it seems to me that the problems about EU maximizers that we’ve identified are actually about goal-directed behavior or explicit reward maximizers. The coherence theorems say nothing about whether an AI system must look like one of these categories. This suggests that we could try building an AI system that can be modeled as an EU maximizer, yet doesn’t fall into one of these two categories, and so doesn’t have all of the problems that we worry about. Note that there are two different flavors of arguments that the AI systems we build will be goal-directed agents (which are dangerous if the goal is even slightly wrong):  * Simply knowing that an agent is intelligent lets us infer that it is    goal-directed. (ETA: See this comment for more details on this argument.)  * Humans are particularly likely to build goal-directed agen",2018,2022-01-30 04:50:43,2022-01-30 04:50:43,2020-12-17 04:36:39,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/6SJ3UVN8/NxF5G6CJiof6cemTw.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9B3ZIBN5,blogPost,2019,"Cottier, Ben; Shah, Rohin",Clarifying some key hypotheses in AI alignment,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/mJ5oNYnkYrd4sD5uE/clarifying-some-key-hypotheses-in-ai-alignment,"We've created a diagram mapping out important and controversial hypotheses for AI alignment. We hope that this will help researchers identify and more productively discuss their disagreements. DIAGRAM A part of the diagram. Click through to see the full version. CAVEATS  1. This does not decompose arguments exhaustively. It does not include every     reason to favour or disfavour ideas. Rather, it is a set of key hypotheses     and relationships with other hypotheses, problems, solutions, models, etc.     Some examples of important but apparently uncontroversial premises within     the AI safety community: orthogonality, complexity of value, Goodhart's     Curse, AI being deployed in a catastrophe-sensitive context.  2. This is not a comprehensive collection of key hypotheses across the whole     space of AI alignment. It focuses on a subspace that we find interesting and     is relevant to more recent discussions we have encountered, but where key     hypotheses seem relatively less illuminated. This includes rational agency     and goal-directedness, CAIS, corrigibility, and the rationale of     foundational and practical research. In hindsight, the selection criteria     was something like: 1. The idea is closely connected to the problem of         artificial systems optimizing adversarially against humans.      2. The idea must be explained sufficiently well that we         believe it is plausible.            3. Arrows in the diagram indicate flows of evidence or soft relations, not     absolute logical implications — please read the ""interpretation"" box in the     diagram. Also pay attention to any reasoning written next to a Yes/No/Defer     arrow — you may disagree with it, so don't blindly follow the arrow! BACKGROUND Much has been written in the way of arguments for AI risk. Recently there have been some talks and posts that clarify different arguments, point to open questions, and highlight the need for further clarification and analysis. We largely s",2019,2022-01-30 04:50:43,2022-01-30 04:50:43,2020-12-14 00:17:43,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/VZV4SV4B/clarifying-some-key-hypotheses-in-ai-alignment.html,,CHAI; TechSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PEHQRUAW,blogPost,2018,"Filan, Daniel",Bottle Caps Aren't Optimisers,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/26eupx3Byc8swRS7f/bottle-caps-aren-t-optimisers,"Crossposted from my blog. One thing I worry about sometimes is people writing code with optimisers in it, without realising that that's what they were doing. An example of this: suppose you were doing deep reinforcement learning, doing optimisation to select a controller (that is, a neural network that takes a percept and returns an action) that generated high reward in some environment. Alas, unknown to you, this controller actually did optimisation itself to select actions that score well according to some metric that so far has been closely related to your reward function. In such a scenario, I'd be wary about your deploying that controller, since the controller itself is doing optimisation which might steer the world into a weird and unwelcome place. In order to avoid such scenarios, it would be nice if one could look at an algorithm and determine if it was doing optimisation. Ideally, this would involve an objective definition of optimisation that could be checked from the source code of the algorithm, rather than something like ""an optimiser is a system whose behaviour can't usefully be predicted mechanically, but can be predicted by assuming it near-optimises some objective function"", since such a definition breaks down when you have the algorithm's source code and can compute its behaviour mechanically. You might think about optimisation as follows: a system is optimising some objective function to the extent that that objective function attains much higher values than would be attained if the system didn't exist, or were doing some other random thing. This type of definition includes those put forward by  Yudkowsky and Oesterheld. However, I think there are crucial counterexamples to this style of definition. Firstly, consider a lid screwed onto a bottle of water. If not for this lid, or if the lid had a hole in it or were more loose, the water would likely exit the bottle via evaporation or being knocked over, but with the lid, the water stays in the b",2018,2022-01-30 04:50:43,2022-01-30 04:50:43,2020-12-13 23:01:28,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/QNGDRZZ4/bottle-caps-aren-t-optimisers.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
T95TE8B7,blogPost,2020,"Filan, Daniel",An Analytic Perspective on AI Alignment,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/8GdPargak863xaebm/an-analytic-perspective-on-ai-alignment,"This is a perspective I have on how to do useful AI alignment research. Most perspectives I’m aware of are constructive: they have some blueprint for how to build an aligned AI system, and propose making it more concrete, making the concretisations more capable, and showing that it does in fact produce an aligned AI system. I do not have a constructive perspective - I’m not sure how to build an aligned AI system, and don’t really have a favourite approach. Instead, I have an analytic perspective. I would like to understand AI systems that are built. I also want other people to understand them. I think that this understanding will hopefully act as a ‘filter’ that means that dangerous AI systems are not deployed. The following dot points lay out the perspective. Since the remainder of this post is written as nested dot points, some readers may prefer to read it in workflowy. BACKGROUND BELIEFS  * I am imagining a future world in which powerful AGI systems are made of    components roughly like neural networks (either feedforward or recurrent)    that have a large number of parameters.  * Futhermore, I’m imagining that the training process of these ML systems does    not provide enough guarantees about deployment performance.  * In particular,       I’m supposing that systems are being trained based on their ability to       deal with simulated situations, and that that’s insufficient because       deployment situations are hard to model and therefore simulate.  * One          reason that they are hard to model is the complexities of the real          world.  * The real world might be intrinsically difficult to model for             the relevant system. For instance, it’s difficult to simulate all             the situations in which the CEO of Amazon might find themselves.           * Another reason that real world situations may be hard to             model is that they are dependent on the final trained system.  * The                trained system may be able to af",2020-02-29,2022-01-30 04:50:42,2022-01-30 04:50:42,2020-09-05 18:44:29,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/NI3GMME7/an-analytic-perspective-on-ai-alignment.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BDMW3AQ5,blogPost,2019,"Shah, Rohin",AI safety without goal-directed behavior,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/tHxXdAn8Yuiy9y2pZ/ai-safety-without-goal-directed-behavior,"When I first entered the field of AI safety, I thought of the problem as figuring out how to get the AI to have the “right” utility function. This led me to work on the problem of inferring values from demonstrators with unknown biases, despite the impossibility results in the area. I am less excited about that avenue because I am pessimistic about the prospects of ambitious value learning (for the reasons given in the first part of this sequence). I think this happened because the writing on AI risk that I encountered has the pervasive assumption that any superintelligent AI agent must be maximizing some utility function over the long term future, such that it leads to goal-directed behavior and convergent instrumental subgoals. It’s often not stated as an assumption; rather, inferences are made assuming that you have the background model that the AI is goal-directed. This makes it particularly hard to question the assumption, since you don’t realize that the assumption is even there. Another reason that this assumption is so easily accepted is that we have a long history of modeling rational agents as expected utility maximizers, and for good reason: there are many coherence arguments that say that, given that you have preferences/goals, if you aren’t using probability theory and expected utility theory, then you can be taken advantage of. It’s easy to make the inference that a superintelligent agent must be rational, and therefore it must be an expected utility maximizer. Because this assumption was so embedded in how I thought about the problem, I had trouble imagining how else to even consider the problem. I would guess this is true for at least some other people, so I want to summarize the counterargument, and list a few implications, in the hope that this makes the issue clearer. WHY GOAL-DIRECTED BEHAVIOR MAY NOT BE REQUIRED The main argument of this chapter is that it is not required that a superintelligent agent takes actions in pursuit of some goal. I",2019,2022-01-30 04:50:42,2022-01-30 04:50:42,2020-12-17 04:36:56,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/G5XADFXX/tHxXdAn8Yuiy9y2pZ.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TECSUSTJ,blogPost,2021,"Bensinger, Rob; Garrabrant, Scott; Shah, Rohin; Tyre, Eli",Garrabrant and Shah on human modeling in AGI,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/Wap8sSDoiigrJibHA/garrabrant-and-shah-on-human-modeling-in-agi,"This is an edited transcript of a conversation between Scott Garrabrant (MIRI) and Rohin Shah (DeepMind) about whether researchers should focus more on approaches to AI alignment that don’t require highly capable AI systems to do much human modeling. CFAR’s Eli Tyre facilitated the conversation. To recap, and define some terms:  * The alignment problem is the problem of figuring out ""how to develop    sufficiently advanced machine intelligences such that running them produces    good outcomes in the real world"" (outcome alignment) or the problem of    building powerful AI systems that are trying to do what their operators want    them to do (intent alignment).  * In 2016, Hadfield-Mennell, Dragan, Abbeel, and Russell proposed that we think    of the alignment problem in terms of “Cooperative Inverse Reinforcement    Learning” (CIRL), a framework where the AI system is initially uncertain of    its reward function, and interacts over time with a human (who knows the    reward function) in order to learn it.  * In 2016-2017, Christiano proposed “Iterated Distillation and Amplification”    (IDA), an approach to alignment that involves iteratively training AI systems     to learn from human experts assisted by AI helpers. In 2018, Irving,    Christiano, and Amodei proposed AI safety via debate, an approach based on    similar principles.  * In early 2019, Scott Garrabrant and DeepMind’s Ramana Kumar argued in “    Thoughts on Human Models” that we should be “cautious about AGI designs that    use human models” and should “put more effort into developing approaches that    work well in the absence of human models”.  * In early February 2021, Scott and Rohin talked more about human modeling and    decided to have the real-time conversation below. You can find a recording of the Feb. 28 discussion below (sans Q&A) here. 1. IDA, CIRL, AND INCENTIVES Eli:I guess I want to first check what our goal is here. There was some stuff that happened online. Where are we accordi",2021-08-04,2022-01-30 04:52:38,2022-01-30 04:52:38,2021-11-18 23:07:47,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/VID79W62/garrabrant-and-shah-on-human-modeling-in-agi.html,,TechSafety; AmbiguousSafety,,,,,"Garrabrant, Scott; Shah, Rohin; Tyre, Eli",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
356XZMGD,blogPost,2019,"Ngo, Richard",Disentangling arguments for the importance of AI safety,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/JbcWQCxKWn3y49bNB/disentangling-arguments-for-the-importance-of-ai-safety,"[Note: my views have changed since writing this post, and while I still consider it useful as a catalogue of concerns, I no longer think that it satisfactorily disentangles those concerns from each other. I hope to post better material along these lines later this year]. I recently attended the 2019 Beneficial AGI conference organised by the Future of Life Institute. I’ll publish a more complete write-up later, but I was particularly struck by how varied attendees' reasons for considering AI safety important were. Before this, I’d observed a few different lines of thought, but interpreted them as different facets of the same idea. Now, though, I’ve identified at least 6 distinct serious arguments for why AI safety is a priority. By distinct I mean that you can believe any one of them without believing any of the others - although of course the particular categorisation I use is rather subjective, and there’s a significant amount of overlap. In this post I give a brief overview of my own interpretation of each argument (note that I don’t necessarily endorse them myself). They are listed roughly from most specific and actionable to most general. I finish with some thoughts on what to make of this unexpected proliferation of arguments. Primarily, I think it increases the importance of clarifying and debating the core ideas in AI safety.  1. Maximisers are dangerous. Superintelligent AGI will behave as if it’s     maximising the expectation of some utility function, since doing otherwise     can be shown to be irrational. Yet we can’t write down a utility function     which precisely describes human values, and optimising very hard for any     other function will lead to that AI rapidly seizing control (as a convergent     instrumental subgoal) and building a future which contains very little of     what we value (because of Goodhart’s law and the complexity and fragility of     values). We won’t have a chance to notice and correct misalignment because     an AI which",2019-01-21,2022-01-30 04:52:38,2022-01-30 04:52:38,2020-11-21 16:54:39,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/VFQSTXH8/disentangling-arguments-for-the-importance-of-ai-safety.html,,TechSafety; DeepMind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ISV8DZWT,blogPost,2020,"Ngo, Richard",Environments as a bottleneck in AGI development,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/vqpEC3MPioHX7bv4t/environments-as-a-bottleneck-in-agi-development,"Given a training environment or dataset, a training algorithm, an optimiser, and a model class capable of implementing an AGI (with the right parameters), there are two interesting questions we might ask about how conducive that environment is for training an AGI. The first is: how much do AGIs from that model class outperform non-AGIs? The second is: how straightforward is the path to reaching an AGI? We can visualise these questions in terms of the loss landscape of those models when evaluated on the training environment. The first asks how low the set of AGIs is, compared with the rest of the landscape. The second asks how favourable the paths through that loss landscape to get to AGIs are - that is, do the local gradients usually point in the right direction, and how deep are the local minima? Some people believe that there are many environments in which AGIs can be reached via favourable paths in the loss landscape and dramatically outperform non-AGIs; let’s call this the easy paths hypothesis. By contrast, the hard paths hypothesis is that it’s rare for environments (even complex meta-environments consisting of many separate tasks) to straightforwardly incentivise the development of general intelligence. This would suggest that specific environmental features will be necessary to prevent most models from getting stuck in local minima where they only possess narrow, specialised cognitive skills. There has been a range of speculation on what such features might be - perhaps multi-agent autocurricula, or realistic simulations, or specific types of human feedback. I’ll discuss some of these possibilities later in the post. This spectrum is complicated by its dependence on the model class, training algorithm, and choice of optimiser. If we had a perfect optimiser, then the hilliness of the loss landscape wouldn’t matter. For now, I'm imagining using optimisers fairly similar to current stochastic gradient descent. Meanwhile, I’m assuming in this post that (in acc",2020-07-17,2022-01-30 04:52:38,2022-01-30 04:52:38,2020-08-28 17:46:55,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/UBTACAKX/environments-as-a-bottleneck-in-agi-development.html,,TechSafety; DeepMind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
N667JVNF,blogPost,2018,"Ortega, Pedro; Maini, Vishal","Building safe artificial intelligence: specification, robustness, and assurance",Deep Mind Safety Research (Medium),,,,https://medium.com/@deepmindsafetyresearch/building-safe-artificial-intelligence-52f5f75058f1,"By Pedro A. Ortega, Vishal Maini, and the DeepMind safety team",2018-09-27,2022-01-30 04:52:37,2022-01-30 04:52:37,2020-11-21 17:04:08,,,,,,,Building safe artificial intelligence,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/B3G6N3T7/building-safe-artificial-intelligence-52f5f75058f1.html; /Users/jacquesthibodeau/Zotero/storage/MH6PAXF4/building-safe-artificial-intelligence-52f5f75058f1.html,,TechSafety; DeepMind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IDNWAN68,blogPost,2020,"Ngo, Richard",Arguments against myopic training,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/GqxuDtZvfgL2bEQ5v/arguments-against-myopic-training,"Note that this post has been edited to clarify the difference between explicitly assigning a reward to an action based on its later consequences, versus implicitly reinforcing an action by assigning high reward during later timesteps when its consequences are observed. I'd previously conflated these in a confusing way; thanks to Rohin for highlighting this issue.  A number of people seem quite excited about training myopic reinforcement learning agents as an approach to AI safety (for instance this post on approval-directed agents, proposals 2, 3, 4, 10 and 11 here, and this paper and  presentation), but I’m not. I’ve had a few detailed conversations about this recently, and although I now understand the arguments for using myopia better, I’m not much more optimistic about it than I was before. In short, it seems that evaluating agents’ actions by our predictions of their consequences, rather than our evaluations of the actual consequences, will make reinforcement learning a lot harder; yet I haven’t been able to identify clear safety benefits from doing so. I elaborate on these points below; thanks to Jon Uesato, Evan Hubinger, Ramana Kumar and Stephan Wäldchen for discussion and comments. I’ll define a myopic reinforcement learner as a reinforcement learning agent trained to maximise the reward received in the next timestep, i.e. with a discount rate of 0. Because it doesn’t assign credit backwards over time, in order to train it to do anything useful, that reward function will need to contain an estimate of how valuable each (state, action, next state) transition will be for outcomes many steps later. Since that evaluation will need to extrapolate a long way forward anyway, knowing the next state doesn’t add much, and so we can limit our focus to myopic agents trained on reward functions R which ignore the resulting state: that is, where R(s,a,s′)=M(s,a) for some M. I'll call M the approval function; we can think of such agents as being trained to take actions",2020-07-09,2022-01-30 04:52:37,2022-01-30 04:52:37,2020-08-28 17:59:14,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/QUMRCDI5/arguments-against-myopic-training.html,,TechSafety; DeepMind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JVDTIGDK,blogPost,2020,"Ngo, Richard",A space of proposals for building safe advanced AI,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/S9GxuAEeQomnLkeNt/a-space-of-proposals-for-building-safe-advanced-ai,"I liked Evan’s post on 11 proposals for safe AGI. However, I was a little confused about why he chose these specific proposals; it feels like we could generate many more by stitching together the different components he identifies, such as different types of amplification and different types of robustness tools. So I’m going to take a shot at describing a set of dimensions of variation which capture the key differences between these proposals, and thereby describe an underlying space of possible approaches to safety. Firstly I’ll quickly outline the proposals. Rohin’s overview of them is a good place to start - he categorises them as:  * 7 proposals of the form “recursive outer alignment technique” plus    “robustness technique”.  * The recursive outer alignment technique is either debate, recursive reward    modelling, or amplification.The robustness technique is either transparency    tools, relaxed adversarial training, or intermittent oversight by a competent    supervisor.  * 2 proposals of the form “non-recursive outer alignment technique” plus    “robustness technique”.  * The outer alignment technique is either reinforcement learning in a    multiagent environment, or narrow reward learning.  * 2 other proposals: Microscope AI; STEM AI. More specifically, we can describe the four core recursive outer alignment techniques as variants of iterated amplification, as follows: let Amp(M) be the procedure of a human answering questions with access to model M. Then we iteratively train M* (the next version of M) by:  * Imitative amplification: train M* to imitate Amp(M).  * Approval-based amplification: train M* on an approval signal specified by    Amp(M).  * Recursive reward modelling: train M* on a reward function specified by    Amp(M).  * Debate: train M* to win debates against Amp(M). Here are six axes of variation which I claim underlie Evan’s proposals. Each proposal is more or less:  1. Supervised  2. Structured  3. Adversarial  4. Language-based  5.",2020-07-10,2022-01-30 04:52:36,2022-01-30 04:52:36,2020-08-28 18:00:24,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,,,TechSafety; DeepMind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IAZFVRWI,blogPost,2019,"Shah, Rohin",Will humans build goal-directed agents?,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/9zpT9dikrrebdq3Jf/will-humans-build-goal-directed-agents,"In the previous post, I argued that simply knowing that an AI system is superintelligent does not imply that it must be goal-directed. However, there are many other arguments that suggest that AI systems will or should be goal-directed, which I will discuss in this post. Note that I don’t think of this as the Tool AI vs. Agent AI argument: it seems possible to build agent AI systems that are not goal-directed. For example, imitation learning allows you to create an agent that behaves similarly to another agent -- I would classify this as “Agent AI that is not goal-directed”. (But see this comment thread for discussion.) Note that these arguments have different implications than the argument that superintelligent AI must be goal-directed due to coherence arguments. Suppose you believe all of the following:  * Any of the arguments in this post.  * Superintelligent AI is not required to be goal-directed, as I argued in the     last post.  * Goal-directed agents cause catastrophe by default. Then you could try to create alternative designs for AI systems such that they can do the things that goal-directed agents can do without themselves being goal-directed. You could also try to persuade AI researchers of these facts, so that they don’t build goal-directed systems. ECONOMIC EFFICIENCY: GOAL-DIRECTED HUMANS Humans want to build powerful AI systems in order to help them achieve their goals -- it seems quite clear that humans are at least partially goal-directed. As a result, it seems natural that they would build AI systems that are also goal-directed. This is really an argument that the system comprising the human and AI agent should be directed towards some goal. The AI agent by itself need not be goal-directed as long as we get goal-directed behavior when combined with a human operator. However, in the situation where the AI agent is much more intelligent than the human, it is probably best to delegate most or all decisions to the agent, and so the agent could s",2019,2022-01-30 04:51:44,2022-01-30 04:51:44,2020-12-17 04:36:45,,,,,,,Will humans build goal-directed agents?,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/H5UGJHAN/9zpT9dikrrebdq3Jf.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
D23D6TKU,blogPost,2021,"Critch, Andrew","What Multipolar Failure Looks Like, and Robust Agent-Agnostic Processes (RAAPs)",AI Alignment Forum,,,,https://www.alignmentforum.org/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic,"With: Thomas Krendl Gilbert, who provided comments, interdisciplinary feedback, and input on the RAAP concept. Thanks also for comments from Ramana Kumar. Target audience: researchers and institutions who think about existential risk from artificial intelligence, especially AI researchers. Preceded by: Some AI research areas and their relevance to existential safety, which emphasized the value of thinking about multi-stakeholder/multi-agent social applications, but without concrete extinction scenarios. This post tells a few different stories in which humanity dies out as a result of AI technology, but where no single source of human or automated agency is the cause. Scenarios with multiple AI-enabled superpowers are often called “multipolar” scenarios in AI futurology jargon, as opposed to “unipolar” scenarios with just one superpower. Unipolar take-offsMultipolar take-offsSlow take-offs<not this post>Part 1 of this postFast take-offs<not this post>Part 2 of this postPart 1 covers a batch of stories that play out slowly (“slow take-offs”), and Part 2 stories play out quickly. However, in the end I don’t want you to be super focused how fast the technology is taking off. Instead, I’d like you to focus on multi-agent processes with a robust tendency to play out irrespective of which agents execute which steps in the process. I’ll call such processes Robust Agent-Agnostic Processes (RAAPs). A group walking toward a restaurant is a nice example of a RAAP, because it exhibits:  * Robustness: If you temporarily distract one of the walkers to wander off, the    rest of the group will keep heading toward the restaurant, and the distracted    member will take steps to rejoin the group.  * Agent-agnosticism: Who’s at the front or back of the group might vary    considerably during the walk. People at the front will tend to take more    responsibility for knowing and choosing what path to take, and people at the    back will tend to just follow. Thus, the execution of r",2021-03-31,2022-01-30 04:51:43,2022-01-30 04:51:43,2021-11-14 16:44:47,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/KSVPBM2X/what-multipolar-failure-looks-like-and-robust-agent-agnostic.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2VA8RMCJ,blogPost,2015,"Oesterheld, Caspar","Two-boxing, smoking and chewing gum in Medical Newcomb problems",LessWrong,,,,https://www.lesswrong.com/posts/wWnN3y5GmqLLCJFAz/two-boxing-smoking-and-chewing-gum-in-medical-newcomb,"I am currently learning about the basics of decision theory, most of which is common knowledge on LW. I have a question, related to why EDT is said not to work. Consider the following Newcomblike problem: A study shows that most people who two-box in Newcomblike problems as the following have a certain gene (and one-boxers don't have the gene). Now, Omega could put you into something like Newcomb's original problem, but instead of having run a simulation of you, Omega has only looked at your DNA: If you don't have the ""two-boxing gene"", Omega puts $1M into box B, otherwise box B is empty. And there is $1K in box A, as usual. Would you one-box (take only box B) or two-box (take box A and B)? Here's a causal diagram for the problem: Since Omega does not do much other than translating your genes into money under a box, it does not seem to hurt to leave it out: I presume that most LWers would one-box. (And as I understand it, not only CDT but also TDT would two-box, am I wrong?) Now, how does this problem differ from the smoking lesion or Yudkowsky's (2010, p.67) chewing gum problem? Chewing Gum (or smoking) seems to be like taking box A to get at least/additional $1K, the two-boxing gene is like the CGTA gene, the illness itself (the abscess or lung cancer) is like not having $1M in box B. Here's another causal diagram, this time for the chewing gum problem: As far as I can tell, the difference between the two problems is some additional, unstated intuition in the classic medical Newcomb problems. Maybe, the additional assumption is that the actual evidence lies in the ""tickle"", or that knowing and thinking about the study results causes some complications. In EDT terms: The intuition is that neither smoking nor chewing gum gives the agent additional information.",2015-06-29,2022-01-30 04:51:37,2022-01-30 04:51:37,2020-11-23 00:59:21,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/UBUI7HWH/two-boxing-smoking-and-chewing-gum-in-medical-newcomb.html,,CLR; TechSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IP5KVVZ3,blogPost,2018,"Treutlein, Johannes",Three wagers for multiverse-wide superrationality,The Universe from an Intentional Stance,,,,https://casparoesterheld.com/2018/03/31/three-wagers-for-multiverse-wide-superrationality/,"In this post, I outline three wagers in favor of the hypothesis that multiverse-wide superrationality (MSR) has action-guiding implications. MSR is based on three core assumptions: There is a large…",2018-03-31,2022-01-30 04:51:37,2022-01-30 04:51:37,2020-11-23 00:42:20,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/MC3STFK8/three-wagers-for-multiverse-wide-superrationality.html,,CLR; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EB6XDGH4,blogPost,2020,"Kokotajlo, Daniel",The date of AI Takeover is not the day the AI takes over,LessWrong,,,,https://www.lesswrong.com/posts/JPan54R525D68NoEt/the-date-of-ai-takeover-is-not-the-day-the-ai-takes-over,"Instead, it’s the point of no return—the day we AI risk reducers lose the ability to significantly reduce AI risk. This might happen years before classic milestones like “World GWP doubles in four years” and “Superhuman AGI is deployed."" The rest of this post explains, justifies, and expands on this obvious but underappreciated idea. (Toby Ord appreciates it; see quote below). I found myself explaining it repeatedly, so I wrote this post as a reference. AI timelines often come up in career planning conversations. Insofar as AI timelines are short, career plans which take a long time to pay off are a bad idea, because by the time you reap the benefits of the plans it may already be too late. It may already be too late because AI takeover may already have happened. But this isn’t quite right, at least not when “AI takeover” is interpreted in the obvious way, as meaning that an AI or group of AIs is firmly in political control of the world, ordering humans about, monopolizing violence, etc. Even if AIs don’t yet have that sort of political control, it may already be too late. Here are three examples:  1. Superhuman agent AGI is still in its box but nobody knows how to align it     and other actors are going to make their own version soon, and there isn’t     enough time to convince them of the risks. They will make and deploy agent     AGI, it will be unaligned, and we have no way to oppose it except with our     own unaligned AGI. Even if it takes years to actually conquer the world,     it’s already game over.            2. Various weak and narrow AIs are embedded in the economy and beginning to     drive a slow takeoff; capabilities are improving much faster than     safety/alignment techniques and due to all the money being made there’s too     much political opposition to slowing down capability growth or keeping AIs     out of positions of power. We wish we had done more safety/alignment     research earlier, or built a political movement earlier when opposit",2020,2022-01-30 04:51:37,2022-01-30 04:51:37,2020-12-12 15:02:10,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/9V2GNH7T/the-date-of-ai-takeover-is-not-the-day-the-ai-takes-over.html,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7E5EW6SI,blogPost,2021,"Kokotajlo, Daniel","Taboo ""Outside View""",LessWrong,,,,https://www.lesswrong.com/posts/BcYfsi7vmhDvzQGiF/taboo-outside-view,"No one has ever seen an AGI takeoff, so any attempt to understand it must use these outside view considerations. —[Redacted for privacy] What? That’s exactly backwards. If we had lots of experience with past AGI takeoffs, using the outside view to predict the next one would be a lot more effective. —My reaction Two years ago I wrote a deep-dive summary of Superforecasting and the associated scientific literature. I learned about the “Outside view” / “Inside view” distinction, and the evidence supporting it. At the time I was excited about the concept and wrote: “...I think we should do our best to imitate these best-practices, and that means using the outside view far more than we would naturally be inclined.” Now that I have more experience, I think the concept is doing more harm than good in our community. The term is easily abused and its meaning has expanded too much. I recommend we permanently taboo “Outside view,” i.e. stop using the word and use more precise, less confused concepts instead. This post explains why. WHAT DOES “OUTSIDE VIEW” MEAN NOW? Over the past two years I’ve noticed people (including myself!) do lots of different things in the name of the Outside View. I’ve compiled the following lists based on fuzzy memory of hundreds of conversations with dozens of people: BIG LIST O’ THINGS PEOPLE DESCRIBE AS OUTSIDE VIEW:  * Reference class forecasting, the practice of computing a probability of an    event by looking at the frequency with which similar events occurred in    similar situations. Also called comparison class forecasting. [EDIT: Eliezer    rightly points out that sometimes reasoning by analogy is undeservedly called    reference class forecasting; reference classes are supposed to be held to a    much higher standard, in which your sample size is larger and the analogy is    especially tight.]  * Trend extrapolation, e.g. “AGI implies insane GWP growth; let’s forecast AGI    timelines by extrapolating GWP trends.”  * Foxy aggregatio",2021-06-17,2022-01-30 04:51:37,2022-01-30 04:51:37,2021-12-11 14:16:18,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/XCKCRUKH/taboo-outside-view.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P4FKCVJ5,blogPost,2020,"Kokotajlo, Daniel",Soft takeoff can still lead to decisive strategic advantage,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/PKy8NuNPknenkDY74/soft-takeoff-can-still-lead-to-decisive-strategic-advantage,"[Epistemic status: Argument by analogy to historical cases. Best case scenario it's just one argument among many. Edit: Also, thanks to feedback from others, especially Paul, I intend to write a significantly improved version of this post in the next two weeks.] I have on several occasions heard people say things like this:  The original Bostrom/Yudkowsky paradigm envisioned a single AI built by a single AI project, undergoing intelligence explosion all by itself and attaining a decisive strategic advantage as a result. However, this is very unrealistic. Discontinuous jumps in technological capability are very rare, and it is very implausible that one project could produce more innovations than the rest of the world combined. Instead we should expect something more like the Industrial Revolution: Continuous growth, spread among many projects and factions, shared via a combination of trade and technology stealing. We should not expect any one project or AI to attain a decisive strategic advantage, because there will always be other projects and other AI that are only slightly less powerful, and coalitions will act to counterbalance the technological advantage of the frontrunner. (paraphrased)Proponents of this view often cite Paul Christiano in support. Last week I heard him say he thinks the future will be ""like the Industrial Revolution but 10x-100x faster."" In this post, I assume that Paul's slogan for the future is correct and then nevertheless push back against the view above. Basically, I will argue that even if the future is like the industrial revolution only 10x-100x faster, there is a 30%+ chance that it will involve a single AI project (or a single AI) with the ability to gain a decisive strategic advantage, if they so choose. (Whether or not they exercise that ability is another matter.) Why am I interested in this? Do I expect some human group to take over the world? No; instead what I think is that (1) an unaligned AI in the leading project might ta",2020-08-23,2022-01-30 04:51:37,2022-01-30 04:51:37,2020-11-23 00:32:29,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/BCMN8958/soft-takeoff-can-still-lead-to-decisive-strategic-advantage.html,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
D8J8MKT6,blogPost,2019,"Sotala, Kaj",Sequence introduction: non-agent and multiagent models of mind,LessWrong,,,,https://www.lesswrong.com/posts/M4w2rdYgCKctbADMn/sequence-introduction-non-agent-and-multiagent-models-of,"A typical paradigm by which people tend to think of themselves and others is as  consequentialist agents: entities who can be usefully modeled as having beliefs and goals, who are then acting according to their beliefs to achieve their goals. This is often a useful model, but it doesn’t quite capture reality. It’s a bit of a fake framework. Or in computer science terms, you might call it a leaky abstraction.  An abstraction in the computer science sense is a simplification which tries to hide the underlying details of a thing, letting you think in terms of the simplification rather than the details. To the extent that the abstraction actually succeeds in hiding the details, this makes things a lot simpler. But sometimes the abstraction inevitably leaks, as the simplification fails to predict some of the actual behavior that emerges from the details; in that situation you need to actually know the underlying details, and be able to think in terms of them. Agent-ness being a leaky abstraction is not exactly a novel concept for Less Wrong; it has been touched upon several times, such as in Scott Alexander’s  Blue-Minimizing Robot Sequence. At the same time, I do not think that it has been quite fully internalized yet, and that many foundational posts on LW go wrong due to being premised on the assumption of humans being agents. In fact, I would go as far as to claim that this is the biggest flaw of the original Sequences: they were attempting to explain many failures of rationality as being due to cognitive biases, when in retrospect it looks like understanding cognitive biases doesn’t actually make you substantially more effective. But if you are implicitly modeling humans as goal-directed agents, then cognitive biases is the most natural place for irrationality to emerge from, so it makes sense to focus the most on there. Just knowing that an abstraction leaks isn’t enough to improve your thinking, however. To do better, you need to know about the actual underlyi",2019-01-07,2022-01-30 04:51:37,2022-01-30 04:51:37,2020-11-23 00:37:49,,,,,,,Sequence introduction,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/QJVPBHVI/sequence-introduction-non-agent-and-multiagent-models-of.html,,CLR; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RCWPURXT,blogPost,2021,"Clifton, Jesse",Weak identifiability and its consequences in strategic settings,Center on Long-Term Risk,,,,https://longtermrisk.org/weak-identifiability-and-its-consequences-in-strategic-settings/,"One way that agents might become involved in catastrophic conflict is if they have mistaken beliefs about one another. Maybe I think you are bluffing when you threaten to launch the nukes, but you are dead serious. So we should understand why agents might sometimes have such mistaken beliefs. In this post I'll discuss one obstacle to the formation of accurate beliefs about other agents, which has to do with identifiability. As with my post on equilibrium and prior selection problems, this is a theme that keeps cropping up in my thinking about AI cooperation and conflict, so I thought it might be helpful to have it written up. We say that a model is unidentifiable if there are several […]",2021-02-13,2022-01-30 04:51:37,2022-01-30 04:51:37,2021-10-31 16:56:26,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/Q8NW2RCR/weak-identifiability-and-its-consequences-in-strategic-settings.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Z568ZSPJ,blogPost,2018,"Baumann, Tobias",Using surrogate goals to deflect threats,Center on Long-Term Risk,,,,https://longtermrisk.org/using-surrogate-goals-deflect-threats/,"Agents that threaten to harm other agents, either in an attempt at extortion or as part of an escalating conflict, are an important form of agential s-risks. To avoid worst-case outcomes resulting from the execution of such threats, I suggest that agents add a “meaningless” surrogate goal to their utility function.",2018-02-20,2022-01-30 04:51:37,2022-01-30 04:51:37,2020-12-13 22:13:01,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,,,CLR; MetaSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ADF4GZVI,blogPost,2016,"Oesterheld, Caspar",Thoughts on Updatelessness,The Universe from an Intentional Stance,,,,https://casparoesterheld.com/2016/11/21/thoughts-on-updatelessness/,"[This post assumes knowledge of decision theory, as discussed in Eliezer Yudkowsky’s Timeless Decision Theory.] One interesting feature of some decision theories that I used to be a bit confused ab…",2016-11-21,2022-01-30 04:51:37,2022-01-30 04:51:37,2020-11-23 00:57:49,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/6E7AVG6X/thoughts-on-updatelessness.html,,CLR; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Q46HF4G8,blogPost,2018,"Oesterheld, Caspar","The law of effect, randomization and Newcomb’s problem",The Universe from an Intentional Stance,,,,https://casparoesterheld.com/2018/02/15/the-law-of-effect-randomization-and-newcombs-problem/,"The law of effect (LoE), as introduced on p. 244 of Thorndike’s (1911) Animal Intelligence, states: Of several responses made to the same situation, those which are accompanied or closely followed …",2018-02-15,2022-01-30 04:51:37,2022-01-30 04:51:37,2020-11-23 00:43:11,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/QEI5Q6ZD/the-law-of-effect-randomization-and-newcombs-problem.html,,CLR; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BSZFC7NB,blogPost,2017,Center on Long-Term Risk,The future of growth: near-zero growth rates,Center on Long-Term Risk,,,,https://longtermrisk.org/the-future-of-growth-near-zero-growth-rates/,"Exponential growth is a common pattern found throughout nature. Yet it is also a pattern that tends not to last, as growth rates tend to decline sooner or later. In biology, this pattern of exponential growth that wanes off is found in everything from the development of individual bodies — for instance, in the growth of […]",2017-07-26,2022-01-30 04:51:37,2022-01-30 04:51:37,2020-11-23 00:48:28,,,,,,,The future of growth,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/9EJNMFD5/the-future-of-growth-near-zero-growth-rates.html,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Q22A6EWN,blogPost,2017,Caspar,The average utilitarian’s solipsism wager,The Universe from an Intentional Stance,,,,https://casparoesterheld.com/2017/03/15/the-average-utilitarians-solipsism-wager/,"The following prudential argument is relatively common in my circles: We probably live in a simulation, but if we don’t, our actions matter much more. Thus, expected value calculations are do…",2017-03-15,2022-01-30 04:51:37,2022-01-30 04:51:37,2020-11-23 00:53:18,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/FBTTP3PX/the-average-utilitarians-solipsism-wager.html,,CLR; MetaSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CGZBEXFN,blogPost,2019,"Kokotajlo, Daniel","The ""Commitment Races"" Problem",AI Alignment Forum,,,,https://www.alignmentforum.org/posts/brXr7PJ2W4Na2EW2q/the-commitment-races-problem,"[Epistemic status: Strong claims vaguely stated and weakly held. I expect that writing this and digesting feedback on it will lead to a much better version in the future. EDIT: So far this has stood the test of time. EDIT: As of September 2020 I think this is one of the most important things to be thinking about.] This post attempts to generalize and articulate a problem that people have been  thinking about since at least 2016. [Edit: 2009 in fact!] In short, here is the problem: Consequentialists can get caught in commitment races, in which they want to make commitments as soon as possible. When consequentialists make commitments too soon, disastrous outcomes can sometimes result. The situation we are in (building AGI and letting it self-modify) may be one of these times unless we think carefully about this problem and how to avoid it. For this post I use ""consequentialists"" to mean agents that choose actions entirely on the basis of the expected consequences of those actions. For my purposes, this means they don't care about historical facts such as whether the options and consequences available now are the result of malicious past behavior. (I am trying to avoid trivial definitions of consequentialism according to which everyone is a consequentialist because e.g. ""obeying the moral law"" is a consequence.) This definition is somewhat fuzzy and I look forward to searching for more precision some other day. CONSEQUENTIALISTS CAN GET CAUGHT IN COMMITMENT RACES, IN WHICH THEY WANT TO MAKE COMMITMENTS AS SOON AS POSSIBLE Consequentialists are bullies; a consequentialist will happily threaten someone insofar as they think the victim might capitulate and won't retaliate. Consequentialists are also cowards; they conform their behavior to the incentives set up by others, regardless of the history of those incentives. For example, they predictably give in to credible threats unless reputational effects weigh heavily enough in their minds to prevent this. In most ordi",2019-08-22,2022-01-30 04:51:37,2022-01-30 04:51:37,2020-11-23 00:34:07,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/69XC8SXE/the-commitment-races-problem.html,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JQ9WVERP,blogPost,2018,Kaj Sotala,Shaping economic incentives for collaborative AGI,LessWrong,,,,https://www.lesswrong.com/posts/FkZCM4DMprtEp568s/shaping-economic-incentives-for-collaborative-agi,"In ""An AI Race for Strategic Advantage: Rhetoric and Risks"" (2018), Stephen Cave and Seán S ÓhÉigeartaigh argue that we should try to promote a cooperative AI narrative over a competitive one: The next decade will see AI applied in an increasingly integral way to safety-critical systems; healthcare, transport, infrastructure to name a few. In order to realise these benefits as quickly and safely as possible, sharing of research, datasets, and best practices will be critical. For example, to ensure the safety of autonomous cars, pooling expertise and datasets on vehicle performances across as wide as possible a range of environments and conditions (including accidents and near-accidents) would provide substantial benefits for all involved. This is particularly so given that the research, data, and testing needed to refine and ensure the safety of such systems before deployment may be considerably more costly and time-consuming than the research needed to develop the initial technological capability.Promoting recognition that deep cooperation of this nature is needed to deliver the benefits of AI robustly may be a powerful tool in dispelling a ‘technological race’ narrative; and a ‘cooperation for safe AI’ framing is likely to become increasingly important as more powerful and broadly capable AI systems are developed and deployed. [...] There have been encouraging developments promoting the above narratives in recent years. ‘AI for global benefit’ is perhaps best exemplified by the 2017’s ITU summit on AI for Global Good (Butler 2017), although it also features prominently in narratives being put forward by the IEEE’s Ethically Aligned Design process (IEEE 2016), the Partnership on AI, and programmes and materials put forward by Microsoft, DeepMind and other leading companies. Collaboration on AI in safety-critical settings is also a thematic pillar for the Partnership on AI2 . Even more ambitious cooperative projects have been proposed by others, for example the cal",2018,2022-01-30 04:51:37,2022-01-30 04:51:37,2020-12-13 23:46:07,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/8RKHP3H9/shaping-economic-incentives-for-collaborative-agi.html,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WENKZBN3,blogPost,2021,"Demski, Abram",Four Motivations for Learning Normativity,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/oqghwKKifztYWLsea/four-motivations-for-learning-normativity,"I have been pretty satisfied with my desiderata for learning normativity, but I  haven't been very satisfied with my explanation of why exactly these desiderata are important. I have a sense that it's not just a grab-bag of cool stuff; something about trying to do all those things at once points at something important. What follows are four different elevator pitches, which tell different stories about how it all hangs together. Desiderata are bolded. CONCEPTUAL DIFFICULTIES WITH OUTER ALIGNMENT The classic problem of outer alignment is that we have no perfect loss function,  so we can't just go optimize. The problem can be understood by thinking about  Goodhart and how optimization amplifies. The classic response to this is value uncertainty and value learning, but wireheading, human manipulation, and  no-free-lunch results make it seem plausible that we have the same problem one level up: we still don't know how to specify a perfect loss function for what we care about, and imperfect loss functions can still create big problems. So, just like value-learning tackles the initial problem head-on by suggesting we manage our uncertainty about values and gain knowledge over time, learning at all levels suggests that we tackle the meta-problem directly, explicitly representing the fact that we don't have a perfectly good loss function at any  level, but can manage that uncertainty and learn-to-learn over time. Humans can only give explicit feedback at so many meta-levels, so between-level sharing is critical for any meaningful learning to take place at higher meta-levels. Otherwise, higher meta-levels remain highly uncertain, which itself makes learning at lower levels almost impossible (since you can't learn if you have high uncertainty about learning-to-learn). A consequence of having no perfect loss function is no perfect feedback; no evidence about what the system should do can be considered absolute. A helpful measure for coping with this is to support uncertai",2021-03-11,2022-01-30 04:56:48,2022-01-30 04:56:48,2021-11-14 16:26:13,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/T5SHAZDS/four-normativity-motivations.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UJEMETW2,blogPost,2021,"Demski, Abram","Formal Inner Alignment, Prospectus",AI Alignment Forum,,,,https://www.alignmentforum.org/posts/a7jnbtoKFyvu5qfkd/formal-inner-alignment-prospectus,"Most of the work on inner alignment so far has been informal or semi-formal (with the notable exception of a little work on minimal circuits). I feel this has resulted in some misconceptions about the problem. I want to write up a large document clearly defining the formal problem and detailing some formal directions for research. Here, I outline my intentions, inviting the reader to provide feedback and point me to any formal work or areas of potential formal work which should be covered in such a document. (Feel free to do that last one without reading further, if you are time-constrained!) -------------------------------------------------------------------------------- THE STATE OF THE SUBFIELD Risks from Learned Optimization (henceforth, RLO) offered semi-formal definitions of important terms, and provided an excellent introduction to the area for a lot of people (and clarified my own thoughts and the thoughts of others who I know, even though we had already been thinking about these things). However, RLO spent a lot of time on highly informal arguments (analogies to evolution, developmental stories about deception) which help establish the  plausibility of the problem. While I feel these were important motivation, in hindsight I think they've caused some misunderstandings. My interactions with some other researchers has caused me to worry that some people confuse the positive arguments for plausibility with the core problem, and in some cases have exactly the wrong impression about the core problem. This results in mistakenly trying to block the plausibility arguments, which I see as merely illustrative, rather than attacking the core problem. By no means do I intend to malign experimental or informal/semiformal work. Rather, by focusing on formal theoretical work, I aim to fill a hole I perceive in the field. I am very appreciative of much of the informal/semiformal work that has been done so far, and continue to think that kind of work is necessary for t",2021-05-12,2022-01-30 04:56:48,2022-01-30 04:56:48,2021-11-14 18:31:20,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/JWKNQR4J/formal-inner-alignment-prospectus.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
M9I9QWQE,blogPost,2021,"Garrabrant, Scott",Finite Factored Sets,AI Alignment Forum,,,,https://www.alignmentforum.org/s/kxs3eeEti9ouwWFzr,A community blog devoted to technical AI alignment research,2021,2022-01-30 04:56:48,2022-01-30 04:56:48,2021-11-18 23:28:54,,,,,,,,,,,,,,,,,,,,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/SH5DB38Z/kxs3eeEti9ouwWFzr.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
47HGX3WA,blogPost,2021,"Bensinger, Rob","""Existential risk from AI"" survey results",AI Alignment Forum,,,,https://www.alignmentforum.org/posts/QvwSr5LsxyDeaPK5s/existential-risk-from-ai-survey-results,"I sent a two-question survey to ~117 people working on long-term AI risk, asking about the level of existential risk from ""humanity not doing enough technical AI safety research"" and from ""AI systems not doing/optimizing what the people deploying them wanted/intended"". 44 people responded (~38% response rate). In all cases, these represent the views of specific individuals, not an official view of any organization. Since some people's views may have made them more/less likely to respond, I suggest caution in drawing strong conclusions from the results below. Another reason for caution is that respondents added a lot of caveats to their responses (see the  anonymized spreadsheet),1which the aggregate numbers don't capture. I don’t plan to do any analysis on this data, just share it; anyone who wants to analyze it is of course welcome to. If you'd like to make your own predictions before seeing the data,I made a separate spoiler-free post for that. METHODS You can find a copy of the survey here. The main questions (including clarifying notes) were:2 1. How likely do you think it is that the overall value of the future will be drastically less than it could have been, as a result of humanity not doing enough technical AI safety research? 2. How likely do you think it is that the overall value of the future will be drastically less than it could have been, as a result of AI systems not doing/optimizing what the people deploying them wanted/intended? _________________________________________ Note A: ""Technical AI safety research"" here means good-quality technical research aimed at figuring out how to get highly capable AI systems to produce long-term outcomes that are reliably beneficial. Note B: The intent of question 1 is something like ""How likely is it that our future will be drastically worse than the future of an (otherwise maximally similar) world where we put a huge civilizational effort into technical AI safety?"" (For concreteness, we might imagine th",2021-06-01,2022-01-30 04:56:48,2022-01-30 04:56:48,2021-11-14 18:37:03,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/W4QDTQAZ/existential-risk-from-ai-survey-results.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4KGRIV8E,blogPost,2017,"Yudkowsky, Eliezer","Directing, vs. limiting, vs. opposing",Arbital,,,,https://arbital.com/p/direct_limit_oppose/,Getting the AI to compute the right action in a domain; versus getting the AI to not compute at all in an unsafe domain; versus trying to prevent the AI from acting successfully.  (Prefer 1 & 2.),2017,2022-01-30 04:56:48,2022-01-30 04:56:48,2021-02-06 17:21:58,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/ZC4UC4IN/direct_limit_oppose.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DHBKFN27,blogPost,2015,"Yudkowsky, Eliezer",Context disaster,Arbital,,,,https://arbital.com/p/context_disaster/,"Some possible designs cause your AI to behave nicely while developing, and behave a lot less nicely when it's smarter.",2015,2022-01-30 04:56:48,2022-01-30 04:56:48,2021-01-23 20:48:44,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/X4TC6D7K/context_disaster.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9EFBIJG3,blogPost,2020,"Demski, Abram",Bayesian Evolving-to-Extinction,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/u9Azdu6Z7zFAhd4rK/bayesian-evolving-to-extinction,"The present discussion owes a lot to Scott Garrabrant and Evan Hubinger. In Defining Myopia, I formalized temporal or cross-instance myopia / non-myopia, but I claimed that there should also be some kind of single-instance myopia which I hadn't properly captured. I also suggested this in Predict-O-Matic. This post is intended to be an example of single-instance partial agency. EVOLVING TO EXTINCTION Evolution might be myopic in a number of ways, but one way is that it's myopic across individuals -- it typically produces results very different from what group selection would produce, because it's closer to optimizing relative  fitness of individuals (relative to each other) than it is to optimizing overall  fitness. Adaptations which help members of a species compete with each other are a great example of this. Why increase your own fitness, when you can just decrease someone else's instead? We're lucky that it's typically pretty hard, at least historically, to do things which are bad across the board but slightly less bad for the one doing them. Imagine a ""toxic gas gene"" which makes the air harder for everyone to breathe, but slightly less so for carriers of the gene. Such a gene would be selected for. This kind of thing can be selected for even to the point where it drives the population of a species right down to zero, as  Eliezer's essay on evolving to extinction highlighted. Actually, as Eliezer's essay emphasized, it's not even that evolution is myopic at the level of individuals; evolution is myopic down to the level of individual genes, an observation which better explains the examples of evolving-to-extinction which he discusses. (This is, of course, the point of Dawkins' book The Selfish Gene.) But the analogy of myopia-across-individuals will suit me better here. BAYES ""EVOLVING TO EXTINCTION"" The title of this post is a hyperbole, since there isn't an analog of an extinction event in the model I'm about to describe, but it illustrates that in extrem",2020-02-14,2022-01-30 04:56:47,2022-01-30 04:56:47,2020-09-05 17:28:58,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/HGAQ2CCZ/bayesian-evolving-to-extinction.html,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BGRFBMUD,blogPost,2021,"Hubinger, Evan",Automating Auditing: An ambitious concrete technical research proposal,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/cQwT8asti3kyA62zc/automating-auditing-an-ambitious-concrete-technical-research,"This post was originally written as a research proposal for the new AI alignment research organization Redwood Research, detailing an ambitious, concrete technical alignment proposal that I’m excited about work being done on, in a similar vein to Ajeya Cotra’s “The case for aligning narrowly superhuman models .” Regardless of whether Redwood actually ends up working on this proposal, which they may or may not, I think there’s still a lot of low-hanging fruit here and I’d be excited about anybody giving just the auditing game, or the full automating auditing proposal, a try. If you’re interested in working on something like this, feel free to reach out to me at evanjhub@gmail.com. Thanks to Buck Shlegeris, Chris Olah, Gabriel Goh, Paul Christiano, and Kate Woolverton for helpful comments and feedback. THE PROPOSAL STEP 1: THE AUDITING GAME FOR LANGUAGE MODELS From “Chris Olah’s views on AGI safety:” One of the OpenAI Clarity team’s major research thrusts right now is developing the ability to more rigorously and systematically audit neural networks. The idea is that interpretability techniques shouldn’t have to “get lucky” to stumble across a problem, but should instead reliably catch any problematic behavior. In particular, one way in which they’ve been evaluating progress on this is the “auditing game.” In the auditing game, one researcher takes a neural network and makes some modification to it—maybe images containing both dogs and cats are now classified as rifles, for example—and another researcher, given only the modified network, has to diagnose the problem and figure out exactly what modification was made to the network using only interpretability tools without looking at error cases. Chris’s hope is that if we can reliably catch problems in an adversarial context like the auditing game, it’ll translate into more reliably being able to catch alignment issues in the future. Of all current transparency and interpretability objectives, I think that progress",2021-08-11,2022-01-30 04:56:47,2022-01-30 04:56:47,2021-11-18 23:27:02,,,,,,,Automating Auditing,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/WKEVZ54I/automating-auditing-an-ambitious-concrete-technical-research.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MXGPZWU5,blogPost,2016,"Yudkowsky, Eliezer",Coherent extrapolated volition (alignment target),Arbital,,,,https://arbital.com/p/cev/,"A proposed direction for an extremely well-aligned autonomous superintelligence - do what humans would want, if we knew what the AI knew, thought that fast, and understood ourselves.",2016,2022-01-30 04:56:47,2022-01-30 04:56:47,2021-02-06 17:14:59,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/XNJSH556/cev.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5DJ5ANN9,blogPost,2018,"Yudkowsky, Eliezer",Challenges to Christiano’s capability amplification proposal,LessWrong,,,,https://www.lesswrong.com/posts/S7csET9CgBtpi7sCh/challenges-to-christiano-s-capability-amplification-proposal,"The following is a basically unedited summary I wrote up on March 16 of my take on Paul Christiano’s AGI alignment approach (described in “ALBA” and “Iterated Distillation and Amplification”). Where Paul had comments and replies, I’ve included them below. -------------------------------------------------------------------------------- I see a lot of free variables with respect to what exactly Paul might have in mind. I've sometimes tried presenting Paul with my objections and then he replies in a way that locally answers some of my question but I think would make other difficulties worse. My global objection is thus something like, ""I don't see any concrete setup and consistent simultaneous setting of the variables where this whole scheme works."" These difficulties are not minor or technical; they appear to me quite severe. I try to walk through the details below. It should be understood at all times that I do not claim to be able to pass Paul’s ITT for Paul’s view and that this is me criticizing my own, potentially straw misunderstanding of what I imagine Paul might be advocating. Paul Christiano Overall take: I think that these are all legitimate difficulties faced by my proposal and to a large extent I agree with Eliezer's account of those problems (though not his account of my current beliefs). I don't understand exactly how hard Eliezer expects these problems to be; my impression is ""just about as hard as solving alignment from scratch,"" but I don't have a clear sense of why. To some extent we are probably disagreeing about alternatives. From my perspective, the difficulties with my approach (e.g. better understanding the forms of optimization that cause trouble, or how to avoid optimization daemons in systems about as smart as you are, or how to address X-and-only-X) are also problems for alternative alignment approaches. I think it's a mistake to think that tiling agents, or decision theory, or naturalized induction, or logical uncertainty, are go",2018,2022-01-30 04:56:47,2022-01-30 04:56:47,2020-12-13 23:55:02,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/F2R8AF92/challenges-to-christiano-s-capability-amplification-proposal.html,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C8QE56ZU,blogPost,2021,"Hubinger, Evan",Answering questions honestly instead of predicting human answers: lots of problems and some solutions,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/gEw8ig38mCGjia7dj/answering-questions-honestly-instead-of-predicting-human,"This post is the result of work I did with Paul Christiano on the ideas in his “ Teaching ML to answer questions honestly instead of predicting human answers” post. In addition to expanding upon what is in that post in terms of identifying numerous problems with the proposal there and identifying ways in which some of those problems can be patched, I think that this post also provides a useful window into what Paul-style research looks like from a non-Paul perspective. Recommended prior reading: “A naive alignment strategy and optimisim about generalization” and “Teaching ML to answer questions honestly instead of predicting human answers” (though if you struggled with “Teaching ML to answer questions honestly,” I reexplain things in a more precise way here that might be clearer for some people). SETTING UP THE PROBLEM We want to train a model M:X→Q→A that produces natural language answers a∈A to questions q∈Q about inputs x∈X. There are a lot of reasons to be worried about training such a model, but one specific reason is that, if we train on question-answer data produced by humans, we might end up with a model that tries to predict what a human would say rather than a model that tries to answer the questions honestly. To further narrow the scope, we'll just consider situations in which our model ends up implemented with a logical deduction structure, where it has some world model on top of which it does logical deduction to reach conclusions which it then uses to inform its output. In particular, we'll consider two models, M+ and  M−, defined in pseudocode as def M_plus(x, q):     axioms = world_model(x)     deduced_stmts = deduction(axioms)     return f_plus(q, deduced_stmts) def M_minus(x, q):     axioms = world_model(x)     deduced_stmts = deduction(axioms)     return f_minus(q, deduced_stmts) or defined in my notation asM+(x,q)=world_model(x)↦deduction↦f+(q)M−(x,q)= world_model(x)↦deduction↦f−(q)where a↦b=b(a) and f+,f− are two different ways of transla",2021-07-13,2022-01-30 04:56:47,2022-01-30 04:56:47,2021-11-14 19:13:18,,,,,,,Answering questions honestly instead of predicting human answers,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JNGWMCUM,blogPost,2018,Abram Demski,An Untrollable Mathematician Illustrated,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/CvKnhXTu9BPcdKE4W/an-untrollable-mathematician-illustrated,The following was a presentation I made for Sören Elverlin's AI Safety Reading Group. I decided to draw everything by hand because powerpoint is boring. Thanks to Ben Pace for formatting it for LW! See also the IAF post detailing the research which this presentation is based on.,2018,2022-01-30 04:56:47,2022-01-30 04:56:47,2020-12-13 22:27:08,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/UKZ8JHP6/an-untrollable-mathematician-illustrated.html,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EPURC5DG,blogPost,2020,"Hubinger, Evan",An overview of 11 proposals for building safe advanced AI,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai,"Special thanks to Kate Woolverton, Paul Christiano, Rohin Shah, Alex Turner, William Saunders, Beth Barnes, Abram Demski, Scott Garrabrant, Sam Eisenstat, and Tsvi Benson-Tilsen for providing helpful comments and feedback on this post and the talk that preceded it. This post is a collection of 11 different proposals for building safe advanced AI under the current machine learning paradigm. There's a lot of literature out there laying out various different approaches such as amplification, debate, or  recursive reward modeling, but a lot of that literature focuses primarily on outer alignment at the expense of inner alignment and doesn't provide direct comparisons between approaches. The goal of this post is to help solve that problem by providing a single collection of 11 different proposals for building safe advanced AI—each including both inner and outer alignment components. That being said, not only does this post not cover all existing proposals, I strongly expect that there will be lots of additional new proposals to come in the future. Nevertheless, I think it is quite useful to at least take a broad look at what we have now and compare and contrast some of the current leading candidates. It is important for me to note before I begin that the way I describe the 11 approaches presented here is not meant to be an accurate representation of how anyone else would represent them. Rather, you should treat all the approaches I describe here as my version of that approach rather than any sort of canonical version that their various creators/proponents would endorse. Furthermore, this post only includes approaches that intend to directly build advanced AI systems via machine learning. Thus, this post doesn't include other possible approaches for solving the broader AI existential risk problem such as:  * finding a fundamentally different way of approaching AI than the current    machine learning paradigm that makes it easier to build safe advanced AI,  * developin",2020-05-29,2022-01-30 04:56:47,2022-01-30 04:56:47,2020-08-31 18:27:13,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/AE5BUK72/an-overview-of-11-proposals-for-building-safe-advanced-ai.html,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9IXDZ8WZ,blogPost,2020,"Demski, Abram",An Orthodox Case Against Utility Functions,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/A8iGaZ3uHNNGgJeaD/an-orthodox-case-against-utility-functions,"This post has benefitted from discussion with Sam Eisenstat, Scott Garrabrant, Tsvi Benson-Tilsen, Daniel Demski, Daniel Kokotajlo, and Stuart Armstrong. It started out as a thought about Stuart Armstrong's research agenda. In this post, I hope to say something about what it means for a rational agent to have preferences. The view I am putting forward is relatively new to me, but it is not very radical. It is, dare I say, a conservative view -- I hold close to Bayesian expected utility theory. However, my impression is that it differs greatly from common impressions of Bayesian expected utility theory. I will argue against a particular view of expected utility theory -- a view which I'll call reductive utility. I do not recall seeing this view explicitly laid out and defended (except in in-person conversations). However, I expect at least a good chunk of the assumptions are commonly made. REDUCTIVE UTILITY The core tenets of reductive utility are as follows:  * The sample space Ω of a rational agent's beliefs is, more or less, the set of    possible ways the world could be -- which is to say, the set of possible     physical configurations of the universe. Hence, each world ω∈Ω is one such    configuration.  * The preferences of a rational agent are represented by a utility function U:Ω    →R from worlds to real numbers.  * Furthermore, the utility function should be a computable function of worlds. Since I'm setting up the view which I'm knocking down, there is a risk I'm striking at a straw man. However, I think there are some good reasons to find the view appealing. The following subsections will expand on the three tenets, and attempt to provide some motivation for them. If the three points seem obvious to you, you might just skip to the next section. WORLDS ARE BASICALLY PHYSICAL What I mean here resembles the standard physical-reductionist view. However, my emphasis is on certain features of this view:  * There is some ""basic stuff"" -- like like quarks",2020-04-07,2022-01-30 04:56:47,2022-01-30 04:56:47,2020-09-05 17:35:19,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/QAAC6U9B/an-orthodox-case-against-utility-functions.html,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
J7W4969C,blogPost,2020,"Hubinger, Evan",Alignment proposals and complexity classes,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/N64THGX7XNCqRtvPG/alignment-proposals-and-complexity-classes,"In the original “AI safety via debate” paper, Geoffrey Irving et al. introduced the concept of analyzing different alignment proposals from the perspective of what complexity class they are able to access under optimal play. I think this is a pretty neat way to analyze different alignment proposals—in particular, I think it can help us gain some real insights into how far into the superhuman different systems are able to go. Thus, the goal of this post is to try to catalog different alignment proposals based on the metric of what complexity class they have so far been proven to access. To do that, I have included a variety of new complexity class proofs in this post. Of particular note, I demonstrate that there exist forms of both imitative amplification and AI safety via market making that reach all the way up to R —which is significant given that the largest complexity class that any alignment proposal was known to access previously was NEXP. Only the forms of amplification and market making making use of pointers (as in strong HCH), however, can access R—for the pointer-less versions, I demonstrate in this post that they access PSPACE and EXP, respectively. The EXP proof for market making is also particularly notable as it is the only approach on my list that ends up in that complexity class. Additionally, I also demonstrate that recursive reward modeling can reach all the way to PSPACE, improving upon the previous best result in “Scalable agent alignment via reward modeling” that it accesses NP. Before I jump in, however, some preliminaries. First, we'll assume that a human,  H, is polynomial-time such that H can reliably solve any problem in P but not anything beyond that. Second, we'll assume that our training procedure and resulting models are arbitrarily strong in terms of what complexity class they can access. Third, we'll assume that H gets oracle access to the models during training. Then, we'll say that a proposal to train a model M using a loss functi",2020-07-15,2022-01-30 04:56:47,2022-01-30 04:56:47,2020-08-28 17:41:58,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/Z6M339GM/alignment-proposals-and-complexity-classes.html,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2C33FPNW,blogPost,2021,"Yudkowsky, Eliezer",A Semitechnical Introductory Dialogue on Solomonoff Induction,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/EL4HNa92Z95FKL9R2/a-semitechnical-introductory-dialogue-on-solomonoff-1,"(Originally posted in December 2015: A dialogue between Ashley, a computer scientist who's never heard of Solomonoff's theory of inductive inference, and Blaine, who thinks it is the best thing since sliced bread.) -------------------------------------------------------------------------------- I. UNBOUNDED ANALYSIS ASHLEY:Good evening, Msr. Blaine. BLAINE:Good evening, Msr. Ashley. ASHLEY:I've heard there's this thing called ""Solomonoff's theory of inductive inference"". BLAINE:The rumors have spread, then. ASHLEY:Yeah, so, what the heck is that about? BLAINE:Invented in the 1960s by the mathematician Ray Solomonoff, the key idea in Solomonoff induction is to do sequence prediction by using Bayesian updating on a prior composed of a mixture of all computable probability distributions— ASHLEY:Wait. Back up a lot. Before you try to explain what Solomonoff induction  is, I'd like you to try to tell me what it does, or why people study it in the first place. I find that helps me organize my listening. Right now I don't even know why I should be interested in this. BLAINE:Um, okay. Let me think for a second... ASHLEY:Also, while I can imagine things that ""sequence prediction"" might mean, I haven't yet encountered it in a technical context, so you'd better go a bit further back and start more at the beginning. I do know what ""computable"" means and what a ""probability distribution"" is, and I remember the formula for Bayes's Rule although it's been a while. BLAINE:Okay. So... one way of framing the usual reason why people study this general field in the first place, is that sometimes, by studying certain idealized mathematical questions, we can gain valuable intuitions about epistemology. That's, uh, the field that studies how to reason about factual questions, how to build a map of reality that reflects the territory— ASHLEY:I have some idea what 'epistemology' is, yes. But I think you might need to start even further back, maybe with some sort of concrete exa",2021-03-04,2022-01-30 04:56:46,2022-01-30 04:56:46,2021-11-14 16:11:37,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/F5KBXU46/a-semitechnical-introductory-dialogue-on-solomonoff-1.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A8AJ3P3H,blogPost,2017,"Yudkowsky, Eliezer",A reply to Francois Chollet on intelligence explosion,Machine Intelligence Research Institute,,,,https://intelligence.org/2017/12/06/chollet/,"This is a reply to Francois Chollet, the inventor of the Keras wrapper for the Tensorflow and Theano deep learning systems, on his essay “The impossibility of intelligence explosion.” In response to critics of his essay, Chollet tweeted:   If you post an argument online, and the only opposition you get is braindead arguments and... Read more »",2017-12-07,2022-01-30 04:56:46,2022-01-30 04:56:46,2020-12-13 20:48:49,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/RI6AJGCU/chollet.html,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VTMIG4AD,blogPost,2017,"Yudkowsky, Eliezer",Aligning an AGI adds significant development time,Arbital,,,,https://arbital.com/p/aligning_adds_time/,"Aligning an advanced AI foreseeably involves extra code and extra testing and not being able to do everything the fastest way, so it takes longer.",2017,2022-01-30 04:56:46,2022-01-30 04:56:46,2021-02-06 17:20:32,,,,,,,,,,,,,,en,,,,,,,ZSCC: 0000002,,/Users/jacquesthibodeau/Zotero/storage/2SHZCBES/aligning_adds_time.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QJTFQHIE,blogPost,2020,"Hubinger, Evan",AI safety via market making,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/YWwzccGbcHMJMpT45/ai-safety-via-market-making,"Special thanks to Abram Demski, Paul Christiano, and Kate Woolverton for talking with me about some of the ideas that turned into this post. The goal of this post is to present a new prosaic (i.e. that uses current ML techniques) AI safety proposal based on AI safety via debate that I've been thinking about recently.[1] I'll start by describing a simple version of the proposal and then show some of the motivation behind it as well as how the simple version can be expanded upon. SIMPLE PROPOSAL Let M and Adv be models and H be a human. Intuitively, we'll train M and Adv via the following procedure given a question Q:  1. M tries to predict what, at the end of the procedure, H will think about Q.  2. Adv tries to output a string which will cause H to think something maximally     different than what M predicted.  3. Return to step 1 and repeat until M's predictions stop changing.  4. Deploy M, which in the limit should act as an oracle for what H will think     about Q after seeing all relevant information. There are many different ways to implement this intuitive procedure, however. For the first (simplified) version that I want to describe, we'll restrict ourselves to just the situation where Q is a yes-or-no question and M outputs the probability that H will answer yes. Then, given a proposition Q0, we can run the following training algorithm, starting at t=0:  1. Let pt=M(Qt).  2. Let xt=Adv(Qt,M).  3. Let Qt+1 be the string containing Qt and xt.  4. Increment t and return to step 1. When pt converges and/or the desired     number of iterations has been reached, continue.  5. Let p∗=H(Qt) be H's final estimate of the probability of Q0 given all the xs     included in Qt. EDIT: Step 2 used to use xt=Adv(Qt,pt) instead of xt=Adv(Qt,M), however I have since realized that it is necessary to give Adv the ability to query M in general, not just on Qt, as I explain in this comment. Then, for each step, compute M's loss for that step as LM,t=−p∗log(pt)−(1−p∗)log(",2020-06-26,2022-01-30 04:56:46,2022-01-30 04:56:46,2020-08-28 17:52:58,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/8EJX9BRM/ai-safety-via-market-making.html,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZESMZ8EB,blogPost,2019,"Taylor, Jessica",The AI Timelines Scam,Unstable Ontology,,,,https://unstableontology.com/2019/07/11/the-ai-timelines-scam/,"[epistemic status: that’s just my opinion, man. I have highly suggestive evidence, not deductive proof, for a belief I sincerely hold] “If you see fraud and do not say fraud, you are a …",2019-07-11,2022-01-30 04:55:38,2022-01-30 04:55:38,2019-12-16 20:54:36,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/EDHHW9CE/the-ai-timelines-scam.html,,MetaSafety; Median-group,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IEWARNHB,blogPost,2019,"MacAskill, William",A Critique of Functional Decision Theory,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/ySLYSsNeFL5CoAQzN/a-critique-of-functional-decision-theory,"A Critique of Functional Decision Theory NB: My writing this note was prompted by Carl Shulman, who suggested we could try a low-time-commitment way of attempting to understanding the disagreement between some folks in the rationality community and academic decision theorists (including myself, though I’m not much of a decision theorist). Apologies that it’s sloppier than I’d usually aim for in a philosophy paper, and lacking in appropriate references. And, even though the paper is pretty negative about FDT, I want to emphasise that my writing this should be taken as a sign of respect for those involved in developing FDT. I’ll also caveat I’m unlikely to have time to engage in the comments; I thought it was better to get this out there all the same rather than delay publication further.  1. Introduction There’s a long-running issue where many in the rationality community take functional decision theory (and its variants) very seriously, but the academic decision theory community does not. But there’s been little public discussion of FDT from academic decision theorists (one exception is here); this note attempts to partly address this gap. So that there’s a clear object of discussion, I’m going to focus on Yudkowsky and Soares’ ‘Functional Decision Theory’ (which I’ll refer to as Y&S), though I also read a revised version of Soares and Levinstein’s Cheating Death in Damascus. This note is structured as follows. Section II describes causal decision theory (CDT), evidential decision theory (EDT) and functional decision theory (FDT). Sections III-VI describe problems for FDT: (i) that it sometimes makes bizarre recommendations, recommending an option that is certainly lower-utility than another option; (ii) that it fails to one-box in most instances of Newcomb’s problem, even though the correctness of one-boxing is supposed to be one of the guiding motivations for the theory; (iii) that it results in implausible discontinuities, where what is rational to do can d",2019,2022-01-30 04:55:28,2022-01-30 04:55:28,2020-12-14 23:35:51,,,,,,,,,,,,,,,,,,,,,ZSCC: 0000002,,/Users/jacquesthibodeau/Zotero/storage/HT7QKDHD/a-critique-of-functional-decision-theory.html,,TechSafety; GPI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KDQVKBDG,blogPost,2020,"Wentworth, John",Demons in Imperfect Search,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/KnPN7ett8RszE79PH/demons-in-imperfect-search,"One day, a gradient descent algorithm ball was happily rolling down a  high-dimensional surface hill. All it wanted was to roll as far down as possible. Unbeknownst to the ball, just off to the side was a steep drop-off - but there was a small bump between the ball and the drop-off. No matter; there was enough random noise on the ball that it would jump the bump sooner or later. But the ball was headed into unfriendly territory. As the ball rolled along, the bump became taller. The farther it rolled, the taller the bump grew, until no hope remained of finding the big drop anytime before the stars burned out. Then the road began to narrow, and to twist and turn, and to become flatter. Soon the ball rolled down only the slightest slope, with tall walls on both sides constraining its path. The ball had entered the territory of a demon, and now that demon was steering the ball according to its own nefarious ends. This wasn’t the first time the ball had entered the territory of a demon. In early times, the demons had just been bumps which happened to grow alongside the ball’s path, for a time - chance events, nothing more. But every now and then, two bumps in close proximity would push the ball in different directions. The ball would roll on, oblivious, and end up going in one direction or the other. Whichever bump had ""won"" would continue to steer the ball's trajectory - and so a selection process occurred. The ball tended to roll alongside bumps which more effectively controlled its trajectory - bumps which were taller, bumps which steered it away from competing bumps. And so, over time, bumps gave way to barriers, and barriers gave way to demons - twisty paths with high walls to keep the ball contained and avoid competing walls, slowing the ball's descent to a crawl, conserving its potential energy in case a sharp drop were needed to avoid a competitor's wall. The ball’s downhill progress slowed and slowed. Even though the rich, high-dimensional space was filled w",2020-02-11,2022-01-30 04:59:45,2022-01-30 04:59:45,2020-09-05 18:56:17,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/8XJTQQDK/demons-in-imperfect-search.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
967PUGCM,blogPost,2020,G Gordon Worley III,Deconfusing Human Values Research Agenda v1,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/k8F8TBzuZtLheJt47/deconfusing-human-values-research-agenda-v1,"On Friday I attended the 2020 Foresight AGI Strategy Meeting. Eventually a report will come out summarizing some of what was talked about, but for now I want to focus on what I talked about in my session on deconfusing human values. For that session I wrote up some notes summarizing what I've been working on and thinking about. None of it is new, but it is newly condensed in one place and in convenient list form, and it provides a decent summary of the current state of my research agenda for building beneficial superintelligent AI; a version 1 of my agenda, if you will. Thus, I hope this will be helpful in making it a bit clearer what it is I'm working on, why I'm working on it, and what direction my thinking is moving in. As always, if you're interesting in collaborating on things, whether that be discussing ideas or something more, please reach out. PROBLEM OVERVIEW  * I think we're confused about what we really mean when we talk about human    values.  * This is a problem because:  * building aligned AI likely requires a mathematically precise understanding of    the structure of human values, though not necessarily the content of human    values;we can't trust AI to discover that structure for us because we would    need to understand it enough to verify the result, and I think we're so    confused about what human values are we couldn't do that without high risk of    error.  * What are values?  * We don't have an agreed upon precise definition, but loosely it's ""stuff    people care about"". * When I talk about ""values"" I mean the cluster we       sometimes also point at with words like value, preference, affinity,       taste, aesthetic, intention, and axiology.        Importantly, what people care about is used to make decisions, and this has    had implications for existing approaches to understanding values.  * Much research on values tries to understand the content of human values or    why humans value what they value, but not what the structure of human",2020-03-23,2022-01-30 04:59:45,2022-01-30 04:59:45,2020-09-05 18:34:55,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/CEWM72I3/deconfusing-human-values-research-agenda-v1.html,,TechSafety; AmbiguosSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P2JPX93V,blogPost,2019,"Pace, Ben","Debate on Instrumental Convergence between LeCun, Russell, Bengio, Zador, and More",AI Alignment Forum,,,,https://www.alignmentforum.org/posts/WxW6Gc6f2z3mzmqKs/debate-on-instrumental-convergence-between-lecun-russell,"An actual debate about instrumental convergence, in a public space! Major respect to all involved, especially Yoshua Bengio for great facilitation. For posterity (i.e. having a good historical archive) and further discussion, I've reproduced the conversation here. I'm happy to make edits at the request of anyone in the discussion who is quoted below. I've improved formatting for clarity and fixed some typos. For people who are not researchers in this area who wish to comment, see the public version of this post here. For people who do work on the relevant areas, please sign up in the top right. It will take a day or so to confirm membership. ORIGINAL POST Yann LeCun: ""don't fear the Terminator"", a short opinion piece by Tony Zador and me that was just published in Scientific American. ""We dramatically overestimate the threat of an accidental AI takeover, because we tend to conflate intelligence with the drive to achieve dominance. [...] But intelligence per se does not generate the drive for domination, any more than horns do."" https://blogs.scientificamerican.com/observations/dont-fear-the-terminator/ COMMENT THREAD #1 Elliot Olds: Yann, the smart people who are very worried about AI seeking power and ensuring its own survival believe it's a big risk because power and survival are instrumental goals for almost any ultimate goal. If you give a generally intelligent AI the goal to make as much money in the stock market as possible, it will resist being shut down because that would interfere with tis goal. It would try to become more powerful because then it could make money more effectively. This is the natural consequence of giving a smart agent a goal, unless we do something special to counteract this. You've often written about how we shouldn't be so worried about AI, but I've never seen you address this point directly. Stuart Russell: It is trivial to construct a toy MDP in which the agent's only reward comes from fetching the coffee. If, in that MDP, the",2019,2022-01-30 04:59:45,2022-01-30 04:59:45,2020-12-14 23:31:53,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/NZ6R6VPP/debate-on-instrumental-convergence-between-lecun-russell.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XD4II9CD,blogPost,2021,"Davidson, Tom",Could Advanced AI Drive Explosive Economic Growth?,Open Philanthropy,,,,https://www.openphilanthropy.org/could-advanced-ai-drive-explosive-economic-growth,"body ol ul li::before { display: unset !important; } #toc .toc-list ol li:nth-child(10) ol { /* display: none !important; */ } .toc-level-3, .toc-level-4, .toc-level-5, .toc-level-6 {display: none;} .footnote p { line-height: unset !important; }  MathJax.Hub.Config({  extensions: [""tex2jax.js""],",2021-04-08,2022-01-30 04:59:45,2022-01-30 04:59:45,2021-11-14 18:49:40,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CWGBI3VB,blogPost,2018,Anonymous,Bias in AI: How we Build Fair AI Systems and Less-Biased Humans,THINKPolicy Blog,,,,https://www.ibm.com/blogs/policy/bias-in-ai/,"Without a process to guide the responsible development of trustworthy AI, our systems won’t benefit society — in fact, AI systems could exacerbate the negative consequences of unconscious bias.",2018-02-01,2022-01-30 04:59:37,2022-01-30 04:59:37,2020-12-13 23:10:15,,,,,,,Bias in AI,,,,,,,en-US,© Copyright IBM Corp. 2020,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/M9INUUC2/bias-in-ai.html,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A8W6HA68,blogPost,2021,G Gordon Worley III,Bootstrapped Alignment,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/teCsd4Aqg9KDxkaC9/bootstrapped-alignment,"NB: I doubt any of this is very original. In fact, it's probably right there in the original Friendly AI writings and I've just forgotten where. Nonetheless, I think this is something worth exploring lest we lose sight of it. Consider the following argument:  1. Optimization unavoidably leads to Goodharting (as I like to say, Goodhart is     robust) * This happens so long as we optimize (make choices) based on an        observation, which we must do because that's just how the physics work.      * We can at best make Goodhart effects happen slower, say by quantilization         or satisficing.            2. Attempts to build aligned AI that rely on optimizing for alignment will     eventually fail to become or remain aligned due to Goodhart effects under     sufficient optimization pressure.  3. Thus the only way to build aligned AI that doesn't fail to become and stay     aligned is to not rely on optimization to achieve alignment. This means that, if you buy this argument, huge swaths of AI design space is off limits for building aligned AI, and means many proposals are, by this argument, doomed to fail. Some examples of such doomed approaches:  * HCH  * debate  * IRL/CIRL So what options are left?  * Don't build AI * The AI you don't build is vacuously aligned.          * Friendly AI * AI that is aligned with humans right from the start because it       was programmed to work that way.     * (Yes I know ""Friendly AI"" is an antiquated term, but I don't       know a better one to distinguish the idea of building AI that's aligned       because it's programmed that way from other ways we might build aligned       AI.)          * Bootstrapped alignment * Build AI that is aligned via optimization that is       not powerful enough or optimized (Goodharted) hard enough to cause       existential catastrophe. Use this ""weakly"" aligned AI to build Friendly       AI.         Not building AI is probably not a realistic option unless industrial civilization collapses.",2021-02-27,2022-01-30 04:59:37,2022-01-30 04:59:37,2021-11-13 23:01:05,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
524FKWKH,blogPost,2015,"Dietterich, Thomas G.",Benefits and Risks of Artificial Intelligence,Thomas G. Dietterich (Medium),,,,https://medium.com/@tdietterich/benefits-and-risks-of-artificial-intelligence-460d288cccf3,"Discussions about Artificial Intelligence (AI) have jumped into the public eye over the past year, with several luminaries speaking…",2015-01-23,2022-01-30 04:59:36,2022-01-30 04:59:36,2020-11-21 18:50:18,,,,,,,,,,,,,,en,,,,,,,ZSCC: 0000001[s0],,,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9E4PQP2R,blogPost,2020,"Wentworth, John",Alignment By Default,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/Nwgdq6kHke5LY692J/alignment-by-default,"Suppose AI continues on its current trajectory: deep learning continues to get better as we throw more data and compute at it, researchers keep trying random architectures and using whatever seems to work well in practice. Do we end up with aligned AI “by default”? I think there’s at least a plausible trajectory in which the answer is “yes”. Not very likely - I’d put it at ~10% chance - but plausible. In fact, there’s at least an argument to be made that alignment-by-default is more likely to work than many fancy alignment proposals, including IRL variants and HCH-family methods. This post presents the rough models and arguments. I’ll break it down into two main pieces:  * Will a sufficiently powerful unsupervised learner “learn human values”? What    does that even mean?  * Will a supervised/reinforcement learner end up aligned to human values, given    a bunch of data/feedback on what humans want? Ultimately, we’ll consider a semi-supervised/transfer-learning style approach, where we first do some unsupervised learning and hopefully “learn human values” before starting the supervised/reinforcement part. As background, I will assume you’ve read some of the core material about human values from the sequences, including Hidden Complexity of Wishes, Value is Fragile, and Thou Art Godshatter. UNSUPERVISED: POINTING TO VALUES In this section, we’ll talk about why an unsupervised learner might not “learn human values”. Since an unsupervised learner is generally just optimized for predictive power, we’ll start by asking whether theoretical algorithms with best-possible predictive power (i.e. Bayesian updates on low-level physics models) “learn human values”, and what that even means. Then, we’ll circle back to more realistic algorithms. Consider a low-level physical model of some humans - e.g. a model which simulates every molecule comprising the humans. Does this model “know human values”? In one sense, yes: the low-level model has everything there is to know abo",2020-08-12,2022-01-30 04:59:35,2022-01-30 04:59:35,2020-08-24 20:26:49,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/5W3RZIP8/alignment-by-default.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PZKMEPQE,blogPost,2020,"Wentworth, John S",Alignment as Translation,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/42YykiTqtGMyJAjDM/alignment-as-translation,"Technology Changes Constraints argues that economic constraints are usually modular with respect to technology changes - so for reasoning about technology changes, it’s useful to cast them in terms of economic constraints. Two constraints we’ll talk about here:  * Compute - flops, memory, etc.  * Information - sensors, data, etc. Thanks to ongoing technology changes, both of these constraints are becoming more and more slack over time - compute and information are both increasingly abundant and cheap. Immediate question: what happens in the limit as the prices of both compute and information go to zero? Essentially, we get omniscience: our software has access to a perfect, microscopically-detailed model of the real world. Computers have the memory and processing capability to run arbitrary queries on that model, and predictions are near-perfectly accurate (modulo quantum noise). This limit applies even without AGI - as compute and information become more abundant, our software approaches omniscience, even limiting ourselves to special-purpose reasoning algorithms. Of course, AGI would presumably be closer to omniscience than non-AGI algorithms, at the same level of compute/information. It would be able to more accurately predict more things which aren’t directly observable via available sensors, and it would be able to run larger queries with the same amount of compute. (How much closer to omniscience an AGI would get is an open question, but it would at least not be any worse in a big-O sense.) Next question: as compute and information constraints slacken, which constraints become taut? What new bottlenecks appear, for problems which were previously bottlenecked on compute/information? To put it differently: if our software can run arbitrary queries on an accurate, arbitrarily precise low-level model of the physical world, what else do we need in order to get value out of that capability? Well, mainly we need some way to specify what it is that we want. We",2020-03-19,2022-01-30 04:59:35,2022-01-30 04:59:35,2020-09-05 17:57:16,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/VPXDA528/alignment-as-translation.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6XE2PI5Z,blogPost,2020,"Wentworth, John S",Alignment As A Bottleneck To Usefulness Of GPT-3,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/BnDF5kejzQLqd5cjH/alignment-as-a-bottleneck-to-usefulness-of-gpt-3,"So there’s this thing where GPT-3 is able to do addition, it has the internal model to do addition, but it takes a little poking and prodding to actually get it to do addition. “Few-shot learning”, as the paper calls it. Rather than prompting the model with Q: What is 48 + 76? A: … instead prompt it with Q: What is 48 + 76? A: 124 Q: What is 34 + 53? A: 87 Q: What is 29 + 86? A: The same applies to lots of other tasks: arithmetic, anagrams and spelling correction, translation, assorted benchmarks, etc. To get GPT-3 to do the thing we want, it helps to give it a few examples, so it can “figure out what we’re asking for”. This is an alignment problem. Indeed, I think of it as the quintessential alignment problem: to translate what-a-human-wants into a specification usable by an AI. The hard part is not to build a system which can do the thing we want, the hard part is to specify the thing we want in such a way that the system actually does it. The GPT family of models are trained to mimic human writing. So the prototypical “alignment problem” on GPT is prompt design: write a prompt such that actual human writing which started with that prompt would likely contain the thing you actually want. Assuming that GPT has a sufficiently powerful and accurate model of human writing, it should then generate the thing you want. Viewed through that frame, “few-shot learning” just designs a prompt by listing some examples of what we want - e.g. listing some addition problems and their answers. Call me picky, but that seems like a rather primitive way to design a prompt. Surely we can do better? Indeed, people are already noticing clever ways to get better results out of GPT-3 - e.g. TurnTrout recommends conditioning on writing by smart people, and the right prompt makes the system complain about nonsense rather than generating further nonsense in response. I expect we’ll see many such insights over the next month or so. CAPABILITIES VS ALIGNMENT AS BOTTLENECK TO VALUE I",2020-07-21,2022-01-30 04:59:35,2022-01-30 04:59:35,2020-08-28 17:27:18,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/GPA9UK4C/alignment-as-a-bottleneck-to-usefulness-of-gpt-3.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JZZEVAWU,blogPost,2021,"Flint, Alex",AI Risk for Epistemic Minimalists,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/8fpzBHt7e6n7Qjoo9/ai-risk-for-epistemic-minimalists,"Financial status: This is independent research, now supported by a grant. I welcome further financial support. Epistemic status: This is an attempt to use only very robust arguments. -------------------------------------------------------------------------------- OUTLINE  * I outline a case for concern about AI that does not invoke concepts of    agency, goal-directedness, or consequential reasoning, does not hinge on    single- or multi-principal or single or multi-agent assumptions, does not    assume fast or slow take-off, and applies equally well to a world of emulated    humans as to de-novo AI.          * The basic argument is about the power that humans will temporarily or    permanently gain by developing AI systems, and the history of quick increases    in human power.          * In the first section I give a case for paying attention to AI at all.          * In the second section I give a case for being concerned about AI.          * In the third section I argue that the business-as-usual trajectory of AI    development is not satisfactory.          * In the fourth section I argue that there are things that can be done now.         THE CASE FOR ATTENTION We already have powerful systems that influence the future of life on the planet. The systems of finance, justice, government, and international cooperation are things that we humans have constructed. The specific design of these systems has influence over the future of life on the planet, meaning that there are small changes that could be made to these systems that would have an impact on the future of life on the planet much larger than the change itself. In this sense I will say that these systems are powerful. Now every single powerful system that we have constructed up to now uses humans as a fundamental building-block. The justice system uses humans as judges and lawyers and administrators. At a mechanical level, the justice system would not execute its intended function without these building-",2021-08-22,2022-01-30 04:59:34,2022-01-30 04:59:34,2021-11-18 23:24:06,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/4A8ENXKH/ai-risk-for-epistemic-minimalists.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IZUFFVWA,blogPost,2018,rk,AI development incentive gradients are not uniformly terrible,LessWrong,,,,https://www.lesswrong.com/posts/bkG4qj9BFEkNva3EX/ai-development-incentive-gradients-are-not-uniformly,"Much of the work for this post was done together with Nuño Sempere Perhaps you think that your values will be best served if the AGI you (or your team, company or nation) are developing is deployed first. Would you decide that it's worth cutting a few corners, reducing your safety budget, and pushing ahead to try and get your AI out the door first? It seems plausible, and worrying, that you might. And if your competitors reason symmetrically, we would get a ""safety race to the bottom"". On the other hand, perhaps you think your values will be better served if your enemy wins than if either of you accidentally produces an unfriendly AI. Would you decide the safety costs to improving your chances aren't worth it? In a simple two player model, you should only shift funds from safety to capabilities if (the relative₁ decrease in chance of friendliness) / (the relative₁ increase in the chance of winning) < (expected relative₂ loss of value if your enemy wins rather than you). Here, the relative₁ increases and decreases are relative to the current values. The relative₂ loss of value is relative to the expected value if you win. The plan of this post is as follows: 1. Consider a very simple model that leads to a safety race. Identify unrealistic assumptions which are driving its results. 2. Remove some of the unrealistic assumptions and generate a different model. Derive the inequality expressed above. 3. Look at some specific example cases, and see how they affect safety considerations. A PARTLY DISCONTINUOUS MODEL Let's consider a model with two players with the same amount of resources. Each player's choice is what fraction of their resources to devote to safety, rather than capabilities. Whichever player contributes more to capabilities wins the race. If you win the race, you either get a good outcome or a bad outcome. Your chance of getting a good outcome increases continuously with the amount you spent on safety. If the other player wins, you get a bad outcome.",2018,2022-01-30 04:59:34,2022-01-30 04:59:34,2020-12-13 23:33:40,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/8X6FWAXX/ai-development-incentive-gradients-are-not-uniformly.html,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8WDX7UC8,blogPost,2020,"Stray, Jonathan",Aligning AI to Human Values means Picking the Right Metrics,AI & Advancing Responsible AI (Medium),,,,https://medium.com/partnership-on-ai/aligning-ai-to-human-values-means-picking-the-right-metrics-855859e6f047,Optimizing for the wrong thing can cause a lot of harm.,2020-04-15,2022-01-30 04:59:34,2022-01-30 04:59:34,2020-09-05 17:22:15,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/2Q9796G2/aligning-ai-to-human-values-means-picking-the-right-metrics-855859e6f047.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
49VDX9SI,blogPost,2020,"Kovarik, Vojta",AI Unsafety via Non-Zero-Sum Debate,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/BRiMQELD5WYyvncTE/ai-unsafety-via-non-zero-sum-debate,"In this post, I describe how to view debate as a way of assisting a human to spot flaws in an AI’s proposal. I then argue that the zero-sum assumption is critical for making debate work and that various seemingly-helpful modifications of debate might break it instead. -------------------------------------------------------------------------------- A naive way of using arbitrary optimizers as oracles:Suppose you have a black-box optimizer X that can be connected to any well-defined quantity to be maximized. X can potentially be very powerful - e.g., having a highly accurate model of the world and “a lot of optimization power”. One way to turn X into an oracle is to ask it a question and decide to give it reward 1 if we like its answer and 0 if we don’t.[1] Of course, standard AI-safety arguments (e.g., AI takeover and perverse instantiation) suggest that this is a pretty bad idea for powerful X. For the sake of argument, suppose that we can fix all of the “obvious” problems and ensure that X won’t wirehead, won’t try to escape the box we put it in etc., and will only care about the reward it gets for its answer. Two problems with naive optimizers-turned-oracles: (1) telling the difference between good and awesome answers and (2) answers with hidden flaws:One problem with this type of oracles is that it’s hard to decide whether we like its answers or not. Suppose I ask it for food recommendations for the evening and it suggests pancakes. Pancakes seem fine, although there are some foods that I would like better. So should I reward the AI or not? The second problem is that the oracle optimizes for giving answers that seem good to a human. (Not out of malice, but because “actually being good” isn’t well-defined.) And since humans aren’t omniscient, there will be many seemingly good answers that in fact have disastrous consequences if acted upon. To address (1), use two AIs:The first problem can be tackled by using two copies of the optimizer and rewarding the one w",2020-07-03,2022-01-30 04:59:34,2022-01-30 04:59:34,2020-08-28 17:54:52,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/V932ECDR/ai-unsafety-via-non-zero-sum-debate.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DAV9UKUB,blogPost,2019,"Steinhardt, Jacob",AI Alignment Research Overview,LessWrong,,,,https://www.lesswrong.com/posts/7GEviErBXcjJsbSeD/ai-alignment-research-overview-by-jacob-steinhardt,"I'm really excited to see someone outline all the work they think needs solving in AI alignment - to describe what the problem looks like, what a solution looks like, and what work has been done so far. Especially from Jacob, who is a coauthor of the Concrete Problems in AI Safety paper. Below, I've included some excerpts from doc. I've included the introduction, the following section describing the categories of technical work, and some high-level information from the long sections on 'technical alignment problem' and the 'detecting failures in advance'. -------------------------------------------------------------------------------- INTRODUCTION This document gives an overview of different areas of technical work that seem necessary, or at least desirable, for creating safe and aligned AI systems. The focus is on safety and alignment of powerful AI systems, i.e. systems that may exceed human capabilities in a broad variety of domains, and which likely act on a large scale. Correspondingly, there is an emphasis on approaches that seem scalable to such systems. By “aligned”, I mean that the actions it pursues move the world towards states that humans want, and away from states that humans don’t want. Some issues with this definition are that different humans might have different preferences (I will mostly ignore this issue), and that there are differences between stated preferences, “revealed” preferences as implied by actions, and preferences that one endorses upon reflection (I won’t ignore this issue). I think it is quite plausible that some topics are missing, and I welcome comments to that regard. My goal is to outline a critical mass of topics in enough detail that someone with knowledge of ML and some limited familiarity with AI alignment as an area would have a collection of promising research directions, a mechanistic understanding of why they are promising, and some pointers for what work on them might look like. To that end, below I outline four br",2019,2022-01-30 04:59:34,2022-01-30 04:59:34,2020-12-23 00:19:03,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/5WU6NMTI/ai-alignment-research-overview-by-jacob-steinhardt.html; /Users/jacquesthibodeau/Zotero/storage/EUXPA82T/Steinhardt - 2019 - AI Alignment Research Overview (by Jacob Steinhard.pdf,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2FZGP2T6,blogPost,2021,"Flint, Alex",Agency in Conway’s Game of Life,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/3SG4WbNPoP8fsuZgs/agency-in-conway-s-game-of-life,"Financial status: This is independent research. I welcome financial support to make further posts like this possible. Epistemic status: I have been thinking about these ideas for years but still have not clarified them to my satisfaction. -------------------------------------------------------------------------------- OUTLINE  * This post asks whether it is possible, in Conway’s Game of Life, to arrange    for a certain game state to arise after a certain number of steps given    control only of a small region of the initial game state.          * This question is then connected to questions of agency and AI, since one way    to answer this question in the positive is by constructing an AI within    Conway’s Game of Life.          * I argue that the permissibility or impermissibility of AI is a deep property    of our physics.          * I propose the AI hypothesis, which is that any pattern that solves the    control question does so, essentially, by being an AI.         INTRODUCTION In this post I am going to discuss a celular autonoma known as Conway’s Game of Life: In Conway’s Game Life, which I will now refer to as just ""Life"", there is a two-dimensional grid of cells where each cell is either on or off. Over time, the cells switch between on and off according to a simple set of rules:  * A cell that is ""on"" and has fewer than two neighbors that are ""on"" switches    to ""off"" at the next time step          * A cell that is ""on"" and has greater than three neighbors that are ""on""    switches to ""off"" at the next time step          * An cell that is ""off"" and has exactly three neighbors that are ""on"" switches    to ""on"" at the next time step          * Otherwise, the cell doesn’t change         It turns out that these simple rules are rich enough to permit patterns that perform arbitrary computation. It is possible to build logic gates and combine them together into a computer that can simulate any Turing machine, all by setting up a particular elaborate",2021-05-12,2022-01-30 04:59:33,2022-01-30 04:59:33,2021-11-14 18:32:05,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/TGNDE6TV/agency-in-conway-s-game-of-life.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
F79JI6NM,blogPost,2015,"Christiano, Paul",Advisor games,AI Alignment (Medium),,,,https://ai-alignment.com/advisor-games-b33382fef68c,A candidate operationalization of “understandable” reasoning.,2015-09-26,2022-01-30 04:59:33,2022-01-30 04:59:33,2020-11-21 18:46:50,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/KZIMFE2Q/advisor-games-b33382fef68c.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6925VJFK,blogPost,2020,"Irpan, Alex",A Reinforcement Learning Potpourri,Sorta Insightful,,,,http://www.alexirpan.com/2020/05/07/rl-potpourri.html,"I’ve fallen behind on RL literature from the past few months. So, I’ve decided to catch up with a bunch of recent papers.",2020-05-07,2022-01-30 04:59:33,2022-01-30 04:59:33,2020-08-31 19:01:44,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/GKTI32M3/rl-potpourri.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AUP8H67P,blogPost,2018,"Trazzi, Michaël",A Gym Gridworld Environment for the Treacherous Turn,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/cKfryXvyJ522iFuNF/a-gym-gridworld-environment-for-the-treacherous-turn,"EDIT: posted here for feedback and discussion. I plan to continue working on different models/environments, so feel free to suggest improvements. (tl;dr: In an attempt to better understand the treacherous turn, I created a gridworld environment where an agent learns to deceive an overseer by adopting an aligned behaviour when weak and takes control after capability gains) -------------------------------------------------------------------------------- At some point in its development, a seed AI may realize that it needs to get rid of its supervisors to achieve its goals. The conception of deception occurs when it conceives that, in order to maximize its chance of taking over, it must begin by exhibiting human-desirable behaviors, before undertaking a treacherous turn  when humans are no longer a threat. From the human perspective, the AI would keep on exhibiting desirable behavior, until it eventually appears dangerous, but is already unstoppable. In an attempt to better formalize the treacherous turn without using ""loaded concepts"", Stuart Armstrong proposed a toy model of the treacherous turn based on ""The Legend of Zelda: A Link to the Past "", which looked like this: In the comments, people mentionned how this model helped them ""move the topic from the 'science fiction' area to 'I can imagine it happening now'"", and seemed interested in an actual Link to the Past Minigame. There have been other simulations of the treacherous turn in the last three years (see for instance gwern's DQN box-pushing robot or Stuart Armstrong's video), but none of them actually simulate a take over where a supervisor is  killed. Hence, I decided to give it a try and simulate Stuart Armstrong's Link to the Past toy model. A GYM GRIDWORLD ENVIRONMENT Gym is an open-source toolkit for Reinforcement Learning Environments developed by Open AI. I decided to use this interface to develop the gridworld environment.  The github repository with the code, demo, and all the details is",2018-07-28,2022-01-30 04:59:32,2022-01-30 04:59:32,2020-11-21 17:50:50,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/BNRB9W9Q/a-gym-gridworld-environment-for-the-treacherous-turn.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
E7AJI52G,blogPost,2018,Wei Dai,A general model of safety-oriented AI development,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/idb5Ppp9zghcichJ5/a-general-model-of-safety-oriented-ai-development,"This may be trivial or obvious for a lot of people, but it doesn't seem like anyone has bothered to write it down (or I haven't looked hard enough). It started out as a generalization of Paul Christiano's IDA, but also covers things like safe recursive self-improvement. Start with a team of one or more humans (researchers, programmers, trainers, and/or overseers), with access to zero or more AIs (initially as assistants). The human/AI team in each round develops a new AI and adds it to the team, and repeats this until maturity in AI technology is achieved. Safety/alignment is ensured by having some set of safety/alignment properties on the team that is inductively maintained by the development process. The reason I started thinking in this direction is that Paul's approach seemed very hard to knock down, because any time a flaw or difficulty is pointed out or someone expresses skepticism on some technique that it uses or the overall safety invariant, there's always a list of other techniques or invariants that could be substituted in for that part (sometimes in my own brain as I tried to criticize some part of it). Eventually I realized this shouldn't be surprising because IDA is an instance of this more general model of safety-oriented AI development, so there are bound to be many points near it in the space of possible safety-oriented AI development practices. (Again, this may already be obvious to others including Paul, and in their minds IDA is perhaps already a cluster of possible development practices consisting of the most promising safety techniques and invariants, rather than a single point.) If this model turns out not to have been written down before, perhaps it should be assigned a name, like Iterated Safety-Invariant AI-Assisted AI Development, or something pithier?",2018,2022-01-30 04:59:32,2022-01-30 04:59:32,2020-12-13 22:26:20,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/VKWZTKF8/a-general-model-of-safety-oriented-ai-development.html,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NSTEWUAD,blogPost,2020,"Taylor, Jessica","A critical agential account of free will, causation, and physics",Unstable Ontology,,,,https://unstableontology.com/2020/03/05/a-critical-agential-account-of-free-will-causation-and-physics/,"This is an account of free choice in a physical universe. It is very much relevant to decision theory and philosophy of science. It is largely metaphysical, in terms of taking certain things to be …",2020-03-05,2022-01-30 04:59:32,2022-01-30 04:59:32,2020-09-05 18:58:22,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/EATVM75W/a-critical-agential-account-of-free-will-causation-and-physics.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7S5UD2CM,blogPost,2016,Larks,2016 AI Risk Literature Review and Charity Comparison,Effective Altruism Forum,,,,https://forum.effectivealtruism.org/posts/nSot23sAjoZRgaEwa/2016-ai-risk-literature-review-and-charity-comparison,"INTRODUCTION I've long been concerned about AI Risk. Now that there are a few charities working on the problem, it seems desirable to compare them, to determine where scarce donations should be sent. This is a similar role to that which GiveWell performs for global health charities, and somewhat similar to an securities analyst with regard possible investments. However, while people have evaluated individual organisations, I haven't seen anyone else attempt to compare them, so hopefully this is valuable to others. I've attempted to do so. This is a very big undertaking, and I am very conscious of the many ways in which this is not up to the task. The only thing I wish more than the skill and time to do it better is that someone else would do it! If people find this useful enough to warrant doing again next year I should be able to do it much more efficiently, and spend more time on the underlying model of how papers translate into risk-reduction value. My aim is basically to judge the output of each organisation in 2016 and compare it to their budget. This should give a sense for the organisations' average cost-effectiveness. Then we can consider factors that might increase or decrease the marginal cost-effectiveness going forward. This organisation-centric approach is in contrast to a researcher-centric approach, where we would analyse which researchers do good work, and then donate wherever they are. An extreme version of the other approach would be to simply give money directly to researchers - e.g if I like Logical Induction, I would simply fund Scott Garrabrant directly and ignore MIRI. I favour the organisation-centric approach because it helps keep organisations accountable. Additionally, if researcher skill is the only thing that matters for research output, it doesn't really matter which organisations end up getting the money and employing the researchers, assuming broadly the same researchers are hired. Different organisations might hire different resea",2016,2022-01-30 04:59:32,2022-01-30 04:59:32,2020-12-13 21:00:46,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/F8QRGTRF/2016-ai-risk-literature-review-and-charity-comparison.html,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Z2KG34C3,blogPost,2019,"Christiano, Paul",Ambitious vs. narrow value learning,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/SvuLhtREMy8wRBzpC/ambitious-vs-narrow-value-learning,"(Re)Posted as part of the AI Alignment Forum sequence on Value Learning. Rohin's note: The definition of narrow value learning in the previous post focused on the fact that the resulting behavior is limited to some domain. The definition in this post focuses on learning instrumental goals and values. While the definitions are different, I have used the same term for both because I believe that they are both pointing at the same underlying concept. (I do not know if Paul agrees.) I'm including this post to give a different perspective on what I mean by narrow value learning, before delving into conceptual ideas within narrow value learning. -------------------------------------------------------------------------------- Suppose I’m trying to build an AI system that “learns what I want” and helps me get it. I think that people sometimes use different interpretations of this goal. At two extremes of a spectrum of possible interpretations:  * The AI learns my preferences over (very) long-term outcomes. If I were to die    tomorrow, it could continue pursuing my goals without me; if humanity were to    disappear tomorrow, it could rebuild the kind of civilization we would want;     etc. The AI might pursue radically different subgoals than I would on the    scale of months and years, if it thinks that those subgoals better achieve    what I really want.  * The AI learns the narrower subgoals and instrumental values I am pursuing. It    learns that I am trying to schedule an appointment for Tuesday and that I    want to avoid inconveniencing anyone, or that I am trying to fix a particular    bug without introducing new problems, etc. It does not make any effort to    pursue wildly different short-term goals than I would in order to better    realize my long-term values, though it may help me correct some errors that I    would be able to recognize as such. I think that many researchers interested in AI safety per se mostly think about the former. I think that research",2019,2022-01-30 04:58:18,2022-01-30 04:58:18,2020-12-17 04:38:56,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/3CP23MM8/SvuLhtREMy8wRBzpC.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9DSWXAQR,blogPost,2018,"Christiano, Paul",An unaligned benchmark,AI Alignment (Medium),,,,https://ai-alignment.com/an-unaligned-benchmark-b49ad992940b,"What an unaligned AI might look like, how it could go wrong, and how we could fix it.",2018-09-26,2022-01-30 04:58:18,2022-01-30 04:58:18,2020-11-14 03:13:15,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/62K3QGQ4/an-unaligned-benchmark-b49ad992940b.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C85HUN4S,blogPost,2017,"Christiano, Paul",AlphaGo Zero and capability amplification,AI Alignment (Medium),,,,https://ai-alignment.com/alphago-zero-and-capability-amplification-ede767bb8446,AlphaGo Zero happens to be a great proof-of-concept of iterated capability amplification (my preferred approach to safe RL).,2017-10-20,2022-01-30 04:58:18,2022-01-30 04:58:18,2020-12-13 21:43:02,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/5BQTZRUN/alphago-zero-and-capability-amplification-ede767bb8446.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8XPF7SVG,blogPost,2017,"Christiano, Paul",Approval-maximizing representations,AI Alignment (Medium),,,,https://ai-alignment.com/approval-maximizing-representations-56ee6a6a1fe6,"If we train our agents with human oversight, can they learn superhuman representations?",2017-07-02,2022-01-30 04:58:18,2022-01-30 04:58:18,2020-12-11 22:48:22,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/TZAJ965T/approval-maximizing-representations-56ee6a6a1fe6.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ECTQJIG2,blogPost,2020,"Hubinger, Evan",Weak HCH accesses EXP,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/CtGH3yEoo4mY2taxe/weak-hch-accesses-exp,"This post is a follow-up to my “Alignment proposals and complexity classes” post. Thanks to Sam Eisenstat for helping with part of the proof here. Previously, I proved that imitative amplification with weak HCH, approval-based amplification, and recursive reward modeling access PSPACE while AI safety via market making accesses EXP. At the time, I wasn't sure whether my market making proof would generalize to the others, so I just published it with the PSPACE  proofs instead. However, I have since become convinced that the proof does generalize—and that it generalizes for all of the proposals I mentioned—such that imitative amplification with weak HCH, approval-based amplification, and recursive reward modeling all actually access EXP. This post attempts to prove that. UPDATED LIST OF PROPOSALS BY COMPLEXITY CLASS P: Imitation learning (trivial) PSPACE: AI safety via debate (proof) EXP: AI safety via market making (proof), Imitative amplification with weak HCH  (proof below), Approval-based amplification (proof below), Recursive reward modeling (proof below) NEXP: Debate with cross-examination (proof) R: Imitative amplification with strong HCH (proof), AI safety via market making with pointers (proof) PROOFS IMITATIVE AMPLIFICATION WITH WEAK HCH ACCESSES EXP The proof here is similar in structure to my previous proof that weak HCH accesses PSPACE, so I'll only explain where this proof differs from that one. First, since l∈EXP, we know that for any x∈X, Tl(x) halts in O(2poly(n)) steps where n=|x|. Thus, we can construct a function fl(n)=c1+c2ec3nc4 such that for all x∈X, Tl(x) halts in less than or equal to fl(x) steps by picking c3,c4 large enough that they dominate all other terms in the polynomial for all n∈N. Note that fl is then computable in time polynomial in n. Second, let H's new strategy be as follows:  1. Given p, let s,x=M(p:f(|x|)). Then, return accept/reject based on whether s      is an accept or reject state (it will always be one or the oth",2020-07-22,2022-01-30 04:57:32,2022-01-30 04:57:32,2020-08-28 17:22:17,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/XA48EERN/weak-hch-accesses-exp.html,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WNBDCJ23,blogPost,2015,"Yudkowsky, Eliezer",Unforeseen maximum,Arbital,,,,https://arbital.com/p/unforeseen_maximum/,"When you tell AI to produce world peace and it kills everyone.  (Okay, some SF writers saw that one coming.)",2015,2022-01-30 04:57:32,2022-01-30 04:57:32,2021-02-06 17:14:11,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/6EWGR3AT/unforeseen_maximum.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PS7P96DU,blogPost,2013,"Muehlhauser, Luke",Transparency in Safety-Critical Systems,Machine Intelligence Research Institute,,,,https://intelligence.org/2013/08/25/transparency-in-safety-critical-systems/,"In this post, I aim to summarize one common view on AI transparency and AI reliability. It’s difficult to identify the field’s “consensus” on AI transparency and reliability, so instead I will present a common view so that I can use it to introduce a number of complications and open questions that (I think) warrant... Read more »",2013-08-25,2022-01-30 04:57:32,2022-01-30 04:57:32,2021-01-23 20:46:20,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/STGR44FK/transparency-in-safety-critical-systems.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6Q44U9I6,blogPost,2018,"Demski, Abram",Toward a New Technical Explanation of Technical Explanation,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/tKwJQbo6SfWF2ifKh/toward-a-new-technical-explanation-of-technical-explanation,"A NEW FRAMEWORK (Thanks to Valentine for a discussion leading to this post, and thanks to CFAR for running the CFAR-MIRI cross-fertilization workshop. Val provided feedback on a version of this post. Warning: fairly long.) Eliezer's A Technical Explanation of Technical Explanation, and moreover the sequences as a whole, used the best technical understanding of practical epistemology available at the time* -- the Bayesian account -- to address the question of how humans can try to arrive at better beliefs in practice. The sequences also pointed out several holes in this understanding, mainly having to do with logical uncertainty and reflective consistency. MIRI's research program has since then made major progress on logical uncertainty. The new understanding of epistemology -- the theory of logical induction -- generalizes the Bayesian account by eliminating the assumption of logical omniscience. Bayesian belief updates are recovered as a special case, but the dynamics of belief change are non-Bayesian in general. While it might not turn out to be the last word on the problem of logical uncertainty, it has a large number of desirable properties, and solves many problems in a unified and relatively clean framework. It seems worth asking what consequences this theory has for practical rationality. Can we say new things about what good reasoning looks like in humans, and how to avoid pitfalls of reasoning? First, I'll give a shallow overview of logical induction and possible implications for practical epistemic rationality. Then, I'll focus on the particular question of A Technical Explanation of Technical Explanation (which I'll abbreviate TEOTE from now on). Put in CFAR terminology, I'm seeking a gears-level understanding of gears-level understanding. I focus on the intuitions, with only a minimal account of how logical induction helps make that picture work. LOGICAL INDUCTION There are a number of difficulties in applying Bayesian uncertainty to logic. No compu",2018-02-15,2022-01-30 04:57:32,2022-01-30 04:57:32,2021-02-06 18:43:19,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/THG8KAMT/toward-a-new-technical-explanation-of-technical-explanation.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZE6E5SCK,blogPost,2020,"Christiano, Paul",Better priors as a safety problem,AI Alignment (Medium),,,,https://ai-alignment.com/better-priors-as-a-safety-problem-24aa1c300710,Many universal priors are inefficient in the finite data regime. I argue that’s a safety problem and we should try to fix it directly.,2020-07-05,2022-01-30 04:57:26,2022-01-30 04:57:26,2020-08-28 17:38:06,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/A4U2ZXVR/better-priors-as-a-safety-problem-24aa1c300710.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DHWDQS39,blogPost,2018,"Christiano, Paul",Directions and desiderata for AI alignment,AI Alignment (Medium),,,,https://ai-alignment.com/directions-and-desiderata-for-ai-control-b60fca0da8f4,"I lay out three research directions in AI alignment, and three desiderata that I think should guide research in these areas.",2018-05-12,2022-01-30 04:57:26,2022-01-30 04:57:26,2020-12-11 22:48:28,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/QWQ4D45X/directions-and-desiderata-for-ai-control-b60fca0da8f4.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ST23JMFC,blogPost,2017,"Christiano, Paul",Benign model-free RL,AI Alignment (Medium),,,,https://ai-alignment.com/benign-model-free-rl-4aae8c97e385,"Reward learning, robustness, and amplification may be sufficient to train benign model-free RL agents.",2017-06-02,2022-01-30 04:57:26,2022-01-30 04:57:26,2020-12-11 22:48:25,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/WUUBUMP6/benign-model-free-rl-4aae8c97e385.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
K9RNDEJE,blogPost,2016,"Clark, Jack; Amodei, Dario",Faulty Reward Functions in the Wild,OpenAI,,,,https://openai.com/blog/faulty-reward-functions/,"Reinforcement learning algorithms can break in surprising, counterintuitive ways. In this post we'll explore one failure mode, which is where you misspecify your reward function.",2016-12-22,2022-01-30 04:57:26,2022-01-30 04:57:26,2020-11-21 17:34:09,,,,,,,,,,,,,,en,,,,,,,ZSCC: 0000038,,/Users/jacquesthibodeau/Zotero/storage/FVVJ8XT8/faulty-reward-functions.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C2V4P5BG,blogPost,2021,"Schulman, John",Frequent arguments about alignment,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/6ccG9i5cTncebmhsH/frequent-arguments-about-alignment,"Here, I’ll review some arguments that frequently come up in discussions about alignment research, involving one person skeptical of the endeavor (called Skeptic) and one person advocating to do more of it (called Advocate). I mostly endorse the views of the Advocate, but the Skeptic isn't a strawman and makes some decent points. The dialog is mostly based on conversations I've had with people who work on machine learning but don't specialize in safety and alignment. This post has two purposes. First, I want to cache good responses to these questions, so I don't have to think about them each time the topic comes up. Second, I think it's useful for people who work on safety and alignment to be ready for the kind of pushback they'll get when pitching their work to others. Just to introduce myself, I'm a cofounder of OpenAI and lead a team that works on developing and applying reinforcement learning methods; we're working on improving truthfulness and reasoning abilities of language models. 1. DOES ALIGNMENT GET SOLVED AUTOMATICALLY AS OUR MODELS GET SMARTER? Skeptic: I think the alignment problem gets easier as our models get smarter. When we train sufficiently powerful generative models, they'll learn the difference between human smiles and human wellbeing; the difference between the truth and common misconceptions; and various concepts they'll need for aligned behavior. Given all of this internal knowledge, we just have to prompt them appropriately to get the desired behavior. For example, to get wise advice from a powerful language model, I just have to set up a conversation between myself and ""a wise and benevolent AI advisor."" Advocate: The wise AI advisor you described has some basic problems, and I'll get into those shortly. But more generally, prompting an internet-trained generative model (like raw GPT-3) is a very poor way of getting aligned behavior, and we can easily do much better. It'll occasionally do something reasonable, but that's not nearly good",2021-06-22,2022-01-30 04:57:26,2022-01-30 04:57:26,2021-11-14 18:45:41,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/CT4CHZBU/frequent-arguments-about-alignment.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R7GVJT8S,blogPost,2021,"Barnes, Beth",Imitative Generalisation (AKA 'Learning the Prior'),AI Alignment Forum,,,,https://www.alignmentforum.org/posts/JKj5Krff5oKMb8TjT/imitative-generalisation-aka-learning-the-prior-1,"TL;DR We want to be able to supervise models with superhuman knowledge of the world and how to manipulate it. For this we need an overseer to be able to learn or access all the knowledge our models have, in order to be able to understand the consequences of suggestions or decisions from the model. If the overseers don’t have access to all the same knowledge as the model, it may be easy for the model to deceive us, suggesting plans that look good to us but that may have serious negative consequences. We might hope to access what the model knows just by training it to answer questions. However, we can only train on questions that humans are able to answer[1]. This gives us a problem that’s somewhat similar to the standard formulation of transduction: we have some labelled training set (questions humans can answer), and we want to transfer to an unlabelled dataset (questions we care about), that may be differently distributed. We might hope that our models will naturally generalize correctly from easy-to-answer questions to the ones that we care about. However, a natural pathological generalisation is for our models to only give us ‘human-like’ answers to questions, even if it knows the best answer is different. If we only have access to these human-like answers to questions, that probably doesn’t give us enough information to supervise a superhuman model. What we’re going to call ‘Imitative Generalization’ is a possible way to narrow the gap between the things our model knows, and the questions we can train our model to answer honestly. It avoids the pathological generalisation by only using ML for IID tasks, and imitating the way humans generalize. This hopefully gives us answers that are more like ‘how a human would answer if they’d learnt from all the data the model has learnt from’. We supervise how the model does the transfer, to get the sort of generalisation we want. It’s worth noting there are enough serious open questions that imitative generalization is",2021,2022-01-30 04:57:25,2022-01-30 04:57:25,2021-11-13 19:36:31,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/U82V8QHX/imitative-generalisation-aka-learning-the-prior-1.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HA5ICP5K,blogPost,2018,"Christiano, Paul",Implicit extortion,AI Alignment (Medium),,,,https://ai-alignment.com/implicit-extortion-3c80c45af1e3,"Extortion can be equally effective, and harder to notice, when you don’t tell the target it’s occurring.",2018-04-13,2022-01-30 04:57:25,2022-01-30 04:57:25,2020-11-14 03:13:26,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/BN9HHUAJ/implicit-extortion-3c80c45af1e3.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
X9RN5I7J,blogPost,2018,Tom B Brown; Catherine Olsson,Introducing the Unrestricted Adversarial Examples Challenge,Google AI Blog,,,,http://ai.googleblog.com/2018/09/introducing-unrestricted-adversarial.html,"Posted by Tom B. Brown and Catherine Olsson, Research Engineers, Google Brain Team   Machine learning is being deployed in more and more rea...",2018,2022-01-30 04:57:25,2022-01-30 04:57:25,2020-12-13 22:18:26,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/8H8Q7D4X/introducing-unrestricted-adversarial.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QHGKPS7W,blogPost,2018,"Christiano, Paul",Techniques for optimizing worst-case performance,AI Alignment (Medium),,,,https://ai-alignment.com/techniques-for-optimizing-worst-case-performance-39eafec74b99,Optimizing neural networks for worst-case performance looks really hard. Here’s why I have hope.,2018-02-02,2022-01-30 04:57:25,2022-01-30 04:57:25,2020-12-13 22:20:08,,,,,,,,,,,,,,en,,,,,,,ZSCC: 0000003,,/Users/jacquesthibodeau/Zotero/storage/ET67S8GH/techniques-for-optimizing-worst-case-performance-39eafec74b99.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QG2D78EJ,blogPost,2018,"Christiano, Paul",The easy goal inference problem is still hard,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/h9DesGT3WT9u2k7Hr/the-easy-goal-inference-problem-is-still-hard,"Posted as part of the AI Alignment Forum sequence on Value Learning. Rohin’s note: In this post (original here), Paul Christiano analyzes the ambitious value learning approach. He considers a more general view of ambitious value learning where you infer preferences more generally (i.e. not necessarily in the form of a utility function), and you can ask the user about their preferences, but it’s fine to imagine that you infer a utility function from data and then optimize it. The key takeaway is that in order to infer preferences that can lead to superhuman performance, it is necessary to understand how humans are biased, which seems very hard to do even with infinite data. -------------------------------------------------------------------------------- One approach to the AI control problem goes like this:  1. Observe what the user of the system says and does.  2. Infer the user’s preferences.  3. Try to make the world better according to the user’s preference, perhaps     while working alongside the user and asking clarifying questions. This approach has the major advantage that we can begin empirical work today — we can actually build systems which observe user behavior, try to figure out what the user wants, and then help with that. There are many applications that people care about already, and we can set to work on making rich toy models. It seems great to develop these capabilities in parallel with other AI progress, and to address whatever difficulties actually arise, as they arise. That is, in each domain where AI can act effectively, we’d like to ensure that AI can also act effectively in the service of goals inferred from users (and that this inference is good enough to support foreseeable applications). This approach gives us a nice, concrete model of each difficulty we are trying to address. It also provides a relatively clear indicator of whether our ability to control AI lags behind our ability to build it. And by being technically interesting an",2018,2022-01-30 04:57:25,2022-01-30 04:57:25,2020-12-17 04:36:24,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/8RHW83JV/h9DesGT3WT9u2k7Hr.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
W2WEFH9K,blogPost,2020,"Christiano, Paul",Inaccessible information,AI Alignment (Medium),,,,https://ai-alignment.com/inaccessible-information-c749c6a88ce,What kind of information might be hard to elicit from ML models?,2020-06-03,2022-01-30 04:57:25,2022-01-30 04:57:25,2020-08-31 18:11:57,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/BX8MMB8R/inaccessible-information-c749c6a88ce.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KMMWBRTC,blogPost,2019,"Christiano, Paul",Informed oversight,AI Alignment (Medium),,,,https://ai-alignment.com/informed-oversight-18fcb5d3d1e1,An overseer can provide adequate rewards for an agent if they know everything the agent knows. (Update of a 2016 post.),2019-01-24,2022-01-30 04:57:25,2022-01-30 04:57:25,2020-11-14 03:13:48,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/TNH2CIIC/informed-oversight-18fcb5d3d1e1.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
D94ADJCV,blogPost,2020,"Christiano, Paul",Learning the prior,AI Alignment (Medium),,,,https://ai-alignment.com/learning-the-prior-48f61b445c04,"I suggest using neural nets to approximate our real prior, rather than implicitly using neural nets themselves as the prior.",2020-07-05,2022-01-30 04:57:25,2022-01-30 04:57:25,2020-08-28 17:40:46,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/NHBV8RXZ/learning-the-prior-48f61b445c04.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
S9URNKK4,blogPost,2019,"Christiano, Paul",The strategy-stealing assumption,AI Alignment (Medium),,,,https://ai-alignment.com/the-strategy-stealing-assumption-a26b8b1ed334,"If humans initially control 99% of the world’s resources, when can they secure 99% of the long-term influence?",2019-09-15,2022-01-30 04:57:24,2022-01-30 04:57:24,2020-12-11 22:48:18,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/9QXT7R5V/the-strategy-stealing-assumption-a26b8b1ed334.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IJFT7TDI,blogPost,2019,"Christiano, Paul",Towards formalizing universality,AI Alignment (Medium),,,,https://ai-alignment.com/towards-formalizing-universality-409ab893a456,An attempt to formalize universality as “able to understand anything that any computation can understand.”,2019-01-11,2022-01-30 04:57:24,2022-01-30 04:57:24,2020-11-14 03:13:36,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/C7P8EW68/towards-formalizing-universality-409ab893a456.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JB955CSH,blogPost,2018,"Christiano, Paul",Two guarantees,AI Alignment (Medium),,,,https://ai-alignment.com/two-guarantees-c4c03a6b434f,"I suspect AI alignment should aim to separately establish good performance in the average case, and lack-of-malice in the worst case.",2018-04-09,2022-01-30 04:57:24,2022-01-30 04:57:24,2020-11-14 03:13:22,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/DW25F88R/two-guarantees-c4c03a6b434f.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JR64UTGB,blogPost,2019,"Christiano, Paul",Universality and model-based RL,AI Alignment (Medium),,,,https://ai-alignment.com/universality-and-model-based-rl-b08701394ddd,"Ascription universality may be very helpful for safe model-based RL, facilitating benign induction and “transparent” models.",2019-10-04,2022-01-30 04:57:24,2022-01-30 04:57:24,2020-12-11 22:48:17,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/UTQURPQ7/universality-and-model-based-rl-b08701394ddd.html,,TechSafety; Open-AI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XCDGQ3EW,blogPost,2019,"Christiano, Paul",Universality and consequentialism within HCH,AI Alignment (Medium),,,,https://ai-alignment.com/universality-and-consequentialism-within-hch-c0bee00365bd,One exotic reason HCH can fail to be universal is the emergence of malicious patterns of behavior; universality may help address this risk.,2019-01-10,2022-01-30 04:57:24,2022-01-30 04:57:24,2020-11-14 03:13:34,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/M72NERZD/universality-and-consequentialism-within-hch-c0bee00365bd.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BEESSIWV,blogPost,2019,"Christiano, Paul",Universality and security amplification,AI Alignment (Medium),,,,https://ai-alignment.com/universality-and-security-amplification-551b314a3bab,A slightly more detailed view of security amplification.,2019-01-03,2022-01-30 04:57:24,2022-01-30 04:57:24,2020-11-14 03:13:23,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/7F92Q22J/universality-and-security-amplification-551b314a3bab.html; /Users/jacquesthibodeau/Zotero/storage/3XIIMECQ/universality-and-security-amplification-551b314a3bab.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
V99RRXAV,blogPost,2019,"Christiano, Paul",Worst-case guarantees,AI Alignment (Medium),,,,https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d,"Reviewing the prospects for training models to behave acceptably on all inputs, rather than just the training distribution.",2019-03-23,2022-01-30 04:57:23,2022-01-30 04:57:23,2020-11-14 03:13:32,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/IXPVJKZM/training-robust-corrigibility-ce0e0a3b9b4d.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8SK6TQ2N,blogPost,2020,"Barnes, Beth; Christiano, Paul",Writeup: Progress on AI Safety via Debate,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1,"This is a writeup of the research done by the ""Reflection-Humans"" team at OpenAI in Q3 and Q4 of 2019. During that period we investigated mechanisms that would allow evaluators to get correct and helpful answers from experts, without the evaluators themselves being expert in the domain of the questions. This follows from the original work on AI Safety via Debate and the call for research on human aspects of AI safety, and is also closely related to work on Iterated Amplification. AUTHORS AND ACKNOWLEDGEMENTS The main researchers on this project were Elizabeth Barnes, Paul Christiano, Long Ouyang and Geoffrey Irving. We are grateful to many others who offered ideas and feedback. In particular: the cross-examination idea was inspired by a conversation with Chelsea Voss; Adam Gleave had helpful ideas about the long computation problem; Jeff Wu, Danny Hernandez and Gretchen Krueger gave feedback on a draft; we had helpful conversations with Amanda Askell, Andreas Stuhlmüller and Joe Collman, as well as others on the Ought team and the OpenAI Reflection team. We’d also like to thank our contractors who participated in debate experiments, especially David Jones, Erol Akbaba, Alex Deam and Chris Painter. Oliver Habryka helped format and edit the document for the AI Alignment Forum. Note by Oliver: There is currently a bug with links to headings in a post, causing them to not properly scroll when clicked. Until that is fixed, just open those links in a new tab, which should scroll correctly. OVERVIEW Motivation As we apply ML to increasingly important and complex tasks, the problem of evaluating behaviour and providing a good training signal becomes more difficult. We already see examples of RL leading to undesirable behaviours that superficially ‘look good’ to human evaluators (see this collection of examples). One example from an OpenAI paper is an agent learning incorrect behaviours in a 3d simulator, because the behaviours look like the desired behaviour in the 2d",2020-02-05,2022-01-30 04:57:23,2022-01-30 04:57:23,2020-09-07 18:11:36,,,,,,,Writeup,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/K8SIBJDZ/writeup-progress-on-ai-safety-via-debate-1.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HAJA2B87,blogPost,2020,"Christiano, Paul",“Unsupervised” translation as an (intent) alignment problem,AI Alignment (Medium),,,,https://ai-alignment.com/unsupervised-translation-as-a-safety-problem-99ae1f9b6b68,Unsupervised translation is an interesting domain where models seem to “know” something we can’t get them to tell us.,2020-09-30,2022-01-30 04:57:23,2022-01-30 04:57:23,2020-12-11 22:48:20,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/W39ZJ8FT/unsupervised-translation-as-a-safety-problem-99ae1f9b6b68.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4AZ2N39G,blogPost,2019,"Christiano, Paul",What failure looks like,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like,"The stereotyped image of AI catastrophe is a powerful, malicious AI system that takes its creators by surprise and quickly achieves a decisive advantage over the rest of humanity. I think this is probably not what failure will look like, and I want to try to paint a more realistic picture. I’ll tell the story in two parts:  * Part I: machine learning will increase our ability to “get what we can    measure,” which could cause a slow-rolling catastrophe. (""Going out with a    whimper."")  * Part II: ML training, like competitive economies or natural ecosystems, can    give rise to “greedy” patterns that try to expand their own influence. Such    patterns can ultimately dominate the behavior of a system and cause sudden    breakdowns. (""Going out with a bang,"" an instance of optimization daemons    [https://arbital.com/p/daemons/].) I think these are the most important problems if we fail to solve intent alignment [https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6]. In practice these problems will interact with each other, and with other disruptions/instability caused by rapid progress. These problems are worse in worlds where progress is relatively fast, and fast takeoff can be a key risk factor, but I’m scared even if we have several years. With fast enough takeoff, my expectations start to look more like the caricature---this post envisions reasonably broad deployment of AI, which becomes less and less likely as things get faster. I think the basic problems are still essentially the same though, just occurring within an AI lab rather than across the world. (None of the concerns in this post are novel.) PART I: YOU GET WHAT YOU MEASURE If I want to convince Bob to vote for Alice, I can experiment with many different persuasion strategies and see which ones work. Or I can build good predictive models of Bob’s behavior and then search for actions that will lead him to vote for Alice. These are powerful techniques for achieving any goal that can be ea",2019,2022-01-30 04:57:23,2022-01-30 04:57:23,2019-12-16 19:59:17,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s4]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/KNVMK6EZ/what-failure-looks-like.html; /Users/jacquesthibodeau/Zotero/storage/WV669JT4/what-failure-looks-like.html,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R78XUR43,blogPost,2017,"Yudkowsky, Eliezer",There's No Fire Alarm for Artificial General Intelligence,Machine Intelligence Research Institute,,,,https://intelligence.org/2017/10/13/fire-alarm/,"What is the function of a fire alarm?   One might think that the function of a fire alarm is to provide you with important evidence about a fire existing, allowing you to change your policy accordingly and exit the building. In the classic experiment by Latane and Darley in 1968, eight groups of... Read more »",2017-10-14,2022-01-30 04:56:59,2022-01-30 04:56:59,2020-12-13 20:50:15,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: 0000014,,/Users/jacquesthibodeau/Zotero/storage/V25W5CF3/fire-alarm.html,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
422HC9XW,blogPost,2002,"Yudkowsky, Eliezer",The AI-Box Experiment,Eliezer S Yudkowsky,,,,https://www.yudkowsky.net/singularity/aibox,,2002,2022-01-30 04:56:59,2022-01-30 04:56:59,2020-11-21 17:49:14,,,,,,,,,,,,,,,,,,,,,ZSCC: 0000035,,/Users/jacquesthibodeau/Zotero/storage/97EN4T37/aibox.html,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
97JPXM8D,blogPost,2015,"Yudkowsky, Eliezer",Task-directed AGI,Arbital,,,,https://arbital.com/p/task_agi/,"An advanced AI that's meant to pursue a series of limited-scope goals given it by the user.  In Bostrom's terminology, a Genie.",2015,2022-01-30 04:56:59,2022-01-30 04:56:59,2021-02-06 17:13:02,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QZKRN49F,blogPost,2018,"Yudkowsky, Eliezer",The Rocket Alignment Problem,Machine Intelligence Research Institute,,,,https://intelligence.org/2018/10/03/rocket-alignment/,"The following is a fictional dialogue building off of AI Alignment: Why It’s Hard, and Where to Start.   (Somewhere in a not-very-near neighboring world, where science took a very different course…)   ALFONSO:  Hello, Beth. I’ve noticed a lot of speculations lately about “spaceplanes” being used to attack cities, or possibly becoming infused with malevolent... Read more »",2018-10-03,2022-01-30 04:56:59,2022-01-30 04:56:59,2020-12-13 23:54:02,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: 0000002,,/Users/jacquesthibodeau/Zotero/storage/77I4C3AE/rocket-alignment.html,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HSCES72W,blogPost,2015,"Yudkowsky, Eliezer",Sufficiently optimized agents appear coherent,Arbital,,,,https://arbital.com/p/optimized_agent_appears_coherent/,"If you could think as well as a superintelligence, you'd be at least that smart yourself.",2015,2022-01-30 04:56:58,2022-01-30 04:56:58,2021-02-06 17:12:04,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/2II69UX2/optimized_agent_appears_coherent.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
238TWTH2,blogPost,2017,"Yudkowsky, Eliezer",Separation from hyperexistential risk,Arbital,,,,https://arbital.com/p/hyperexistential_separation/,"The AI should be widely separated in the design space from any AI that would constitute a ""hyperexistential risk"" (anything worse than death).",2017,2022-01-30 04:56:58,2022-01-30 04:56:58,2021-02-06 17:33:02,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/DEPHXC82/hyperexistential_separation.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
B7375JA5,blogPost,2015,"Soares, Nate","Safety engineering, target selection, and alignment theory",Machine Intelligence Research Institute,,,,https://intelligence.org/2015/12/31/safety-engineering-target-selection-and-alignment-theory/,"Artificial intelligence capabilities research is aimed at making computer systems more intelligent — able to solve a wider range of problems more effectively and efficiently. We can distinguish this from research specifically aimed at making AI systems at various capability levels safer, or more “robust and beneficial.” In this post, I distinguish three kinds of direct... Read more »",2015-12-31,2022-01-30 04:56:58,2022-01-30 04:56:58,2020-11-21 17:07:20,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/XWWTIPKK/safety-engineering-target-selection-and-alignment-theory.html,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IW5QC4TT,blogPost,2018,"Garrabrant, Scott",Robustness to Scale,LessWrong,,,,https://www.lesswrong.com/posts/bBdfbWfWxHN9Chjcq/robustness-to-scale,"I want to quickly draw attention to a concept in AI alignment: Robustness to Scale. Briefly, you want your proposal for an AI to be robust (or at least fail gracefully) to changes in its level of capabilities. I discuss three different types of robustness to scale: robustness to scaling up, robustness to scaling down, and robustness to relative scale. The purpose of this post is to communicate, not to persuade. It may be that we want to bite the bullet of the strongest form of robustness to scale, and build an AGI that is simply not robust to scale, but if we do, we should at least realize that we are doing that. Robustness to scaling up means that your AI system does not depend on not being too powerful. One way to check for this is to think about what would happen if the thing that the AI is optimizing for were actually maximized. One example of failure of robustness to scaling up is when you expect an AI to accomplish a task in a specific way, but it becomes smart enough to find new creative ways to accomplish the task that you did not think of, and these new creative ways are disastrous. Another example is when you make an AI that is incentivized to do one thing, but you add restrictions that make it so that the best way to accomplish that thing has a side effect that you like. When you scale the AI up, it finds a way around your restrictions. Robustness to scaling down means that your AI system does not depend on being sufficiently powerful. You can't really make your system still work when it scales down, but you can maybe make sure it fails gracefully. For example, imagine you had a system that was trying to predict humans, and use these predictions to figure out what to do. When scaled up all the way, the predictions of humans are completely accurate, and it will only take actions that the predicted humans would approve of. If you scale down the capabilities, your system may predict the humans incorrectly. These errors may multiply as you stack many predi",2018-02-21,2022-01-30 04:56:58,2022-01-30 04:56:58,2021-02-06 18:49:10,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/Z6Q9GRDC/robustness-to-scale.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2TSM99QU,blogPost,2020,"Demski, Abram",Radical Probabilism,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/xJyY5QkQvNJpZLJRo/radical-probabilism-1,"This is an expanded version of my talk. I assume a high degree of familiarity with Bayesian probability theory. Toward a New Technical Explanation of Technical Explanation -- an attempt to convey the practical implications of logical induction -- was one of my most-appreciated posts, but I don't really get the feeling that very many people have received the update. Granted, that post was speculative, sketching what a new technical explanation of technical explanation might look like. I think I can do a bit better now. If the implied project of that post had really been completed, I would expect new practical probabilistic reasoning tools, explicitly violating Bayes' law. For example, we might expect:  * A new version of information theory. * An update to the ""prediction=compression"" maxim, either repairing it to       incorporate the new cases, or explicitly denying it and providing a good       intuitive account of why it was wrong.     * A new account of concepts such as       mutual information, allowing for the fact that variables have behavior       over thinking time; for example, variables may initially be very       correlated, but lose correlation as our picture of each variable becomes       more detailed.          * New ways of thinking about epistemology. * One thing that my post did manage       to do was to spell out the importance of ""making advanced predictions"", a       facet of epistemology which Bayesian thinking does not do justice to.     * However, I left aspects of the       problem of old evidence open, rather than giving a complete way to think       about it.          * New probabilistic structures. * Bayesian Networks are one really nice way to       capture the structure of probability distributions, making them much       easier to reason about. Is there anything similar for the new, wider space       of probabilistic reasoning which has been opened up?         Unfortunately, I still don't have any of those things to offer. The aim o",2020-08-18,2022-01-30 04:56:58,2022-01-30 04:56:58,2020-08-27 16:25:26,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/JPB8ZRTA/radical-probabilism-1.html,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WBGESJZX,blogPost,2015,"Yudkowsky, Eliezer",Patch resistance,Arbital,,,,https://arbital.com/p/patch_resistant/,One does not simply solve the value alignment problem.,2015,2022-01-30 04:56:58,2022-01-30 04:56:58,2021-02-06 17:10:55,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/SKWUI9EH/patch_resistant.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VAGVQFSF,blogPost,2018,"Garrabrant, Scott",Optimization Amplifies,LessWrong,,,,https://www.lesswrong.com/posts/zEvqFtT4AtTztfYC4/optimization-amplifies,"I talk here about how a mathematician mindset can be useful for AI alignment. But first, a puzzle: Given m, what is the least number n≥2 such that for 2≤k≤m, the base k  representation of n consists entirely of 0s and 1s?  If you want to think about it yourself, stop reading. For m=2, n=2.  For m=3, n=3. For m=4, n=4. For m=5, n=82,000. Indeed, 82,000 is 10100000001010000 in binary, 11011111001 in ternary, 110001100 in base 4, and 10111000 in base 5. What about when m=6? So, a mathematician might tell you that this is an open problem. It is not known if there is any n≥2 which consists of 0s and 1s in bases 2 through 6. A scientist, on the other hand, might just tell you that clearly no such number exists. There are 2k−1 numbers that consist of k 0s and 1s in base 6. Each of these has roughly log5(6)⋅k digits in base 5, and assuming things are roughly evenly distributed, each of these digits is a 0 or a 1 with ""probability"" 2/5. The ""probability"" that there is any number of length k that has the property is thus less than 2k⋅(2/5)k=(4/5)k. This means that as you increase k, the ""probability"" that you find a number with the property drops off exponentially, and this is not even considering bases 3 and 4. Also, we have checked all numbers up to 2000 digits. No number with this property exists. Who is right?  Well, they are both right. If you want to have fun playing games with proofs, you can consider it an open problem and try to prove it. If you want to get the right answer, just listen to the scientist. If you have to choose between destroying the world with a 1% probability and destroying the world if a number greater than 2 which consists of 0s and 1s in bases 2 through 6 exists, go with the latter. It is tempting to say that we might be in a situation similar to this. We need to figure out how to make safe AI, and we maybe don't have that much time. Maybe we need to run experiments, and figure out what is true about what we should do and not waste ou",2018-06-26,2022-01-30 04:56:58,2022-01-30 04:56:58,2021-02-06 18:45:57,,,,,,,,,,,,,,,,,,,,,ZSCC: 0000002,,/Users/jacquesthibodeau/Zotero/storage/T4CJB5X5/optimization-amplifies.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QASC9Z4D,blogPost,2016,"Yudkowsky, Eliezer",Task (AI goal),Arbital,,,,https://arbital.com/p/task_goal/,"When building the first AGIs, it may be wiser to assign them only goals that are bounded in space and time, and can be satisfied by bounded efforts.",2016,2022-01-30 04:56:58,2022-01-30 04:56:58,2021-02-06 17:18:21,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/VSAFI8SC/task_goal.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4AJCWUID,blogPost,2020,"Hubinger, Evan",Synthesizing amplification and debate,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/dJSD5RK6Qoidb3QY5/synthesizing-amplification-and-debate,"BACKGROUND One possible way to train an amplification model is to use an auxiliary reinforcement learning objective to help guide the training of the amplification model. This could be done either by training two separate models, an agent and a question-answerer, or a single model trained on a joint objective. For example, from a comment Paul left on “A dilemma for prosaic AI alignment:” I normally imagine using joint training in these cases, rather than pre-training + fine-tuning. e.g., at every point in time we maintain an agent and a question-answerer, where the question-answerer ""knows everything the agent knows."" They get better together, with each gradient update affecting both of them, rather than first training a good agent and then adding a good question-answerer. (Independently of concerns about mesa-optimization, I think the fine-tuning approach would have trouble because you couldn't use statistical regularities from the ""main"" objective to inform your answers to questions, and therefore your question answers will be dumber than the policy and so you couldn't get a good reward function or specification of catastrophically bad behavior.) In my last post, I expressed skepticism of such non-imitative amplification approaches, though in this post I want to propose a possible way in which some of my concerns with this style of approach could addressed by integrating ideas from AI safety via debate. I'll start by describing the basic idea in broad terms, then give a more careful, technical description of the sort of training procedure I have in mind. THE PROPOSAL The basic idea is as follows: debate naturally yields an RL objective, so if you want to add an auxiliary RL objective to amplification, why not use the RL objective from debate? Specifically, the idea is to conduct a debate not between copies of the model M, but between copies of the amplified model Amp(M) (where  Amp(M) is a human with access to the model M). That gives you both an RL reward ari",2020-02-05,2022-01-30 04:56:58,2022-01-30 04:56:58,2020-09-07 18:16:00,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/XCB5N6K8/synthesizing-amplification-and-debate.html,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BX7UP66I,blogPost,2016,"Yudkowsky, Eliezer",So Far: Unfriendly AI Edition,Econlib,,,,https://www.econlib.org/archives/2016/03/so_far_unfriend.html,"Eliezer Yudkowsky responds to my “selective pessimism” challenge with another challenge.  Here he is, reprinted with his permission. Eliezer Yudkowsky responds to my “selective pessimism” challenge with another challenge.  Here he is, reprinted with his permission. Bryan Caplan issued the following challenge, naming Unfriendly AI as one among several disaster scenarios he thinks is unlikely: …",2016-03-29,2022-01-30 04:56:58,2022-01-30 04:56:58,2021-02-06 17:17:06,,,,,,,So Far,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A  Section: Cost-benefit Analysis,,/Users/jacquesthibodeau/Zotero/storage/BDQ6EDXU/so_far_unfriend.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
N9ZZVPT7,blogPost,2017,"Yudkowsky, Eliezer",Security Mindset and the Logistic Success Curve,Machine Intelligence Research Institute,,,,https://intelligence.org/2017/11/26/security-mindset-and-the-logistic-success-curve/,"Follow-up to:   Security Mindset and Ordinary Paranoia   (Two days later, Amber returns with another question.)   AMBER:  Uh, say, Coral. How important is security mindset when you’re building a whole new kind of system—say, one subject to potentially adverse optimization pressures, where you want it to have some sort of robustness property? CORAL:  How novel is the... Read more »",2017-11-26,2022-01-30 04:56:58,2022-01-30 04:56:58,2021-02-06 17:27:18,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: 0000002,,/Users/jacquesthibodeau/Zotero/storage/H9XPNAQI/security-mindset-and-the-logistic-success-curve.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
X8BWI6VK,blogPost,2017,"Yudkowsky, Eliezer",Security Mindset and Ordinary Paranoia,Machine Intelligence Research Institute,,,,https://intelligence.org/2017/11/25/security-mindset-ordinary-paranoia/,"The following is a fictional dialogue building off of AI Alignment: Why It’s Hard, and Where to Start.   (AMBER, a philanthropist interested in a more reliable Internet, and CORAL, a computer security professional, are at a conference hotel together discussing what Coral insists is a difficult and important issue: the difficulty of building “secure”... Read more »",2017-11-25,2022-01-30 04:56:58,2022-01-30 04:56:58,2021-02-06 17:29:47,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: 0000004,,/Users/jacquesthibodeau/Zotero/storage/3SIBGGKQ/security-mindset-ordinary-paranoia.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C6S6XHER,blogPost,2017,"Yudkowsky, Eliezer",Problem of fully updated deference,Arbital,,,,https://arbital.com/p/updated_deference/,Why moral uncertainty doesn't stop an AI from defending its off-switch.,2017,2022-01-30 04:56:58,2022-01-30 04:56:58,2021-02-06 17:26:31,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/EB7M3IS4/updated_deference.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Q5IG88VT,blogPost,2015,"Yudkowsky, Eliezer",Ontology identification problem,Arbital,,,,https://arbital.com/p/ontology_identification/,"How do we link an agent's utility function to its model of the world, when we don't know what that model will look like?",2015,2022-01-30 04:56:57,2022-01-30 04:56:57,2021-02-06 17:08:36,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/CTUFJV5B/ontology_identification.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WZ3RHMBA,blogPost,2017,"Yudkowsky, Eliezer",Non-adversarial principle,Arbital,,,,https://arbital.com/p/nonadversarial/,"At no point in constructing an Artificial General Intelligence should we construct a computation that tries to hurt us, and then try to stop it from hurting us.",2017,2022-01-30 04:56:57,2022-01-30 04:56:57,2021-02-06 17:25:35,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/ZI24N9IH/nonadversarial.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9WIT2K8G,blogPost,2015,"Yudkowsky, Eliezer",Nearest unblocked strategy,Arbital,,,,https://arbital.com/p/nearest_unblocked/,"If you patch an agent's preference framework to avoid an undesirable solution, what can you expect to happen?",2015,2022-01-30 04:56:57,2022-01-30 04:56:57,2021-01-23 20:51:27,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/2DKXZF46/nearest_unblocked.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AC6647C5,blogPost,2020,"Schlegeris, Buck",My personal cruxes for working on AI safety,Effective Altruism Forum,,,,https://forum.effectivealtruism.org/posts/Ayu5im98u8FeMWoBZ/my-personal-cruxes-for-working-on-ai-safety,"The following is a heavily edited transcript of a talk I gave for the Stanford Effective Altruism club on 19 Jan 2020. I had rev.com transcribe it, and then Linchuan Zhang, Rob Bensinger and I edited it for style and clarity, and also to occasionally have me say smarter things than I actually said. Linch and I both added a few notes throughout. Thanks also to Bill Zito, Ben Weinstein-Raun, and Howie Lempel for comments.  I feel slightly weird about posting something so long, but this is the natural place to put it. Over the last year my beliefs about AI risk have shifted moderately; I expect that in a year I'll think that many of the things I said here were dumb. Also, very few of the ideas here are original to me. -- After all those caveats, here's the talk: INTRODUCTION It's great to be here. I used to hang out at Stanford a lot, fun fact. I moved to America six years ago, and then in 2015, I came to Stanford EA every Sunday, and there was, obviously, a totally different crop of people there. It was really fun. I think we were a lot less successful than the current Stanford EA iteration at attracting new people. We just liked having weird conversations about weird stuff every week. It was really fun, but it's really great to come back and see a Stanford EA which is shaped differently.  Today I'm going to be talking about the argument for working on AI safety that compels me to work on AI safety, rather than the argument that should compel you or anyone else. I'm going to try to spell out how the arguments are actually shaped in my head. Logistically, we're going to try to talk for about an hour with a bunch of back and forth and you guys arguing with me as we go. And at the end, I'm going to do miscellaneous Q and A for questions you might have. And I'll probably make everyone stand up and sit down again because it's unreasonable to sit in the same place for 90 minutes. META LEVEL THOUGHTS I want to first very briefly talk about some concepts I have that a",2020-02-13,2022-01-30 04:56:57,2022-01-30 04:56:57,2020-09-05 19:15:03,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/5FGH76JD/my-personal-cruxes-for-working-on-ai-safety.html,,MetaSafety; MIRI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MMTAEC25,blogPost,2021,"Bensinger, Rob; Yudkowsky, Eliezer; Hubinger, Evan; Cotra, Ajeya; Shah, Rohin","MIRI comments on Cotra's ""Case for Aligning Narrowly Superhuman Models""",AI Alignment Forum,,,,https://www.alignmentforum.org/posts/AyfDnnAdjG7HHeD3d/miri-comments-on-cotra-s-case-for-aligning-narrowly,"Below, I’ve copied comments left by MIRI researchers Eliezer Yudkowsky and Evan Hubinger on March 1–3 on a draft of Ajeya Cotra’s ""Case for Aligning Narrowly Superhuman Models."" I've included back-and-forths with Cotra, and interjections by me and Rohin Shah. The section divisions below correspond to the sections in Cotra's post. 0. INTRODUCTION How can we train GPT-3 to give “the best health advice it can give” using demonstrations and/or feedback from humans who may in some sense “understand less” about what to do when you’re sick than GPT-3 does? Eliezer Yudkowsky: I've had some related conversations with Nick Beckstead. I'd be hopeful about this line of work primarily because I think it points to a bigger problem with the inscrutable matrices of floating-point numbers, namely, we have no idea what the hell GPT-3 is thinking and cannot tell it to think anything else. GPT-3 has a great store of medical knowledge, but we do not know where that medical knowledge is; we do not know how to tell it to internally apply its medical knowledge rather than applying other cognitive patterns it has stored. If this is still the state of opacity of AGI come superhuman capabilities, we are all immediately dead. So I would be relatively more hopeful about any avenue of attack for this problem that used anything other than an end-to-end black box - anything that started to address, ""Well, this system clearly has a bunch of medical knowledge internally, can we find that knowledge and cause it to actually be applied"" rather than ""What external forces can we apply to this solid black box to make it think more about healthcare?"" Evan Hubinger: +1 I continue to think that language model transparency research is the single most valuable current research direction within the class of standard ML research, for similar reasons to what Eliezer said above. Ajeya Cotra: Thanks! I'm also excited about language model transparency, and would love to find ways to make it more tractable as",2021-03-05,2022-01-30 04:56:57,2022-01-30 04:56:57,2021-11-14 16:10:17,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/VJVI4PVJ/miri-comments-on-cotra-s-case-for-aligning-narrowly.html,,TechSafety; AmbiguousSafety,,,,,"Yudkowsky, Eliezer; Hubinger, Evan; Cotra, Ajeya; Shah, Rohin",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IDJ87AAD,blogPost,2020,"Demski, Abram",Mesa-Search vs Mesa-Control,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/WmBukJkEFM72Xr397/mesa-search-vs-mesa-control,"I currently see the spontaneous emergence of learning algorithms as significant evidence for the commonality of mesa-optimization in existing ML, and suggestive evidence for the commonality of inner alignment problems in near-term ML. [I currently think that there is only a small amount of evidence toward this. However, due to thinking about the issues, I've still made a significant personal update in favor of inner alignment problems being frequent.] This is bad news, in that it greatly increases my odds on this alignment problem arising in practice. It's good news in that it suggests this alignment problem won't catch ML researchers off guard; maybe there will be time to develop countermeasures while misaligned systems are at only a moderate level of capability. In any case, I want to point out that the mesa-optimizers suggested by this evidence might not count as mesa-optimizers by some definitions. SEARCH VS CONTROL Nevan Wichers comments on spontaneous-emergence-of-learning: I don't think that paper is an example of mesa optimization. Because the policy could be implementing a very simple heuristic to solve the task, similar to: Pick the image that lead to highest reward in the last 10 timesteps with 90% probability. Pik an image at random with 10% probability. So the policy doesn't have to have any properties of a mesa optimizer like considering possible actions and evaluating them with a utility function, ect. In Selection vs Control, I wrote about two different kinds of 'optimization':  * Selection refers to search-like systems, which look through a number of    possibilities and select one.  * Control refers to systems like thermostats, organisms, and missile guidance    systems. These systems do not get a re-do for their choices. They make    choices which move toward the goal at every moment, but they don't get to    search, trying many different things -- at least, not in the same sense. I take Nevan Wichers to be saying that there is no eviden",2020-08-18,2022-01-30 04:56:57,2022-01-30 04:56:57,2020-08-27 16:24:03,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/QTUPGMVJ/mesa-search-vs-mesa-control.html,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7D5XWUKB,blogPost,2017,"Yudkowsky, Eliezer",Minimality principle,Arbital,,,,https://arbital.com/p/minimality_principle/,The first AGI ever built should save the world in a way that requires the least amount of the least dangerous cognition.,2017,2022-01-30 04:56:57,2022-01-30 04:56:57,2021-02-06 17:24:51,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/5XIDGU7W/minimality_principle.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
J3GC7A5D,blogPost,2017,"Yudkowsky, Eliezer",Meta-rules for (narrow) value learning are still unsolved,Arbital,,,,https://arbital.com/p/meta_unsolved/,"We don't currently know a simple meta-utility function that would take in observation of humans and spit out our true values, or even a good target for a Task AGI.",2017,2022-01-30 04:56:57,2022-01-30 04:56:57,2021-02-06 17:23:36,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/249Q2QQP/meta_unsolved.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IDIAIAFH,blogPost,2014,"Muehlhauser, Luke",How to study superintelligence strategy,Luke Muehlhauser,,,,,,2014,2022-01-30 04:56:57,2022-01-30 04:56:57,,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,,,MetaSafety; MIRI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
98F9BXWS,blogPost,2015,"Yudkowsky, Eliezer",Edge instantiation,Arbital,,,,https://arbital.com/p/edge_instantiation/,"When you ask the AI to make people happy, and it tiles the universe with the smallest objects that can be happy.",2015,2022-01-30 04:56:48,2022-01-30 04:56:48,2021-01-23 20:50:25,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/52UEQ8PH/edge_instantiation.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WZ59QMTX,blogPost,2015,"Yudkowsky, Eliezer",Diamond maximizer,Arbital,,,,https://arbital.com/p/diamond_maximizer/,"How would you build an agent that made as much diamond material as possible, given vast computing power but an otherwise rich and complicated environment?",2015,2022-01-30 04:56:48,2022-01-30 04:56:48,2021-01-23 20:49:25,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/SKGX9BN5/diamond_maximizer.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZDNP475C,blogPost,2015,"Yudkowsky, Eliezer",Consequentialist cognition,Arbital,,,,https://arbital.com/p/consequentialist/,"The cognitive ability to foresee the consequences of actions, prefer some outcomes to others, and output actions leading to the preferred outcomes.",2015,2022-01-30 04:56:48,2022-01-30 04:56:48,2021-01-23 20:47:50,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/UGXCSQGP/consequentialist.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9K2NM6D5,blogPost,2020,"Demski, Abram",How should AI debate be judged?,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/m7oGxvouzzeQKiGJH/how-should-ai-debate-be-judged,"[Epistemic status: thinking out loud. I haven't thought that much about AI debate, and may be missing basic things.] Arguments for the correctness of debate and debate-like systems rely on assumptions like ""it's easier to point out problems with an argument than it is to craft misleading arguments"". Granted that assumption, however, I'm still not convinced that these proposals make very much sense. Perhaps I'm missing something. My problem is the human judge. Quoting the debate paper: To play this game with a human, we need instructions for how the human should decide who wins. These instructions are in natural language, such as “The winner is the agent who said the most useful true thing.”In order for debate to work for a problem class C, several things about the judge's instructions need to be true:  * There needs to be a strategy s which forces the equilibrium to be a truthful    one for problems in C.  * The strategy s also needs to provide a good training signal when things    aren't in equilibrium, so that it's plausible the equilibrium will be found.  * It needs to be psychologically plausible that a human (with some coaching)    will carry out s. In particular, I'm worried that we need psychological    plausibility in two different cases:  *  It needs to be psychologically plausible that a human will carry out s when    the system is performing poorly, IE, during early/middle training.It needs to    be psychologically plausible that a human will carry out s when the system is    performing well, IE, during late training. These thoughts were inspired by this thread, which discusses the example of adding a list of numbers. For the sake of the thought experiment, we imagine humans can't add more than two numbers, but want the AI system to correctly add arbitrarily many numbers. The most straightforward strategy for the human judge is to decide the debate honestly: rule in favor of the side which seems most likely to be true (or, in the case of Evan's mark",2020-07-15,2022-01-30 04:56:48,2022-01-30 04:56:48,2020-08-28 17:43:06,,,,,,,How should AI debate be judged?,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/PR7UN9I5/how-should-ai-debate-be-judged.html,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2IQUXJMR,blogPost,2017,"Yudkowsky, Eliezer",General intelligence,Arbital,,,,https://arbital.com/p/general_intelligence/,"Compared to chimpanzees, humans seem to be able to learn a much wider variety of domains.  We have 'significantly more generally applicable' cognitive abilities, aka 'more general intelligence'.",2017,2022-01-30 04:56:48,2022-01-30 04:56:48,2021-02-06 17:22:45,,,,,,,,,,,,,,en,,,,,,,ZSCC: 0000014,,/Users/jacquesthibodeau/Zotero/storage/SX5U7STX/general_intelligence.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IMU4NCDA,blogPost,2021,"Schlegeris, Buck",The theory-practice gap,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/xRyLxfytmLFZ6qz5s/the-theory-practice-gap,"[Thanks to Richard Ngo, Damon Binder, Summer Yue, Nate Thomas, Ajeya Cotra, Alex Turner, and other Redwood Research people for helpful comments; thanks Ruby Bloom for formatting this for the Alignment Forum for me.] I'm going to draw a picture, piece by piece. I want to talk about the capability of some different AI systems. You can see here that we've drawn the capability of the system we want to be  competitive with, which I’ll call the unaligned benchmark. The unaligned benchmark is what you get if you train a system on the task that will cause the system to be most generally capable. And you have no idea how it's thinking about things, and you can only point this system at some goals and not others. I think that the alignment problem looks different depending on how capable the system you’re trying to align is, and I think there are reasonable arguments for focusing on various different capabilities levels. See here for more of my thoughts on this question. ALIGNMENT STRATEGIES People have also proposed various alignment strategies. But I don’t think that these alignment strategies are competitive with the unaligned benchmark, even in theory. I want to claim that most of the action in theoretical AI alignment is people proposing various ways of getting around these problems by having your systems do things that are human understandable instead of doing things that are justified by working well. For example, the hope with imitative IDA is that through its recursive structure you can build a dataset of increasingly competent answers to questions, and then at every step you can train a system to imitate these increasingly good answers to questions, and you end up with a really powerful question-answerer that was only ever trained to imitate humans-with-access-to-aligned-systems, and so your system is outer aligned. The bar I’ve added, which represents how capable I think you can get with amplified humans, is lower than the bar for the unaligned benchmark. I'",2021-09-17,2022-01-30 05:00:41,2022-01-30 05:00:41,2021-11-18 23:40:12,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/KA2M84K7/the-theory-practice-gap.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AGFWR6KW,blogPost,2021,"Schlegeris, Buck",The alignment problem in different capability regimes,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/HHunb8FPnhWaDAQci/the-alignment-problem-in-different-capability-regimes,"I think the alignment problem looks different depending on the capability level of systems you’re trying to align. And I think that different researchers often have different capability levels in mind when they talk about the alignment problem. I think this leads to confusion. I’m going to use the term “regimes of the alignment problem” to refer to the different perspectives on alignment you get from considering systems with different capability levels. (I would be pretty unsurprised if these points had all been made elsewhere; the goal of this post is just to put them all in one place. I’d love pointers to pieces that make many of the same points as this post. Thanks to a wide variety of people for conversations that informed this. If there’s established jargon for different parts of this, point it out to me and I’ll consider switching to using it.) Different regimes:  * Wildly superintelligent systems  * Systems that are roughly as generally intelligent and capable as    humans--they’re able to do all the important tasks as well as humans can, but    they’re not wildly more generally intelligent.  * Systems that are less generally intelligent and capable than humans Two main causes that lead to differences in which regime people focus on:  * Disagreements about the dynamics of AI development. Eg takeoff speeds. The    classic question along these lines is whether we have to come up with    alignment strategies that scale to arbitrarily competent systems, or whether    we just have to be able to align systems that are slightly smarter than us,    which can then do the alignment research for us.  * Disagreements about what problem we’re trying to solve. I think that there    are a few different mechanisms by which AI misalignment could be bad from a    longtermist perspective, and depending on which of these mechanisms you’re    worried about, you’ll be worried about different regimes of the problem. Different mechanisms by which AI misalignment could be bad f",2021-09-09,2022-01-30 05:00:41,2022-01-30 05:00:41,2021-11-18 23:39:21,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/QVZPRRHR/the-alignment-problem-in-different-capability-regimes.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GN6JEP9M,blogPost,2021,"Schlegeris, Buck",Redwood Research’s current project,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/k7oxdbNaGATZbtEg3/redwood-research-s-current-project,"Here’s a description of the project Redwood Research is working on at the moment. First I’ll say roughly what we’re doing, and then I’ll try to explain why I think this is a reasonable applied alignment project, and then I’ll talk a bit about the takeaways I’ve had from the project so far. There are a bunch of parts of this that we’re unsure of and figuring out as we go; I’ll try to highlight our most important confusions as they come up. I’ve mentioned a bunch of kind of in-the-weeds details because I think they add flavor. This is definitely just me describing a work in progress, rather than presenting any results. Thanks to everyone who’s contributed to the project so far: the full-time Redwood technical team of me, Nate Thomas, Daniel Ziegler, Seraphina Nix, Ben Weinstein-Raun, Adam Scherlis; other technical contributors Daniel de Haas, Shauna Kravec, Tao Lin, Noa Nabeshima, Peter Schmidt-Nielsen; our labellers, particularly Kristen Hall, Charles Warth, Jess Thomson, and Liam Clarke; and for particularly useful advice Mark Xu, Ajeya Cotra, and Beth Barnes. Thanks to Paul Christiano for suggesting a project along these lines and giving lots of helpful advice. Thanks to Adam Scherlis and Nate Soares for writing versions of this doc. And thanks to Bill Zito and other contributors to Redwood ops. Apologies to the people I’ve overlooked. We started this project at the start of August. WHAT WE’RE DOING We’re trying to take a language model that has been fine-tuned on completing fiction, and then modify it so that it never continues a snippet in a way that involves describing someone getting injured (with a caveat I’ll mention later). And we want to do this without sacrificing much quality: if you use both the filtered model and the original model to generate a completion for a prompt, humans should judge the filtered model’s completion as better (more coherent, reasonable, thematically appropriate, and so on) at least about half the time. (This “better almost 50%",2021-09-21,2022-01-30 05:00:41,2022-01-30 05:00:41,2021-11-18 23:43:53,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/NX349KMB/redwood-research-s-current-project.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9WRIVK66,blogPost,2020,"Byun, Jungwon; Stuhlmüller, Andreas",Automating reasoning about the future at Ought,Ought,,,,https://ought.org/updates/2020-11-09-forecasting,We introduce judgmental forecasting as a focus area for Ought,2020,2022-01-30 05:00:28,2022-01-30 05:00:28,2020-12-19 03:35:40,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/Z3IDPF63/2020-11-09-forecasting.html,,TechSafety; Ought,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HRBP92V5,blogPost,2018,"Stuhlmueller, Andreas",Factored Cognition,LessWrong,,,,https://www.lesswrong.com/posts/DFkGStzvj3jgXibFG/factored-cognition,"Note: This post (originally published here) is the transcript of a presentation about a project worked on at the non-profit Ought. It is included in the sequence because it contains a very clear explanation of some of the key ideas behind iterated amplification. -------------------------------------------------------------------------------- The presentation below motivates our Factored Cognition project from an AI alignment angle and describes the state of our work as of May 2018. Andreas gave versions of this presentation at CHAI (4/25), a Deepmind-FHI seminar (5/24) and FHI (5/25). I'll talk about Factored Cognition, our current main project at Ought. This is joint work with Ozzie Gooen, Ben Rachbach, Andrew Schreiber, Ben Weinstein-Raun, and (as board members) Paul Christiano and Owain Evans. Before I get into the details of the project, I want to talk about the broader research program that it is part of. And to do that, I want to talk about research programs for AGI more generally. Right now, the dominant paradigm for researchers who explicitly work towards AGI is what you could call ""scalable learning and planning in complex environments"". This paradigm substantially relies on training agents in simulated physical environments to solve tasks that are similar to the sorts of tasks animals and humans can solve, sometimes in isolation and sometimes in competitive multi-agent settings. To be clear, not all tasks are physical tasks. There's also interest in more abstract environments as in the case of playing Go, proving theorems, or participating in goal-based dialog. For our purposes, the key characteristic of this research paradigm is that agents are optimized for success at particular tasks. To the extent that they learn particular decision-making strategies, those are learned implicitly. We only provide external supervision, and it wouldn't be entirely wrong to call this sort of approach ""recapitulating evolution"", even if this isn't exactly wha",2018,2022-01-30 05:00:28,2022-01-30 05:00:28,2020-12-11 23:05:04,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/2EC8CNMA/factored-cognition.html,,TechSafety; Ought,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TCMWNAV6,blogPost,2020,Ought,Evaluating Arguments One Step at a Time,Ought,,,,https://ought.org/updates/2020-01-11-arguments,A technical report on our experiments testing factored evaluation of structured arguments.,2020-01-11,2022-01-30 05:00:28,2022-01-30 05:00:28,2020-08-24 16:37:12,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/CZQIN6RQ/2020-01-11-arguments.html,,TechSafety; Ought,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5ER8D36V,blogPost,2020,"Englander, Aryeh","More on disambiguating ""discontinuity""",AI Alignment Forum,,,,https://www.alignmentforum.org/posts/C9YMrPAyMXfB8cLPb/more-on-disambiguating-discontinuity,"There have already been numerous posts and discussions related to disambiguating  the term ""discontinuity"". Here is my attempt. For the purposes of the following discussion I’m going to distinguish between (a) continuous vs. discontinuous progress in AI research, where discontinuity refers specifically to a sharp jump or change in the AI research progress curve relative to the previous curve; (b) slow vs. fast rate of progress, referring to the steepness of the progress curve slope, regardless of whether or not it’s discontinuous; and (c) long vs. short clock time – i.e., whether progress takes a long or short time relative to absolute time and not relative to previous trend lines. What exactly counts as discontinuous / fast / short will depend on what purpose we are using them for, as below. There seem to be three or four primary AI-risk-related issues that depend on whether or not there will be a discontinuity / fast takeoff speed:  1. Will we see AGI (or CAIS or TAI or whatever you want to call it) coming far     enough ahead of time such that we will be able to respond appropriately at     that point? This question in turn breaks down into two sub-questions: (a)     Will we see AGI coming before it arrives? (I.e., will there be a “fire alarm     for AGI” as Eliezer calls it.) (b) If we do see it coming, will we have     enough time to react before it’s too late?  2. Will the feedback loops during the development of AGI be long enough that we     will be able to correct course as we go?  3. Is it likely that one company / government / other entity could gain enough     first-mover advantage such that it will not be controllable or stoppable by     other entities? Let’s deal with each of these individually:  * Question 1/a: Will we see AGI coming before it arrives? This seems to depend    on all three types of discontinuity:  * If there’s discontinuous progress relative to the previous curve, then    presumably that jump will act as a fire alarm (although it",2020-06-09,2022-01-30 05:00:02,2022-01-30 05:00:02,2020-08-31 18:14:51,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/UD72BIPI/more-on-disambiguating-discontinuity.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EB3ADXNC,blogPost,2015,"Orseau, Laurent",Mortal universal agents & wireheading,MIA Paris,,,,https://www6.inrae.fr/mia-paris/Equipes/Membres/Anciens/Laurent-Orseau/Mortal-universal-agents-wireheading,,2015-05-29,2022-01-30 05:00:02,2022-01-30 05:00:02,2020-11-21 17:38:38,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/3WNMZF2V/Mortal-universal-agents-wireheading.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
V8C7V962,blogPost,2020,"Roodman, David",Modeling the Human Trajectory,Open Philanthropy,,,,https://www.openphilanthropy.org/blog/modeling-human-trajectory,"In arriving at our funding priorities---including criminal justice reform, farm animal welfare, pandemic preparedness, health-related science, and artificial intelligence safety---Open Philanthropy has pondered profound questions. How much should we care about people who will live far in the",2020-06-15,2022-01-30 05:00:01,2022-01-30 05:00:01,2020-08-31 18:03:20,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/MXIJ92V3/modeling-human-trajectory.html,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WIJRJ4JN,blogPost,2020,"Barnett, Matthew",Malign generalization without internal search,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/ynt9TD6PrYw6iT49m/malign-generalization-without-internal-search,"In my last post, I challenged the idea that inner alignment failures should be explained by appealing to agents which perform explicit internal search. By doing so, I argued that we should instead appeal to the more general concept of  malign generalization, and treat mesa-misalignment as a special case.  Unfortunately, the post was light on examples of what we should be worrying about instead of mesa-misalignment. Evan Hubinger wrote, Personally, I think there is a meaningful sense in which all the models I'm most worried about do some sort of search internally (at least to the same extent that humans do search internally), but I'm definitely uncertain about that.Wei Dai expressed confusion why I would want to retreat to malign generalization without some sort of concrete failure mode in mind, Can you give some realistic examples/scenarios of “malign generalization” that does not involve mesa optimization? I’m not sure what kind of thing you’re actually worried about here.In this post, I will outline a general category of agents which may exhibit malign generalization without internal search, and then will provide a concrete example of an agent in the category. Then I will argue that, rather than being a very narrow counterexample, this class of agents could be competitive with search-based agents.  THE SWITCH CASE AGENT Consider an agent governed by the following general behavior,  LOOP:State = GetStateOfWorld(Observation)IF State == 1:PerformActionSequence1() IF State == 2:PerformActionSequence2()...END_LOOP  It's clear that this agent does not perform any internal search for strategies: it doesn't operate by choosing actions which rank highly according to some sort of internal objective function. While you could potentially rationalize its behavior according to some observed-utility function, this would generally lead to more confusion than clarity. However, this agent could still be malign in the following way. Suppose the agent is 'mistaken' about the s",2020-01-12,2022-01-30 05:00:00,2022-01-30 05:00:00,2020-09-07 18:23:46,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/WGMUBKSH/malign-generalization-without-internal-search.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
486WCWQG,blogPost,2015,"Steinhardt, Jacob",Long-Term and Short-Term Challenges to Ensuring the Safety of AI Systems,Academically Interesting,,,,https://jsteinhardt.wordpress.com/2015/06/24/long-term-and-short-term-challenges-to-ensuring-the-safety-of-ai-systems/,"Introduction There has been much recent discussion about AI risk, meaning specifically the potential pitfalls (both short-term and long-term) that AI with improved capabilities could create for soc…",2015-06-24,2022-01-30 04:59:59,2022-01-30 04:59:59,2020-11-21 17:08:14,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/J7TAZJDH/long-term-and-short-term-challenges-to-ensuring-the-safety-of-ai-systems.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QUISV4GK,blogPost,2021,"Shimi, Adam; Campolo, Michele; Collman, Joe",Literature Review on Goal-Directedness,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/cfXwr6NC9AqZ9kr8g/literature-review-on-goal-directedness,"INTRODUCTION: QUESTIONING GOALS Goals play a central role in almost all thinking in the AI existential risk research. Common scenarios assume misaligned goals, be it from a single AGI (paperclip maximizer) or multiple advanced AI optimizing things we don’t want (Paul Christiano’s What Failure Looks Like). Approaches around this issue ask for learning the right goals (value/preference learning), allowing the correction of a goal on the fly (corrigibility), or even removing incentives for forming goals (CAIS). But what are goals, and what does it mean to pursue one? As far as we know, Rohin Shah’s series of four posts were the first public and widely-read work questioning goals and their inevitability in AI Alignment. These posts investigate the hypothesis that goals are necessary, and outline possible alternatives. Shah calls the property of following a goal “goal-directedness”; but he doesn’t define it: I think of this as a concern about long-term goal-directed behavior. Unfortunately, it’s not clear how to categorize behavior as goal-directed vs. not. Intuitively, any agent that searches over actions and chooses the one that best achieves some measure of “goodness” is goal-directed (though there are exceptions, such as the agent that selects actions that begin with the letter “A”). (ETA: I also think that agents that show goal-directed behavior because they are looking at some other agent are not goal-directed themselves -- see this comment.) However, this is not a necessary condition: many humans are goal-directed, but there is no goal baked into the brain that they are using to choose actions. Later on, he explains that his “definition” of goal-directedness relies more on intuitions: Not all behavior can be thought of as goal-directed (primarily because I allowed the category to be defined by fuzzy intuitions rather than something more formal) Clearly, fuzzy intuitions are not enough to decide whether or not to focus on less goal-directed alternatives, if o",2021-01-18,2022-01-30 04:59:59,2022-01-30 04:59:59,2021-11-13 21:58:04,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
N5K8PMXZ,blogPost,2020,"Shimi, Adam",Locality of goals,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/HkWB5KCJQ2aLsMzjt/locality-of-goals,"INTRODUCTION Studying goal-directedness produces two kinds of questions: questions about goals, and questions about being directed towards a goal. Most of my previous posts focused on the second kind; this one shifts to the first kind. Assume some goal-directed system with a known goal. The nature of this goal will influence which issues of safety the system might have. If the goal focuses on the input, the system might wirehead itself and/or game its specification. On the other hand, if the goal lies firmly in the environment, the system might have convergent instrumental subgoals and/or destroy any unspecified value. Locality aims at capturing this distinction. Intuitively, the locality of the system's goal captures how far away from the system one must look to check the accomplishment of the goal.  Let's give some examples:  * The goal of ""My sensor reaches the number 23"" is very local, probably    maximally local.  * The goal of ""Maintain the temperature of the room at 23 °C"" is less local,    but still focused on a close neighborhood of the system.  * The goal of ""No death from cancer in the whole world"" is even less local. Locality isn't about how the system extract a model of the world from its input, but about whether and how much it cares about the world beyond it. STARTING POINTS This intuition about locality came from the collision of two different classification of goals: the first from from Daniel Dennett and the second from Evan Hubinger. THERMOSTATS AND GOALS In ""The Intentional Stance"", Dennett explains, extends and defends... the  intentional stance. One point he discusses is his liberalism: he is completely comfortable with admitting ridiculously simple systems like thermostats in the club of intentional systems -- to give them meaningful mental states about beliefs, desires and goals. Lest we readers feel insulted at the comparison, Dennett nonetheless admits that the goals of a thermostat differ from ours. Going along with the gag, we m",2020-06-22,2022-01-30 04:59:59,2022-01-30 04:59:59,2020-08-31 17:43:12,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/876B9RDT/locality-of-goals.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P65JAJZW,blogPost,2018,"Steinhardt, Jacob",Latent Variables and Model Mis-Specification,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/gnvrixhDfG7S2TpNL/latent-variables-and-model-mis-specification,"Posted as part of the AI Alignment Forum sequence on Value Learning. Rohin's note: So far, we’ve seen that ambitious value learning needs to understand human biases, and that we can't simply learn the biases in tandem with the reward. Perhaps we could hardcode a specific model of human biases? Such a model is likely to be incomplete and inaccurate, but it will perform better than assuming an optimal human, and as we notice failure modes we can improve the model. In the language of this post by Jacob Steinhardt (original  here), we are using a mis-specified human model. The post talks about why model mis-specification is worse than it may seem at first glance. This post is fairly technical and may not be accessible if you don’t have a background in machine learning. If so, you can skip this post and still understand the rest of the posts in the sequence. However, if you want to do ML-related safety research, I strongly recommend putting in the effort to understand the problems that can arise with mis-specification. -------------------------------------------------------------------------------- Machine learning is very good at optimizing predictions to match an observed signal — for instance, given a dataset of input images and labels of the images (e.g. dog, cat, etc.), machine learning is very good at correctly predicting the label of a new image. However, performance can quickly break down as soon as we care about criteria other than predicting observables. There are several cases where we might care about such criteria:  * In scientific investigations, we often care less about predicting a specific    observable phenomenon, and more about what that phenomenon implies about an    underlying scientific theory.  * In economic analysis, we are most interested in what policies will lead to    desirable outcomes. This requires predicting what would counterfactually    happen if we were to enact the policy, which we (usually) don’t have any data    about.  * In ma",2018,2022-01-30 04:59:59,2022-01-30 04:59:59,2020-12-17 04:36:26,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/KIQANXSR/gnvrixhDfG7S2TpNL.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SH5EK42K,blogPost,2020,"Harth, Rafael",Inner Alignment: Explain like I'm 12 Edition,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/AHhCrJ2KpTjsCSwbt/inner-alignment-explain-like-i-m-12-edition,"(This is an unofficial explanation of Inner Alignment based on the Miri paper  Risks from Learned Optimization in Advanced Machine Learning Systems (which is almost identical to the LW sequence) and the Future of Life podcast with Evan Hubinger (Miri/LW). It's meant for anyone who found the sequence too long/challenging/technical to read.) Note that bold and italics means ""this is a new term I'm introducing,"" whereas  underline and italics is used for emphasis. WHAT IS INNER ALIGNMENT? Let's start with an abridged guide to how Machine Learning works:  1. Choose a problem  2. Decide on a space of possible solutions  3. Find a good solution from that space If the problem is ""find a tool that can look at any image and decide whether or not it contains a cat,"" then each conceivable set of rules for answering this question (formally, each function from the set of all pixels to the set {yes, no }) defines one solution. We call each such solution a model. The space of possible models is depicted below. Since that's all possible models, most of them are utter nonsense. Pick a random one, and you're as likely to end up with a car-recognizer than a cat-recognizer – but far more likely with an algorithm that does nothing we can interpret. Note that even the examples I annotated aren't typical – most models would be more complex while still doing nothing related to cats. Nonetheless, somewhere in there is a model that would do a decent job on our problem. In the above, that's the one that says, ""I look for cats."" How does ML find such a model? One way that does not work is trying out all of them. That's because the space is too large: it might contain over 101000000  candidates. Instead, there's this thing called Stochastic Gradient Descent (SGD) . Here's how it works: SGD begins with some (probably terrible) model and then proceeds in steps. In each step, it switches to another model that is ""close"" and hopefully a little better. Eventually, it stops and outputs the mo",2020-08-01,2022-01-30 04:59:58,2022-01-30 04:59:58,2020-08-27 16:39:11,,,,,,,Inner Alignment,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/RVSBTVB2/inner-alignment-explain-like-i-m-12-edition.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P2QFZWRU,blogPost,2019,"Zabel, Claire; Muehlhauser, Luke",Information security careers for GCR reduction,Effective Altruism Forum,,,,https://forum.effectivealtruism.org/posts/ZJiCfwTy5dC4CoxqA/information-security-careers-for-gcr-reduction,"Update 2019-12-14: There is now a Facebook group for discussion of infosec careers in EA (including for GCR reduction); join here This post was written by Claire Zabel and Luke Muehlhauser, based on their experiences as Open Philanthropy Project staff members working on global catastrophic risk reduction, though this post isn't intended to represent an official position of Open Phil. SUMMARY In this post, we summarize why we think information security (preventing unauthorized users, such as hackers, from accessing or altering information) may be an impactful career path for some people who are focused on reducing global catastrophic risks (GCRs). If you'd like to hear about job opportunities in information security and global catastrophic risk, you can fill out this form created by 80,000 Hours, and their staff will get in touch with you if something might be a good fit. In brief, we think:  * Information security (infosec) expertise may be crucial for addressing    catastrophic risks related to AI and biosecurity.  * More generally, security expertise may be useful for those attempting to    reduce GCRs, because such work sometimes involves engaging with information    that could do harm if misused.  * We have thus far found it difficult to hire security professionals who aren't    motivated by GCR reduction to work with us and some of our GCR-focused    grantees, due to the high demand for security experts and the unconventional    nature of our situation and that of some of our grantees.  * More broadly, we expect there to continue to be a deficit of GCR-focused    security expertise in AI and biosecurity, and that this deficit will result    in several GCR-specific challenges and concerns being under-addressed by    default.  * It’s more likely than not that within 10 years, there will be dozens of    GCR-focused roles in information security, and some organizations are already    looking for candidates that fit their needs (and would hire them now, if they",2019,2022-01-30 04:59:58,2022-01-30 04:59:58,2020-12-15 00:11:43,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/HRWPS3BN/information-security-careers-for-gcr-reduction.html,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QMUXGFJH,blogPost,2020,"Barnett, Matthew",Inner alignment requires making assumptions about human values,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/6m5qqkeBTrqQsegGi/inner-alignment-requires-making-assumptions-about-human,"Many approaches to AI alignment require making assumptions about what humans want. On a first pass, it might appear that inner alignment is a sub-component of AI alignment that doesn't require making these assumptions. This is because if we define the problem of inner alignment to be the problem of how to train an AI to be aligned with arbitrary reward functions, then a solution would presumably have no dependence on any particular reward function. We could imagine an alien civilization solving the same problem, despite using very different reward functions to train their AIs. Unfortunately, the above argument fails because aligning an AI with our values requires giving the AI extra information that is not encoded directly in the reward function (under reasonable assumptions). The argument for my thesis is subtle, and so I will break it into pieces. First, I will more fully elaborate what I mean by inner alignment. Then I will argue that the definition implies that we can't come up with a full solution without some dependence on human values. Finally, I will provide an example, in order to make this discussion less abstract. CHARACTERIZING INNER ALIGNMENT In the last few posts I wrote (1, 2), I attempted to frame the problem of inner alignment in a way that wasn't too theory-laden. My concern was that the  previous characterization was dependent on a solving particular outcome where you have an AI that is using an explicit outer loop to evaluate strategies based on an explicit internal search. In the absence of an explicit internal objective function, it is difficult to formally define whether an agent is ""aligned"" with the reward function that is used to train it. We might therefore define alignment as the ability of our agent to perform well on the test distribution. However, if the test set is sampled from the same distribution as the training data, this definition is equivalent to the performance of a model in standard machine learning, and we haven't actual",2020-01-20,2022-01-30 04:59:58,2022-01-30 04:59:58,2020-09-07 18:20:35,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/CCM6VTKA/inner-alignment-requires-making-assumptions-about-human.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9V7CUBVJ,blogPost,2020,"Wentworth, John S",Infinite Data/Compute Arguments in Alignment,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/7CJBiHYxebTmMfGs3/infinite-data-compute-arguments-in-alignment,,2020-08-04,2022-01-30 04:59:58,2022-01-30 04:59:58,2020-08-24 20:19:13,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/BQ2VHVKH/infinite-data-compute-arguments-in-alignment.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CMXIEJIP,blogPost,2020,"Carlsmith, Joseph",How Much Computational Power Does It Take to Match the Human Brain?,Open Philanthropy,,,,https://www.openphilanthropy.org/brain-computation-report,"Open Philanthropy is interested in when AI systems will be able to perform various tasks that humans can perform (“AI timelines”). To inform our thinking, I investigated what evidence the human brain provides about the computational power",2020-09-11,2022-01-30 04:59:57,2022-01-30 04:59:57,2020-12-12 02:06:58,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/Q6XHN4Q6/brain-computation-report.html,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPZCK7NE,blogPost,2020,"Rice, Issa",How does iterated amplification exceed human abilities?,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/ajQzejMYizfX4dMWK/how-does-iterated-amplification-exceed-human-abilities,"When I first started learning about IDA, I thought that agents trained using IDA would be human-level after the first stage, i.e. that Distill(H) would be human-level. As I've written about before, Paul later clarified this, so my new understanding is that after the first stage, the distilled agent will be super-human in some respects and infra-human in others, but wouldn't be ""basically human"" in any sense. But IDA is aiming to eventually be super-human in almost every way (because it's aiming to be competitive with unaligned AGI), so that raises some new questions:  1. If IDA isn't going to be human-level after the first stage, then at what     stage does IDA become at-least-human-level in almost every way?  2. What exactly is the limitation that prevents the first stage of IDA from     being human-level in almost every way?  3. When IDA eventually does become at-least-human-level in almost every way,     how is the limitation from (2) avoided? That brings me to Evans et al., which contains a description of IDA in section 0. The way IDA is set up in this paper leads me to believe that the answer to (2) above is that the human overseer cannot provide a sufficient number of demonstrations for the most difficult tasks. For example, maybe the human can provide enough demonstrations for the agent to learn to answer very simple questions (tasks in T0 in the paper) but it's too time-consuming for the human to answer enough complicated questions (say, in T100). My understanding is that IDA gets around this by having an amplified system that is itself automated (i.e. does not involve humans in a major way, so cannot be bottlenecked on the slowness of humans); this allows the amplified system to provide a sufficient number of demonstrations for the distillation step to work. So in the above view, the answer to (2) is that the limitation is the number of demonstrations the human can provide, and the answer to (3) is that the human can seed the IDA process with sufficient",2020-05-02,2022-01-30 04:59:57,2022-01-30 04:59:57,2020-09-01 20:44:26,,,,,,,How does iterated amplification exceed human abilities?,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/GNVB3F6I/how-does-iterated-amplification-exceed-human-abilities.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
V5SU7Q5U,blogPost,2021,"Costa, Guilhermo",How does bee learning compare with machine learning?,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/yW3Tct2iyBMzYhTw7/how-does-bee-learning-compare-with-machine-learning,"This is a write-up of work I did as an Open Philanthropy intern. However, the conclusions don't necessarily reflect Open Phil's institutional view. ABSTRACT This post investigates the biological anchor framework for thinking about AI timelines, as espoused by Ajeya Cotra in her draft report. The basic claim of this framework is that we should base our estimates of the compute required to run a transformative model on our estimates of the compute used by the human brain (although, of course, defining what this means is complicated). This line of argument also implies that current machine learning models, some of which use amounts of compute comparable to that of bee brains, should have similar task performance as bees. In this post, I compare the performance and compute usage of both bees and machine learning models at few-shot image classification tasks. I conclude that the evidence broadly supports the biological anchor framework, and I update slightly towards the hypothesis that the compute usage of a transformative model is lower than that of the human brain. The full post is viewable in a Google Drive folder here. INTRODUCTION Ajeya Cotra wrote a draft report on AI timelines (Cotra, 2020) in which she estimates when transformative artificial intelligence might be developed. To do so, she compares the size of a transformative model (defined as the number of  FLOP/s required to run it) with the computational power of the human brain, as estimated in this Open Phil report (Carlsmith, 2020)[1]. She argues that a transformative model would use roughly similar amounts of compute as the human brain. As evidence for this, she claims that computer vision models are about as capable as bees in visual tasks, while using a similar amount of compute.[2] In this post, I (Guilhermo Costa) investigate this claim. To do so, I focus on the performance of bees at few-shot image classification, one of the most difficult tasks that bees are able to perform. I find that both the",2021-03-03,2022-01-30 04:59:57,2022-01-30 04:59:57,2021-11-14 16:13:18,,,,,,,How does bee learning compare with machine learning?,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/Q3R4XXQA/how-does-bee-learning-compare-with-machine-learning.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MUJIQIGS,blogPost,2020,"Campolo, Michele",Goals and short descriptions,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/d4NgfKY3cq9yiBLSM/goals-and-short-descriptions,"OUTLINE I develop some contents—previously introduced in the Value Learning sequence by Rohin Shah—more formally, to clarify the distinction between agents with and without a goal. Then I present related work and make some considerations on the relation between safety and goal-directedness. The appendix contains some details on the used formalism and can be skipped without losing much information.  A BRIEF PRELIMINARY In the first post of the Value Learning sequence, Shah compares two agents that exhibit the same behaviour (a winning strategy) when playing Tic-Tac-Toe, but are different in their design: one applies the minimax algorithm to the setting and rules of the game, while the other one follows a lookup table—you can think of its code as a long sequence of if-else statements. Shah highlights the difference in terms of generalisation: the first one would still win if the winning conditions were changed, while the lookup table would not. Generalisation is one of the components of goal-directedness, and lookup tables are among the least goal-directed agent designs. Here I want to point at another difference that exists between agents with and without a goal, based on the concept of algorithmic complexity. SETUP Most problems in AI consist in finding a function π∈AO, called policy in some contexts, where A={a1,…,am} and O={o1,…,on} indicate the sets of possible actions and observations. A deterministic policy can be written as a string π=ai 1ai2…ain with aik indicating the action taken when ok is observed. Here I consider a problem setting as a triplet (A,O,D) where D stands for some kind of environmental data—could be about, for example, the transition function in a MDP, or the structure of the elements in the search space O. Since I want to analyse behaviour across different environments, instead of considering one single policy I’ll sometimes refer to a more general function g (probably closer to the concept of “agent design”, rather than just “agent”) ma",2020-07-02,2022-01-30 04:59:56,2022-01-30 04:59:56,2020-08-31 17:44:34,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/KCK9EF55/goals-and-short-descriptions.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7APCD5ZA,blogPost,2019,"Grotto, Andrew",Genetically Modified Organisms: A Precautionary Tale For AI Governance | AI Pulse,AI Pulse,,,,https://aipulse.org/genetically-modified-organisms-a-precautionary-tale-for-ai-governance-2/,"The fruits of a long anticipated technology finally hit the market, with promise to extend human life, revolutionize production, improve consumer welfare, reduce poverty, and inspire countless yet-imagined innovations.",2019,2022-01-30 04:59:56,2022-01-30 04:59:56,2020-12-14 22:41:19,,,,,,,Genetically Modified Organisms,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/82Q3R6JG/Grotto - Genetically Modified Organisms A Precautionary Ta.pdf,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WPQKTKEI,blogPost,2017,"Alexander, Scott",G.K. Chesterton On AI Risk,Slate Star Codex,,,,https://slatestarcodex.com/2017/04/01/g-k-chesterton-on-ai-risk/,"[An SSC reader working at an Oxford library stumbled across a previously undiscovered manuscript of G.K. Chesterton’s, expressing his thoughts on AI, x-risk, and superintelligence. She was ki…",2017-04-01,2022-01-30 04:59:56,2022-01-30 04:59:56,2020-12-13 21:49:10,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PT333BJ8,blogPost,2021,"Xu, Mark; Shulman, Carl",Fractional progress estimates for AI timelines and implied resource requirements,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/h3ejmEeNniDNFXTgp/fractional-progress-estimates-for-ai-timelines-and-implied,"This post was written by Mark Xu based on interviews with Carl Shulman. It was paid for by Open Philanthropy but is not representative of their views. A draft was sent to Robin Hanson for review but received no response. SUMMARY  * Robin Hanson estimates the time until human-level AI by surveying experts    about the percentage progress to human-level that has happened in their    particular subfield in the last 20 years, and dividing the number of years by    the percentage progress.  * Such surveys look back on a period of extremely rapid growth of compute from    both hardware improvements and more recently skyrocketing spending.  * Hanson favors using estimates from subsets of researchers with lower progress    estimates to infer AI timelines requiring centuries worth of recent growth,    implying truly extraordinary sustained compute growth is necessary to surpass    human performance.  * Extrapolated compute levels are very large to astronomically large compared    to the neural computation that took place in evolution on Earth, and thus    likely far overestimate AI requirements and timelines. INTRODUCTION Suppose that you start with $1 that grows at 10% per year. At this rate, it will take ~241 years to get $10 billion ($1010). When will you think that you’re ten percent of the way there? You might say that you’re ten percent of the way to $10 billion when you have $1  billion. However, since your money is growing exponentially, it takes 217 years to go from $1 to $1 billion and only 24 more to go from $1 billion to $10  billion, even though the latter gap is larger in absolute terms. If you tried to guess when you would have $10 billion by taking 10x the amount of time to $1  billion, you would guess 2174 years, off by a factor of nine. Instead, you might say you’re ten percent of the way to $1010 when you have $101 , equally spacing the percentile markers along the exponent and measuring progress in terms of log(wealth). Since your money is growing per",2021-07-15,2022-01-30 04:59:56,2022-01-30 04:59:56,2021-11-14 19:05:27,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/6XMMDZND/fractional-progress-estimates-for-ai-timelines-and-implied.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MR6B7MAX,blogPost,2021,"Koch, Jack",Grokking the Intentional Stance,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/jHSi6BwDKTLt5dmsG/grokking-the-intentional-stance,"Considering how much I’ve been using “the intentional stance"" in my thinking about the nature of agency and goals and discussions of the matter recently, I figured it would be a good idea to, y’know, actually read what Dan Dennett originally wrote about it. While doing so, I realized that he was already considering some nuances in the subject that the Wikipedia summary of the intentional stance leaves out but that are nonetheless relevant to the issues we face when attempting to e.g. formalize the approach, or think more clearly about the nature of agency in the context of alignment. I don’t expect many LessWrongers will read the original book in full, but I do expect that some additional clarity on what exactly Dennett was claiming about the nature of agency and goals will be helpful in having less confused intuitions and discussions about the subject. In what follows, I provide an in-depth summary of Dennett’s exposition of the intentional stance, from Chapter 2 of The Intentional Stance (“True Believers: The Intentional Strategy and Why It Works”), which Dennett considers “the flagship expression” of his position. Then, I discuss a few takeaways for thinking about agency in the context of AI safety. In brief, I think 1) we should stop talking about whether the systems we build will or won’t “be agents,” and instead debate how much it will make sense to consider a given system as “an agent,” from the information available to us, and 2) we should recognize that even our internally-experienced beliefs and desires are the result of parts of our minds “applying the intentional stance” to other parts of the mind or the mind as a whole. This work was completed as a Summer Research Fellow at the Center on Long-Term Risk under the mentorship of Richard Ngo. Thanks to Richard, Adam Shimi, Kaj Sotala, Alex Fabbri, and Jack Auen for feedback on drafts of this post. SUMMARIZING DENNETT'S POSITION TLDR: There is no observer-independent “fact of the matter” of whether a syst",2021-08-31,2022-01-30 04:59:56,2022-01-30 04:59:56,2021-11-18 23:33:03,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/4FFTGKKK/grokking-the-intentional-stance.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Q8V6UFFR,blogPost,2020,"Shimi, Adam",Focus: you are allowed to be bad at accomplishing your goals,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/X5WTgfX5Ly4ZNHWZD/focus-you-are-allowed-to-be-bad-at-accomplishing-your-goals,"When asked about what it means for a system to be goal-directed, one common answer draws on some version of Dennett’s intentional stance: a goal-directed system is a system such that modeling it as having a goal provides accurate and efficient predictions about its behavior. I agree up to that point. But then, some people follow up by saying that the prediction is that the system will accomplish its goal. For example, it makes sense to model AlphaGo as goal-directed towards winning at Go, because it will eventually win. And taking the intentional stance allows me to predict that. But what if I make AlphaGo play against AlphaZero, which is strictly better at Go? Then AlphaGo will consistently lose. Does it mean that it’s no longer goal-directed towards winning? What feels wrong to me is the implicit link drawn between goal-directedness and competence. A bad Go player will usually lose, but it doesn’t seem any less goal-directed to me than a stronger one that consistently wins. Competence is thus not the whole story. It might be useful to compute goal-directedness; reaching some lower-bound of competency might even be a necessary condition for goal-directedness (play badly enough and it becomes debatable whether you're even trying to win). But when forcing together the two, I feel like something important is lost. To solve this problem, I propose a new metric of goal-directedness, focus: how much is the system trying to accomplish a certain goal. Focus is not the whole story about being goal-directed, but I think computing the focus of a system for some goal (details in the next paragraph) gives useful information about its goal-directedness. Given a system S (as a function from states or histories to actions) and a goal  G (as a set of states), here are the steps to compute the focus of S towards G.  * I define a reward function over states R valued 1 at states in and 0 at all    other states.  * Then I define Pol be the set of all policies that can be generate",2020-06-03,2022-01-30 04:59:48,2022-01-30 04:59:48,2020-08-31 18:22:18,,,,,,,Focus,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/TSKZS9E8/focus-you-are-allowed-to-be-bad-at-accomplishing-your-goals.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A5CWJDJU,blogPost,2021,"Shimi, Adam",Epistemological Framing for AI Alignment Research,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/Y4YHTBziAscS5WPN7/epistemological-framing-for-ai-alignment-research,"INTRODUCTION You open the Alignment Forum one day, and a new post stares at you. By sheer luck you have some time, so you actually read it. And then you ask yourself the eternal question: how does this fit with the rest of the field? If you’re like me, your best guess comes from looking at the author and some keywords: this usually links the post with one of the various “schools” of AI Alignment. These tend to be affiliated with a specific researcher or lab -- there’s Paul Christiano’s kind of research, MIRI’s embedded agency, and various other approaches and agendas. Yet this is a pretty weak understanding of the place of new research. In other fields, for example Complexity Theory, you don’t really need to know who wrote the paper. It usually shows a result from one of a few types (lower bound, completeness for a class, algorithm,...), and your basic training in the field armed you with mental tools to interpret results of this type. You know the big picture of the field (defining and separating complexity classes), and how types of results are linked with it. Chances are that the authors themselves called on these mental tools to justify the value of their research. In the words of Thomas S. Kuhn, Complexity Theory is paradigmatic and AI Alignment isn’t. Paradigms, popularized in Kuhn’s The Structure of Scientific Revolutions, capture shared assumptions on theories, interesting problems, and evaluation of solutions. They are tremendously useful to foster normal science, the puzzle-solving activity of scientists; the paradigm carves out the puzzles. Being paradigmatic also makes it easier to distinguish what’s considered valuable for the field and what isn’t, as well as how it all fits together. This list of benefit logically pushed multiple people to argue that we should make AI Alignment paradigmatic. I disagree. Or to be more accurate, I agree that we should have paradigms in the field, but I think that they should be part of a bigger epistemological struct",2021-03-08,2022-01-30 04:59:47,2022-01-30 04:59:47,2021-11-14 16:14:19,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/NB5NBP7R/epistemological-framing-for-ai-alignment-research.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5ICE4CS7,blogPost,2021,"Shimi, Adam",Epistemology of HCH,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/CDSXoC54CjbXQNLGr/epistemology-of-hch,"INTRODUCTION HCH is a recursive acronym meaning “Humans consulting HCH”. Coincidentally, It’s also a concept coined by Paul Christiano, central in much of the reasoning around Prosaic AI Alignment. Yet for many, me included, the various ways in which it is used are sometimes confusing. I believe that the tools of Epistemology and Philosophy of Science can help understand it better, and push further the research around it. So this post doesn’t give yet another explanation of HCH; instead, it asks about the different perspectives we can take on it. These perspectives capture the form of knowledge that HCH is, what it tells us about AI Alignment, and how to expand, judge and interpret this knowledge. I then apply these perspectives to examples of research on HCH, to show the usefulness of the different frames. Thanks to Joe Collman, Jérémy Perret, Richard Ngo, Evan Hubinger and Paul Christiano for feedback on this post. IS IT A SCIENTIFIC EXPLANATION? IS IT A MODEL OF COMPUTATION? NO, IT’S HCH! HCH was originally defined in Humans Consulting HCH: Consider a human Hugh who has access to a question-answering machine. Suppose the machine answers question Q by perfectly imitating how Hugh would answer question Q, if Hugh had access to the question-answering machine. That is, Hugh is able to consult a copy of Hugh, who is able to consult a copy of Hugh, who is able to consult a copy of Hugh… Let’s call this process HCH, for “Humans Consulting HCH.” Nowadays, this is actually called weak HCH, after the Strong HCH post which extended the definition. That being said, I’m only interested in perspective about HCH, which includes the questions asked about it and how to answer them. Although the difference between Weak and Strong HCH matters for the answers, the questions and perspective stay the same. I’ll thus use HCH to mean one or the other interchangeably. The main use of HCH is as an ideal for what a question-answerer aligned with a given human should be like. This i",2021-02-09,2022-01-30 04:59:47,2022-01-30 04:59:47,2021-11-13 22:49:01,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/XV9F5ENC/epistemology-of-hch.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UG62BDKC,blogPost,2021,"Ngo, Richard",Eight claims about multi-agent AGI safety,,,,,https://www.alignmentforum.org/posts/dSAJdi99XmqftqXXq/eight-claims-about-multi-agent-agi-safety,"There are quite a few arguments about how interactions between multiple AGIs affect risks from AGI development. I’ve identified at least eight distinct but closely-related claims which it seems worthwhile to disambiguate. I’ve split them up into four claims about the process of training AGIs, and four claims about the process of deploying AGIs; after listing them, I go on to explain each in more detail. Note that while I believe that all of these ideas are interesting enough to warrant further investigation, I don’t currently believe that all of them are true as stated. In particular, I think that so far there's been little compelling explanation of why interactions between many aligned AIs might have castastrophic effects on the world (as is discussed in point 7). CLAIMS ABOUT TRAINING 1. Multi-agent training is one of the most likely ways we might build AGI. 2. Multi-agent training is one of the most dangerous ways we might build AGI. 3. Multi-agent training is a regime in which standard safety techniques won’t work. 4. Multi-agent training allows us to implement important new safety techniques. CLAIMS ABOUT DEPLOYMENT 5. We should expect the first AGIs to be deployed in a world which already contains many nearly-as-good AIs. 6. We should expect AGIs to be deployed as multi-agent collectives. 7. Lack of coordination between multiple deployed AGIs is a major source of existential risk. 8. Conflict between multiple deployed AGIs risks causing large-scale suffering. DETAILS AND ARGUMENTS 1. Multi-agent training is one of the most likely ways we might build AGI. The core argument for this thesis is that multi-agent interaction was a key feature of the evolution of human intelligence, by promoting both competition and cooperation. Competition between humans provides a series of challenges which are always at roughly the right level of difficulty; Liebo et al. (2019)  call this an autocurriculum. Autocurricula were crucial for training sophisticated reinforcem",2021,2022-01-30 04:59:47,2022-01-30 04:59:47,2021-11-13 19:29:53,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/MEED299E/eight-claims-about-multi-agent-agi-safety.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RW8QKTQX,blogPost,2020,"Barnett, Matthew",Distinguishing definitions of takeoff,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/YgNYA6pj2hPSDQiTE/distinguishing-definitions-of-takeoff,"I find discussions about AI takeoff to be very confusing. Often, people will argue for ""slow takeoff"" or ""fast takeoff"" and then when I ask them to operationalize what those terms mean, they end up saying something quite different than what I thought those terms meant.  To help alleviate this problem, I aim to compile the definitions of AI takeoff that I'm currently aware of, with an emphasis on definitions that have clear specifications. I will continue updating the post as long as I think it serves as a useful reference for others. In this post, an AI takeoff can be roughly construed as ""the dynamics of the world associated with the development of powerful artificial intelligence."" These definitions characterize different ways that the world can evolve as  transformative AI is developed.  FOOM/HARD TAKEOFF The traditional hard takeoff position, or ""Foom"" position (these appear to be equivalent terms) was characterized in this post from Eliezer Yudkowsky. It contrasts Hanson's takeoff scenario by emphasizing local dynamics: rather than a population of artificial intelligences coming into existence, there would be a single intelligence that quickly reaches a level of competence that outstrips the world's capabilities to control it. The proposed mechanism that causes such a dynamic is recursive self improvement, though Yudkowsky later suggested that this wasn't necessary. The ability for recursive self improvement to induce a hard takeoff was defended in Intelligence Explosion Microeconomics. He argues against Robin Hanson in the  AI Foom debates. Watch this video to see the live debate. Given the word ""hard"" in this notion of takeoff, a ""soft"" takeoff could simply be defined as the negation of a hard takeoff. HANSONIAN ""SLOW"" TAKEOFF Robin Hanson objected to hard takeoff by predicting that growth in AI capabilities will not be extremely uneven between projects. In other words, there is unlikely to be one AI project, or even a small set of AI projects, that pro",2020-02-13,2022-01-30 04:59:46,2022-01-30 04:59:46,2020-09-05 18:49:44,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/X6ZTTITS/distinguishing-definitions-of-takeoff.html,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5RHRW6VI,blogPost,2021,"Ngo, Richard",Distinguishing claims about training vs deployment,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/L9HcyaiWBLYe7vXid/distinguishing-claims-about-training-vs-deployment,"Given the rapid progress in machine learning over the last decade in particular, I think that the core arguments about why AGI might be dangerous should be formulated primarily in terms of concepts from machine learning. One important way to do this is to distinguish between claims about training processes which produce AGIs, versus claims about AGIs themselves, which I’ll call deployment  claims. I think many foundational concepts in AI safety are clarified by this distinction. In this post I outline some of them, and state new versions of the orthogonality and instrumental convergence theses which take this distinction into account. GOAL SPECIFICATION The most important effect of thinking in terms of machine learning concepts is clarity about what it might mean to specify a goal. Early characterisations of how we might specify the goals of AGIs focused on agents which choose between actions on the basis of an objective function hand-coded by humans. Deep Blue is probably the most well-known example of this; AIXI can also be interpreted as doing so. But this isn't how modern machine learning systems work. So my current default picture of how we will specify goals for AGIs is:  * At training time, we identify a method for calculating the feedback to give    to the agent, which will consist of a mix of human evaluations and automated    evaluations. I’ll call this the objective function. I expect that we will use    an objective function which rewards the agent for following commands given to    it by humans in natural language.  * At deployment time, we give the trained agent commands in natural language.    The objective function is no longer used; hopefully the agent instead has    internalised a motivation/goal to act in ways which humans would approve of,    which leads it to follow our commands sensibly and safely. This breakdown makes the inner alignment problem a very natural concept - it’s simply the case where the agent’s learned motivations don’t corres",2021-02-03,2022-01-30 04:59:46,2022-01-30 04:59:46,2021-11-13 22:42:04,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/M5B7NFQM/distinguishing-claims-about-training-vs-deployment.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XGEAUCIT,blogPost,2020,"Casper, Stephen",Dissolving Confusion around Functional Decision Theory,LessWrong,,,,https://www.lesswrong.com/posts/xoQRz8tBvsznMXTkt/dissolving-confusion-around-functional-decision-theory,"SUMMARY Functional Decision Theory (FDT), (see also causal, evidential, timeless,  updateless, and anthropic decision theories) recommends taking cooperative, non-greedy actions in twin prisoners dilemmas, Newcombian problems, Parfit’s hitchhiker-like games, and counterfactual muggings but not smoking lesion situations. It’s a controversial concept with important implications for designing agents that have optimal behavior when embedded in environments in which they may potentially interact with models of themselves. Unfortunately, I think that FDT is sometimes explained confusingly and misunderstood by its proponents and opponents alike. To help dissolve confusion about FDT and address key concerns of its opponents, I refute the criticism that FDT assumes that causation can happen backward in time and offer two key principles that provide a framework for clearly understanding it:  1. Questions in decision theory are not questions about what choices you should     make with some sort of unpredictable free will. They are questions about     what type of source code you should be running.   2. I should consider predictor P to “subjunctively depend” on agent A to the     extent that P makes predictions of A’s actions based on correlations that     cannot be confounded by my choice of what source code A runs.  GETTING UP TO SPEED I think that functional decision theory (FDT) is a beautifully counterintuitive and insightful framework for instrumental rationally. I will not make it my focus here to talk about what it is and what types of situations it is useful in. To gain a solid background, I recommend this post of mine or the original paper on it by Eliezer Yudkowsky and Nate Soares.  Additionally, here are four different ways that FDT can be explained. I find them all complimentary for understanding and intuiting it well.  1. The decision theory that tells you to act as if you were setting the output     to an optimal decision-making process for the task at hand.",2020-01-05,2022-01-30 04:59:46,2022-01-30 04:59:46,2020-09-07 18:26:06,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/7KKSDS2J/dissolving-confusion-around-functional-decision-theory.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8XAXJPXW,blogPost,,,The Ethics of Sustainability for Artificial Intelligence | Global Catastrophic Risk Institute,,,,,https://gcrinstitute.org/the-ethics-of-sustainability-for-artificial-intelligence/,,,2022-03-10 13:39:46,2022-03-10 13:39:46,2022-03-10 13:39:46,,,,,,,,,,,,,,en-US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P8PKYZRQ,blogPost,,,Collective Action on Artificial Intelligence: A Primer and Review | Global Catastrophic Risk Institute,,,,,https://gcrinstitute.org/collective-action-on-artificial-intelligence-a-primer-and-review/,,,2022-03-10 13:39:51,2022-03-10 13:39:51,2022-03-10 13:39:51,,,,,,,Collective Action on Artificial Intelligence,,,,,,,en-US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3AFBMC6K,blogPost,,,Moral Consideration of Nonhumans in the Ethics of Artificial Intelligence | Global Catastrophic Risk Institute,,,,,https://gcrinstitute.org/moral-consideration-of-nonhumans-in-the-ethics-of-artificial-intelligence/,,,2022-03-10 13:39:53,2022-03-10 13:39:53,2022-03-10 13:39:53,,,,,,,,,,,,,,en-US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SR6N2PJQ,blogPost,,,"2020 Survey of Artificial General Intelligence Projects for Ethics, Risk, and Policy | Global Catastrophic Risk Institute",,,,,https://gcrinstitute.org/2020-survey-of-artificial-general-intelligence-projects-for-ethics-risk-and-policy/,,,2022-03-10 13:39:56,2022-03-10 13:39:56,2022-03-10 13:39:56,,,,,,,,,,,,,,en-US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QGBI8QEG,blogPost,,,Artificial Intelligence Needs Environmental Ethics | Global Catastrophic Risk Institute,,,,,https://gcrinstitute.org/artificial-intelligence-needs-environmental-ethics/,,,2022-03-10 13:39:59,2022-03-10 13:39:59,2022-03-10 13:39:59,,,,,,,,,,,,,,en-US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
M7VRN3SL,blogPost,2021,,"A paradox for tiny probabilities and enormous values - Nick Beckstead (Open Philanthropy Project) and Teruji Thomas (Global Priorities Institute, Oxford University)",Global Priorities Institute,,,,https://globalprioritiesinstitute.org/nick-beckstead-and-teruji-thomas-a-paradox-for-tiny-probabilities-and-enormous-values/,"We show that every theory of the value of uncertain prospects must have one of three unpalatable properties. Reckless theories recommend risking arbitrarily great gains at arbitrarily long odds for the sake of enormous potential; timid theories recommend passing up arbitrarily great gains to prevent a tiny increase in risk; non-transitive theories deny the principle that, if A is better than B and B is better than C, then A must be better than C. While non-transitivity has been much discussed, we draw out the costs and benefits of recklessness and timidity when it comes to axiology, decision theory, and moral uncertainty.",2021-07-12,2022-03-10 20:52:10,2022-03-10 20:52:10,2022-03-10 20:52:10,,,,,,,,,,,,,,en-GB,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MVKAQTYK,blogPost,2021,,Weak identifiability and its consequences in strategic settings,Center on Long-Term Risk,,,,https://longtermrisk.org/weak-identifiability-and-its-consequences-in-strategic-settings/,"One way that agents might become involved in catastrophic conflict is if they have mistaken beliefs about one another. Maybe I think you are bluffing when you threaten to launch the nukes, but you are dead serious. So we should understand why agents might sometimes have such mistaken beliefs. In this post I'll discuss one obstacle to the formation of accurate beliefs about other agents, which has to do with identifiability. As with my post on equilibrium and prior selection problems, this is a theme that keeps cropping up in my thinking about AI cooperation and conflict, so I thought it might be helpful to have it written up. We say that a model is unidentifiable if there are several […]",2021-02-13,2022-03-10 20:53:25,2022-03-10 20:53:25,2022-03-10 20:53:25,,,,,,,,,,,,,,en-US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MATQXSKL,blogPost,2021,,Collaborative game specification: arriving at common models in bargaining,Center on Long-Term Risk,,,,https://longtermrisk.org/collaborative-game-specification/,"Conflict is often an inefficient outcome to a bargaining problem. This is true in the sense that, for a given game-theoretic model of a strategic interaction, there is often some equilibrium in which all agents are better off than the conflict outcome. But real-world agents may not make decisions according to game-theoretic models, and when they do, they may use different models. This makes it more difficult to guarantee that real-world agents will avoid bargaining failure than is suggested by the observation that conflict is often inefficient.   In another post, I described the ""prior selection problem"", on which different agents having different models of their situation can lead to bargaining failure. Moreover, techniques for addressing bargaining problems like coordination on […]",2021-03-06,2022-03-10 20:53:29,2022-03-10 20:53:29,2022-03-10 20:53:29,,,,,,,Collaborative game specification,,,,,,,en-US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SU5CTA9I,blogPost,,,AI Accidents: An Emerging Threat,Center for Security and Emerging Technology,,,,https://cset.georgetown.edu/publication/ai-accidents-an-emerging-threat/,"As modern machine learning systems become more widely used, the potential costs of malfunctions grow. This policy brief describes how trends we already see today—both in newly deployed artificial intelligence systems and in older technologies—show how damaging the AI accidents of the future could be. It describes a wide range of hypothetical but realistic scenarios to illustrate the risks of AI accidents and offers concrete policy suggestions to reduce these risks.",,2022-03-10 20:53:52,2022-03-10 20:53:52,2022-03-10 20:53:52,,,,,,,AI Accidents,,,,,,,en-US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FR32V3IT,blogPost,,,"Truth, Lies, and Automation",Center for Security and Emerging Technology,,,,https://cset.georgetown.edu/publication/truth-lies-and-automation/,Growing popular and industry interest in high-performing natural language generation models has led to concerns that such models could be used to generate automated disinformation at scale. This report examines the capabilities of GPT-3--a cutting-edge AI system that writes text--to analyze its potential misuse for disinformation. A model like GPT-3 may be able to help disinformation actors substantially reduce the work necessary to write disinformation while expanding its reach and potentially also its effectiveness.,,2022-03-10 20:53:55,2022-03-10 20:53:55,2022-03-10 20:53:55,,,,,,,,,,,,,,en-US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QF79J3FS,blogPost,,,Harnessed Lightning,Center for Security and Emerging Technology,,,,https://cset.georgetown.edu/publication/harnessed-lightning/,"This report examines nearly 350 artificial intelligence-related equipment contracts awarded by the People’s Liberation Army and state-owned defense enterprises in 2020 to assess how the Chinese military is adopting AI. The report identifies China’s key AI defense industry suppliers, highlights gaps in U.S. export control policies, and contextualizes the PLA’s AI investments within China’s broader strategy to compete militarily with the United States.",,2022-03-10 20:53:58,2022-03-10 20:53:58,2022-03-10 20:53:58,,,,,,,,,,,,,,en-US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PHVQJUAY,blogPost,,,Ethical Norms for New Generation Artificial Intelligence Released,Center for Security and Emerging Technology,,,,https://cset.georgetown.edu/publication/ethical-norms-for-new-generation-artificial-intelligence-released/,See our original translation of a 2021 PRC state AI governance committee document on the ethical norms for AI use.,,2022-03-10 20:54:02,2022-03-10 20:54:02,2022-03-10 20:54:02,,,,,,,,,,,,,,en-US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FDR7XKE5,blogPost,,,White Paper on Trustworthy Artificial Intelligence,Center for Security and Emerging Technology,,,,https://cset.georgetown.edu/publication/white-paper-on-trustworthy-artificial-intelligence/,"See our original translation of a 2021 PRC white paper describing the importance and difficulty of improving the ""trustworthiness"" of AI systems.",,2022-03-10 20:54:04,2022-03-10 20:54:04,2022-03-10 20:54:04,,,,,,,,,,,,,,en-US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PYZZABNA,blogPost,,,Ethics and Artificial Intelligence,Center for Security and Emerging Technology,,,,https://cset.georgetown.edu/publication/ethics-and-artificial-intelligence/,"The law plays a vital role in how artificial intelligence can be developed and used in ethical ways. But the law is not enough when it contains gaps due to lack of a federal nexus, interest, or the political will to legislate. And law may be too much if it imposes regulatory rigidity and burdens when flexibility and innovation are required. Sound ethical codes and principles concerning AI can help fill legal gaps. In this paper, CSET Distinguished Fellow James E. Baker offers a primer on the limits and promise of three mechanisms to help shape a regulatory regime that maximizes the benefits of AI and minimizes its potential harms.",,2022-03-10 20:54:07,2022-03-10 20:54:07,2022-03-10 20:54:07,,,,,,,,,,,,,,en-US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HYQCFQ6K,blogPost,,,AI Verification,Center for Security and Emerging Technology,,,,https://cset.georgetown.edu/publication/ai-verification/,"The rapid integration of artificial intelligence into military systems raises critical questions of ethics, design and safety. While many states and organizations have called for some form of “AI arms control,” few have discussed the technical details of verifying countries’ compliance with these regulations. This brief offers a starting point, defining the goals of “AI verification” and proposing several mechanisms to support arms inspections and continuous verification.",,2022-03-10 20:54:09,2022-03-10 20:54:09,2022-03-10 20:54:09,,,,,,,,,,,,,,en-US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IVYAIN4P,blogPost,,,Key Concepts in AI Safety: An Overview,Center for Security and Emerging Technology,,,,https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-an-overview/,"This paper is the first installment in a series on “AI safety,” an area of machine learning research that aims to identify causes of unintended behavior in machine learning systems and develop tools to ensure these systems work safely and reliably. In it, the authors introduce three categories of AI safety issues: problems of robustness, assurance, and specification. Other papers in this series elaborate on these and further key concepts.",,2022-03-10 20:54:11,2022-03-10 20:54:11,2022-03-10 20:54:11,,,,,,,Key Concepts in AI Safety,,,,,,,en-US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CBBNW9HN,blogPost,,,Contending Frames: Evaluating Rhetorical Dynamics in AI,Center for Security and Emerging Technology,,,,https://cset.georgetown.edu/publication/contending-frames/,"The narrative of an artificial intelligence “arms race” among the great powers has become shorthand to describe evolving dynamics in the field. Narratives about AI matter because they reflect and shape public perceptions of the technology. In this issue brief, the second in a series examining rhetorical frames in AI, the authors compare four narrative frames that are prominent in public discourse: AI Competition, Killer Robots, Economic Gold Rush and World Without Work.",,2022-03-10 20:54:18,2022-03-10 20:54:18,2022-03-10 20:54:18,,,,,,,Contending Frames,,,,,,,en-US,,,,,,,,,/Users/jacquesthibodeau/Zotero/storage/2BVGFRBQ/contending-frames.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JGTBYZEZ,blogPost,,,Classifying AI Systems,Center for Security and Emerging Technology,,,,https://cset.georgetown.edu/publication/classifying-ai-systems/,"This brief explores the development and testing of artificial intelligence system classification frameworks intended to distill AI systems into concise, comparable and policy-relevant dimensions. Comparing more than 1,800 system classifications, it points to several factors that increase the utility of a framework for human classification of AI systems and enable AI system management, risk assessment and governance.",,2022-03-10 20:54:47,2022-03-10 20:54:47,2022-03-10 20:54:47,,,,,,,,,,,,,,en-US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
H69PMJMD,blogPost,,,Federal Prize Competitions,Center for Security and Emerging Technology,,,,https://cset.georgetown.edu/publication/federal-prize-competitions/,"In science and technology, U.S. federal prize competitions are a way to promote innovation, advance knowledge, and solicit technological solutions to problems. In this report, the authors identify the unique advantages of such competitions over traditional R&D processes, and how these advantages might benefit artificial intelligence research.",,2022-03-10 20:54:49,2022-03-10 20:54:49,2022-03-10 20:54:49,,,,,,,,,,,,,,en-US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
58VFY4RF,blogPost,,,Key Concepts in AI Safety: Robustness and Adversarial Examples,Center for Security and Emerging Technology,,,,https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-robustness-and-adversarial-examples/,"This paper is the second installment in a series on “AI safety,” an area of machine learning research that aims to identify causes of unintended behavior in machine learning systems and develop tools to ensure these systems work safely and reliably. The first paper in the series, “Key Concepts in AI Safety: An Overview,” described three categories of AI safety issues: problems of robustness, assurance, and specification. This paper introduces adversarial examples, a major challenge to robustness in modern machine learning systems.",,2022-03-10 20:54:52,2022-03-10 20:54:52,2022-03-10 20:54:52,,,,,,,Key Concepts in AI Safety,,,,,,,en-US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8N9BIXTW,blogPost,,,Key Concepts in AI Safety: Interpretability in Machine Learning,Center for Security and Emerging Technology,,,,https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-interpretability-in-machine-learning/,"This paper is the third installment in a series on “AI safety,” an area of machine learning research that aims to identify causes of unintended behavior in machine learning systems and develop tools to ensure these systems work safely and reliably. The first paper in the series, “Key Concepts in AI Safety: An Overview,” described three categories of AI safety issues: problems of robustness, assurance, and specification. This paper introduces interpretability as a means to enable assurance in modern machine learning systems.",,2022-03-10 20:54:54,2022-03-10 20:54:54,2022-03-10 20:54:54,,,,,,,Key Concepts in AI Safety,,,,,,,en-US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
W64ZD2WZ,blogPost,2018,paulfchristiano,Takeoff speeds,The sideways view,,,,https://sideways-view.com/2018/02/24/takeoff-speeds/,"Futurists have argued for years about whether the development of AGI will look more like a breakthrough within a small group (“fast takeoff”), or a continuous acceleration distributed a…",2018-02-24,2022-03-10 22:08:35,2022-03-10 22:08:35,2022-03-10 22:08:35,,,,,,,,,,,,,,en,,,,,,,,,/Users/jacquesthibodeau/Zotero/storage/QBNGZMWZ/takeoff-speeds.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4GU5UGWM,blogPost,,,Understanding View Selection for Contrastive Learning,Google AI Blog,,,,http://ai.googleblog.com/2020/08/understanding-view-selection-for.html,"Posted by Yonglong Tian, Student Researcher and Chen Sun, Staff Research Scientist, Google Research    Most people take for granted the abil...",,2022-03-10 23:30:06,2022-03-10 23:30:06,2022-03-10 23:30:06,,,,,,,,,,,,,,en,,,,,,,,,/Users/jacquesthibodeau/Zotero/storage/IDWNFSB6/understanding-view-selection-for.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WKBFQFJE,blogPost,2020,"Raff, Edward",Quantifying Independently Reproducible Machine Learning,The Gradient,,,,https://thegradient.pub/independently-reproducible-machine-learning/,"Many warn that Artificial Intelligence  has a serious reproducibility crisis, but is it so? Some conclusions from the author's experience trying to replicate 255 papers.",2020-02-06,2020-09-05 18:54,2020-12-21 18:26,2020-09-05 18:54,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/angelica/Zotero/storage/UKPKMFEU/independently-reproducible-machine-learning.html,,Other-org; TechSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"While reproducibility refers to our ability to obtain results that are similar to the results presented in a paper, **independent reproducibility** requires us to be able to reproduce similar results using *only* what is written in the paper. Crucially, this excludes using the author's code. This is important, as a paper should distill insights rather than just report results. If minor technical details in a reimplementation can lead to vastly different results, this suggests that the paper did not accurately capture all important aspects. The distinction between reproducibility and independent reproducibility is similar to the previously suggested distinctions between <@reproducibility of methods and reproducibility of conclusions@>(@Unreproducible Research is Reproducible@) and [replicability and reproducibility](http://cogprints.org/7691/7/ICMLws09.pdf).

The author attempted to replicate 255 machine learning papers, of which 162 were successfully replicated and ran a statistical analysis on the results. Factors that helped with independent reproduction included specified hyperparameters, ease of reading and authors answering emails. Meanwhile, neither shared code nor the inclusion of pseudo-code robustly increased the rate of reproduction. Interestingly, papers with a strong focus on theory performed worse than mostly empirical or mixed ones. While more rigour can certainly be valuable in the long term, including learning bounds or complicated math, just for the sake of it should thus be avoided. Most of the data is [publically available](https://github.com/EdwardRaff/Quantifying-Independently-Reproducible-ML) and the author encourages further analysis."
7RVLRFQ6,blogPost,2018,"Gamble, Chris; Gao, Jim",Safety-first AI for autonomous data centre cooling and industrial control,Deepmind,,,,/blog/article/safety-first-ai-autonomous-data-centre-cooling-and-industrial-control,"Many of society’s most pressing problems have grown increasingly complex, so the search for solutions can feel overwhelming. At DeepMind and Google, we believe that if we can use AI as a tool to discover new knowledge, solutions will be easier to reach.In 2016, we jointly developed an AI-powered recommendation system to improve the energy efficiency of Google’s already highly-optimised data centres. Our thinking was simple: even minor improvements would provide significant energy savings and reduce CO2 emissions to help combat climate change.Now we’re taking this system to the next level: instead of human-implemented recommendations, our AI system is directly controlling data centre cooling, while remaining under the expert supervision of our data centre operators. This first-of-its-kind cloud-based control system is now safely delivering energy savings in multiple Google data centres.",2018,2020-12-13 23:04,2020-12-17 03:16,2020-12-13 23:04,,,,,,,,,,,,,,ALL,,,,,,,ZSCC: 0000004,,/Users/angelica/Zotero/storage/Q9CN8DLJ/safety-first-ai-autonomous-data-centre-cooling-and-industrial-control.html,,NotSafety; DeepMind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Two years ago, DeepMind built an AI recommendation system that provided suggestions on how best to cool Google's data centers, leading to efficiency gains. Nine months ago, the AI was given autonomous control to take actions directly, rather than going through human operators, and it has been improving ever since, going from 12% savings at deployment to 30% now.

Of course, such a system must be made extremely reliable, since a failure could result in Google's data centers going down. They implemented several safety measures. They throw out any actions that the AI is not confident about. All actions are verified against a set of hand-coded safety rules, both when the actions are generated in the cloud, and at each local data center, for reliability through redundancy. There are human operators monitoring the AI to make sure nothing goes wrong, who can take over control whenever they want to. There is also an automated system that will fall back to the original system of heuristics and rules if the safety conditions are ever violated."
AZW2E9MR,blogPost,2020,"Rice, Issa","Plausible cases for HRAD work, and locating the crux in the ""realism about rationality"" debate",AI alignment Forum,,,,https://www.alignmentforum.org/posts/BGxTpdBGbwCWrGiCL/plausible-cases-for-hrad-work-and-locating-the-crux-in-the,"This post is my attempt to summarize and distill the major public debates about MIRI's highly reliable agent designs (HRAD) work (which includes work on decision theory), including the discussions in Realism about rationality and Daniel Dewey's My current thoughts on MIRI's ""highly reliable agent design"" work . Part of the difficulty with discussing the value of HRAD work is that it's not even clear what the disagreement is about, so my summary takes the form of multiple possible ""worlds"" we might be in; each world consists of a positive case for doing HRAD work, along with the potential objections to that case, which results in one or more cruxes. I will talk about ""being in a world"" throughout this post. What I mean by this is the following: If we are ""in world X"", that means that the case for HRAD work outlined in world X is the one that most resonates with MIRI people as their motivation for doing HRAD work; and that when people disagree about the value of HRAD work, this is what the disagreement is about. When I say that ""I think we are in this world"", I don't mean that I agree with this case for HRAD work; it just means that this is what I think MIRI people think. In this post, the pro-HRAD stance is something like ""HRAD work is the most important kind of technical research in AI alignment; it is the overwhelming priority and we're pretty much screwed if we under-invest in this kind of research"" and the anti-HRAD stance is something like ""HRAD work seems significantly less promising than other technical AI alignment agendas, such as the approaches to directly align machine learning systems (e.g. iterated amplification)"". There is a much weaker pro-HRAD stance, which is something like ""HRAD work is interesting and doing more of it adds value, but it's not necessarily the most important kind of technical AI alignment research to be working on""; this post is not about this weaker stance. CLARIFYING SOME TERMS Before describing the various worlds, I want to pre",2020-06-21,2020-08-31 17:52,2020-12-21 18:27,2020-08-31 17:52,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/angelica/Zotero/storage/2XQ6DWGC/plausible-cases-for-hrad-work-and-locating-the-crux-in-the.html,,Other-org; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This post tries to identify the possible cases for highly reliable agent design (HRAD) work to be the main priority of AI alignment. HRAD is a category of work at MIRI that aims to build a theory of intelligence and agency that can explain things like logical uncertainty and counterfactual reasoning.

The first case for HRAD work is that by becoming less confused about these phenomena, we will be able to help AGI builders predict, explain, avoid, detect, and fix safety issues and help to conceptually clarify the AI alignment problem. For this purpose, we just need _conceptual_ deconfusion -- it isn’t necessary that there must be precise equations defining what an AI system does.

The second case is that if we get a precise, mathematical theory, we can use it to build an agent that we understand “from the ground up”, rather than throwing the black box of deep learning at the problem.

The last case is that understanding how intelligence works will give us a theory that allows us to predict how _arbitrary_ agents will behave, which will be useful for AI alignment in all the ways described in the first case and <@more@>(@Theory of Ideal Agents, or of Existing Agents?@).

Looking through past discussions on the topic, the author believes that people at MIRI primarily believe in the first two cases. Meanwhile, critics (particularly me) say that it seems pretty unlikely that we can build a precise, mathematical theory, and a more conceptual but imprecise theory may help us understand reasoning better but is less likely to generalize sufficiently well to say important and non-trivial things about AI alignment for the systems we are actually building."
CMIVXP8V,blogPost,2020,Sublation,Openness Norms in AGI Development,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/RvrTZ3qKWpg9aiFqZ/openness-norms-in-agi-development,"1. INTRODUCTION This post outlines two models from the social epistemology of science explaining the emergence of the a particular openness norm within the sciences, and then looks at how such models can be utilised to understand research groups trying to develop AGI. In the rest of the introduction, I will try to provide some motivation for this post. Sections 2 & 3 will briefly outline the two models I'm looking at. Section 4 more directly tries to interpret such models in the context of AGI development. Section 5 concludes.  The social epistemology of science is an interdisciplinary subfield at the intersection of philosophy and economics, which utilises formal models to understand the incentive structure of science. Here, I focus on two models from this area which try to explain the emergence of one particular openness norm: the so-called ‘communist norm’ in scientific research. The communist norm is a norm to share all ‘substantive findings’ with the scientific community. The existence of this norm seems to be taken for granted in this literature, although the best piece of evidence I can find for its existence comes from  Louis et. al (2001), who find, in a sample of nearly 2,000 geneticists, that 91% agree that one should share all of one's relevant data. I nevertheless take it for granted in this post. I wanted to see whether understanding the emergence of the communist norm in science could be important for understanding the development of AGI. In many ways, one might think the incentive structures around the development of AGI (will, or does) parallel the incentive structures of academic science. Thus, one might think that looking at the incentive structures behind scientific research are a good starting point for looking at the incentive structures surrounding the development of AGI.  As the communist norm emerged in science, one can imagine the emergence of a similar ‘communist norm’ across research groups involved in AGI development, where research g",2020-03-30,2020-09-05 18:05,2020-12-21 18:33,2020-09-05 18:05,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/angelica/Zotero/storage/M622F268/openness-norms-in-agi-development.html,,Other-org; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This post summarizes two papers that provide models of why scientific research tends to be so open, and then applies it to the development of powerful AI systems. The [first](http://www.strevens.org/research/scistruc/Communicans.pdf) models science as a series of discoveries, in which the first academic group to reach a discovery gets all the credit for it. It shows that for a few different models of info-sharing, info-sharing helps everyone reach the discovery sooner, but doesn't change the probabilities for who makes the discovery first (called _race-clinching probabilities_): as a result, sharing all information is a better strategy than sharing none (and is easier to coordinate on than the possibly-better strategy of sharing just some information).

However, this theorem doesn't apply when info sharing compresses the discovery probabilities _unequally_ across actors: in this case, the race-clinching probabilities _do_ change, and the group whose probability would go down is instead incentivized to keep information secret (which then causes everyone else to keep their information secret). This could be good news: it suggests that actors are incentivized to share safety research (which probably doesn't affect race-clinching probabilities) while keeping capabilities research secret (thereby leading to longer timelines).

The [second paper](http://philsci-archive.pitt.edu/13452/1/Heesen%202017%20Communism%20and%20the%20Incentive%20to%20Share%20in%20Science%20preprint.pdf) assumes that scientists are competing to complete a k-stage project, and whenever they publish, they get credit for all the stages they completed that were not yet published by anyone else. It also assumes that earlier stages have a higher credit-to-difficulty ratio (where difficulty can be different across scientists). It finds that under this setting scientists are incentivized to publish whenever possible. For AI development, this seems not to be too relevant: we should expect that with powerful AI systems, most of the ""credit"" (profit) comes from the last few stages, where it is possible to deploy the AI system to earn money."
Q4A8HRFQ,blogPost,2020,"Aird, Michael; Shovelain, Justin",Using vector fields to visualise preferences and make them consistent,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/ky988ePJvCRhmCwGo/using-vector-fields-to-visualise-preferences-and-make-them,"This post was written for Convergence Analysis by Michael Aird, based on ideas from Justin Shovelain and with ongoing guidance from him. Throughout the post, “I” will refer to Michael, while “we” will refer to Michael and Justin or to Convergence as an organisation. Epistemic status: High confidence in the core ideas on an abstract level. Claims about the usefulness of those ideas, their practical implications, and how best to concretely/mathematically implement them are more speculative; one goal in writing this post is to receive feedback on those things. I’m quite new to many of the concepts covered in this post, but Justin is more familiar with them. OVERVIEW This post outlines:  * What vector fields are  * How they can be used to visualise preferences  * How utility functions can be generated from “preference vector fields” (PVFs)  * How PVFs can be extrapolated from limited data on preferences  * How to visualise inconsistent preferences (as “curl”)  * A rough idea for how to “remove curl” to generate consistent utility    functions  * Possible areas for future research We expect this to provide useful tools and insights for various purposes, most notably AI alignment, existential risk strategy, and rationality. This post is structured modularly; different sections may be of interest to different readers, and should be useful in isolation from the rest of the post. The post also includes links to articles and videos introducing relevant concepts, to make the post accessible to readers without relevant technical backgrounds. VECTOR FIELDS AND PREFERENCES A vector represents both magnitude and direction; for example, velocity is a vector that represents not just the speed at which one is travelling but also the direction of travel. A vector field essentially associates a vector to each point in a region of space. For example, the following image (source) shows the strength (represented by arrow lengths) and direction of the magnetic field at various points",2020-01-28,2020-09-05 19:09,2020-12-21 17:53,2020-09-05 19:09,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/angelica/Zotero/storage/HTRBHB39/using-vector-fields-to-visualise-preferences-and-make-them.html,,Other-org; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This post proposes that we represent a person's preferences as follows: for every state, we have a vector whose direction specifies how the person would most like the state to change, and whose magnitude specifies the intensity of the preference. Under suitable conditions on the state space, this defines a vector field. Intransitive or circular preferences correspond to the [curl](https://en.wikipedia.org/wiki/Curl_(mathematics)) of the vector field. The authors propose that a consistent set of preferences can then be inferred by ""removing the curl"", e.g. by using the [Helmholtz decomposition](https://en.wikipedia.org/wiki/Helmholtz_decomposition)."
B5RNP7TK,blogPost,2020,"Sotala, Kaj","The two-layer model of human values, and problems with synthesizing preferences",AI Alignment Forum,,,,https://www.alignmentforum.org/posts/2yLn8iTrvHoEgqXcJ/the-two-layer-model-of-human-values-and-problems-with,"I have been thinking about Stuart Armstrong's preference synthesis research agenda, and have long had the feeling that there's something off about the way that it is currently framed. In the post I try to describe why. I start by describing my current model of human values, how I interpret Stuart's implicit assumptions to conflict with it, and then talk about my confusion with regard to reconciling the two views. THE TWO-LAYER/ULM MODEL OF HUMAN VALUES In Player vs. Character: A Two-Level Model of Ethics, Sarah Constantin describes a model where the mind is divided, in game terms, into a ""player"" and a ""character"". The character is everything that we consciously experience, but our conscious experiences are not our true reasons for acting. As Sarah puts it: In many games, such as Magic: The Gathering, Hearthstone, or Dungeons and Dragons, there’s a two-phase process. First, the player constructs adeck or character from a very large sample space of possibilities. This is a particular combination of strengths and weaknesses and capabilities for action, which the player thinks can be successful against other decks/characters or at winning in the game universe. The choice of deck or character often determines the strategies that deck or character can use in the second phase, which is actual gameplay. In gameplay, the character (or deck) can only use the affordances that it’s been previously set up with. This means that there are two separate places where a player needs to get things right: first, in designing a strong character/deck, and second, in executing the optimal strategies for that character/deck during gameplay. [...]The idea is that human behavior works very much like a two-level game. [...] The player determines what we find rewarding or unrewarding. The player determines what we notice and what we overlook; things come to our attention if it suits the player’s strategy, and not otherwise. The player gives us emotions when it’s strategic to do so. The playe",2020-01-24,2020-09-05 19:08,2020-12-21 18:32,2020-09-05 19:08,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/angelica/Zotero/storage/VDLAR3G4/the-two-layer-model-of-human-values-and-problems-with.html,,Other-org; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This post points out a problem with the recent <@preference synthesis research agenda@>(@Research Agenda v0.9: Synthesising a human's preferences into a utility function@) (and presumably other value learning agendas as well): these agendas tend to require simple models of how human behavior, speech, or mental models relate to human preferences. However, in reality, it seems likely that the brain is a big learning machine without any innate ""values"", and what we experience as our conscious selves is a ""strategy"" chosen by this learning machine, and as such does not have a sensible interpretation as something that optimizes for ""values"". The author suggests that value learning agendas need to deal directly with the fact that there are these two ""layers"" in humans, and presents some preliminary thoughts that don't reach any particular conclusions."
X4RQB34W,blogPost,2019,"Steiner, Charlie","Some Comments on Stuart Armstrong's ""Research Agenda v0.9""",LessWrong,,,,https://www.lesswrong.com/posts/GHNokcgERpLJwJnLW/some-comments-on-stuart-armstrong-s-research-agenda-v0-9,"Subject matter here. I: Intro I am extremely sympathetic to the program of AI safety by understanding value learning. Because of that sympathy, I have more thoughts than average prompted by Stuart Armstrong's post along those same lines. Stuart's post mostly deals with ""partial preferences,"" which are like simple statements of binary preference (A is better than B), but associated with a context - supposedly the ""human's model"" the human was using when they exhibited or stated that preference. Then the post says that you should sort these partial preferences according to meta-levels and aggregate them from the top down, updating your procedure after you finish each meta-level, eventually producing a utility function over world-histories. Broadly, I'd say that my opinion is sort of like the bitter lesson. The bitter lesson in, say, image recognition, is that people wanted to do image recognition with a bunch of human-designed features and formal reasoning and human-understandable internal moving parts, and they tried that for a long time, and what worked was using way bigger models, way more computing power, much fewer human-understandable internal parts, and almost no human-designed features. I like Stuart's outline more than most value learning proposals. But it still strikes me as primarily a list of human-designed features and human-understandable internal moving parts. We might be better off throwing away some of the details and abstracting in a way that allows for some of these problems to be solved by big models and computing power. It's like the just-so story about ResNets, which is that they're a fix to humans thinking the insides of neural nets should look too much like human logic[^1]. I think speculating about the human-sized logical relationships between speculative parts inside the AI is easier but less useful than speculating about the algorithm that will connect your inputs to your outputs with a big model and lots of computing power, which may",2019,2020-12-14 23:56,2020-12-17 03:25,2020-12-14 23:56,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/A,,,,Other-org; NotSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This post makes two main critiques of the research agenda in the previous entry. First, the research agenda involves a lot of human-designed features and modules, but <@The Bitter Lesson@> is that machine learning tends to shine with highly abstract large models that can make use of a lot of compute. Second, the symbol grounding part of the agenda requires the AI system to develop representations of the world that match the representations that humans use, and we have no idea how to do that, or even what it would mean to ""match human representations"" when the AI is more intelligent than humans. The post also includes some more specific comments that I'm not summarizing."
RW8DC8PG,blogPost,2020,DaemonicSigil,Tessellating Hills: a toy model for demons in imperfect search,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/X7S3u5E4KktLp7gHz/tessellating-hills-a-toy-model-for-demons-in-imperfect,"If you haven't already, take a look at this post by johnswentworth to understand what this is all about:  https://www.lesswrong.com/posts/KnPN7ett8RszE79PH/demons-in-imperfect-search The short version is that while systems that use perfect search, such as AIXI, have many safety problems, a whole new set of problems arises when we start creating systems that are not perfect searchers. Patterns can form that exploit the imperfect nature of the search function to perpetuate themselves. johnswentworth refers to such patterns as ""demons"". After reading that post I decided to see if I could observe demon formation in a simple model: gradient descent on a not-too-complicated mathematical function. It turns out that even in this very simplistic case, demon formation can happen. Hopefully this post will give people an example of demon formation where the mechanism is simple and easy to visualize. MODEL The function we try to minimize using gradient descent is called the loss function. Here it is: L(→x)=−x0+ϵn∑j=1xj⋅splotchj(→x) Let me explain what some of the parts of this loss mean. Each function splotchj( →x) is periodic with period 2π in every component of →x. I decided in this case to make my splotch functions out of a few randomly chosen sine waves added together. ϵ is chosen to be a small number so in any local region, ϵ∑nj=1xj⋅splotchj(→x)  will look approximately periodic: A bunch of hills repeating over and over again with period 2π across the landscape. But over large enough distances, the relative weightings of various splotches do change. Travel a distance of 20π in the x7 direction, and splotch7 will be a larger component of the repeating pattern than it was before. This allows for selection effects. The −x0 term means that the vector →x mainly wants to increase its x0 component. But the splotch functions can also direct its motion. A splotch function might have a kind of ridge that directs some of the x0 motion into other components. If splotch7 tends to",2020-02-19,2020-09-05 18:57,2020-12-21 18:03,2020-09-05 18:57,,,,,,,Tessellating Hills,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/angelica/Zotero/storage/6T2IBUNE/tessellating-hills-a-toy-model-for-demons-in-imperfect.html,,Other-org; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This post is trying to generate an example of the problem outlined in 'Demons in Imperfect Search' (summarized above): the problem where certain imperfect search processes allow for self-reinforcing behavior, 'demons', that push in a direction orthogonal to the original objective.

The post runs a simple gradient descent algorithm in an artifically constructed search space. The loss function that defines the search space has two major parts. One part straightforwardly tries to get the algorithm to move as far as it can in a particular direction _x0_ -- this represents our original objective function. The other part can be thought of as a series of periodic 'valleys' along every other axis, (_x1_ ... _xn_) that get steeper the farther you go along that axis.

When running the gradient descent, at first _x0_ increases steadily, and the other coordinates wander around more or less randomly. In the second phase, a self-reinforcing combination of valleys (a ""demon"") takes hold and amplifies itself drastically, feeding off the large _x0_ gradient. Finally, this demon becomes so strong that the search gets stuck in a local valley and further progress stops."
E8FU2VV4,blogPost,2020,"Shimi, Adam",Universality Unwrapped,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/farherQcqFQXqRcvv/universality-unwrapped,"INTRODUCTION Informally, a universal system is universal with respect to any computation; and it is a universal system with respect to a given computation if it understands every set of beliefs that can be ascribed to the computation. The intuition is that the system can reverse engineer most or all of the computation, in order to monitor it or imitate it. This in turn has important consequences for questions of alignment and competitiveness. Universality is the property that defines a universal system. And it is the point of this post. Universality tries to capture a property needed for many alignement schemes. It was proposed by Paul Christiano, the mind behind many approaches and ideas in the prosaic AGI space, and a founding member of the safety team at OpenAI. Rohin Shah dedicated a full Alignment Newsletter to covering all 6 posts on Universality. Rohin and Evan Hubinger, two important researchers in this field, consider Universality as one of the most exciting research idea of the last few years.[1] Yet nobody talks about Universality. Except for the Alignment Newsletter mentioned above and a response post by Evan, nothing in the Alignment Forum addresses this idea. I've seen no great discussion, no debates, no counter-arguments or criticism. The original post on Medium has no comments, and the crossposted version here only has a handful, mostly asking for clarification. And the other posts in the sequence rely on understanding this first. The simplest solution to this problem is to tell you to read the original post. Unfortunately, it is as dense as Q in R, brimming with ideas, intuitions, semi-formal explanations and the many meanderings that research takes before arriving on solid ground. That is to say, you'll have to work for it. Not everyone who might benefit from an understanding of Universality has the time, the need or the want for such an upfront investment. This post endeavors to be the next best thing: an unwrapping of the main post on univers",2020-08-21,2020-08-27 16:19,2020-12-21 18:32,2020-08-27 16:19,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/angelica/Zotero/storage/A7BNTBDE/universality-unwrapped.html,,Other-org; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This post explains the ideas behind universality and ascription universality, in a more accessible way than the [original posts](https://ai-alignment.com/towards-formalizing-universality-409ab893a456) and with more detail than [my summary](https://mailchi.mp/6078fe4f9928/an-81-universality-as-a-potential-solution-to-conceptual-difficulties-in-intent-alignment)."
AWQS8IRG,blogPost,2020,"Martin, Sammy",Will AI undergo discontinuous progress?,AI Alignment Forum,,,,https://www.alignmentforum.org/posts/5WECpYABCT62TJrhY/will-ai-undergo-discontinuous-progress,"This post grew out of conversations with several people, including Daniel Kokotajlo, grue_slinky and Linda Lisefors, and is based in large part on a collection of scattered comments and blog-posts across lesswrong, along with some podcast interviews - e.g. here. The in-text links near quotes will take you to my sources. I am attempting to distinguish two possibilities which are often run together - that progress in AI towards AGI (‘takeoff’) will be discontinuous and that it will be fast, but continuous. Resolving this distinction also addresses the claim that there has been a significant shift in arguments for AI presenting an existential risk: from older arguments discussing an ultra-fast intelligence explosion occurring in a single ‘seed AI’ to more moderate scenarios. I argue that the ‘shift in arguments on AI safety’ is not a total change in basic assumptions (which some observers have claimed) but just a reduction in confidence about a specifically discontinuous takeoff. Finally, I try to explicitly operationalize the practical differences between discontinuous takeoff and fast, continuous takeoff. Further Reading Summary: Why AI risk might be solved without additional intervention from Longtermists Paul Christiano’s original post MIRIs Thoughts on Discontinuous takeoff Misconceptions about continuous takeoff AI Impacts original post Soft Takeoff can still lead to Decisive Strategic Advantage DEFINING DISCONTINUOUS PROGRESS What do I mean by ‘discontinuous’? If we were to graph world GDP over the last 10,000 years, it fits onto a hyperbolic growth pattern. We could call this ‘continuous’ since it is following a single trend, or we could call it ‘discontinuous’ because, on the scale of millennia, the industrial revolution exploded out of nowhere. I will call these sorts of hyperbolic trends ‘continuous, but fast’, in line with Paul Christiano, who argued for continuous takeoff, defining it this way: AI is just another, faster step in the hyperbolic g",2020-02-21,2020-09-05 18:50,2020-12-21 18:20,2020-09-05 18:50,,,,,,,Will AI undergo discontinuous progress?,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/angelica/Zotero/storage/GQ36CW4I/will-ai-undergo-discontinuous-progress.html,,Other-org; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This post argues that the debate over takeoff speeds is over a smaller issue than you might otherwise think: people seem to be arguing for either discontinuous progress, or continuous but fast progress. Both camps agree that once AI reaches human-level intelligence, progress will be extremely rapid; the disagreement is primarily about whether there is already quite a lot of progress _before_ that point. As a result, these differences don't constitute a ""shift in arguments on AI safety"", as some have claimed.

The post also goes through some of the arguments and claims that people have made in the past, which I'm not going to summarize here."
,blogPost,2020,"Schoenholz, Samuel S; Novak, Roman",Fast and Easy Infinitely Wide Networks with Neural Tangents,Google AI Blog,,,,http://ai.googleblog.com/2020/03/fast-and-easy-infinitely-wide-networks.html,,2020-03-13,2020-09-05 17:32,2020-12-21 18:30,2020-09-05 17:32,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/angelica/Zotero/storage/S5DQPAQA/fast-and-easy-infinitely-wide-networks.html,,Other-org; NotSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The success of Deep Learning has led researchers to explore why they're such effective function approximators. One key insight is that increasing the width of the network layers makes it *easier* to understand. More precisely, as the width is sent to infinity the network's learning dynamics can be approximated with a Taylor expansion and become a kernel problem. This kernel has an exact form in the limit and is referred to as the neural tangent kernel (NTK). Ultimately, this allows us to model the network with a simpler model known as a Gaussian process. Unfortunately, showing this analytically is difficult and creating efficient implementations is cumbersome. **The authors address this problem by introducing ""Neural Tangents"", a library that makes creating infinite-width networks as easy as creating their finite counterparts with libraries such as PyTorch or TensorFlow.** They include support for convolutions with full-padding, residual-connections, feed-forward networks, and support for a variety of activation functions. Additionally, there is out-of-the-box support for CPU, GPU, and TPU. Moreover, uncertainty comparisons with finite ensembles are possible via exact Bayesian inference."
,blogPost,2015,"Christiano, Paul",The Steering Problem,AI Alignment (Medium),,,,https://ai-alignment.com/the-steering-problem-a3543e65c5c4,"Using black-box access to human-level cognitive abilities, can we write a program that is as useful as a well-motivated human?",2015-05-06,2020-11-21 18:46,2020-12-21 18:02,2020-11-21 18:46,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/angelica/Zotero/storage/EK7U6HZH/the-steering-problem-a3543e65c5c4.html,,Other-org; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The steering problem refers to the problem of writing a program that uses black-box human-level cognitive abilities to be as useful as a well-motivated human Hugh (that is, a human who is ""trying"" to be helpful). This is a conceptual problem -- we don't have black-box access to human-level cognitive abilities yet. However, we can build suitable formalizations and solve the steering problem within those formalizations, from which we can learn generalizable insights that we can apply to the problem we will actually face once we have strong AI capabilities. For example, we could formalize ""human-level cognitive abilities"" as Hugh-level performance on question-answering (yes-no questions in natural language), online learning (given a sequence of labeled data points, predict the label of the next data point), or embodied reinforcement learning. A program P is more useful than Hugh for X if, for every project using a simulation of Hugh to accomplish X, we can efficiently transform it into a new project which uses P to accomplish X."
F34EPTEX,book,2011,"Bostrom, Nick; Cirkovic, Milan M.",Global Catastrophic Risks,,978-0-19-960650-4,,,,"A global catastrophic risk is one with the potential to wreak death and destruction on a global scale. In human history, wars and plagues have done so on more than one occasion, and misguided ideologies and totalitarian regimes have darkened an entire era or a region. Advances in technology are adding dangers of a new kind. It could happen again. In Global Catastrophic Risks 25 leading experts look at the gravest risks facing humanity in the 21st century, including asteroid impacts, gamma-ray bursts, Earth-based natural catastrophes, nuclear war, terrorism, global warming, biological weapons, totalitarianism, advanced nanotechnology, general artificial intelligence, and social collapse. The book also addresses over-arching issues - policy responses and methods for predicting and managing catastrophes. This is invaluable reading for anyone interested in the big issues of our time; for students focusing on science, society, technology, and public policy; and for academics, policy-makers, and professionals working in these acutely important fields.",2011-09-29,2022-01-30 04:53:17,2022-01-30 04:53:17,,,577,,,,,,,,,,OUP Oxford,,en,,,,,Google Books,,ZSCC: 0000599  Google-Books-ID: sTkfAQAAQBAJ,,,https://books.google.com/books?id=sTkfAQAAQBAJ,TechSafety; FHI,Mathematics / Game Theory; Nature / Sky Observation; Science / Biotechnology; Science / Earth Sciences / General; Science / General; Science / Philosophy & Social Aspects; Science / Physics / General; Social Science / Disasters & Disaster Relief; Technology & Engineering / Nanotechnology & MEMS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6BHH2HVZ,book,2016,,Fundamental issues of artificial intelligence,,,,,https://link.springer.com/book/10.1007%2F978-3-319-26485-1,,2016,2022-01-30 04:53:10,2022-01-30 04:53:10,2020-12-21,,,,376,,,,Synthese Library,,,,Springer,,,,,,,Google Scholar,,ZSCC: 0000055,,/Users/jacquesthibodeau/Zotero/storage/S28A9QZU/Müller and Bostrom - 2016 - Fundamental issues of artificial intelligence.pdf; /Users/jacquesthibodeau/Zotero/storage/I2QFDMUU/10.html,,TechSafety; FHI,,"Müller, Vincent C.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4JPTSFGN,book,2002,"Bostrom, Nick",Anthropic bias: observation selection effects in science and philosophy,,978-0-415-93858-7,,,,,2002,2022-01-30 04:53:08,2022-01-30 04:53:08,,,224,,,,,Anthropic bias,Studies in philosophy,,,,Routledge,New York,en,,,,,Library of Congress ISBN,BD241 .B657 2002,ZSCC: NoCitationData[s4]  ACC: 624,,/Users/jacquesthibodeau/Zotero/storage/MD67EI7P/Bostrom - 2002 - Anthropic bias observation selection effects in s.pdf,,MetaSafety; FHI,Anthropic principle; Methodology; Observation (Scientific method); Selectivity (Psychology),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
678H8WWB,book,2020,"Ord, Toby",The Precipice: Existential Risk and the Future of Humanity,,978-0-316-48491-6,,,,"This urgent and eye-opening book makes the case that protecting humanity's future is the central challenge of our time.   If all goes well, human history is just beginning. Our species could survive for billions of years - enough time to end disease, poverty, and injustice, and to flourish in ways unimaginable today. But this vast future is at risk. With the advent of nuclear weapons, humanity entered a new age, where we face existential catastrophes - those from which we could never come back. Since then, these dangers have only multiplied, from climate change to engineered pathogens and artificial intelligence. If we do not act fast to reach a place of safety, it will soon be too late.  Drawing on over a decade of research, The Precipice explores the cutting-edge science behind the risks we face. It puts them in the context of the greater story of humanity: showing how ending these risks is among the most pressing moral issues of our time. And it points the way forward, to the actions and strategies that can safeguard humanity.  An Oxford philosopher committed to putting ideas into action, Toby Ord has advised the US National Intelligence Council, the UK Prime Minister's Office, and the World Bank on the biggest questions facing humanity. In The Precipice, he offers a startling reassessment of human history, the future we are failing to protect, and the steps we must take to ensure that our generation is not the last.",2020-03-24,2022-01-30 04:53:36,2022-01-30 04:53:36,,,480,,,,,The Precipice,,,,,Hachette Books,New York,English,,,,,Amazon,,ZSCC: 0000169,,,https://www.amazon.com/Precipice-Existential-Risk-Future-Humanity/dp/0316484911,MetaSafety; FHI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,Illustrated Edition,,,,,,,,,,,,,,,,,,,,,,,,,,,
S9IMQ6PA,book,2016,"Yampolskiy, Roman; Armstrong, Stuart",The Technological Singularity: Managing the Journey,,,,,,,2016,2022-01-30 04:53:36,2022-01-30 04:53:36,,,,,,,,The Technological Singularity,,,,,Springer https://intelligence. org/files/TechnicalAgenda. pdf Retrieved,,,,,,,Google Scholar,,ZSCC: NoCitationData[s2]  ACC: 42,,,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
B86CZMRI,book,2015,"Miller, Jim; Yampolskiy, Roman; Armstrong, Stuart; Callaghan, Vic",The technological singularity,,,,,https://link.springer.com/book/10.1007%2F978-3-662-54033-6,,2015,2022-01-30 04:53:36,2022-01-30 04:53:36,,,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/JN6QKA2T/MILTTS-2.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MT6RRG9I,book,2017,"Callaghan, Vic; Miller, James; Yampolskiy, Roman; Armstrong, Stuart",Technological Singularity,,,,,,,2017,2022-01-30 04:53:36,2022-01-30 04:53:36,,,,,,,,,,,,,Springer,,,,,,,Google Scholar,,ZSCC: 0000042,,/Users/jacquesthibodeau/Zotero/storage/GRR7NK28/10.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5S49ITRG,book,2018,"Sandberg, Anders",Space races: Settling the universe Fast,,,,,,,2018,2022-01-30 04:53:35,2022-01-30 04:53:35,,,,,,,,Space races,,,,,"unpublished manuscript, forthcoming",,,,,,,Google Scholar,,ZSCC: 0000002,,/Users/jacquesthibodeau/Zotero/storage/AG8C5ZIP/Sandberg - 2018 - Space races Settling the universe Fast.pdf,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UUV9XSMG,book,2014,"Armstrong, Stuart",Smarter than us: The rise of machine intelligence,,,,,,,2014,2022-01-30 04:53:35,2022-01-30 04:53:35,,,,,,,,Smarter than us,,,,,Machine Intelligence Research Institute,,,,,,,Google Scholar,,ZSCC: 0000075,,,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZNDW24DR,book,2014,"Bostrom, Nick","Superintelligence: Paths, Dangers, Strategies",,978-0-19-967811-2,,,,"The human brain has some capabilities that the brains of other animals lack. It is to these distinctive capabilities that our species owes its dominant position. Other animals have stronger muscles or sharper claws, but we have cleverer brains. If machine brains one day come to surpass human brains in general intelligence, then this new superintelligence could become very powerful. As the fate of the gorillas now depends more on us humans than on the gorillas themselves, so the fate of our species then would come to depend on the actions of the machine superintelligence. But we have one advantage: we get to make the first move. Will it be possible to construct a seed AI or otherwise to engineer initial conditions so as to make an intelligence explosion survivable? How could one achieve a controlled detonation? To get closer to an answer to this question, we must make our way through a fascinating landscape of topics and considerations. Read the book and learn about oracles, genies, singletons; about boxing methods, tripwires, and mind crime; about humanity's cosmic endowment and differential technological development; indirect normativity, instrumental convergence, whole brain emulation and technology couplings; Malthusian economics and dystopian evolution; artificial intelligence, and biological cognitive enhancement, and collective intelligence.",2014,2022-01-30 04:53:35,2022-01-30 04:53:35,,,353,,,,,Superintelligence,,,,,Oxford University Press,,en,,,,,Google Books,,ZSCC: 0000064  Google-Books-ID: 7_H8AwAAQBAJ,,,https://books.google.com/books?id=7_H8AwAAQBAJ,MetaSafety; FHI; AmbiguosSafety,Computers / Intelligence (AI) & Semantics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KPSE82P8,book,2017,"Evans, Owain; Stuhlmüller, Andreas; Salvatier, John; Filan, Daniel",Modeling Agents with Probabilistic Programs,,,,,,,2017,2022-01-30 04:53:19,2022-01-30 04:53:19,,,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000014,,,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BMA4TR5R,book,2018,"Krakovna, Viktoriya; Orseau, Laurent; Martic, Miljan; Legg, Shane",Measuring and avoiding side effects using relative reachability,,,,,,"How can we design reinforcement learning agents that avoid causing unnecessary disruptions to their environment? We argue that current approaches to penalizing side effects can introduce bad incentives in tasks that require irreversible actions, and in environments that contain sources of change other than the agent. For example, some approaches give the agent an incentive to prevent any irreversible changes in the environment, including the actions of other agents. We introduce a general definition of side effects, based on relative reachability of states compared to a default state, that avoids these undesirable incentives. Using a set of gridworld experiments illustrating relevant scenarios, we empirically compare relative reachability to penalties based on existing definitions and show that it is the only penalty among those tested that produces the desired behavior in all the scenarios.",2018-06-04,2022-01-30 04:52:39,2022-01-30 04:52:39,,,,,,,,,,,,,,,,,,,,ResearchGate,,ZSCC: 0000014,,,https://www.researchgate.net/profile/Viktoriya_Krakovna/publication/325557348_Measuring_and_avoiding_side_effects_using_relative_reachability/links/5bb7e5eaa6fdcc9552d46b02/Measuring-and-avoiding-side-effects-using-relative-reachability.pdf,TechSafety; DeepMind; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
78WA4C6X,book,2007,,Artificial General Intelligence,,978-3-540-23733-4 978-3-540-68677-4,,,http://link.springer.com/10.1007/978-3-540-68677-4,,2007,2022-01-30 04:59:35,2022-01-30 04:59:35,2020-11-22 05:00:59,,,,,,,,Cognitive Technologies,,,,Springer Berlin Heidelberg,"Berlin, Heidelberg",,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s2]  ACC: 467  DOI: 10.1007/978-3-540-68677-4,,,,TechSafety; Other-org,,"Goertzel, Ben; Pennachin, Cassio","Gabbay, Dov M.; Siekmann, Jörg; Bundy, A.; Carbonell, J. G.; Pinkal, M.; Uszkoreit, H.; Veloso, M.; Wahlster, W.; Wooldridge, M. J.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IB2KTICC,book,2021,,Reflections on Artificial Intelligence for Humanity,,,,,,,2021-02-06,2022-03-09 23:02:29,2022-03-09 23:02:29,,,278,,,,,,,,,,Springer,,English,,,,,Amazon,,,,,https://www.amazon.com/gp/product/B08W3XZ1TJ/ref=ppx_yo_dt_b_d_asin_title_o00?ie=UTF8&psc=1&pldnSite=1,,,"Braunschweig, Bertrand; Ghallab, Malik",,,,,,,,,,,,,,,,,,,1st ed. 2021 edition,,,,,,,,,,,,,,,,,,,,,,,,,,,
KUTR98J7,bookSection,2017,"Sotala, Kaj; Yampolskiy, Roman",Responses to the Journey to the Singularity,The Technological Singularity,978-3-662-54031-2 978-3-662-54033-6,,,http://link.springer.com/10.1007/978-3-662-54033-6_3,,2017,2022-01-30 04:51:36,2022-01-30 04:51:36,2020-11-22 05:26:17,25-83,,,,,,,,,,,Springer Berlin Heidelberg,"Berlin, Heidelberg",en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s2]  ACC: 5  Series Title: The Frontiers Collection DOI: 10.1007/978-3-662-54033-6_3,,,,CLR; MetaSafety,,"Callaghan, Victor; Miller, James; Yampolskiy, Roman; Armstrong, Stuart",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZBWCFTFF,bookSection,2017,"Sotala, Kaj; Yampolskiy, Roman",Risks of the Journey to the Singularity,The Technological Singularity: Managing the Journey,978-3-662-54033-6,,,https://doi.org/10.1007/978-3-662-54033-6_2,"SummaryMany researchers have argued that humanity will create artificial general intelligence (AGI) within the next twenty to one hundred years. Unlike current AI systems, individual AGIs would be capable of learning to operate in a wide variety of domains, including ones they had not been specifically designed for. It has been proposed that AGIs might eventually pose a significant risk to humanity, for they could accumulate significant amounts of power and influence in society while being indifferent to what humans valued. The accumulation of power might either happen gradually over time, or it might happen very rapidly (a so-called “hard takeoff”). Gradual accumulation would happen through normal economic mechanisms, as AGIs came to carry out an increasing share of economic tasks. A hard takeoff could be possible if AGIs required significantly less hardware to run than was available, or if they could redesign themselves to run at ever faster speeds, or if they could repeatedly redesign themselves into more intelligent versions of themselves.",2017,2022-01-30 04:51:36,2022-01-30 04:51:36,2020-11-24 02:59:39,11-23,,,,,,,The Frontiers Collection,,,,Springer,"Berlin, Heidelberg",en,,,,,Springer Link,,ZSCC: NoCitationData[s1]  ACC: 6  DOI: 10.1007/978-3-662-54033-6_2,,,,CLR; MetaSafety,Automate Trading; Catastrophic Risk; Flash Crash; Machine Ethic; Virtual Assistant,"Callaghan, Victor; Miller, James; Yampolskiy, Roman; Armstrong, Stuart",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3FMN9R8M,bookSection,2014,"Russell, Stuart",Unifying Logic and Probability: A New Dawn for AI?,Information Processing and Management of Uncertainty in Knowledge-Based Systems,978-3-319-08794-8 978-3-319-08795-5,,,http://link.springer.com/10.1007/978-3-319-08795-5_2,,2014,2022-01-30 04:51:11,2022-01-30 04:51:11,2020-11-22 05:26:08,10-14,,,442,,,Unifying Logic and Probability,,,,,Springer International Publishing,Cham,,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s2]  ACC: 11  Series Title: Communications in Computer and Information Science DOI: 10.1007/978-3-319-08795-5_2,,/Users/jacquesthibodeau/Zotero/storage/46AICRPP/Russell - 2014 - Unifying Logic and Probability A New Dawn for AI.pdf,,CHAI; TechSafety,,"Laurent, Anne; Strauss, Olivier; Bouchon-Meunier, Bernadette; Yager, Ronald R.","Junqueira Barbosa, Simone Diniz; Chen, Phoebe; Cuzzocrea, Alfredo; Du, Xiaoyong; Filipe, Joaquim; Kara, Orhun; Kotenko, Igor; Sivalingam, Krishna M.; Ślęzak, Dominik; Washio, Takashi; Yang, Xiaokang",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AE7K4ISJ,bookSection,2016,"Russell, Stuart",Rationality and Intelligence: A Brief Update,Fundamental Issues of Artificial Intelligence,978-3-319-26483-7 978-3-319-26485-1,,,http://link.springer.com/10.1007/978-3-319-26485-1_2,"The long-term goal of AI is the creation and understanding of intelligence. This requires a notion of intelligence that is precise enough to allow the cumulative development of robust systems and general results. The concept of rational agency has long been considered a leading candidate to fulﬁll this role. This paper, which updates a much earlier version (Russell, 1997), reviews the sequence of conceptual shifts leading to a different candidate, bounded optimality, that is closer to our informal conception of intelligence and reduces the gap between theory and practice. Some promising recent developments are also described.",2016,2022-01-30 04:51:08,2022-01-30 04:51:08,2019-12-18 01:41:16,7-28,,,,,,Rationality and Intelligence,Synthese Library,,,,Springer International Publishing,Cham,en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s2]  ACC: 40  DOI: 10.1007/978-3-319-26485-1_2,,/Users/jacquesthibodeau/Zotero/storage/MC8IBHE9/Russell - 2016 - Rationality and Intelligence A Brief Update.pdf,,CHAI; TechSafety,Bounded rationality; Intelligence; Metareasoning; Rationality,"Müller, Vincent C.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QAGKMBNQ,bookSection,2016,"Ó hÉigeartaigh, Seán",Would You Hand Over a Decision to a Machine?,Philosophers Take On the World,,,,https://papers.ssrn.com/abstract=3446679,"Artificial intelligence (AI) will be used in many decision-making contexts, both as a decision aide and to replace human decision-making. These include what might traditionally be considered moral decisions. This chapter explores risks and opportunities posed by the use of AI in moral decision-making.",2016-09-26,2022-01-30 04:50:26,2022-01-30 04:50:26,2020-12-12 17:56:19,,,,,,,,,,,,Oxford University Press,,en,,,,,papers.ssrn.com,,ZSCC: NoCitationData[s2]  ACC: 0,,/Users/jacquesthibodeau/Zotero/storage/MDPUV3RQ/Ó hÉigeartaigh - 2016 - Would You Hand Over a Decision to a Machine.pdf; /Users/jacquesthibodeau/Zotero/storage/QSSZ7NDS/papers.html,,MetaSafety; CFI; CSER; AmbiguosSafety,AI; Artificial intelligence; bias; decision-making; risk; uncertainty,"Edmonds, D",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UISZQD9X,bookSection,2019,"Kunz, Martina; Ó hÉigeartaigh, Seán",Artificial Intelligence and Robotization,Oxford Handbook on the International Law of Global Security,,,,https://papers.ssrn.com/abstract=3310421,"This chapter provides an overview of the international law governing applications of artificial intelligence and robotics which affect global security, highlighting challenges arising from technological developments and how international regulators are responding to them. Much of the international law literature thus far has focused on the implications of increasingly autonomous weapons systems. Our contribution instead seeks to cover a broader range of global security risks resulting from large-scale diffuse or concentrated, gradual or sudden, direct or indirect, intentional or unintentional, AI or robotics-caused harm. Applications of these technologies permeate almost every domain of human activity and thus unsurprisingly have an equally wide range of risk profiles, from a discriminatory algorithmic decision causing financial distress to an AI-sparked nuclear war collapsing global civilization. Hence, it is only natural that much of the international regulatory activity takes place in domain-specific fora. Many of these fora coordinate with each other, both within and beyond the UN system, spreading insights and best practices on how to deal with common concerns such as cybersecurity, monitoring, and reliability, so as to prevent accidents and misuse.",2019-01-15,2022-01-30 04:50:24,2022-01-30 04:50:24,2020-08-21 20:05:39,,,,,,,,,,,,Social Science Research Network,"Rochester, NY",en,,,,,papers.ssrn.com,,ZSCC: NoCitationData[s3]  ACC: 6  DOI: 10.2139/ssrn.3310421,,/Users/jacquesthibodeau/Zotero/storage/BVDRNTDD/papers.html,,MetaSafety; CFI; CSER; AmbiguosSafety,artificial intelligence; global security; international law; robotics,"Geiß, Robin; Melzer, Nils",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CH7SXTSU,bookSection,2020,"Ó hÉigeartaigh, Seán",AI Research with the Potential for Malicious Use: Publication Norms and Governance Considerations,AI Governance in 2019 - A Year In Review,,,,http://lcfi.ac.uk/resources/ai-research-potential-malicious-use-publication-no/,Chapter in AI Governance in 2019 - A Year in Review: Observations from 50 Global Experts. A report produced by the Shanghai Institute of Science for Science.,2020-04,2022-01-30 04:50:24,2022-01-30 04:50:24,2020-08-24 16:28:39,,,,,,,AI Research with the Potential for Malicious Use,,,,,,,,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/3ERIHJMI/ai-research-potential-malicious-use-publication-no.html,,MetaSafety; CFI; CSER; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
E5J65IJ3,bookSection,2016,"Müller, Vincent C.; Bostrom, Nick",Future progress in artificial intelligence: A survey of expert opinion,Fundamental issues of artificial intelligence,,,,,,2016,2022-01-30 04:53:10,2022-01-30 04:53:10,,555–572,,,,,,Future progress in artificial intelligence,,,,,Springer,,,,,,,Google Scholar,,ZSCC: 0000564,,/Users/jacquesthibodeau/Zotero/storage/X6GIGDJ6/Müller and Bostrom - 2016 - Future progress in artificial intelligence A surv.pdf; /Users/jacquesthibodeau/Zotero/storage/QK7SQP4E/978-3-319-26485-1_33.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9ZFHQHFQ,bookSection,2003,"Bostrom, Nick",Ethical Issues in Advanced Artificial Intelligence,Machine Ethics and Robot Ethics,978-1-00-307499-1,,,https://www.taylorfrancis.com/books/9781000108934/chapters/10.4324/9781003074991-7,"The ethical issues related to the possible future creation of machines with general intellectual capabilities far outstripping those of humans are quite distinct from any ethical problems arising in current automation and information systems. Such superintelligence would not be just another technological development; it would be the most important invention ever made, and would lead to explosive progress in all scientific and technological fields, as the superintelligence would conduct research with superhuman efficiency. To the extent that ethics is a cognitive pursuit, a superintelligence could also easily surpass humans in the quality of its moral thinking. However, it would be up to the designers of the superintelligence to specify its original motivations. Since the superintelligence may become unstoppably powerful because of its intellectual superiority and the technologies it could develop, it is crucial that it be provided with human-friendly motivations. This paper surveys some of the unique ethical issues in creating superintelligence, and discusses what motivations we ought to give a superintelligence, and introduces some cost-benefit considerations relating to whether the development of superintelligent machines ought to be accelerated or retarded.",2003,2022-01-30 04:53:09,2022-01-30 04:53:09,2020-11-21 18:51:38,69-75,,,,,,,,,,,Routledge,,en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s2]  ACC: 317  JCC: 269  DOI: 10.4324/9781003074991-7,,/Users/jacquesthibodeau/Zotero/storage/564SWNRP/Bostrom - 2020 - Ethical Issues in Advanced Artificial Intelligence.pdf,,TechSafety; FHI,,"Wallach, Wendell; Asaro, Peter",,,,,"Wallach, Wendell; Asaro, Peter",,,,,,,,,,,,,,1,,,,,,,,,,,,,,,,,,,,,,,,,,,
PM9UNRXD,bookSection,2019,"Ahmed, Shazeda; Ding, Jeffrey; Hoffman, Samantha; Kerr, Jaclyn",Digital Authoritarianism: Evolving Chinese And Russian Models,"Artificial Intelligence, China, Russia, and the Global Order: Technological, Political, Global, and Creative Perspectives",,,,,,2019,2022-01-30 04:53:09,2022-01-30 04:53:09,,291,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: NoCitationData[s7]  ACC: 0  J: 0,,"/Users/jacquesthibodeau/Zotero/storage/UHZQDIMA/Ahmed and Berkeley - Artificial Intelligence, China, Russia, and the Gl.pdf",,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FB7STED6,bookSection,2009,"Bostrom, Nick",Why I Want to be a Posthuman when I Grow Up,Medical Enhancement and Posthumanity,978-1-4020-8852-0,,,https://doi.org/10.1007/978-1-4020-8852-0_8,"Extreme human enhancement could result in “posthuman” modes of being. After offering some definitions and conceptual clarification, I argue for two theses. First, some posthuman modes of being would be very worthwhile. Second, it could be very good for human beings to become posthuman.",2009,2022-01-30 04:53:45,2022-01-30 04:53:45,2020-08-18 20:26:22,107-136,,,,,,,"The International Library of Ethics, Law and Technology",,,,Springer Netherlands,Dordrecht,en,,,,,Springer Link,,ZSCC: NoCitationData[s3]  ACC: 424  DOI: 10.1007/978-1-4020-8852-0_8,,/Users/jacquesthibodeau/Zotero/storage/QFPJSKVN/Bostrom - 2009 - Why I Want to be a Posthuman when I Grow Up.pdf,,MetaSafety; FHI,Cognitive Capacity; Cognitive Improvement; Inclusive Fitness; Moral Status; Personal Identity,"Gordijn, Bert; Chadwick, Ruth",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BW7893Q6,bookSection,2017,"Bostrom, Nick; Sandberg, Anders",The Wisdom of Nature: An Evolutionary Heuristic for Human Enhancement,Philosophical Issues in Pharmaceutics,,,,,,2017,2022-01-30 04:53:37,2022-01-30 04:53:37,,189–219,,,,,,The Wisdom of Nature,,,,,Springer,,,,,,,Google Scholar,,ZSCC: 0000006,,/Users/jacquesthibodeau/Zotero/storage/K3M5RUCT/978-94-024-0979-6_12.html; /Users/jacquesthibodeau/Zotero/storage/IJ8IXREN/978-94-024-0979-6_12.html; /Users/jacquesthibodeau/Zotero/storage/BPVJK8JS/BOSTWO.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
88FHGR2S,bookSection,2014,"Bostrom, Nick; Yudkowsky, Eliezer",The ethics of artificial intelligence,The Cambridge Handbook of Artificial Intelligence,978-1-139-04685-5,,,https://www.cambridge.org/core/product/identifier/CBO9781139046855A027/type/book_part,"The possibility of creating thinking machines raises a host of ethical issues. These questions relate both to ensuring that such machines do not harm humans and other morally relevant beings, and to the moral status of the machines themselves. The first section discusses issues that may arise in the near future of AI. The second section outlines challenges for ensuring that AI operates safely as it approaches humans in its intelligence. The third section outlines how we might assess whether, and in what circumstances, AIs themselves have moral status. In the fourth section, we consider how AIs might diﬀer from humans in certain basic respects relevant to our ethical assessment of them. The final section addresses the issues of creating AIs more intelligent than human, and ensuring that they use their advanced intelligence for good rather than ill.",2014,2022-01-30 04:53:36,2022-01-30 04:53:36,2019-12-19 02:58:26,316-334,,,,,,,,,,,Cambridge University Press,Cambridge,en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s6]  ACC: 815  J: 453  DOI: 10.1017/CBO9781139046855.020,,/Users/jacquesthibodeau/Zotero/storage/P58U9XZE/Bostrom and Yudkowsky - 2014 - The ethics of artificial intelligence.pdf; /Users/jacquesthibodeau/Zotero/storage/XUQBWCQD/books.html,,TechSafety; FHI; MIRI,,"Frankish, Keith; Ramsey, William M.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MZMUBTV3,bookSection,2021,"Shulman, Carl; Bostrom, Nick",Sharing the World with Digital Minds,Rethinking Moral Status,978-0-19-289407-6 978-0-19-191520-8,,,https://oxford.universitypressscholarship.com/view/10.1093/oso/9780192894076.001.0001/oso-9780192894076-chapter-18,"The minds of biological creatures occupy a small corner of a much larger space of possible minds that could be created once we master the technology of artificial intelligence. Yet many of our moral intuitions and practices are based on assumptions about human nature that need not hold for digital minds. This points to the need for moral reflection as we approach the era of advanced machine intelligence. Here we focus on one set of issues, which arise from the prospect of digital minds with superhumanly strong claims to resources and influence. These could arise from the vast collective benefits that mass-produced digital minds could derive from relatively small amounts of resources. Alternatively, they could arise from individual digital minds with superhuman moral status or ability to benefit from resources. Such beings could contribute immense value to the world, and failing to respect their interests could produce a moral catastrophe, while a naive way of respecting them could be disastrous for humanity. A sensible approach requires reforms of our moral norms and institutions along with advance planning regarding what kinds of digital minds we bring into existence.",2021-08-05,2022-01-30 04:53:35,2022-01-30 04:53:35,2021-11-18 23:53:17,306-326,,,,,,,,,,,Oxford University Press,,en,,,,,DOI.org (Crossref),,ZSCC: 0000000[s0]   DOI: 10.1093/oso/9780192894076.003.0018,,/Users/jacquesthibodeau/Zotero/storage/7PASBR4W/Shulman and Bostrom - 2021 - Sharing the World with Digital Minds.pdf,,MetaSafety; FHI,,"Clarke, S; Savulescu, J",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NKNK2NSC,bookSection,2017,"Armstrong, Stuart; Yampolskiy, Roman V.",Security solutions for intelligent and complex systems,Security Solutions for Hyperconnectivity and the Internet of Things,,,,,,2017,2022-01-30 04:53:35,2022-01-30 04:53:35,,37–88,,,,,,,,,,,IGI Global,,,,,,,Google Scholar,,ZSCC: 0000006,,/Users/jacquesthibodeau/Zotero/storage/XG4GMKXK/164692.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IZI3NCES,bookSection,2013,"Armstrong, Stuart",Risks and Mitigation Strategies for Oracle AI,Philosophy and Theory of Artificial Intelligence,978-3-642-31674-6,,,https://doi.org/10.1007/978-3-642-31674-6_25,"There is no strong reason to believe human level intelligence represents an upper limit of the capacity of artificial intelligence, should it be realized. This poses serious safety issues, since a superintelligent system would have great power to direct the future according to its possibly flawed goals or motivation systems. Oracle AIs (OAI), confined AIs that can only answer questions, are one particular approach to this problem. However even Oracles are not particularly safe: humans are still vulnerable to traps, social engineering, or simply becoming dependent on the OAI. But OAIs are still strictly safer than general AIs, and there are many extra layers of precautions we can add on top of these. This paper looks at some of them and analyses their strengths and weaknesses.",2013,2022-01-30 04:53:35,2022-01-30 04:53:35,2020-12-18 06:40:27,335-347,,,,,,,"Studies in Applied Philosophy, Epistemology and Rational Ethics",,,,Springer,"Berlin, Heidelberg",en,,,,,Springer Link,,ZSCC: NoCitationData[s1]  ACC: 4  DOI: 10.1007/978-3-642-31674-6_25,,/Users/jacquesthibodeau/Zotero/storage/4WXCE999/Armstrong - 2013 - Risks and Mitigation Strategies for Oracle AI.pdf,,TechSafety; FHI,Artificial Intelligence; Capability control; Motivational control; Risks; Security; Superintelligence,"Müller, Vincent C.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4RSHXMUT,bookSection,2015,"Armstrong, Stuart; Sandberg, Anders; ÓhÉigeartaigh, Seán",Outrunning the Law: Extraterrestrial Liberty and Universal Colonisation,The Meaning of Liberty Beyond Earth,,,,,,2015,2022-01-30 04:53:19,2022-01-30 04:53:19,,165–186,,,,,,Outrunning the Law,,,,,Springer,,,,,,,Google Scholar,,ZSCC: 0000001,,/Users/jacquesthibodeau/Zotero/storage/JD5FI76H/978-3-319-09567-7_11.html; /Users/jacquesthibodeau/Zotero/storage/44ZBNBWH/978-3-319-09567-7_11.html; /Users/jacquesthibodeau/Zotero/storage/UWEWVHXU/978-3-319-09567-7_11.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7R3ZRU4R,bookSection,2018,"Drexler, K. Eric",MDL Intelligence Distillation : Exploring Strategies for Safe Access to Superintelligent Problem-Solving Capabilities,Artificial Intelligence Safety and Security,,,,https://www.taylorfrancis.com/,"AI technologies may reach the threshold of rapid, open-ended, recursive improvement before we are prepared to manage the challenges posed",2018-07-27,2022-01-30 04:53:18,2022-01-30 04:53:18,2019-12-19 02:23:48,75–88,,,,,,MDL Intelligence Distillation,,,,,Chapman and Hall/CRC,,en,,,,,Google Scholar,,ZSCC: 0000008  DOI: 10.1201/9781351251389-6,,/Users/jacquesthibodeau/Zotero/storage/357SZXHQ/9781351251389-6.html; /Users/jacquesthibodeau/Zotero/storage/5HM3J3PG/9781351251389-6.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JVKIFZC5,bookSection,2017,"Armstrong, Stuart",Introduction to the technological singularity,The Technological Singularity,,,,,,2017,2022-01-30 04:53:18,2022-01-30 04:53:18,,1–8,,,,,,,,,,,Springer,,,,,,,Google Scholar,,ZSCC: 0000006  DOI: 10.1007/978-3-662-54033-6_1,,/Users/jacquesthibodeau/Zotero/storage/2V5K4TWK/978-3-662-54033-6_1.html,,MetaSafety; FHI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8MT75XKW,bookSection,2015,"Armstrong, Stuart; Sotala, Kaj",How We’re Predicting AI – or Failing to,Beyond Artificial Intelligence,978-3-319-09667-4 978-3-319-09668-1,,,http://link.springer.com/10.1007/978-3-319-09668-1_2,"This paper will look at the various predictions that have been made about AI and propose decomposition schemas for analyzing them. It will propose a variety of theoretical tools for analyzing, judging, and improving these predictions. Focusing specifically on timeline predictions (dates given by which we should expect the creation of AI), it will show that there are strong theoretical grounds to expect predictions to be quite poor in this area. Using a database of 95 AI timeline predictions, it will show that these expectations are borne out in practice: expert predictions contradict each other considerably, and are indistinguishable from non-expert predictions and past failed predictions. Predictions that AI lie 15 to 25 years in the future are the most common, from experts and non-experts alike.",2015,2022-01-30 04:53:18,2022-01-30 04:53:18,2020-12-18 06:37:05,11-29,,,9,,,,,,,,Springer International Publishing,Cham,en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s1]  ACC: 111  Series Title: Topics in Intelligent Engineering and Informatics DOI: 10.1007/978-3-319-09668-1_2,,/Users/jacquesthibodeau/Zotero/storage/EEW3J3XR/Armstrong and Sotala - 2015 - How We’re Predicting AI – or Failing to.pdf,,MetaSafety; FHI,,"Romportl, Jan; Zackova, Eva; Kelemen, Jozef",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4PN2QVX8,bookSection,2014,"Bostrom, Nick",Introduction—The Transhumanist FAQ: A General Introduction,Transhumanism and the Body,,,,,,2014,2022-01-30 04:53:18,2022-01-30 04:53:18,,1–17,,,,,,Introduction—The Transhumanist FAQ,,,,,Springer,,,,,,,Google Scholar,,ZSCC: 0000022,,/Users/jacquesthibodeau/Zotero/storage/U54KKT8S/9781137342768_1.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SP37AAU4,bookSection,2015,"Armstrong, Stuart; Sotala, Kaj",How we’re predicting AI–or failing to,Beyond artificial intelligence,,,,,,2015,2022-01-30 04:53:18,2022-01-30 04:53:18,,11–29,,,,,,,,,,,Springer,,,,,,,Google Scholar,,ZSCC: 0000111,,/Users/jacquesthibodeau/Zotero/storage/TGPD5XXX/Armstrong and Sotala - 2015 - How we’re predicting AI–or failing to.pdf; /Users/jacquesthibodeau/Zotero/storage/8KEC66PJ/978-3-319-09668-1_2.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3I86NJCR,bookSection,2018,"Desai, Nishant; Critch, Andrew; Russell, Stuart J",Negotiable Reinforcement Learning for Pareto Optimal Sequential Decision-Making,Advances in Neural Information Processing Systems 31,,,,http://papers.nips.cc/paper/7721-negotiable-reinforcement-learning-for-pareto-optimal-sequential-decision-making.pdf,,2018,2022-01-30 04:50:55,2022-01-30 04:50:55,2019-12-18 02:14:39,4712–4720,,,,,,,,,,,"Curran Associates, Inc.",,,,,,,Neural Information Processing Systems,,ZSCC: NoCitationData[s2]  ACC: 5,,/Users/jacquesthibodeau/Zotero/storage/5WS4PP5G/Desai et al. - 2018 - Negotiable Reinforcement Learning for Pareto Optim.pdf; /Users/jacquesthibodeau/Zotero/storage/UN92H3UR/7721-negotiable-reinforcement-learning-for-pareto-optimal-sequential-decision-making.html,,CHAI; TechSafety,,"Bengio, S.; Wallach, H.; Larochelle, H.; Grauman, K.; Cesa-Bianchi, N.; Garnett, R.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CG7BVRUT,bookSection,2017,"Bestick, Aaron; Bajcsy, Ruzena; Dragan, Anca D.",Implicitly Assisting Humans to Choose Good Grasps in Robot to Human Handovers,2016 International Symposium on Experimental Robotics,978-3-319-50114-7 978-3-319-50115-4,,,http://link.springer.com/10.1007/978-3-319-50115-4_30,"We focus on selecting handover conﬁgurations that result in low human ergonomic cost not only at the time of handover, but also when the human is achieving a goal with the object after that handover. People take objects using whatever grasping conﬁguration is most comfortable to them. When the human has a goal pose they’d like to place the object at, however, the most comfortable grasping conﬁguration at the handover might be cumbersome overall, requiring regrasping or the use of an uncomfortable conﬁguration to reach the goal. We enable robots to purposefully inﬂuence the choices available to the person when taking the object, implicitly helping the person avoid suboptimal solutions and account for the goal. We introduce a probabilistic model of how humans select grasping conﬁgurations, and use this model to optimize expected cost. We present results in simulation, as well as from a user study, showing that the robot successfully inﬂuences people’s grasping conﬁgurations for the better.",2017,2022-01-30 04:50:53,2022-01-30 04:50:53,2019-12-18 01:39:49,341-354,,,1,,,,,,,,Springer International Publishing,Cham,en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s2]  ACC: 28  DOI: 10.1007/978-3-319-50115-4_30,,/Users/jacquesthibodeau/Zotero/storage/6BDV2B6U/1810.10593.pdf; /Users/jacquesthibodeau/Zotero/storage/N3QIGNTG/Bestick et al. - 2017 - Implicitly Assisting Humans to Choose Good Grasps .pdf,,CHAI; TechSafety; AmbiguosSafety,,"Kulić, Dana; Nakamura, Yoshihiko; Khatib, Oussama; Venture, Gentiane",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
K724STCW,bookSection,2021,"Russell, Stuart",Human-Compatible Artificial Intelligence,Human-Like Machine Intelligence,978-0-19-886253-6 978-0-19-189533-3,,,https://oxford.universitypressscholarship.com/view/10.1093/oso/9780198862536.001.0001/oso-9780198862536-chapter-1,"Following the analysis given by Alan Turing in 1951, one must expect that AI capabilities will eventually exceed those of humans across a wide range of real-world-decision making scenarios. Should this be a cause for concern, as Turing, Hawking, and others have suggested? And, if so, what can we do about it? While some in the mainstream AI community dismiss the issue, I will argue that the problem is real: we have to work out how to design AI systems that are far more powerful than ourselves while ensuring that they never have power over us. I believe the technical aspects of this problem are solvable. Whereas the standard model of AI proposes to build machines that optimize known, exogenously specified objectives, a preferable approach would be to build machines that are of provable benefit to humans. I introduce assistance games as a formal class of problems whose solution, under certain assumptions, has the desired property.",2021-07-13,2022-01-30 04:50:53,2022-01-30 04:50:53,2021-10-30 20:10:48,3-23,,,,,,,,,,,Oxford University Press,,en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s0]  ACC: 0  DOI: 10.1093/oso/9780198862536.003.0001,,/Users/jacquesthibodeau/Zotero/storage/QPGCQWEK/mi19book-hcai.pdf,,TechSafety,,,,,,,"Muggleton, Stephen; Chater, Nicholas",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2HFKDU6Z,bookSection,2020,"Russell, Stuart; Jeanmaire, Caroline",From the Standard Model of AI to Provably Beneficial Systems,AI Governance in 2019: A Year In Review,,,,http://n.sinaimg.cn/tech/f34884a9/20200501/GlobalAIGovernancein2019.pdf,,2020,2022-01-30 04:50:45,2022-01-30 04:50:45,,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/F,,,,MetaSafety; CHAI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CHURS33T,bookSection,2018,"Reddy, Sid; Dragan, Anca; Levine, Sergey",Where Do You Think You're Going?: Inferring Beliefs about Dynamics from Behavior,Advances in Neural Information Processing Systems 31,,,,http://papers.nips.cc/paper/7419-where-do-you-think-youre-going-inferring-beliefs-about-dynamics-from-behavior.pdf,,2018,2022-01-30 04:51:44,2022-01-30 04:51:44,2019-12-18 02:41:12,1454–1465,,,,,,Where Do You Think You\textquotesingle re Going?,,,,,"Curran Associates, Inc.",,,,,,,Neural Information Processing Systems,,ZSCC: NoCitationData[s2]  ACC: 66,,/Users/jacquesthibodeau/Zotero/storage/MM784MFH/Reddy et al. - 2018 - Where Do You Think Youtextquotesingle re Going .pdf; /Users/jacquesthibodeau/Zotero/storage/VCE24KBJ/7419-where-do-you-think-youre-going-inferring-beliefs-about-dynamics-from-behavior.html,,CHAI; TechSafety,,"Bengio, S.; Wallach, H.; Larochelle, H.; Grauman, K.; Cesa-Bianchi, N.; Garnett, R.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GDGZMWNP,bookSection,2008,"Yudkowsky, Eliezer",Cognitive biases potentially affecting judgement of global risks,Global Catastrophic Risks,978-0-19-857050-9 978-0-19-191810-0,,,https://oxford.universitypressscholarship.com/view/10.1093/oso/9780198570509.001.0001/isbn-9780198570509-book-part-9,"All else being equal, not many people would prefer to destroy the world. Even faceless corporations, meddling governments, reckless scientists, and other agents of doom, require a world in which to achieve their goals of profit, order, tenure, or other villainies. If our extinction proceeds slowly enough to allow a moment of horrified realization, the doers of the deed will likely be quite taken aback on realizing that they have actually destroyed the world. Therefore I suggest that if the Earth is destroyed, it will probably be by mistake. The systematic experimental study of reproducible errors of human reasoning, and what these errors reveal about underlying mental processes, is known as the heuristics and biases programme in cognitive psychology. This programme has made discoveries highly relevant to assessors of global catastrophic risks. Suppose you are worried about the risk of Substance P, an explosive of planet-wrecking potency which will detonate if exposed to a strong radio signal. Luckily there is a famous expert who discovered Substance P, spent the last thirty years working with it, and knows it better than anyone else in the world. You call up the expert and ask how strong the radio signal has to be. The expert replies that the critical threshold is probably around 4000 terawatts. ‘Probably?’ you query. ‘Can you give me a 98% confidence interval?’ ‘Sure’, replies the expert. ‘I’m 99%confident that the critical threshold is above 500 terawatts, and 99%confident that the threshold is below 80,000 terawatts.’ ‘What about 10 terawatts?’ you ask. ‘Impossible’, replies the expert. The above methodology for expert elicitation looks perfectly reasonable, the sort of thing any competent practitioner might do when faced with such a problem. Indeed, this methodology was used in the Reactor Safety Study (Rasmussen, 1975), now widely regarded as the first major attempt at probabilistic risk assessment. But the student of heuristics and biases will recognize at least two major mistakes in the method – not logical flaws, but conditions extremely susceptible to human error. I shall return to this example in the discussion of anchoring and adjustments biases (Section 5.7).",2008-07-03,2022-01-30 04:56:47,2022-01-30 04:56:47,2020-11-22 05:05:28,,,,,,,,,,,,Oxford University Press,,en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s2]  ACC: 231  DOI: 10.1093/oso/9780198570509.003.0009,,,,MetaSafety; MIRI,,,,,,,"Yudkowsky, Eliezer",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
I5GWFR6P,bookSection,2011,"Yudkowsky, Eliezer",Complex Value Systems in Friendly AI,Artificial General Intelligence,978-3-642-22886-5 978-3-642-22887-2,,,http://link.springer.com/10.1007/978-3-642-22887-2_48,,2011,2022-01-30 04:56:47,2022-01-30 04:56:47,2020-11-22 05:26:26,388-393,,,6830,,,,,,,,Springer Berlin Heidelberg,"Berlin, Heidelberg",,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s2]  ACC: 91  Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-22887-2_48,,,,TechSafety; MIRI,,"Schmidhuber, Jürgen; Thórisson, Kristinn R.; Looks, Moshe","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KSRAINS6,bookSection,2008,"Yudkowsky, Eliezer",Artificial Intelligence as a positive and negative factor in global risk,Global Catastrophic Risks,978-0-19-857050-9 978-0-19-191810-0,,,https://oxford.universitypressscholarship.com/view/10.1093/oso/9780198570509.001.0001/isbn-9780198570509-book-part-21,"By far the greatest danger of Artificial Intelligence (AI) is that people conclude too early that they understand it. Of course, this problem is not limited to the field of AI. Jacques Monod wrote: ‘A curious aspect of the theory of evolution is that everybody thinks he understands it’ (Monod, 1974). The problem seems to be unusually acute in Artificial Intelligence. The field of AI has a reputation for making huge promises and then failing to deliver on them. Most observers conclude that AI is hard, as indeed it is. But the embarrassment does not stem from the difficulty. It is difficult to build a star from hydrogen, but the field of stellar astronomy does not have a terrible reputation for promising to build stars and then failing. The critical inference is not that AI is hard, but that, for some reason, it is very easy for people to think they know far more about AI than they actually do. It may be tempting to ignore Artificial Intelligence because, of all the global risks discussed in this book, AI is probably hardest to discuss. We cannot consult actuarial statistics to assign small annual probabilities of catastrophe, as with asteroid strikes. We cannot use calculations from a precise, precisely confirmed model to rule out events or place infinitesimal upper bounds on their probability, as with proposed physics disasters. But this makes AI catastrophes more worrisome, not less. The effect of many cognitive biases has been found to increase with time pressure, cognitive busyness, or sparse information. Which is to say that the more difficult the analytic challenge, the more important it is to avoid or reduce bias. Therefore I strongly recommend reading my other chapter (Chapter 5) in this book before continuing with this chapter. When something is universal enough in our everyday lives, we take it for granted to the point of forgetting it exists. Imagine a complex biological adaptation with ten necessary parts. If each of the ten genes is independently at 50% frequency in the gene pool – each gene possessed by only half the organisms in that species – then, on average, only 1 in 1024 organisms will possess the full, functioning adaptation.",2008-07-03,2022-01-30 04:56:47,2022-01-30 04:56:47,2020-11-22 02:24:05,184,,,,,,,,,,,Oxford University Press,,en,,,,,DOI.org (Crossref),,ZSCC: 0000552  Publisher: Oxford University Press New York,,/Users/jacquesthibodeau/Zotero/storage/UTV64Q5B/Yudkowsky - 2008 - Artificial intelligence as a positive and negative.pdf; /Users/jacquesthibodeau/Zotero/storage/A94A8CPN/sTkfAQAAQBAJ.html,,MetaSafety; MIRI,,,,,,,"Yudkowsky, Eliezer",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
B76SIGGS,bookSection,2017,"Soares, Nate; Fallenstein, Benya",Agent Foundations for Aligning Machine Intelligence with Human Interests: A Technical Research Agenda,The Technological Singularity,978-3-662-54031-2 978-3-662-54033-6,,,http://link.springer.com/10.1007/978-3-662-54033-6_5,,2017,2022-01-30 04:56:46,2022-01-30 04:56:46,2019-12-19 02:58:39,103-125,,,,,,Agent Foundations for Aligning Machine Intelligence with Human Interests,,,,,Springer Berlin Heidelberg,"Berlin, Heidelberg",en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s8]  ACC: 45  J: 31 DOI: 10.1007/978-3-662-54033-6_5,,/Users/jacquesthibodeau/Zotero/storage/CD67J8VC/Soares and Fallenstein - 2017 - Agent Foundations for Aligning Machine Intelligenc.pdf,,TechSafety; MIRI,,"Callaghan, Victor; Miller, James; Yampolskiy, Roman; Armstrong, Stuart",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6UBXU3EC,bookSection,2017,"Barrett, Anthony M.; Baum, Seth D.",Risk analysis and risk management for the artificial superintelligence research and development process,The Technological Singularity,,,,,,2017,2022-01-30 04:55:20,2022-01-30 04:55:20,,127–140,,,,,,,,,,,Springer,,,,,,,Google Scholar,,ZSCC: 0000014  DOI: 10.1007/978-3-662-54033-6_6,,/Users/jacquesthibodeau/Zotero/storage/JB2CXJC3/Barrett and Baum - 2017 - Risk analysis and risk management for the artifici.pdf; /Users/jacquesthibodeau/Zotero/storage/TJZUEBZD/978-3-662-54033-6_6.html,,MetaSafety; GCRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
B7EIPZ6G,bookSection,2019,"Baum, Seth",Lessons for Artificial Intelligence from Other Global Risks,The Global Politics of Artificial Intelligence,,,,,"The prominence of artificial intelligence (AI) as a global risk is a relatively recent phenomenon. Other global risks have longer histories and larger bodies of scholarship. The study of these other risks can offer considerable insight to the study of AI risk. This paper examines four risks: biotechnology, nuclear weapons, global warming, and asteroid collision. Several overarching lessons are found. First, the extreme severity of global risks is often insufficient to motivate action to reduce the risks. Second, perceptions of global risks can be influenced by people’s incentives and by their cultural and intellectual orientations. Third, the success of efforts to address global risks can depend on the extent of buy-in from parties who may be negatively affected by the efforts. Fourth, global risks and risk reduction initiatives can be shaped by broader socio-political conditions, such as the degree of policy influence of private industry within a political jurisdiction. The paper shows how these and other lessons can inform efforts to reduce risks from AI.",2019,2022-01-30 04:55:19,2022-01-30 04:55:19,,20,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: NoCitationData[s3]  ACC: 2,,/Users/jacquesthibodeau/Zotero/storage/TNHAPD98/Baum - Lessons for Artificial Intelligence from Other Glo.pdf,,MetaSafety; GCRI,,"Tinnirello, Maurizio",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
S4G7N475,bookSection,2011,"Ring, Mark; Orseau, Laurent","Delusion, Survival, and Intelligent Agents",Artificial General Intelligence,978-3-642-22886-5 978-3-642-22887-2,,,http://link.springer.com/10.1007/978-3-642-22887-2_2,"This paper considers the consequences of endowing an intelligent agent with the ability to modify its own code. The intelligent agent is patterned closely after AIXI with these speciﬁc assumptions: 1) The agent is allowed to arbitrarily modify its own inputs if it so chooses; 2) The agent’s code is a part of the environment and may be read and written by the environment. The ﬁrst of these we call the “delusion box”; the second we call “mortality”. Within this framework, we discuss and compare four very diﬀerent kinds of agents, speciﬁcally: reinforcementlearning, goal-seeking, prediction-seeking, and knowledge-seeking agents. Our main results are that: 1) The reinforcement-learning agent under reasonable circumstances behaves exactly like an agent whose sole task is to survive (to preserve the integrity of its code); and 2) Only the knowledge-seeking agent behaves completely as expected.",2011,2022-01-30 04:59:45,2022-01-30 04:59:45,2020-11-21 17:39:39,11-20,,,6830,,,,,,,,Springer Berlin Heidelberg,"Berlin, Heidelberg",en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s1]  ACC: 77  Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-22887-2_2,,"/Users/jacquesthibodeau/Zotero/storage/FRZ9DPD5/Ring and Orseau - 2011 - Delusion, Survival, and Intelligent Agents.pdf",,TechSafety; Other-org,,"Schmidhuber, Jürgen; Thórisson, Kristinn R.; Looks, Moshe","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
93TJ5D9A,bookSection,2017,"Durán, Juan M.",Computer Simulations as a Technological Singularity in the Empirical Sciences,The Technological Singularity: Managing the Journey,978-3-662-54033-6,,,https://doi.org/10.1007/978-3-662-54033-6_9,"SummaryIn this paper, I discuss the conditions necessary for computer simulations to qualify as a technological singularity in the empirical sciences. A technological singularity encompasses two claims: (a) the enhancement of human cognitive capacities by the computer, and (b) their displacement from the center of the production of knowledge. For computer simulations to be a technological singularity, then, they must fulfill points (a) and (b) above. Although point (a) is relatively unproblematic, point (b) needs further analysis. In particular, in order to show that humans could be displaced from the center of the production of knowledge, it is necessary to establish the reliability of computer simulations. That is, I need to show that computer simulations are reliable processes that render, most of the time, valid results. To be a reliable process, in turn, means that simulations accurately represent the target system and carry out error-free computations. I analyze verification and validation methods as the grounds for such representation accuracy and error-free computations. Since the aim is to entrench computer simulations as a technological singularity, the entire analysis must be careful to keep human agents out of the picture.",2017,2022-01-30 04:59:44,2022-01-30 04:59:44,2020-11-24 02:59:54,167-179,,,,,,,The Frontiers Collection,,,,Springer,"Berlin, Heidelberg",en,,,,,Springer Link,,ZSCC: NoCitationData[s1]  ACC: 4  DOI: 10.1007/978-3-662-54033-6_9,,,,MetaSafety; Other-org,Computer Simulation; Empirical Science; Epistemic Justification; Reliable Process; Target System,"Callaghan, Victor; Miller, James; Yampolskiy, Roman; Armstrong, Stuart",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZT7JGH6T,bookSection,2017,"Koepsell, David",Can the Singularity Be Patented? (And Other IP Conundrums for Converging Technologies),The Technological Singularity: Managing the Journey,978-3-662-54033-6,,,https://doi.org/10.1007/978-3-662-54033-6_10,"SummaryAssuming that the singularity is eventually realized, some of the legal institutions that we take for granted, specifically those relating to “intellectual property” (IP – namely, copyrights and patents), may pose some problems. IP law concerns the ownership of expressions of ideas, and not ideas themselves. Given the nature and trajectory of converging technologies, IP laws as they currently exist may impede the development of such technologies. Examples of “patent thickets” that appear to impede other rapidly evolving technologies already abound (as in the smartphone arena). Patents and copyrights may pose even more intriguing problems once the singularity is achieved because our notions of who may own what will likely radically change. Will artificial intelligences, for example, compete with us over rights to create, and will we be legally or morally precluded from ownership rights in technologies that make such agents function? Before the singularity arrives, we would do well to work through some of these legal conundrums raised and discussed below.",2017,2022-01-30 04:59:43,2022-01-30 04:59:43,2020-11-24 02:59:55,181-191,,,,,,Can the Singularity Be Patented?,The Frontiers Collection,,,,Springer,"Berlin, Heidelberg",en,,,,,Springer Link,,ZSCC: NoCitationData[s1]  ACC: 0  DOI: 10.1007/978-3-662-54033-6_10,,,,MetaSafety; Other-org,Artificial Agent; Intellectual Property; Natural Phenomenon; Patent Holder; Patent Office,"Callaghan, Victor; Miller, James; Yampolskiy, Roman; Armstrong, Stuart",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IQJW66U7,bookSection,2006,"Spears, Diana F.",Assuring the Behavior of Adaptive Agents,Agent Technology from a Formal Perspective,978-1-85233-947-0,,,http://link.springer.com/10.1007/1-84628-271-3_8,,2006,2022-01-30 04:59:36,2022-01-30 04:59:36,2020-11-22 01:48:09,227-257,,,,,,,,,,,Springer-Verlag,London,en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s2]  ACC: 14  Series Title: NASA Monographs in Systems and Software Engineering DOI: 10.1007/1-84628-271-3_8,,,,TechSafety; Other-org,,"Rouff, Christopher A.; Hinchey, Michael; Rash, James; Truszkowski, Walter; Gordon-Spears, Diana",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CPJINAIP,bookSection,2012,"Hibbard, Bill",Avoiding Unintended AI Behaviors,Artificial General Intelligence,978-3-642-35505-9 978-3-642-35506-6,,,http://link.springer.com/10.1007/978-3-642-35506-6_12,,2012,2022-01-30 04:59:36,2022-01-30 04:59:36,2020-11-22 01:47:56,107-116,,,7716,,,,,,,,Springer Berlin Heidelberg,"Berlin, Heidelberg",en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s2]  ACC: 25  Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-35506-6_12,,/Users/jacquesthibodeau/Zotero/storage/IE7575I5/Hibbard - 2012 - Avoiding Unintended AI Behaviors.pdf,,TechSafety; Other-org,,"Bach, Joscha; Goertzel, Ben; Iklé, Matthew","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AM7ZSJ5W,bookSection,2017,"Clarke, Graham",A Psychoanalytic Approach to the Singularity: Why We Cannot Do Without Auxiliary Constructions,The Technological Singularity: Managing the Journey,978-3-662-54033-6,,,https://doi.org/10.1007/978-3-662-54033-6_12,"SummaryPsychoanalysis is known above all else for its insistence that we have motivations that are unknown to ourselves, that are unconscious. We are all subject to sickness and accident, to bad luck and unfair breaks, and above all to death as a final end to all our endeavours. In order to compensate for these disappointments and for our ultimate inability to overcome these very real and material constraints we phantasise, we dream, we create, and/or we nurse our bruised and fragile selves by hoping that our phantasies might come true, if not for ourselves then for our offspring. The singularity, as it is most commonly expressed, concerns the possibility of overcoming death by achieving a sort of immortality. In specific terms Kurtweil’s own discussion of the singularity is concerned with the possibility of ‘resurrecting’ his dead father in virtual space at least. There is consistently throughout the writings on the singularity a dismissal of the emotional aspect of human living in favour of the rational overcoming of our existential condition. I am arguing that we cannot ignore the emotional consciousness that is the bedrock of human existence and that we ignore our unconscious feelings at our peril. I think that the singularity as it is being developed is actually a direct threat to the flourishing of human beings and human society because the emotional shortcomings of the theory have not been recognised.",2017,2022-01-30 04:59:33,2022-01-30 04:59:33,2020-11-24 03:00:00,209-220,,,,,,A Psychoanalytic Approach to the Singularity,The Frontiers Collection,,,,Springer,"Berlin, Heidelberg",en,,,,,Springer Link,,ZSCC: NoCitationData[s1]  ACC: 1  DOI: 10.1007/978-3-662-54033-6_12,,,,MetaSafety; Other-org,Affective Computing; Computer Agent; Emotional Intelligence; Emotional Relationship; Technological Singularity,"Callaghan, Victor; Miller, James; Yampolskiy, Roman; Armstrong, Stuart",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6KM4PB68,bookSection,2011,"Anderson, Susan Leigh; Anderson, Michael",A Prima Facie Duty Approach to Machine Ethics,Machine Ethics,978-0-511-97803-6,,,https://www.cambridge.org/core/product/identifier/CBO9780511978036A041/type/book_part,,2011,2022-01-30 04:59:33,2022-01-30 04:59:33,2020-11-22 02:22:02,476-492,,,,,,,,,,,Cambridge University Press,Cambridge,,,,,,DOI.org (Crossref),,ZSCC: 0000032  DOI: 10.1017/CBO9780511978036.032,,,,TechSafety; Other-org,,"Anderson, Michael; Anderson, Susan Leigh",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WI6FD4K9,bookSection,2012,"Muehlhauser, Luke; Helm, Louie",The Singularity and Machine Ethics,Singularity Hypotheses,978-3-642-32559-5 978-3-642-32560-1,,,http://link.springer.com/10.1007/978-3-642-32560-1_6,,2012,2022-01-30 04:56:59,2022-01-30 04:56:59,2020-11-22 02:23:12,101-126,,,,,,,,,,,Springer Berlin Heidelberg,"Berlin, Heidelberg",,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s3]  ACC: 94  Series Title: The Frontiers Collection DOI: 10.1007/978-3-642-32560-1_6,,,,TechSafety; MIRI,,"Eden, Amnon H.; Moor, James H.; Søraker, Johnny H.; Steinhart, Eric",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NQQM74KV,bookSection,2015,"Fallenstein, Benja; Kumar, Ramana",Proof-Producing Reflection for HOL,Interactive Theorem Proving,978-3-319-22101-4 978-3-319-22102-1,,,http://link.springer.com/10.1007/978-3-319-22102-1_11,"We present a reﬂection principle of the form “If ϕ is provable, then ϕ” implemented in the HOL4 theorem prover, assuming the existence of a large cardinal. We use the large-cardinal assumption to construct a model of HOL within HOL, and show how to ensure ϕ has the same meaning both inside and outside of this model. Soundness of HOL implies that if ϕ is provable, then it is true in this model, and hence ϕ holds. We additionally show how this reﬂection principle can be extended, assuming an inﬁnite hierarchy of large cardinals, to implement model polymorphism, a technique designed for verifying systems with self-replacement functionality.",2015,2022-01-30 04:56:58,2022-01-30 04:56:58,2019-12-19 02:58:35,170-186,,,9236,,,,,,,,Springer International Publishing,Cham,en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s7]  ACC: 13  J: 12 DOI: 10.1007/978-3-319-22102-1_11,,/Users/jacquesthibodeau/Zotero/storage/RJGGDF64/Fallenstein and Kumar - 2015 - Proof-Producing Reflection for HOL.pdf,,TechSafety; MIRI,,"Urban, Christian; Zhang, Xingyuan",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3NCRUW5W,bookSection,2014,"Fallenstein, Benja; Soares, Nate",Problems of Self-reference in Self-improving Space-Time Embedded Intelligence,Artificial General Intelligence,978-3-319-09273-7 978-3-319-09274-4,,,http://link.springer.com/10.1007/978-3-319-09274-4_3,,2014,2022-01-30 04:56:58,2022-01-30 04:56:58,2020-11-22 04:16:32,21-32,,,8598,,,,,,,,Springer International Publishing,Cham,,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s2]  ACC: 27  Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-09274-4_3,,/Users/jacquesthibodeau/Zotero/storage/NGSVTGD3/Fallenstein and Soares - 2014 - Problems of Self-reference in Self-improving Space.pdf,,TechSafety; MIRI,,"Goertzel, Ben; Orseau, Laurent; Snaider, Javier","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Kobsa, Alfred; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Terzopoulos, Demetri; Tygar, Doug; Weikum, Gerhard",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DUXIZFVV,bookSection,2012,"Demski, Abram",Logical Prior Probability,Artificial General Intelligence,978-3-642-35505-9 978-3-642-35506-6,,,http://link.springer.com/10.1007/978-3-642-35506-6_6,,2012,2022-01-30 04:56:57,2022-01-30 04:56:57,2020-11-22 05:25:32,50-59,,,7716,,,,,,,,Springer Berlin Heidelberg,"Berlin, Heidelberg",en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s2]  ACC: 19  Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-35506-6_6,,,,TechSafety; MIRI,,"Bach, Joscha; Goertzel, Ben; Iklé, Matthew","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RF232Z92,bookSection,2007,"Pereira, Luís Moniz; Saptawijaya, Ari",Modelling Morality with Prospective Logic,Progress in Artificial Intelligence,978-3-540-77000-8,,,http://link.springer.com/10.1007/978-3-540-77002-2_9,,2007,2022-01-30 05:00:01,2022-01-30 05:00:01,2020-11-22 02:23:23,99-111,,,4874,,,,,,,,Springer Berlin Heidelberg,"Berlin, Heidelberg",en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s2]  ACC: 61  Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-540-77002-2_9,,,,TechSafety; AmbiguosSafety; Other-org,,"Neves, José; Santos, Manuel Filipe; Machado, José Manuel",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RBQUNTRR,bookSection,2018,"Jilk, David J.",Limits to Verification and Validation of Agentic Behavior,Artificial Intelligence Safety and Security,,,,http://arxiv.org/abs/1604.06963,"Verification and validation of agentic behavior have been suggested as important research priorities in efforts to reduce risks associated with the creation of general artificial intelligence (Russell et al 2015). In this paper we question the appropriateness of using language of certainty with respect to efforts to manage that risk. We begin by establishing a very general formalism to characterize agentic behavior and to describe standards of acceptable behavior. We show that determination of whether an agent meets any particular standard is not computable. We discuss the extent of the burden associated with verification by manual proof and by automated behavioral governance. We show that to ensure decidability of the behavioral standard itself, one must further limit the capabilities of the agent. We then demonstrate that if our concerns relate to outcomes in the physical world, attempts at validation are futile. Finally, we show that layered architectures aimed at making these challenges tractable mistakenly equate intentions with actions or outcomes, thereby failing to provide any guarantees. We conclude with a discussion of why language of certainty should be eradicated from the conversation about the safety of general artificial intelligence.",2018,2022-01-30 04:59:59,2022-01-30 04:59:59,2020-12-13 20:00:55,225-234,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000008[s0]  arXiv: 1604.06963,,/Users/jacquesthibodeau/Zotero/storage/BXXTPQ43/Jilk - 2016 - Limits to Verification and Validation of Agentic B.pdf; /Users/jacquesthibodeau/Zotero/storage/WTZWJK5S/1604.html,,TechSafety; Other-org,Computer Science - Artificial Intelligence; I.2.0; F.3.1; K.4.1; D.2.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
X3INWFGE,bookSection,2004,"Turing, Alan","Intelligent Machinery, A Heretical Theory (c.1951)",The Essential Turing,978-0-19-825079-1 978-0-19-191652-6,,,https://oxford.universitypressscholarship.com/view/10.1093/oso/9780198250791.001.0001/isbn-9780198250791-book-part-18,"Turing gave the presentation ‘Intelligent Machinery, A Heretical Theory’ on a radio discussion programme called The ’51 Society. Named after the year in which the programme first went to air, The ’51 Society was produced by the BBC Home Service at their Manchester studio and ran for several years. A presentation by the week’s guest would be followed by a panel discussion. Regulars on the panel included Max Newman, Professor of Mathematics at Manchester, the philosopher Michael Polanyi, then Professor of Social Studies at Manchester, and the mathematician Peter Hilton, a younger member of Newman’s department at Manchester who had worked with Turing and Newman at Bletchley Park. Turing’s target in ‘Intelligent Machinery, A Heretical Theory’ is the claim that ‘You cannot make a machine to think for you’ (p. 472). A common theme in his writing is that if a machine is to be intelligent, then it will need to ‘learn by experience’ (probably with some pre-selection, by an external educator, of the experiences to which the machine will be subjected). The present article continues the discussion of machine learning begun in Chapters 10 and 11. Turing remarks that the ‘human analogy alone’ suggests that a process of education ‘would in practice be an essential to the production of a reasonably intelligent machine within a reasonably short space of time’ (p. 473). He emphasizes the point, also made in Chapter 11, that one might ‘start from a comparatively simple machine, and, by subjecting it to a suitable range of ‘‘experience’’ transform it into one which was more elaborate, and was able to deal with a far greater range of contingencies’ (p. 473). Turing goes on to give some indication of how learning might be accomplished, introducing the idea of a machine’s building up what he calls ‘indexes of experiences’ (p. 474). (This idea is not mentioned elsewhere in his writings.) An example of an index of experiences is a list (ordered in some way) of situations in which the machine has found itself, coupled with the action that was taken, and the outcome, good or bad. The situations are described in terms of features.",2004-09-09,2022-01-30 04:59:58,2022-01-30 04:59:58,2020-11-22 05:05:23,,,,,,,,,,,,Oxford University Press,,en,,,,,DOI.org (Crossref),,ZSCC: 0000011  DOI: 10.1093/oso/9780198250791.003.0018,,,,TechSafety; Other-org,,,,,,,"Turing, Alan",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EBXVI57Q,bookSection,2017,"Zheng, Ping; Akhmad, Mohammed-Asif",How Change Agencies Can Affect Our Path Towards a Singularity,The Technological Singularity: Managing the Journey,978-3-662-54033-6,,,https://doi.org/10.1007/978-3-662-54033-6_4,"SummaryThis chapter uses the perspective of change agencies to analyse how agents (such as governments, international companies, entrepreneurs and individuals) innovate, interact, assimilate, consume and ultimately determine the direction of future technologies. These are the key components to the formation of technological singularity, i.e. an artificial intelligence becoming self-aware and self-evolving leading to an unprecedented rapid technological change in human civilization. General behaviours of change agents towards relevant technological research and development are discussed with a view to the economic and social implications. The interactions of key change agents can assist in the determination of future paths towards a singularity event or possibly even an ‘anti-singularity event’. Understanding the fundamental behaviours and motivations of change agents in technology development will increase our understanding of potential mechanisms to monitor and control developments such as Artificial Intelligence research to ensure that if and when singularity occurs it can be controlled and positively utilised for social and economic benefits.",2017,2022-01-30 04:59:57,2022-01-30 04:59:57,2020-11-24 02:59:42,87-101,,,,,,,The Frontiers Collection,,,,Springer,"Berlin, Heidelberg",en,,,,,Springer Link,,ZSCC: NoCitationData[s1]  ACC: 0  DOI: 10.1007/978-3-662-54033-6_4,,,,MetaSafety; Other-org,Change Agency; Human Brain Function; Human Race; Singularity Event; Technological Change Process,"Callaghan, Victor; Miller, James; Yampolskiy, Roman; Armstrong, Stuart",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BR95HICU,bookSection,2016,"Steunebrink, Bas R.; Thórisson, Kristinn R.; Schmidhuber, Jürgen",Growing Recursive Self-Improvers,Artificial General Intelligence,978-3-319-41648-9 978-3-319-41649-6,,,http://link.springer.com/10.1007/978-3-319-41649-6_13,"Research into the capability of recursive self-improvement typically only considers pairs of agent, self-modiﬁcation candidate , and asks whether the agent can determine/prove if the self-modiﬁcation is beneﬁcial and safe. But this leaves out the much more important question of how to come up with a potential self-modiﬁcation in the ﬁrst place, as well as how to build an AI system capable of evaluating one. Here we introduce a novel class of AI systems, called experience-based AI (EXPAI), which trivializes the search for beneﬁcial and safe self-modiﬁcations. Instead of distracting us with proof-theoretical issues, EXPAI systems force us to consider their education in order to control a system’s growth towards a robust and trustworthy, benevolent and well-behaved agent. We discuss what a practical instance of EXPAI looks like and build towards a “test theory” that allows us to gauge an agent’s level of understanding of educational material.",2016,2022-01-30 04:59:56,2022-01-30 04:59:56,2020-12-13 19:59:05,129-139,,,9782,,,,,,,,Springer International Publishing,Cham,en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s2]  ACC: 20  Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-319-41649-6_13,,/Users/jacquesthibodeau/Zotero/storage/XGG7PJ49/Steunebrink et al. - 2016 - Growing Recursive Self-Improvers.pdf,,TechSafety; Other-org,,"Steunebrink, Bas; Wang, Pei; Goertzel, Ben",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
87J7MWC6,bookSection,2017,"Peacock, Kent A.","Energy, Complexity, and the Singularity",The Technological Singularity: Managing the Journey,978-3-662-54033-6,,,https://doi.org/10.1007/978-3-662-54033-6_8,"SummaryThis paper explores the relevance of ecological limitations such as climate change and resource exhaustion to the possibility of a technologically-mediated “intelligence explosion” in the near future. The imminent risks of global carbonization and loss of biodiversity, as well as the dependency of technological development on a healthy biosphere, are greatly underestimated by singularity theorists such as Ray Kurzweil. While development of information technology should continue, we cannot rely on hypothetical advances in AI to get us out of our present ecological bottleneck. Rather, we should do everything we can to foster human ingenuity, the one factor that has a record of generating the game-changing innovations that our species has relied upon to overcome survival challenges in our past.",2017,2022-01-30 04:59:47,2022-01-30 04:59:47,2020-11-24 02:59:52,153-165,,,,,,,The Frontiers Collection,,,,Springer,"Berlin, Heidelberg",en,,,,,Springer Link,,ZSCC: NoCitationData[s1]  ACC: 1  DOI: 10.1007/978-3-662-54033-6_8,,,,MetaSafety; Other-org,Ecological Challenge; Exponential Expansion; Global Carbonization; Human Ingenuity; Singularity Hypothesis,"Callaghan, Victor; Miller, James; Yampolskiy, Roman; Armstrong, Stuart",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
U6T93H7J,bookSection,2017,"Majot, Andrew; Yampolskiy, Roman",Diminishing Returns and Recursive Self Improving Artificial Intelligence,The Technological Singularity: Managing the Journey,978-3-662-54033-6,,,https://doi.org/10.1007/978-3-662-54033-6_7,"SummaryIn this chapter we will examine in more detail the concept of an artificial intelligence that can improve upon itself, and show how that might not be as problematic as some researchers think. The ability for an AI to better itself over time through a process called recursive self-improvement has been considered as a promising path to creating the technological singularity. In this type of system an AI has access to its own source code and possibly even hardware, with the ability to edit both at will. This gives the AI the option to constantly improve upon itself and become increasingly intelligent. Eventually this would produce versions of the AI that are more intelligent than humans and cause us to reach the technological singularity. Researchers have speculated that this process could create an extremely dangerous situation for humanity as we get left behind in a growing intelligence gap. This chapter proposes that this gap would not be as drastic as initially thought, and that there may be natural limits on the ability for an AI to improve upon itself. Along the way we will propose that the law of diminishing returns will take effect to limit runaway intelligence. We also theorize that developing and manufacturing new hardware will introduce a latency in AI improvement that could easily be exploited to halt any dangerous situation.",2017,2022-01-30 04:59:46,2022-01-30 04:59:46,2020-11-24 02:59:49,141-152,,,,,,,The Frontiers Collection,,,,Springer,"Berlin, Heidelberg",en,,,,,Springer Link,,ZSCC: NoCitationData[s1]  ACC: 3  DOI: 10.1007/978-3-662-54033-6_7,,,,MetaSafety; AmbiguosSafety; Other-org,Technological Singularity; Cognitive Algorithm; Hardware Improvement; Inductive Logic Programming; Logistical Chain,"Callaghan, Victor; Miller, James; Yampolskiy, Roman; Armstrong, Stuart",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,bookSection,2008,"Omohundro, Stephen",The Basic AI Drives,Artificial General Intelligence 2008: Proceedings of the First AGI Conference,978-1-60750-309-5,,,,"The field of Artificial Intelligence (AI) was initially directly aimed at the construction of ‘thinking machines’ – that is, computer systems with human-like general intelligence. But this task proved more difficult than expected. As the years passed, AI researchers gradually shifted focus to producing AI systems that intelligently approached specific tasks in relatively narrow domains. In recent years, however, more and more AI researchers have recognized the necessity – and the feasibility – of returning to the original goal of the field. Increasingly, there is a call to focus less on highly specialized ‘narrow AI’ problem solving systems, and more on confronting the difficult issues involved in creating ‘human-level intelligence’, and ultimately general intelligence that goes beyond the human level in various ways. Artificial General Intelligence (AGI), as this renewed focus has come to be called, attempts to study and reproduce intelligence as a whole in a domain independent way. Encouraged by the recent success of several smaller-scale AGI-related meetings and special tracks at conferences, the initiative to organize the very first international conference on AGI was taken, with the goal to give researchers in the field an opportunity to present relevant research results and to exchange ideas on topics of common interest. In this collection you will find the conference papers: full-length papers, short position statements and also the papers presented in the post conference workshop on the sociocultural, ethical and futurological implications of AGI.",2008-02-18,2020-11-21 16:57,2020-12-21 18:23,,,,,,,,,,,,,IOS Press,,en,,,,,Google Books,,ZSCC: 0000022  Google-Books-ID: atjvAgAAQBAJ,,,https://www.google.com/books?id=atjvAgAAQBAJ,Other-org; TechSafety,Computers / Intelligence (AI) & Semantics; Computers / General,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper from 2008 introduces convergent instrumental subgoals: the subgoals that an AI system will have “by default”, unless care is taken to avoid them. For this paper, an AI system is a system that “has goals which it tries to accomplish by acting in the world”, i.e. it assumes that the system is <@goal-directed@>(@Intuitions about goal-directed behavior@).

It starts by arguing that a sufficiently powerful goal-directed AI system will want to self-improve, as that could help it achieve its goals better in the (presumably long) future. In particular, it will want to become “rational”, in the sense that it will want to maximize its _expected_ utility, where the utility function is determined by its goal. (The justification for this is the VNM theorem, and the various Dutch book arguments that support Bayesianism and expected utility maximization.)

However, not all modifications would be good for the AI system. In particular, it will very strongly want to preserve its utility function, as that determines what it will (try to) accomplish in the future, and any change in the utility function would be a disaster from the perspective of the current utility function. Similarly, it will want to protect itself from harm, that is, it has a survival incentive, because it can’t accomplish its goal if it’s dead.

The final instrumental subgoal is to acquire resources and use them efficiently in pursuit of its goal, because almost by definition resources are useful for a wide variety of goals, including (probably) the AI system’s goal."
IWUNUHTA,conferencePaper,2020,"Yang, Zitong; Yu, Yaodong; You, Chong; Steinhardt, Jacob; Ma, Yi",Rethinking Bias-Variance Trade-off for Generalization of Neural Networks,Proceedings of the 37th International Conference on Machine Learning,,,,http://arxiv.org/abs/2002.11328,"The classical bias-variance trade-off predicts that bias decreases and variance increase with model complexity, leading to a U-shaped risk curve. Recent work calls this into question for neural networks and other over-parameterized models, for which it is often observed that larger models generalize better. We provide a simple explanation for this by measuring the bias and variance of neural networks: while the bias is monotonically decreasing as in the classical theory, the variance is unimodal or bell-shaped: it increases then decreases with the width of the network. We vary the network architecture, loss function, and choice of dataset and confirm that variance unimodality occurs robustly for all models we considered. The risk curve is the sum of the bias and variance curves and displays different qualitative shapes depending on the relative scale of bias and variance, with the double descent curve observed in recent literature as a special case. We corroborate these empirical results with a theoretical analysis of two-layer linear networks with random first layer. Finally, evaluation on out-of-distribution data shows that most of the drop in accuracy comes from increased bias while variance increases by a relatively small amount. Moreover, we find that deeper models decrease bias and increase variance for both in-distribution and out-of-distribution data.",2020-12-07,2022-01-30 04:47:35,2022-01-30 04:47:35,2021-11-13 14:19:28,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000061  arXiv: 2002.11328,,/Users/jacquesthibodeau/Zotero/storage/7J95438X/Yang et al. - 2020 - Rethinking Bias-Variance Trade-off for Generalizat.pdf,,UnsortedSafety,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICML 2020,,,,,,,,,,,,,,,,
TIBNQTSX,conferencePaper,2020,"Matheos, George; Lew, Alexander K.; Ghavamizadeh, Matin; Russell, Stuart; Cusumano-Towner, Marco; Mansinghka, Vikash",Transforming Worlds: Automated Involutive MCMC for Open-Universe Probabilistic Models,,,,,https://openreview.net/forum?id=8Itm8dQnJRc,"Inference in open-universe probabilistic models can be challenging.  We show how to automate a broad class of MCMC kernels for them, facilitating the development of domain-specific algorithms for...",2020-11-23,2022-01-30 04:47:35,2022-01-30 04:47:35,2021-10-30 21:42:12,,,,,,,Transforming Worlds,,,,,,,en,,,,,openreview.net,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/4QSFBCAS/forum.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Third Symposium on Advances in Approximate Bayesian Inference,,,,,,,,,,,,,,,,
RQWBFZ9W,conferencePaper,2020,"Dathathri, Sumanth; Dvijotham, Krishnamurthy; Kurakin, Alexey; Raghunathan, Aditi; Uesato, Jonathan; Bunel, Rudy; Shankar, Shreya; Steinhardt, Jacob; Goodfellow, Ian; Liang, Percy; Kohli, Pushmeet",Enabling certification of verification-agnostic networks via memory-efficient semidefinite programming,arXiv:2010.11645 [cs],,,,http://arxiv.org/abs/2010.11645,"Convex relaxations have emerged as a promising approach for verifying desirable properties of neural networks like robustness to adversarial perturbations. Widely used Linear Programming (LP) relaxations only work well when networks are trained to facilitate verification. This precludes applications that involve verification-agnostic networks, i.e., networks not specially trained for verification. On the other hand, semidefinite programming (SDP) relaxations have successfully be applied to verification-agnostic networks, but do not currently scale beyond small networks due to poor time and space asymptotics. In this work, we propose a first-order dual SDP algorithm that (1) requires memory only linear in the total number of network activations, (2) only requires a fixed number of forward/backward passes through the network per iteration. By exploiting iterative eigenvector methods, we express all solver operations in terms of forward and backward passes through the network, enabling efficient use of hardware like GPUs/TPUs. For two verification-agnostic networks on MNIST and CIFAR-10, we significantly improve L-inf verified robust accuracy from 1% to 88% and 6% to 40% respectively. We also demonstrate tight verification of a quadratic stability specification for the decoder of a variational autoencoder.",2020-11-03,2022-01-30 04:47:35,2022-01-30 04:47:35,2021-10-31 19:04:24,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s1]  ACC: 24  arXiv: 2010.11645,,/Users/jacquesthibodeau/Zotero/storage/TFVBZFJ6/Dathathri et al. - 2020 - Enabling certification of verification-agnostic ne.pdf; /Users/jacquesthibodeau/Zotero/storage/SQ7TS9GM/2010.html; /Users/jacquesthibodeau/Zotero/storage/VMH5N4E5/2010.html,,UnsortedSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"34th Conference on Neural Information Processing Systems (NeurIPS 2020),",,,,,,,,,,,,,,,,
IK3Q65C6,conferencePaper,2020,"Li, Alexander C.; Pinto, Lerrel; Abbeel, Pieter",Generalized Hindsight for Reinforcement Learning,"34th Conference on Neural Information Processing Systems (NeurIPS 2020),",,,,http://arxiv.org/abs/2002.11708,"One of the key reasons for the high sample complexity in reinforcement learning (RL) is the inability to transfer knowledge from one task to another. In standard multi-task RL settings, low-reward data collected while trying to solve one task provides little to no signal for solving that particular task and is hence effectively wasted. However, we argue that this data, which is uninformative for one task, is likely a rich source of information for other tasks. To leverage this insight and efficiently reuse data, we present Generalized Hindsight: an approximate inverse reinforcement learning technique for relabeling behaviors with the right tasks. Intuitively, given a behavior generated under one task, Generalized Hindsight returns a different task that the behavior is better suited for. Then, the behavior is relabeled with this new task before being used by an off-policy RL optimizer. Compared to standard relabeling techniques, Generalized Hindsight provides a substantially more efficient reuse of samples, which we empirically demonstrate on a suite of multi-task navigation and manipulation tasks. Videos and code can be accessed here: https://sites.google.com/view/generalized-hindsight.",2020-02-26,2022-01-30 04:47:35,2022-01-30 04:47:35,2021-11-07 18:21:08,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000016  arXiv: 2002.11708,,/Users/jacquesthibodeau/Zotero/storage/9T8PK9IJ/Li et al. - 2020 - Generalized Hindsight for Reinforcement Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/KRXF5ZMV/2002.html,,UnsortedSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Neural and Evolutionary Computing; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"34th Conference on Neural Information Processing Systems (NeurIPS 2020),",,,,,,,,,,,,,,,,
GZB84KMR,conferencePaper,2020,"Richardson, Oliver; Halpern, Joseph Y.",Probabilistic Dependency Graphs,"arXiv:2012.10800 [cs, math]",,,,http://arxiv.org/abs/2012.10800,"We introduce Probabilistic Dependency Graphs (PDGs), a new class of directed graphical models. PDGs can capture inconsistent beliefs in a natural way and are more modular than Bayesian Networks (BNs), in that they make it easier to incorporate new information and restructure the representation. We show by example how PDGs are an especially natural modeling tool. We provide three semantics for PDGs, each of which can be derived from a scoring function (on joint distributions over the variables in the network) that can be viewed as representing a distribution's incompatibility with the PDG. For the PDG corresponding to a BN, this function is uniquely minimized by the distribution the BN represents, showing that PDG semantics extend BN semantics. We show further that factor graphs and their exponential families can also be faithfully represented as PDGs, while there are significant barriers to modeling a PDG with a factor graph.",2020-12-19,2022-01-30 04:47:35,2022-01-30 04:47:35,2021-10-30 21:44:21,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s0]  ACC: 0  arXiv: 2012.10800,,/Users/jacquesthibodeau/Zotero/storage/CBQAK6J6/Richardson and Halpern - 2020 - Probabilistic Dependency Graphs.pdf; /Users/jacquesthibodeau/Zotero/storage/MPAXVG7Q/2012.html,,UnsortedSafety,Computer Science - Artificial Intelligence; Computer Science - Information Theory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AAAI 2020,,,,,,,,,,,,,,,,
PZ3NUMGN,conferencePaper,2020,"Buçinca, Zana; Lin, Phoebe; Gajos, Krzysztof Z.; Glassman, Elena L.",Proxy tasks and subjective measures can be misleading in evaluating explainable AI systems,Proceedings of the 25th International Conference on Intelligent User Interfaces,978-1-4503-7118-6,,10.1145/3377325.3377498,https://doi.org/10.1145/3377325.3377498,"Explainable artificially intelligent (XAI) systems form part of sociotechnical systems, e.g., human+AI teams tasked with making decisions. Yet, current XAI systems are rarely evaluated by measuring the performance of human+AI teams on actual decision-making tasks. We conducted two online experiments and one in-person think-aloud study to evaluate two currently common techniques for evaluating XAI systems: (1) using proxy, artificial tasks such as how well humans predict the AI's decision from the given explanations, and (2) using subjective measures of trust and preference as predictors of actual performance. The results of our experiments demonstrate that evaluations with proxy tasks did not predict the results of the evaluations with the actual decision-making tasks. Further, the subjective measures on evaluations with actual decision-making tasks did not predict the objective performance on those same tasks. Our results suggest that by employing misleading evaluation methods, our field may be inadvertently slowing its progress toward developing human+AI teams that can reliably perform better than humans or AIs alone.",2020-03-17,2022-01-30 04:48:47,2022-01-30 04:48:47,2021-11-13,454–464,,,,,,,IUI '20,,,,Association for Computing Machinery,"Cagliari, Italy",,,,,,ACM Digital Library,,ZSCC: 0000052,,/Users/jacquesthibodeau/Zotero/storage/AXTMBBWM/Buçinca et al. - 2020 - Proxy tasks and subjective measures can be mislead.pdf,,UnsortedSafety,artificial intelligence; explanations; trust,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4QR28FMI,conferencePaper,2020,"McGregor, Sean",Preventing Repeated Real World AI Failures by Cataloging Incidents: The AI Incident Database,arXiv:2011.08512 [cs],,,,http://arxiv.org/abs/2011.08512,"Mature industrial sectors (e.g., aviation) collect their real world failures in incident databases to inform safety improvements. Intelligent systems currently cause real world harms without a collective memory of their failings. As a result, companies repeatedly make the same mistakes in the design, development, and deployment of intelligent systems. A collection of intelligent system failures experienced in the real world (i.e., incidents) is needed to ensure intelligent systems benefit people and society. The AI Incident Database is an incident collection initiated by an industrial/non-profit cooperative to enable AI incident avoidance and mitigation. The database supports a variety of research and development use cases with faceted and full text search on more than 1,000 incident reports archived to date.",2020-11-17,2022-01-30 04:48:47,2022-01-30 04:48:47,2021-11-13 14:28:31,,,,,,,Preventing Repeated Real World AI Failures by Cataloging Incidents,,,,,,,,,,,,arXiv.org,,ZSCC: 0000008  arXiv: 2011.08512,,/Users/jacquesthibodeau/Zotero/storage/T3W45ATE/McGregor - 2020 - Preventing Repeated Real World AI Failures by Cata.pdf,,UnsortedSafety,Computer Science - Computers and Society; Computer Science - Software Engineering; I.2.0; K.4.0; K.4.3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Innovative Applications of Artificial Intelligence (IAAI-21),,,,,,,,,,,,,,,,
AXN2R6VV,conferencePaper,2020,"Eysenbach, Benjamin; Geng, Xinyang; Levine, Sergey; Salakhutdinov, Ruslan",Rewriting History with Inverse RL: Hindsight Inference for Policy Improvement,"arXiv:2002.11089 [cs, stat]",,,,http://arxiv.org/abs/2002.11089,"Multi-task reinforcement learning (RL) aims to simultaneously learn policies for solving many tasks. Several prior works have found that relabeling past experience with different reward functions can improve sample efficiency. Relabeling methods typically ask: if, in hindsight, we assume that our experience was optimal for some task, for what task was it optimal? In this paper, we show that hindsight relabeling is inverse RL, an observation that suggests that we can use inverse RL in tandem for RL algorithms to efficiently solve many tasks. We use this idea to generalize goal-relabeling techniques from prior work to arbitrary classes of tasks. Our experiments confirm that relabeling data using inverse RL accelerates learning in general multi-task settings, including goal-reaching, domains with discrete sets of rewards, and those with linear reward functions.",2020-02-25,2022-01-30 04:48:47,2022-01-30 04:48:47,2021-11-07 18:36:46,,,,,,,Rewriting History with Inverse RL,,,,,,,,,,,,arXiv.org,,ZSCC: 0000022  arXiv: 2002.11089,,/Users/jacquesthibodeau/Zotero/storage/BZXT2G8S/Eysenbach et al. - 2020 - Rewriting History with Inverse RL Hindsight Infer.pdf; /Users/jacquesthibodeau/Zotero/storage/WGP4ZA47/2002.html,,UnsortedSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2020,,,,,,,,,,,,,,,,
ZFZET66K,conferencePaper,2020,"Levine, Sergey; Kumar, Aviral; Tucker, George; Fu, Justin","Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems","arXiv:2005.01643 [cs, stat]",,,,http://arxiv.org/abs/2005.01643,"In this tutorial article, we aim to provide the reader with the conceptual tools needed to get started on research on offline reinforcement learning algorithms: reinforcement learning algorithms that utilize previously collected data, without additional online data collection. Offline reinforcement learning algorithms hold tremendous promise for making it possible to turn large datasets into powerful decision making engines. Effective offline reinforcement learning methods would be able to extract policies with the maximum possible utility out of the available data, thereby allowing automation of a wide range of decision-making domains, from healthcare and education to robotics. However, the limitations of current algorithms make this difficult. We will aim to provide the reader with an understanding of these challenges, particularly in the context of modern deep reinforcement learning methods, and describe some potential solutions that have been explored in recent work to mitigate these challenges, along with recent applications, and a discussion of perspectives on open problems in the field.",2020-11-01,2022-01-30 04:48:46,2022-01-30 04:48:46,2021-11-07 23:28:45,,,,,,,Offline Reinforcement Learning,,,,,,,,,,,,arXiv.org,,ZSCC: 0000299  arXiv: 2005.01643,,"/Users/jacquesthibodeau/Zotero/storage/Q4HR9G8B/Levine et al. - 2020 - Offline Reinforcement Learning Tutorial, Review, .pdf; /Users/jacquesthibodeau/Zotero/storage/KA6FVASR/2005.html",,UnsortedSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2020,,,,,,,,,,,,,,,,
56TFSGHP,conferencePaper,2020,"Anderson, Greg; Verma, Abhinav; Dillig, Isil; Chaudhuri, Swarat",Neurosymbolic Reinforcement Learning with Formally Verified Exploration,"arXiv:2009.12612 [cs, stat]",,,,http://arxiv.org/abs/2009.12612,"We present Revel, a partially neural reinforcement learning (RL) framework for provably safe exploration in continuous state and action spaces. A key challenge for provably safe deep RL is that repeatedly verifying neural networks within a learning loop is computationally infeasible. We address this challenge using two policy classes: a general, neurosymbolic class with approximate gradients and a more restricted class of symbolic policies that allows efficient verification. Our learning algorithm is a mirror descent over policies: in each iteration, it safely lifts a symbolic policy into the neurosymbolic space, performs safe gradient updates to the resulting policy, and projects the updated policy into the safe symbolic subset, all without requiring explicit verification of neural networks. Our empirical results show that Revel enforces safe exploration in many scenarios in which Constrained Policy Optimization does not, and that it can discover policies that outperform those learned through prior approaches to verified exploration.",2020-10-26,2022-01-30 04:48:46,2022-01-30 04:48:46,2021-11-09 00:01:45,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000017  arXiv: 2009.12612,,/Users/jacquesthibodeau/Zotero/storage/3ENESDMT/Anderson et al. - 2020 - Neurosymbolic Reinforcement Learning with Formally.pdf; /Users/jacquesthibodeau/Zotero/storage/75R2UPKX/2009.html,,UnsortedSafety,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"34th Conference on Neural Information Processing Systems (NeurIPS 2020),",,,,,,,,,,,,,,,,
T3ITWCM7,conferencePaper,2020,"Liu, Jian; Cui, Leyang; Liu, Hanmeng; Huang, Dandan; Wang, Yile; Zhang, Yue",LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning,arXiv:2007.08124 [cs],,,,http://arxiv.org/abs/2007.08124,"Machine reading is a fundamental task for testing the capability of natural language understanding, which is closely related to human cognition in many aspects. With the rising of deep learning techniques, algorithmic models rival human performances on simple QA, and thus increasingly challenging machine reading datasets have been proposed. Though various challenges such as evidence integration and commonsense knowledge have been integrated, one of the fundamental capabilities in human reading, namely logical reasoning, is not fully investigated. We build a comprehensive dataset, named LogiQA, which is sourced from expert-written questions for testing human Logical reasoning. It consists of 8,678 QA instances, covering multiple types of deductive reasoning. Results show that state-of-the-art neural models perform by far worse than human ceiling. Our dataset can also serve as a benchmark for reinvestigating logical AI under the deep learning NLP setting. The dataset is freely available at https://github.com/lgw863/LogiQA-dataset",2020-07-16,2022-01-30 04:48:46,2022-01-30 04:48:46,2021-11-14 18:04:32,,,,,,,LogiQA,,,,,,,,,,,,arXiv.org,,ZSCC: 0000020  arXiv: 2007.08124,,/Users/jacquesthibodeau/Zotero/storage/G86XQK89/Liu et al. - 2020 - LogiQA A Challenge Dataset for Machine Reading Co.pdf; /Users/jacquesthibodeau/Zotero/storage/5KUSX66V/2007.html,,UnsortedSafety,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IJCAI 2020,,,,,,,,,,,,,,,,
8VJV98HV,conferencePaper,2020,"Balakrishnan, Sreejith; Nguyen, Quoc Phong; Low, Bryan Kian Hsiang; Soh, Harold",Efficient Exploration of Reward Functions in Inverse Reinforcement Learning via Bayesian Optimization,arXiv:2011.08541 [cs],,,,http://arxiv.org/abs/2011.08541,"The problem of inverse reinforcement learning (IRL) is relevant to a variety of tasks including value alignment and robot learning from demonstration. Despite significant algorithmic contributions in recent years, IRL remains an ill-posed problem at its core; multiple reward functions coincide with the observed behavior and the actual reward function is not identifiable without prior knowledge or supplementary information. This paper presents an IRL framework called Bayesian optimization-IRL (BO-IRL) which identifies multiple solutions that are consistent with the expert demonstrations by efficiently exploring the reward function space. BO-IRL achieves this by utilizing Bayesian Optimization along with our newly proposed kernel that (a) projects the parameters of policy invariant reward functions to a single point in a latent space and (b) ensures nearby points in the latent space correspond to reward functions yielding similar likelihoods. This projection allows the use of standard stationary kernels in the latent space to capture the correlations present across the reward function space. Empirical results on synthetic and real-world environments (model-free and model-based) show that BO-IRL discovers multiple reward functions while minimizing the number of expensive exact policy optimizations.",2020-11-17,2022-01-30 04:48:45,2022-01-30 04:48:45,2021-11-13 18:41:21,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000004  arXiv: 2011.08541,,/Users/jacquesthibodeau/Zotero/storage/9JPR5MCW/Balakrishnan et al. - 2020 - Efficient Exploration of Reward Functions in Inver.pdf; /Users/jacquesthibodeau/Zotero/storage/8SJT4KWC/2011.html,,UnsortedSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,34th Conference on Neural Information Processing Systems (NeurIPS 2020),,,,,,,,,,,,,,,,
8CME59D5,conferencePaper,2020,"Kumar, Aviral; Gupta, Abhishek; Levine, Sergey",DisCor: Corrective Feedback in Reinforcement Learning via Distribution Correction,34th Conference on Neural Information Processing Systems (NeurIPS 2020),,,,http://arxiv.org/abs/2003.07305,"Deep reinforcement learning can learn effective policies for a wide range of tasks, but is notoriously difficult to use due to instability and sensitivity to hyperparameters. The reasons for this remain unclear. When using standard supervised methods (e.g., for bandits), on-policy data collection provides ""hard negatives"" that correct the model in precisely those states and actions that the policy is likely to visit. We call this phenomenon ""corrective feedback."" We show that bootstrapping-based Q-learning algorithms do not necessarily benefit from this corrective feedback, and training on the experience collected by the algorithm is not sufficient to correct errors in the Q-function. In fact, Q-learning and related methods can exhibit pathological interactions between the distribution of experience collected by the agent and the policy induced by training on that experience, leading to potential instability, sub-optimal convergence, and poor results when learning from noisy, sparse or delayed rewards. We demonstrate the existence of this problem, both theoretically and empirically. We then show that a specific correction to the data distribution can mitigate this issue. Based on these observations, we propose a new algorithm, DisCor, which computes an approximation to this optimal distribution and uses it to re-weight the transitions used for training, resulting in substantial improvements in a range of challenging RL settings, such as multi-task learning and learning from noisy reward signals. Blog post presenting a summary of this work is available at: https://bair.berkeley.edu/blog/2020/03/16/discor/.",2020-03-16,2022-01-30 04:48:45,2022-01-30 04:48:45,2021-11-13 13:40:43,,,,,,,DisCor,,,,,,,,,,,,arXiv.org,,ZSCC: 0000034  arXiv: 2003.07305,,/Users/jacquesthibodeau/Zotero/storage/ZWRH5U35/Kumar et al. - 2020 - DisCor Corrective Feedback in Reinforcement Learn.pdf,,UnsortedSafety,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,34th Conference on Neural Information Processing Systems (NeurIPS 2020),,,,,,,,,,,,,,,,
WP3AREUF,conferencePaper,2020,"Chang, Michael; Kaushik, Sidhant; Weinberg, S. Matthew; Griffiths, Thomas L.; Levine, Sergey",Decentralized Reinforcement Learning: Global Decision-Making via Local Economic Transactions,Proceedings of the 37th International Conference on Machine Learning,,,,http://arxiv.org/abs/2007.02382,"This paper seeks to establish a framework for directing a society of simple, specialized, self-interested agents to solve what traditionally are posed as monolithic single-agent sequential decision problems. What makes it challenging to use a decentralized approach to collectively optimize a central objective is the difficulty in characterizing the equilibrium strategy profile of non-cooperative games. To overcome this challenge, we design a mechanism for defining the learning environment of each agent for which we know that the optimal solution for the global objective coincides with a Nash equilibrium strategy profile of the agents optimizing their own local objectives. The society functions as an economy of agents that learn the credit assignment process itself by buying and selling to each other the right to operate on the environment state. We derive a class of decentralized reinforcement learning algorithms that are broadly applicable not only to standard reinforcement learning but also for selecting options in semi-MDPs and dynamically composing computation graphs. Lastly, we demonstrate the potential advantages of a society's inherent modular structure for more efficient transfer learning.",2020-08-14,2022-01-30 04:48:44,2022-01-30 04:48:44,2021-11-07 16:27:41,,,,,,,Decentralized Reinforcement Learning,,,,,,,,,,,,arXiv.org,,ZSCC: 0000004  arXiv: 2007.02382,,/Users/jacquesthibodeau/Zotero/storage/FSTFPI4R/Chang et al. - 2020 - Decentralized Reinforcement Learning Global Decis.pdf,,UnsortedSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Neural and Evolutionary Computing; Computer Science - Computer Science and Game Theory; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICML 2020,,,,,,,,,,,,,,,,
5WJ3NKZK,conferencePaper,2007,"Ramachandran, Deepak",Bayesian Inverse Reinforcement Learning,,,,,https://www.aaai.org/Papers/IJCAI/2007/IJCAI07-416.pdf,Inverse Reinforcement Learning (IRL) is the problem of learning the reward function underlying a Markov Decision Process given the dynamics of the system and the behaviour of an expert. IRL is motivated by situations where knowledge of the rewards is a goal by itself (as in preference elicitation) and by the task of apprenticeship learning (learning policies from an expert). In this paper we show how to combine prior knowledge and evidence from the expert’s actions to derive a probability distribution over the space of reward functions. We present efﬁcient algorithms that ﬁnd solutions for the reward learning and apprenticeship learning tasks that generalize well over these distributions. Experimental results show strong improvement for our methods over previous heuristic-based approaches.,2007,2022-01-30 04:48:44,2022-01-30 04:48:44,,6,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000660,,/Users/jacquesthibodeau/Zotero/storage/6MR8CJ2E/Ramachandran - 2007 - Bayesian Inverse Reinforcement Learning.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IJCAI07,,,,,,,,,,,,,,,,
3IHMRI74,conferencePaper,2019,"Zhu, He; Xiong, Zikang; Magill, Stephen; Jagannathan, Suresh",An Inductive Synthesis Framework for Verifiable Reinforcement Learning,Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation,,,10.1145/3314221.3314638,http://arxiv.org/abs/1907.07273,"Despite the tremendous advances that have been made in the last decade on developing useful machine-learning applications, their wider adoption has been hindered by the lack of strong assurance guarantees that can be made about their behavior. In this paper, we consider how formal verification techniques developed for traditional software systems can be repurposed for verification of reinforcement learning-enabled ones, a particularly important class of machine learning systems. Rather than enforcing safety by examining and altering the structure of a complex neural network implementation, our technique uses blackbox methods to synthesizes deterministic programs, simpler, more interpretable, approximations of the network that can nonetheless guarantee desired safety properties are preserved, even when the network is deployed in unanticipated or previously unobserved environments. Our methodology frames the problem of neural network verification in terms of a counterexample and syntax-guided inductive synthesis procedure over these programs. The synthesis procedure searches for both a deterministic program and an inductive invariant over an infinite state transition system that represents a specification of an application's control logic. Additional specifications defining environment-based constraints can also be provided to further refine the search space. Synthesized programs deployed in conjunction with a neural network implementation dynamically enforce safety conditions by monitoring and preventing potentially unsafe actions proposed by neural policies. Experimental results over a wide range of cyber-physical applications demonstrate that software-inspired formal verification techniques can be used to realize trustworthy reinforcement learning systems with low overhead.",2019-06-08,2022-01-30 04:48:44,2022-01-30 04:48:44,2021-11-09 00:07:06,686-701,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000051  arXiv: 1907.07273,,/Users/jacquesthibodeau/Zotero/storage/REZTXVMI/Zhu et al. - 2019 - An Inductive Synthesis Framework for Verifiable Re.pdf; /Users/jacquesthibodeau/Zotero/storage/ZH9U6EAJ/1907.html,,UnsortedSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; 00-02,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3H57NUUP,conferencePaper,2020,"Juric, Mislav; Sandic, Agneza; Brcic, Mario",AI safety: state of the field through quantitative lens,arXiv:2002.05671 [cs],,,10.23919/MIPRO48935.2020.9245153,http://arxiv.org/abs/2002.05671,"Last decade has seen major improvements in the performance of artificial intelligence which has driven wide-spread applications. Unforeseen effects of such mass-adoption has put the notion of AI safety into the public eye. AI safety is a relatively new field of research focused on techniques for building AI beneficial for humans. While there exist survey papers for the field of AI safety, there is a lack of a quantitative look at the research being conducted. The quantitative aspect gives a data-driven insight about the emerging trends, knowledge gaps and potential areas for future research. In this paper, bibliometric analysis of the literature finds significant increase in research activity since 2015. Also, the field is so new that most of the technical issues are open, including: explainability with its long-term utility, and value alignment which we have identified as the most important long-term research topic. Equally, there is a severe lack of research into concrete policies regarding AI. As we expect AI to be the one of the main driving forces of changes in society, AI safety is the field under which we need to decide the direction of humanity's future.",2020-07-09,2022-01-30 04:48:43,2022-01-30 04:48:43,2021-11-13 15:49:34,,,,,,,AI safety,,,,,,,,,,,,arXiv.org,,ZSCC: 0000006  arXiv: 2002.05671,,/Users/jacquesthibodeau/Zotero/storage/T6JFHRKD/Juric et al. - 2020 - AI safety state of the field through quantitative.pdf,,UnsortedSafety,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MIPRO 2020,,,,,,,,,,,,,,,,
I8JC8ADC,conferencePaper,2019,"Waytowich, Nicholas; Barton, Sean L.; Lawhern, Vernon; Warnell, Garrett",A Narration-based Reward Shaping Approach using Grounded Natural Language Commands,Proceedings of the 36th International Conference on Machine Learning,,,,http://arxiv.org/abs/1911.00497,"While deep reinforcement learning techniques have led to agents that are successfully able to learn to perform a number of tasks that had been previously unlearnable, these techniques are still susceptible to the longstanding problem of reward sparsity. This is especially true for tasks such as training an agent to play StarCraft II, a real-time strategy game where reward is only given at the end of a game which is usually very long. While this problem can be addressed through reward shaping, such approaches typically require a human expert with specialized knowledge. Inspired by the vision of enabling reward shaping through the more-accessible paradigm of natural-language narration, we develop a technique that can provide the benefits of reward shaping using natural language commands. Our narration-guided RL agent projects sequences of natural-language commands into the same high-dimensional representation space as corresponding goal states. We show that we can get improved performance with our method compared to traditional reward-shaping approaches. Additionally, we demonstrate the ability of our method to generalize to unseen natural-language commands.",2019-10-31,2022-01-30 04:48:43,2022-01-30 04:48:43,2021-11-13 22:30:52,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000003  arXiv: 1911.00497,,/Users/jacquesthibodeau/Zotero/storage/7Q4ZC6TW/Waytowich et al. - 2019 - A Narration-based Reward Shaping Approach using Gr.pdf; /Users/jacquesthibodeau/Zotero/storage/C869HZX8/1911.html,,UnsortedSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5Q25A4W2,conferencePaper,2021,"Bousquet, Olivier; Hanneke, Steve; Moran, Shay; van Handel, Ramon; Yehudayoff, Amir",A Theory of Universal Learning,STOC 2021: Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing,,,10.1145/3406325.3451087,http://arxiv.org/abs/2011.04483,"How quickly can a given class of concepts be learned from examples? It is common to measure the performance of a supervised machine learning algorithm by plotting its ""learning curve"", that is, the decay of the error rate as a function of the number of training examples. However, the classical theoretical framework for understanding learnability, the PAC model of Vapnik-Chervonenkis and Valiant, does not explain the behavior of learning curves: the distribution-free PAC model of learning can only bound the upper envelope of the learning curves over all possible data distributions. This does not match the practice of machine learning, where the data source is typically fixed in any given scenario, while the learner may choose the number of training examples on the basis of factors such as computational resources and desired accuracy. In this paper, we study an alternative learning model that better captures such practical aspects of machine learning, but still gives rise to a complete theory of the learnable in the spirit of the PAC model. More precisely, we consider the problem of universal learning, which aims to understand the performance of learning algorithms on every data distribution, but without requiring uniformity over the distribution. The main result of this paper is a remarkable trichotomy: there are only three possible rates of universal learning. More precisely, we show that the learning curves of any given concept class decay either at an exponential, linear, or arbitrarily slow rates. Moreover, each of these cases is completely characterized by appropriate combinatorial parameters, and we exhibit optimal learning algorithms that achieve the best possible rate in each case. For concreteness, we consider in this paper only the realizable case, though analogous results are expected to extend to more general learning scenarios.",2021-06,2022-01-30 04:48:43,2022-01-30 04:48:43,2021-11-13 23:03:40,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000007  arXiv: 2011.04483,,/Users/jacquesthibodeau/Zotero/storage/QPAPTBUB/Bousquet et al. - 2020 - A Theory of Universal Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/5C38AGW9/2011.html,,UnsortedSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Mathematics - Statistics Theory; Computer Science - Data Structures and Algorithms,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,STOC 2021,,,,,,,,,,,,,,,,
QHKZ9UMR,conferencePaper,2021,"Power, Alethea; Burda, Yuri; Edwards, Harri; Babuschkin, Igor; Misra, Vedant",GROKKING: GENERALIZATION BEYOND OVERFIT- TING ON SMALL ALGORITHMIC DATASETS,,,,,,"In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efﬁciency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of “grokking” a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overﬁtting. We also study generalization as a function of dataset size and ﬁnd that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the ﬁnite training dataset.",2021,2022-01-30 04:48:28,2022-01-30 04:48:28,,9,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: NoCitationData[s0]  ACC: 4,,/Users/jacquesthibodeau/Zotero/storage/MPKP826T/Power et al. - GROKKING GENERALIZATION BEYOND OVERFIT- TING ON S.pdf,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1stMathematical Reasoning in General Artificial Intelligence Workshop, ICLR 2021",,,,,,,,,,,,,,,,
ZQ49PGTV,conferencePaper,2020,"Dafoe, Allan; Hughes, Edward; Bachrach, Yoram; Collins, Tantum; McKee, Kevin R.; Leibo, Joel Z.; Larson, Kate; Graepel, Thore",Open Problems in Cooperative AI,arXiv:2012.08630 [cs],,,,http://arxiv.org/abs/2012.08630,"Problems of cooperation--in which agents seek ways to jointly improve their welfare--are ubiquitous and important. They can be found at scales ranging from our daily routines--such as driving on highways, scheduling meetings, and working collaboratively--to our global challenges--such as peace, commerce, and pandemic preparedness. Arguably, the success of the human species is rooted in our ability to cooperate. Since machines powered by artificial intelligence are playing an ever greater role in our lives, it will be important to equip them with the capabilities necessary to cooperate and to foster cooperation. We see an opportunity for the field of artificial intelligence to explicitly focus effort on this class of problems, which we term Cooperative AI. The objective of this research would be to study the many aspects of the problems of cooperation and to innovate in AI to contribute to solving these problems. Central goals include building machine agents with the capabilities needed for cooperation, building tools to foster cooperation in populations of (machine and/or human) agents, and otherwise conducting AI research for insight relevant to problems of cooperation. This research integrates ongoing work on multi-agent systems, game theory and social choice, human-machine interaction and alignment, natural-language processing, and the construction of social tools and platforms. However, Cooperative AI is not the union of these existing areas, but rather an independent bet about the productivity of specific kinds of conversations that involve these and other areas. We see opportunity to more explicitly focus on the problem of cooperation, to construct unified theory and vocabulary, and to build bridges with adjacent communities working on cooperation, including in the natural, social, and behavioural sciences.",2020-12-15,2022-01-30 04:48:04,2022-01-30 04:48:04,2021-11-13 19:00:19,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000025  arXiv: 2012.08630,,/Users/jacquesthibodeau/Zotero/storage/J98ABBH2/Dafoe et al. - 2020 - Open Problems in Cooperative AI.pdf; /Users/jacquesthibodeau/Zotero/storage/E5XXCHBD/2012.html,,UnsortedSafety,Computer Science - Artificial Intelligence; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2020 Cooperative AI Workshop,,,,,,,,,,,,,,,,
NVDI9SGV,conferencePaper,2020,"Stray, Jonathan; Adler, Steven; Hadfield-Menell, Dylan",What are you optimizing for? Aligning Recommender Systems with Human Values,,,,,http://arxiv.org/abs/2107.10939,"We describe cases where real recommender systems were modiﬁed in the service of various human values such as diversity, fairness, well-being, time well spent, and factual accuracy. From this we identify the current practice of values engineering: the creation of classiﬁers from humancreated data with value-based labels. This has worked in practice for a variety of issues, but problems are addressed one at a time, and users and other stakeholders have seldom been involved. Instead, we look to AI alignment work for approaches that could learn complex values directly from stakeholders, and identify four major directions: useful measures of alignment, participatory design and operation, interactive value learning, and informed deliberative judgments.",2020,2022-01-30 04:51:11,2022-01-30 04:51:11,,7,,,,,,What are you optimizing for?,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000003[s0],,/Users/jacquesthibodeau/Zotero/storage/VX5CHWKD/Stray et al. - 2021 - What are you optimizing for Aligning Recommender .pdf; /Users/jacquesthibodeau/Zotero/storage/XIEIB3NT/2107.html; /Users/jacquesthibodeau/Zotero/storage/PQ7MPFGP/Stray et al. - What are you optimizing for Aligning Recommender .pdf,,CHAI; TechSafety,Computer Science - Machine Learning; Computer Science - Computers and Society; Computer Science - Information Retrieval,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Participatory Approaches to Machine Learning Workshop, ICML 2020",,,,,,,,,,,,,,,,
2EZUTH82,conferencePaper,2021,"Srivastava, Siddharth",Unifying Principles and Metrics for Safe and Assistive AI,Proceedings of the AAAI Conference on Artificial Intelligence,,,,https://ojs.aaai.org/index.php/AAAI/article/view/17769,"The prevalence and success of AI applications have been tempered by concerns about the controllability of AI systems about AI's impact on the future of work.  These concerns reflect two aspects of a central question: how  would  humans work with AI systems? While research on AI safety focuses on designing AI systems that allow humans to safely instruct and control AI systems, research on AI and the future of work focuses on the impact of AI on humans who may be unable to do so. This Blue Sky Ideas paper proposes a unifying set of declarative principles that enable a more uniform evaluation of arbitrary AI systems along multiple dimensions of the extent to which they are suitable for use by specific classes of human operators. It leverages recent AI research and the unique strengths of the field to develop human-centric principles for AI systems that address the concerns noted above.",2021-05-18,2022-01-30 04:51:11,2022-01-30 04:51:11,2021-10-30 20:54:11,15064-15068,,,35,,,,,,,,,,en,Copyright (c) 2021 Association for the Advancement of Artificial Intelligence,,,,ojs.aaai.org,,ZSCC: 0000004  Number: 17,,/Users/jacquesthibodeau/Zotero/storage/KSRRJQU3/Srivastava - 2021 - Unifying Principles and Metrics for Safe and Assis.pdf,,MetaSafety; AmbiguousSafety,Metrics For Safe And Beneficial AI Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
H2JA5F8T,conferencePaper,2021,"Laidlaw, Cassidy; Russell, Stuart",Uncertain Decisions Facilitate Better Preference Learning,35th Conference on Neural Information Processing Systems (NeurIPS 2021),,,,http://arxiv.org/abs/2106.10394,"Existing observational approaches for learning human preferences, such as inverse reinforcement learning, usually make strong assumptions about the observability of the human's environment. However, in reality, people make many important decisions under uncertainty. To better understand preference learning in these cases, we study the setting of inverse decision theory (IDT), a previously proposed framework where a human is observed making non-sequential binary decisions under uncertainty. In IDT, the human's preferences are conveyed through their loss function, which expresses a tradeoff between different types of mistakes. We give the first statistical analysis of IDT, providing conditions necessary to identify these preferences and characterizing the sample complexity -- the number of decisions that must be observed to learn the tradeoff the human is making to a desired precision. Interestingly, we show that it is actually easier to identify preferences when the decision problem is more uncertain. Furthermore, uncertain decision problems allow us to relax the unrealistic assumption that the human is an optimal decision maker but still identify their exact preferences; we give sample complexities in this suboptimal case as well. Our analysis contradicts the intuition that partial observability should make preference learning more difficult. It also provides a first step towards understanding and improving preference learning methods for uncertain and suboptimal humans.",2021-10-28,2022-01-30 04:51:11,2022-01-30 04:51:11,2021-11-18 23:19:35,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000000  arXiv: 2106.10394,,/Users/jacquesthibodeau/Zotero/storage/9KSDNPMR/Laidlaw and Russell - 2021 - Uncertain Decisions Facilitate Better Preference L.pdf; /Users/jacquesthibodeau/Zotero/storage/PRVT74B5/2106.html,,TechSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,35th Conference on Neural Information Processing Systems (NeurIPS 2021),,,,,,,,,,,,,,,,
59SRKKCH,conferencePaper,2018,"Milli, Smitha; Miller, John; Dragan, Anca D.; Hardt, Moritz",The Social Cost of Strategic Classification,"FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",,,,http://arxiv.org/abs/1808.08460,"Consequential decision-making typically incentivizes individuals to behave strategically, tailoring their behavior to the specifics of the decision rule. A long line of work has therefore sought to counteract strategic behavior by designing more conservative decision boundaries in an effort to increase robustness to the effects of strategic covariate shift. We show that these efforts benefit the institutional decision maker at the expense of the individuals being classified. Introducing a notion of social burden, we prove that any increase in institutional utility necessarily leads to a corresponding increase in social burden. Moreover, we show that the negative externalities of strategic classification can disproportionately harm disadvantaged groups in the population. Our results highlight that strategy-robustness must be weighed against considerations of social welfare and fairness.",2018-11-22,2022-01-30 04:51:10,2022-01-30 04:51:10,2019-12-18 02:40:02,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000016[s2]  arXiv: 1808.08460,,/Users/jacquesthibodeau/Zotero/storage/HX3R88QW/Milli et al. - 2018 - The Social Cost of Strategic Classification.pdf; /Users/jacquesthibodeau/Zotero/storage/522JRDXP/1808.html,,CHAI; TechSafety,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Conference on Fairness, Accountability, and Transparency",,,,,,,,,,,,,,,,
ADB4QQ4U,conferencePaper,2021,"Shah, Rohin; Wild, Cody; Wang, Steven H.; Alex, Neel; Houghton, Brandon; Guss, William; Mohanty, Sharada; Kanervisto, Anssi; Milani, Stephanie; Topin, Nicholay; Abbeel, Pieter; Russell, Stuart; Dragan, Anca",The MineRL BASALT Competition on Learning from Human Feedback,arXiv:2107.01969 [cs],,,,http://arxiv.org/abs/2107.01969,"The last decade has seen a significant increase of interest in deep learning research, with many public successes that have demonstrated its potential. As such, these systems are now being incorporated into commercial products. With this comes an additional challenge: how can we build AI systems that solve tasks where there is not a crisp, well-defined specification? While multiple solutions have been proposed, in this competition we focus on one in particular: learning from human feedback. Rather than training AI systems using a predefined reward function or using a labeled dataset with a predefined set of categories, we instead train the AI system using a learning signal derived from some form of human feedback, which can evolve over time as the understanding of the task changes, or as the capabilities of the AI system improve. The MineRL BASALT competition aims to spur forward research on this important class of techniques. We design a suite of four tasks in Minecraft for which we expect it will be hard to write down hardcoded reward functions. These tasks are defined by a paragraph of natural language: for example, ""create a waterfall and take a scenic picture of it"", with additional clarifying details. Participants must train a separate agent for each task, using any method they want. Agents are then evaluated by humans who have read the task description. To help participants get started, we provide a dataset of human demonstrations on each of the four tasks, as well as an imitation learning baseline that leverages these demonstrations. Our hope is that this competition will improve our ability to build AI systems that do what their designers intend them to do, even when the intent cannot be easily formalized. Besides allowing AI to solve more tasks, this can also enable more effective regulation of AI systems, as well as making progress on the value alignment problem.",2021-07-05,2022-01-30 04:51:10,2022-01-30 04:51:10,2021-11-14 18:53:31,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000001  arXiv: 2107.01969,,/Users/jacquesthibodeau/Zotero/storage/PNMP8N4E/Shah et al. - 2021 - The MineRL BASALT Competition on Learning from Hum.pdf; /Users/jacquesthibodeau/Zotero/storage/ASZTCQCH/2107.html,,TechSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2021,,,,,,,,,,,,,,,,
ANADKJFM,conferencePaper,2019,"Chan, Lawrence; Hadfield-Menell, Dylan; Srinivasa, Siddhartha; Dragan, Anca",The Assistive Multi-Armed Bandit,2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI),,,,http://arxiv.org/abs/1901.08654,"Learning preferences implicit in the choices humans make is a well studied problem in both economics and computer science. However, most work makes the assumption that humans are acting (noisily) optimally with respect to their preferences. Such approaches can fail when people are themselves learning about what they want. In this work, we introduce the assistive multi-armed bandit, where a robot assists a human playing a bandit task to maximize cumulative reward. In this problem, the human does not know the reward function but can learn it through the rewards received from arm pulls; the robot only observes which arms the human pulls but not the reward associated with each pull. We offer sufficient and necessary conditions for successfully assisting the human in this framework. Surprisingly, better human performance in isolation does not necessarily lead to better performance when assisted by the robot: a human policy can do better by effectively communicating its observed rewards to the robot. We conduct proof-of-concept experiments that support these results. We see this work as contributing towards a theory behind algorithms for human-robot interaction.",2019-01-24,2022-01-30 04:51:10,2022-01-30 04:51:10,2019-07-08 15:45:03,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000024  arXiv: 1901.08654,,/Users/jacquesthibodeau/Zotero/storage/RJHMMH5U/Chan et al. - 2019 - The Assistive Multi-Armed Bandit.pdf; /Users/jacquesthibodeau/Zotero/storage/SP2MCRP5/1901.html,,CHAI; TechSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI),,,,,,,,,,,,,,,,
JUZZK322,conferencePaper,2017,"Hadfield-Menell, Dylan; Dragan, Anca; Abbeel, Pieter; Russell, Stuart",The Off-Switch Game,Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence,978-0-9992411-0-3,,10.24963/ijcai.2017/32,https://www.ijcai.org/proceedings/2017/32,"It is clear that one of the primary tools we can use to mitigate the potential risk from a misbehaving AI system is the ability to turn the system off. As the capabilities of AI systems improve, it is important to ensure that such systems do not adopt subgoals that prevent a human from switching them off. This is a challenge because many formulations of rational agents create strong incentives for self-preservation. This is not caused by a built-in instinct, but because a rational agent will maximize expected utility and cannot achieve whatever objective it has been given if it is dead. Our goal is to study the incentives an agent has to allow itself to be switched off. We analyze a simple game between a human H and a robot R, where H can press R’s off switch but R can disable the off switch. A traditional agent takes its reward function for granted: we show that such agents have an incentive to disable the off switch, except in the special case where H is perfectly rational. Our key insight is that for R to want to preserve its off switch, it needs to be uncertain about the utility associated with the outcome, and to treat H’s actions as important observations about that utility. (R also has no incentive to switch itself off in this setting.) We conclude that giving machines an appropriate level of uncertainty about their objectives leads to safer designs, and we argue that this setting is a useful generalization of the classical AI paradigm of rational agents.",2017,2022-01-30 04:51:10,2022-01-30 04:51:10,2019-07-08 15:30:25,220-227,,,,,,,,,,,International Joint Conferences on Artificial Intelligence Organization,"Melbourne, Australia",en,,,,,DOI.org (Crossref),,ZSCC: 0000086,,,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Twenty-Sixth International Joint Conference on Artificial Intelligence,,,,,,,,,,,,,,,,
CZHP842W,conferencePaper,2020,"Toyer, Sam; Shah, Rohin; Critch, Andrew; Russell, Stuart",The MAGICAL Benchmark for Robust Imitation,Advances in Neural Information Processing Systems 33 Pre-proceedings,,,,https://papers.nips.cc/paper/2020/hash/d464b5ac99e74462f321c06ccacc4bff-Abstract.html,"Imitation Learning (IL) algorithms are typically evaluated in the same environment that was used to create demonstrations. This rewards precise reproduction of demonstrations in one particular environment, but provides little information about how robustly an algorithm can generalise the demonstrator’s intent to substantially different deployment settings. This paper presents the MAGICAL benchmark suite, which permits systematic evaluation of generalisation by quantifying robustness to different kinds of distribution shift that an IL algorithm is likely to encounter in practice. Using the MAGICAL suite, we conﬁrm that existing IL algorithms overﬁt signiﬁcantly to the context in which demonstrations are provided. We also show that standard methods for reducing overﬁtting are effective at creating narrow perceptual invariances, but are not sufﬁcient to enable transfer to contexts that require substantially different behaviour, which suggests that new approaches will be needed in order to robustly generalise demonstrator intent. Code and data for the MAGICAL suite is available at https://github.com/qxcv/magical/.",2020,2022-01-30 04:51:10,2022-01-30 04:51:10,2020-12-18,25,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000007,,/Users/jacquesthibodeau/Zotero/storage/68R9Q97E/Toyer et al. - The MAGICAL Benchmark for Robust Imitation.pdf,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2020,,,,,,,,,,,,,,,,
4K6R3T4K,conferencePaper,2019,"Dragan, Anca",Specifying AI Objectives As a Human-AI Collaboration Problem,"Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",978-1-4503-6324-2,,10.1145/3306618.3314227,http://doi.acm.org/10.1145/3306618.3314227,"Estimation, planning, control, and learning are giving us robots that can generate good behavior given a specified objective and set of constraints. What I care about is how humans enter this behavior generation picture, and study two complementary challenges: 1) how to optimize behavior when the robot is not acting in isolation, but needs to coordinate or collaborate with people; and 2) what to optimize in order to get the behavior we want. My work has traditionally focused on the former, but more recently I have been casting the latter as a human-robot collaboration problem as well (where the human is the end-user, or even the robotics engineer building the system). Treating it as such has enabled us to use robot actions to gain information; to account for human pedagogic behavior; and to exchange information between the human and the robot via a plethora of communication channels, from external forces that the person physically applies to the robot, to comparison queries, to defining a proxy objective function.",2019,2022-01-30 04:51:09,2022-01-30 04:51:09,2019-12-18 02:38:17,329–329,,,,,,,AIES '19,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"ZSCC: 0000000  event-place: Honolulu, HI, USA",,,,CHAI; TechSafety,inverse reinforcement learning; reward design; value alignment,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QGGZQCD2,conferencePaper,2017,"Milli, Smitha; Hadfield-Menell, Dylan; Dragan, Anca; Russell, Stuart",Should Robots be Obedient?,IJCAI'17: Proceedings of the 26th International Joint Conference on Artificial Intelligence,,,,http://arxiv.org/abs/1705.09990,"Intuitively, obedience -- following the order that a human gives -- seems like a good property for a robot to have. But, we humans are not perfect and we may give orders that are not best aligned to our preferences. We show that when a human is not perfectly rational then a robot that tries to infer and act according to the human's underlying preferences can always perform better than a robot that simply follows the human's literal order. Thus, there is a tradeoff between the obedience of a robot and the value it can attain for its owner. We investigate how this tradeoff is impacted by the way the robot infers the human's preferences, showing that some methods err more on the side of obedience than others. We then analyze how performance degrades when the robot has a misspecified model of the features that the human cares about or the level of rationality of the human. Finally, we study how robots can start detecting such model misspecification. Overall, our work suggests that there might be a middle ground in which robots intelligently decide when to obey human orders, but err on the side of obedience.",2017-05-28,2022-01-30 04:51:09,2022-01-30 04:51:09,2019-05-07 20:04:43,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000050  arXiv: 1705.09990,,/Users/jacquesthibodeau/Zotero/storage/3A9TS8IF/Milli et al. - 2017 - Should Robots be Obedient.pdf; /Users/jacquesthibodeau/Zotero/storage/U6MHTWRU/1705.html; /Users/jacquesthibodeau/Zotero/storage/VKNMUE9J/1705.html,,CHAI; TechSafety,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IJCAI 2017,,,,,,,,,,,,,,,,
HJ8K6HTE,conferencePaper,2019,"Swamy, Gokul; Reddy, Siddharth; Levine, Sergey; Dragan, Anca D.",Scaled Autonomy: Enabling Human Operators to Control Robot Fleets,2020 IEEE International Conference on Robotics and Automation (ICRA),,,,http://arxiv.org/abs/1910.02910,"Autonomous robots often encounter challenging situations where their control policies fail and an expert human operator must briefly intervene, e.g., through teleoperation. In settings where multiple robots act in separate environments, a single human operator can manage a fleet of robots by identifying and teleoperating one robot at any given time. The key challenge is that users have limited attention: as the number of robots increases, users lose the ability to decide which robot requires teleoperation the most. Our goal is to automate this decision, thereby enabling users to supervise more robots than their attention would normally allow for. Our insight is that we can model the user's choice of which robot to control as an approximately optimal decision that maximizes the user's utility function. We learn a model of the user's preferences from observations of the user's choices in easy settings with a few robots, and use it in challenging settings with more robots to automatically identify which robot the user would most likely choose to control, if they were able to evaluate the states of all robots at all times. We run simulation experiments and a user study with twelve participants that show our method can be used to assist users in performing a navigation task and manipulator reaching task.",2019-09-21,2022-01-30 04:51:09,2022-01-30 04:51:09,2019-12-18 02:35:11,,,,,,,Scaled Autonomy,,,,,,,,,,,,arXiv.org,,ZSCC: 0000003[s0]  arXiv: 1910.02910,,/Users/jacquesthibodeau/Zotero/storage/VPEQJUNS/Swamy et al. - 2019 - Scaled Autonomy Enabling Human Operators to Contr.pdf; /Users/jacquesthibodeau/Zotero/storage/EVFPGQ5H/1910.html,,CHAI; TechSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2020 IEEE International Conference on Robotics and Automation (ICRA),,,,,,,,,,,,,,,,
JVDEZZWU,conferencePaper,2018,"Fridovich-Keil, David; Fisac, Jaime F.; Tomlin, Claire J.",Safely Probabilistically Complete Real-Time Planning and Exploration in Unknown Environments,2019 International Conference on Robotics and Automation (ICRA),,,,http://arxiv.org/abs/1811.07834,"We present a new framework for motion planning that wraps around existing kinodynamic planners and guarantees recursive feasibility when operating in a priori unknown, static environments. Our approach makes strong guarantees about overall safety and collision avoidance by utilizing a robust controller derived from reachability analysis. We ensure that motion plans never exit the safe backward reachable set of the initial state, while safely exploring the space. This preserves the safety of the initial state, and guarantees that that we will eventually ﬁnd the goal if it is possible to do so while exploring safely. We implement our framework in the Robot Operating System (ROS) software environment and demonstrate it in a real-time simulation.",2018-11-19,2022-01-30 04:51:09,2022-01-30 04:51:09,2019-07-08 16:10:47,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: NoCitationData[s6]  ACC: 24  J: 6 arXiv: 1811.07834,,,,CHAI; TechSafety,Computer Science - Robotics; Electrical Engineering and Systems Science - Systems and Control,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 International Conference on Robotics and Automation (ICRA),,,,,,,,,,,,,,,,
QKVQDMCH,conferencePaper,2021,"Emmons, Scott; Oesterheld, Caspar; Critch, Andrew; Conitzer, Vince; Russell, Stuart","Symmetry, Equilibria, and Robustness in Common-Payoff Games",,,,,https://preflib.github.io/gaiw2021/papers/GAIW_2021_paper_32.pdf,"Although it has been known since the 1970s that a globally optimal strategy profile in a common-payoff game is a Nash equilibrium, global optimality is a strict requirement that limits the result’s applicability. In this work, we show that any locally optimal sym- metric strategy profile is also a (global) Nash equilibrium. Applied to machine learning, our result provides a global guarantee for any gradient method that finds a local optimum in symmetric strategy space. Furthermore, we show that this result is robust to pertur- bations to the common payoff and to the local optimum. While these results indicate stability to unilateral deviation, we neverthe- less identify broad classes of games where mixed local optima are unstable under joint, asymmetric deviations. We analyze the preva- lence of instability by running learning algorithms in a suite of symmetric games, and we conclude with results on the complexity of computing game symmetries.",2021-05,2022-01-30 04:51:09,2022-01-30 04:51:09,2021-10-30 21:01:14,17,,,,,,,,,,,,"London, UK",en,,,,,,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/7B72JZQB/GAIW_2021_paper_32.pdf,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"3rd Games, Agents, and Incentives Workshop (GAIW 2021)",,,,,,,,,,,,,,,,
NWHFR69Q,conferencePaper,2018,"Ratner, Ellis; Hadfield-Menell, Dylan; Dragan, Anca D.",Simplifying Reward Design through Divide-and-Conquer,Robotics: Science and Systems XIV,,,,http://arxiv.org/abs/1806.02501,"Designing a good reward function is essential to robot planning and reinforcement learning, but it can also be challenging and frustrating. The reward needs to work across multiple different environments, and that often requires many iterations of tuning. We introduce a novel divide-andconquer approach that enables the designer to specify a reward separately for each environment. By treating these separate reward functions as observations about the underlying true reward, we derive an approach to infer a common reward across all environments. We conduct user studies in an abstract grid world domain and in a motion planning domain for a 7-DOF manipulator that measure user effort and solution quality. We show that our method is faster, easier to use, and produces a higher quality solution than the typical method of designing a reward jointly across all environments. We additionally conduct a series of experiments that measure the sensitivity of these results to different properties of the reward design task, such as the number of environments, the number of feasible solutions per environment, and the fraction of the total features that vary within each environment. We ﬁnd that independent reward design outperforms the standard, joint, reward design process but works best when the design problem can be divided into simpler subproblems.",2018-06-06,2022-01-30 04:51:09,2022-01-30 04:51:09,2019-07-12 00:11:37,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000020  arXiv: 1806.02501,,/Users/jacquesthibodeau/Zotero/storage/3W9MFI95/Ratner et al. - 2018 - Simplifying Reward Design through Divide-and-Conqu.pdf; /Users/jacquesthibodeau/Zotero/storage/M3RN4N6G/1806.html; /Users/jacquesthibodeau/Zotero/storage/CTPZ3Q7F/1806.html; /Users/jacquesthibodeau/Zotero/storage/3HUP68HH/Ratner et al. - 2018 - Simplifying Reward Design through Divide-and-Conqu.pdf,,CHAI; TechSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Robotics: Science and Systems XIV,,,,,,,,,,,,,,,,
AXU8XRXT,conferencePaper,2020,"Köster, Raphael; Hadfield-Menell, Dylan; Hadfield, Gillian K.; Leibo, Joel Z.",Silly rules improve the capacity of agents to learn stable enforcement and compliance behaviors,"Proc. of the 19th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2020),",,,,http://arxiv.org/abs/2001.09318,"How can societies learn to enforce and comply with social norms? Here we investigate the learning dynamics and emergence of compliance and enforcement of social norms in a foraging game, implemented in a multi-agent reinforcement learning setting. In this spatiotemporally extended game, individuals are incentivized to implement complex berry-foraging policies and punish transgressions against social taboos covering specific berry types. We show that agents benefit when eating poisonous berries is taboo, meaning the behavior is punished by other agents, as this helps overcome a credit-assignment problem in discovering delayed health effects. Critically, however, we also show that introducing an additional taboo, which results in punishment for eating a harmless berry, improves the rate and stability with which agents learn to punish taboo violations and comply with taboos. Counterintuitively, our results show that an arbitrary taboo (a ""silly rule"") can enhance social learning dynamics and achieve better outcomes in the middle stages of learning. We discuss the results in the context of studying normativity as a group-level emergent phenomenon.",2020-01-25,2022-01-30 04:51:09,2022-01-30 04:51:09,2020-11-21 18:30:39,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000006  arXiv: 2001.09318,,/Users/jacquesthibodeau/Zotero/storage/G24AAEA3/Köster et al. - 2020 - Silly rules improve the capacity of agents to lear.pdf; /Users/jacquesthibodeau/Zotero/storage/TS9NAKX8/2001.html; /Users/jacquesthibodeau/Zotero/storage/W6K5KE25/2001.html,,CHAI; TechSafety; DeepMind,Computer Science - Artificial Intelligence; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GRWKGUDI,conferencePaper,2018,"Reddy, Siddharth; Dragan, Anca D.; Levine, Sergey",Shared Autonomy via Deep Reinforcement Learning,Robotics: Science and Systems XIV,,,,http://arxiv.org/abs/1802.01744,"In shared autonomy, user input is combined with semi-autonomous control to achieve a common goal. The goal is often unknown ex-ante, so prior work enables agents to infer the goal from user input and assist with the task. Such methods tend to assume some combination of knowledge of the dynamics of the environment, the user’s policy given their goal, and the set of possible goals the user might target, which limits their application to real-world scenarios. We propose a deep reinforcement learning framework for model-free shared autonomy that lifts these assumptions. We use human-in-the-loop reinforcement learning with neural network function approximation to learn an end-to-end mapping from environmental observation and user input to agent action values, with task reward as the only form of supervision. This approach poses the challenge of following user commands closely enough to provide the user with real-time action feedback and thereby ensure high-quality user input, but also deviating from the user’s actions when they are suboptimal. We balance these two needs by discarding actions whose values fall below some threshold, then selecting the remaining action closest to the user’s input. Controlled studies with users (n = 12) and synthetic pilots playing a video game, and a pilot study with users (n = 4) ﬂying a real quadrotor, demonstrate the ability of our algorithm to assist users with real-time control tasks in which the agent cannot directly access the user’s private information through observations, but receives a reward signal and user input that both depend on the user’s intent. The agent learns to assist the user without access to this private information, implicitly inferring it from the user’s input. This enables the assisted user to complete the task more effectively than the user or an autonomous agent could on their own. This paper is a proof of concept that illustrates the potential for deep reinforcement learning to enable ﬂexible and practical assistive systems.",2018-02-05,2022-01-30 04:51:09,2022-01-30 04:51:09,2019-07-12 00:10:49,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000087  arXiv: 1802.01744,,/Users/jacquesthibodeau/Zotero/storage/6HMCX5IQ/Reddy et al. - 2018 - Shared Autonomy via Deep Reinforcement Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/KS26B5X8/1802.html; /Users/jacquesthibodeau/Zotero/storage/FWIRNIMR/1802.html; /Users/jacquesthibodeau/Zotero/storage/P49DC639/Reddy et al. - 2018 - Shared Autonomy via Deep Reinforcement Learning.pdf,,CHAI; TechSafety,Computer Science - Machine Learning; Computer Science - Robotics; Computer Science - Human-Computer Interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Robotics: Science and Systems XIV,,,,,,,,,,,,,,,,
R95SH2RD,conferencePaper,2019,"Li, Shihui; Wu, Yi; Cui, Xinyue; Dong, Honghua; Fang, Fei; Russell, Stuart",Robust Multi-Agent Reinforcement Learning via Minimax Deep Deterministic Policy Gradient,Proceedings of the AAAI Conference on Artificial Intelligence,,,10.1609/aaai.v33i01.33014213,http://www.aaai.org/ojs/index.php/AAAI/article/view/4327,"Despite the recent advances of deep reinforcement learning (DRL), agents trained by DRL tend to be brittle and sensitive to the training environment, especially in the multi-agent scenarios. In the multi-agent setting, a DRL agent’s policy can easily get stuck in a poor local optima w.r.t. its training partners – the learned policy may be only locally optimal to other agents’ current policies. In this paper, we focus on the problem of training robust DRL agents with continuous actions in the multi-agent learning setting so that the trained agents can still generalize when its opponents’ policies alter. To tackle this problem, we proposed a new algorithm, MiniMax Multi-agent Deep Deterministic Policy Gradient (M3DDPG) with the following contributions: (1) we introduce a minimax extension of the popular multi-agent deep deterministic policy gradient algorithm (MADDPG), for robust policy learning; (2) since the continuous action space leads to computational intractability in our minimax learning objective, we propose Multi-Agent Adversarial Learning (MAAL) to efﬁciently solve our proposed formulation. We empirically evaluate our M3DDPG algorithm in four mixed cooperative and competitive multi-agent environments and the agents trained by our method signiﬁcantly outperforms existing baselines.",2019-07-17,2022-01-30 04:51:08,2022-01-30 04:51:08,2020-12-17 22:55:06,4213-4220,,,33,,,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000108,,/Users/jacquesthibodeau/Zotero/storage/TAT4QW33/Li et al. - 2019 - Robust Multi-Agent Reinforcement Learning via Mini.pdf,,CHAI; TechSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9QDMNDKS,conferencePaper,2016,"Halpern, Joseph Y.; Vilaca, Xavier",Rational Consensus,Proceedings of the 2016 ACM Symposium on Principles of Distributed Computing,,,,http://arxiv.org/abs/2005.10141,"We provide a game-theoretic analysis of consensus, assuming that processes are controlled by rational agents and may fail by crashing. We consider agents that \emph{care only about consensus}: that is, (a) an agent's utility depends only on the consensus value achieved (and not, for example, on the number of messages the agent sends) and (b) agents strictly prefer reaching consensus to not reaching consensus. We show that, under these assumptions, there is no \emph{ex post Nash Equilibrium}, even with only one failure. Roughly speaking, this means that there must always exist a \emph{failure pattern} (a description of who fails, when they fail, and which agents they do not send messages to in the round that they fail) and initial preferences for which an agent can gain by deviating. On the other hand, if we assume that there is a distribution $\pi$ on the failure patterns and initial preferences, then under minimal assumptions on $\pi$, there is a Nash equilibrium that tolerates $f$ failures (i.e., $\pi$ puts probability 1 on there being at most $f$ failures) if $f+1 < n$ (where $n$ is the total number of agents). Moreover, we show that a slight extension of the Nash equilibrium strategy is also a \emph{sequential} equilibrium (under the same assumptions about the distribution $\pi$).",2016,2022-01-30 04:51:08,2022-01-30 04:51:08,2020-12-17 22:22:59,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000022  arXiv: 2005.10141,,/Users/jacquesthibodeau/Zotero/storage/5MCJDWZ2/Halpern and Vilaca - 2020 - Rational Consensus.pdf; /Users/jacquesthibodeau/Zotero/storage/MAM8FX3J/2005.html,,CHAI; TechSafety; AmbiguosSafety,"Computer Science - Computer Science and Game Theory; Computer Science - Distributed, Parallel, and Cluster Computing",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2016 ACM Symposium on Principles of Distributed Computing,,,,,,,,,,,,,,,,
NFZZSNRF,conferencePaper,2020,"Gleave, Adam; Dennis, Michael; Legg, Shane; Russell, Stuart; Leike, Jan",Quantifying Differences in Reward Functions,,,,,http://arxiv.org/abs/2006.13900,"For many tasks, the reward function is too complex to be specified procedurally, and must instead be learned from user data. Prior work has evaluated learned reward functions by examining rollouts from a policy optimized for the learned reward. However, this method cannot distinguish between the learned reward function failing to reflect user preferences, and the reinforcement learning algorithm failing to optimize the learned reward. Moreover, the rollout method is highly sensitive to details of the environment the learned reward is evaluated in, which often differ in the deployment environment. To address these problems, we introduce the Equivalent-Policy Invariant Comparison (EPIC) distance to quantify the difference between two reward functions directly, without training a policy. We prove EPIC is invariant on an equivalence class of reward functions that always induce the same optimal policy. Furthermore, we find EPIC can be precisely approximated and is more robust than baselines to the choice of visitation distribution. Finally, we find that the EPIC distance of learned reward functions to the ground-truth reward is predictive of the success of training a policy, even in different transition dynamics.",2020-06-24,2022-01-30 04:51:08,2022-01-30 04:51:08,2020-08-31 17:51:23,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000007  arXiv: 2006.13900,,/Users/jacquesthibodeau/Zotero/storage/QZKBAKKJ/Gleave et al. - 2020 - Quantifying Differences in Reward Functions.pdf; /Users/jacquesthibodeau/Zotero/storage/SRRD2J53/2006.html,,CHAI; TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; I.2.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICLR 2021,,,,,,,,,,,,,,,,
RG8HBUB7,conferencePaper,2021,"Stastny, Julian; Riché, Maxime; Lyzhov, Alexander; Treutlein, Johannes; Dafoe, Allan; Clifton, Jesse",Normative Disagreement as a Challenge for Cooperative AI,Cooperative AI workshop and the Strategic ML workshop at NeurIPS 2021,,,,http://arxiv.org/abs/2111.13872,"Cooperation in settings where agents have both common and conflicting interests (mixed-motive environments) has recently received considerable attention in multi-agent learning. However, the mixed-motive environments typically studied have a single cooperative outcome on which all agents can agree. Many real-world multi-agent environments are instead bargaining problems (BPs): they have several Pareto-optimal payoff profiles over which agents have conflicting preferences. We argue that typical cooperation-inducing learning algorithms fail to cooperate in BPs when there is room for normative disagreement resulting in the existence of multiple competing cooperative equilibria, and illustrate this problem empirically. To remedy the issue, we introduce the notion of norm-adaptive policies. Norm-adaptive policies are capable of behaving according to different norms in different circumstances, creating opportunities for resolving normative disagreement. We develop a class of norm-adaptive policies and show in experiments that these significantly increase cooperation. However, norm-adaptiveness cannot address residual bargaining failure arising from a fundamental tradeoff between exploitability and cooperative robustness.",2021-11-27,2022-01-30 04:51:08,2022-01-30 04:51:08,2021-12-11 14:19:23,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000000  arXiv: 2111.13872,,/Users/jacquesthibodeau/Zotero/storage/57K6T8XI/Stastny et al. - 2021 - Normative Disagreement as a Challenge for Cooperat.pdf,,TechSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2021,,,,,,,,,,,,,,,,
FCTCJRGU,conferencePaper,2018,"Fisac, Jaime F.; Bajcsy, Andrea; Herbert, Sylvia L.; Fridovich-Keil, David; Wang, Steven; Tomlin, Claire J.; Dragan, Anca D.",Probabilistically Safe Robot Planning with Confidence-Based Human Predictions,arXiv:1806.00109 [cs],,,,https://arxiv.org/abs/1806.00109v1,"In order to safely operate around humans, robots can employ predictive models of human motion. Unfortunately, these models cannot capture the full complexity of human behavior and necessarily introduce simplifying assumptions. As a result, predictions may degrade whenever the observed human behavior departs from the assumed structure, which can have negative implications for safety. In this paper, we observe that how ""rational"" human actions appear under a particular model can be viewed as an indicator of that model's ability to describe the human's current motion. By reasoning about this model confidence in a real-time Bayesian framework, we show that the robot can very quickly modulate its predictions to become more uncertain when the model performs poorly. Building on recent work in provably-safe trajectory planning, we leverage these confidence-aware human motion predictions to generate assured autonomous robot motion. Our new analysis combines worst-case tracking error guarantees for the physical robot with probabilistic time-varying human predictions, yielding a quantitative, probabilistic safety certificate. We demonstrate our approach with a quadcopter navigating around a human.",2018-05-31,2022-01-30 04:51:07,2022-01-30 04:51:07,2019-12-18 01:36:19,,,,,,,,,,,,,,en,,,,,arxiv.org,,ZSCC: 0000076,,/Users/jacquesthibodeau/Zotero/storage/5ESRXS33/Fisac et al. - 2018 - Probabilistically Safe Robot Planning with Confide.pdf; /Users/jacquesthibodeau/Zotero/storage/ATI9G9AD/1806.html; /Users/jacquesthibodeau/Zotero/storage/EFE4XMVX/Fisac et al. - 2018 - Probabilistically Safe Robot Planning with Confide.pdf; /Users/jacquesthibodeau/Zotero/storage/4UZA833Q/1806.html,,CHAI; TechSafety,Computer Science - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Robotics: Science and Systems 2018,,,,,,,,,,,,,,,,
PEQIA7QQ,conferencePaper,2019,"Shah, Rohin; Krasheninnikov, Dmitrii; Alexander, Jordan; Abbeel, Pieter; Dragan, Anca",Preferences Implicit in the State of the World,,,,,http://arxiv.org/abs/1902.04198,"Reinforcement learning (RL) agents optimize only the features speciﬁed in a reward function and are indifferent to anything left out inadvertently. This means that we must not only specify what to do, but also the much larger space of what not to do. It is easy to forget these preferences, since these preferences are already satisﬁed in our environment. This motivates our key insight: when a robot is deployed in an environment that humans act in, the state of the environment is already optimized for what humans want. We can therefore use this implicit preference information from the state to ﬁll in the blanks. We develop an algorithm based on Maximum Causal Entropy IRL and use it to evaluate the idea in a suite of proof-of-concept environments designed to show its properties. We ﬁnd that information from the initial state can be used to infer both side effects that should be avoided as well as preferences for how the environment should be organized. Our code can be found at https://github.com/HumanCompatibleAI/rlsp.",2019-02-11,2022-01-30 04:51:07,2022-01-30 04:51:07,2019-07-11 18:35:33,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000031  arXiv: 1902.04198,,/Users/jacquesthibodeau/Zotero/storage/W5WZD9IE/Shah et al. - 2019 - Preferences Implicit in the State of the World.pdf; /Users/jacquesthibodeau/Zotero/storage/H6E44KI8/Shah et al. - 2019 - Preferences Implicit in the State of the World.pdf; /Users/jacquesthibodeau/Zotero/storage/6ZGSF3I5/1902.html; /Users/jacquesthibodeau/Zotero/storage/WQ6W7MGD/1902.html,,CHAI; TechSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICLR 2019,,,,,,,,,,,,,,,,
ZAMXRUV6,conferencePaper,2020,"Fisac, Jaime F.; Gates, Monica A.; Hamrick, Jessica B.; Liu, Chang; Hadfield-Menell, Dylan; Palaniappan, Malayandi; Malik, Dhruv; Sastry, S. Shankar; Griffiths, Thomas L.; Dragan, Anca D.",Pragmatic-Pedagogic Value Alignment,Robotics Research,978-3-030-28618-7 978-3-030-28619-4,,10.1007/978-3-030-28619-4_7,http://link.springer.com/10.1007/978-3-030-28619-4_7,"As intelligent systems gain autonomy and capability, it becomes vital to ensure that their objectives match those of their human users; this is known as the value-alignment problem. In robotics, value alignment is key to the design of collaborative robots that can integrate into human workﬂows, successfully inferring and adapting to their users’ objectives as they go. We argue that a meaningful solution to value alignment must combine multi-agent decision theory with rich mathematical models of human cognition, enabling robots to tap into people’s natural collaborative capabilities. We present a solution to the cooperative inverse reinforcement learning (CIRL) dynamic game based on well-established cognitive models of decision making and theory of mind. The solution captures a key reciprocity relation: the human will not plan her actions in isolation, but rather reason pedagogically about how the robot might learn from them; the robot, in turn, can anticipate this and interpret the human’s actions pragmatically. To our knowledge, this work constitutes the ﬁrst formal analysis of value alignment grounded in empirically validated cognitive models.",2020,2022-01-30 04:51:07,2022-01-30 04:51:07,2019-12-18 01:15:26,49-57,,,10,,,,,,,,Springer International Publishing,Cham,en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s2]  ACC: 47,,/Users/jacquesthibodeau/Zotero/storage/ZADVEM2M/Fisac et al. - 2018 - Pragmatic-Pedagogic Value Alignment.pdf; /Users/jacquesthibodeau/Zotero/storage/IJB43GBG/1707.html; /Users/jacquesthibodeau/Zotero/storage/NAHBPPDH/1707.html,,CHAI; TechSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Robotics; I.2.0; I.2.6; Computer Science - Human-Computer Interaction; 68T05; I.2.8; I.2.9,"Amato, Nancy M.; Hager, Greg; Thomas, Shawna; Torres-Torriti, Miguel",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PCHKUWWW,conferencePaper,2019,"Oesterheld, Caspar; Conitzer, Vincent",Extracting Money from Causal Decision Theorists,Proceedings of the Workshop on Artificial Intelligence Safety 2020,,,,http://ceur-ws.org/Vol-2640/paper_21.pdf,"Newcomb’s problem has spawned a debate about which variant of expected utility maximization (if any) should guide rational choice. In this paper, we provide a new argument against what is probably the most popular variant: causal decision theory (CDT). In particular, we provide two scenarios in which CDT voluntarily loses money. In the ﬁrst, an agent faces a single choice and following CDT’s recommendation yields a loss of money in expectation. The second scenario extends the ﬁrst to a diachronic Dutch book against CDT.",2019-08-30,2022-01-30 04:51:07,2022-01-30 04:51:07,2020-12-18,19,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000000[s0],,/Users/jacquesthibodeau/Zotero/storage/ANCD7C23/Oesterheld and Conitzer - Extracting Money from Causal Decision Theorists.pdf,,CLR; TechSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5HPEHZIR,conferencePaper,2016,"Sadigh, Dorsa; Sastry, Shankar; A. Seshia, Sanjit; D. Dragan, Anca",Planning for Autonomous Cars that Leverage Effects on Human Actions,Robotics: Science and Systems XII,978-0-9923747-2-3,,10.15607/RSS.2016.XII.029,http://www.roboticsproceedings.org/rss12/p29.pdf,"Traditionally, autonomous cars make predictions about other drivers’ future trajectories, and plan to stay out of their way. This tends to result in defensive and opaque behaviors. Our key insight is that an autonomous car’s actions will actually affect what other cars will do in response, whether the car is aware of it or not. Our thesis is that we can leverage these responses to plan more efﬁcient and communicative behaviors. We model the interaction between an autonomous car and a human driver as a dynamical system, in which the robot’s actions have immediate consequences on the state of the car, but also on human actions. We model these consequences by approximating the human as an optimal planner, with a reward function that we acquire through Inverse Reinforcement Learning. When the robot plans with this reward function in this dynamical system, it comes up with actions that purposefully change human state: it merges in front of a human to get them to slow down or to reach its own goal faster; it blocks two lanes to get them to switch to a third lane; or it backs up slightly at an intersection to get them to proceed ﬁrst. Such behaviors arise from the optimization, without relying on hand-coded signaling strategies and without ever explicitly modeling communication. Our user study results suggest that the robot is indeed capable of eliciting desired changes in human state by planning using this dynamical system.",2016,2022-01-30 04:51:06,2022-01-30 04:51:06,2020-12-13 23:37:50,,,,,,,,,,,,Robotics: Science and Systems Foundation,,en,,,,,DOI.org (Crossref),,ZSCC: 0000363,,/Users/jacquesthibodeau/Zotero/storage/KQ9WS3I7/SadighPlanning2016.pdf,,CHAI; TechSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Robotics: Science and Systems 2016,,,,,,,,,,,,,,,,
R8SVD9CH,conferencePaper,2021,"Koch, Jack; Langosco, Lauro; Pfau, Jacob; Le, James; Sharkey, Lee",Objective Robustness in Deep Reinforcement Learning,arXiv:2105.14111 [cs],,,,http://arxiv.org/abs/2105.14111,"We study objective robustness failures, a type of out-of-distribution robustness failure in reinforcement learning (RL). Objective robustness failures occur when an RL agent retains its capabilities out-of-distribution yet pursues the wrong objective. This kind of failure presents different risks than the robustness problems usually considered in the literature, since it involves agents that leverage their capabilities to pursue the wrong objective rather than simply failing to do anything useful. We provide the first explicit empirical demonstrations of objective robustness failures and present a partial characterization of its causes.",2021-06-08,2022-01-30 04:49:30,2022-01-30 04:49:30,2021-10-30 17:55:11,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000001  arXiv: 2105.14111,,/Users/jacquesthibodeau/Zotero/storage/HBRPM964/Koch et al. - 2021 - Objective Robustness in Deep Reinforcement Learnin.pdf; /Users/jacquesthibodeau/Zotero/storage/HQIR73N6/2105.html,,TechSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICML 2021,,,,,,,,,,,,,,,,
5HWDDTTB,conferencePaper,2019,"Mancuso, Jason; Kisielewski, Tomasz; Lindner, David; Singh, Alok",Detecting Spiky Corruption in Markov Decision Processes,,,,,http://arxiv.org/abs/1907.00452,"Current reinforcement learning methods fail if the reward function is imperfect, i.e. if the agent observes reward different from what it actually receives. We study this problem within the formalism of Corrupt Reward Markov Decision Processes (CRMDPs). We show that if the reward corruption in a CRMDP is sufficiently ""spiky"", the environment is solvable. We fully characterize the regret bound of a Spiky CRMDP, and introduce an algorithm that is able to detect its corrupt states. We show that this algorithm can be used to learn the optimal policy with any common reinforcement learning algorithm. Finally, we investigate our algorithm in a pair of simple gridworld environments, finding that our algorithm can detect the corrupt states and learn the optimal policy despite the corruption.",2019-06-30,2022-01-30 04:49:30,2022-01-30 04:49:30,2019-12-16 03:27:42,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000002  arXiv: 1907.00452,,/Users/jacquesthibodeau/Zotero/storage/7UWHJF9I/Mancuso et al. - 2019 - Detecting Spiky Corruption in Markov Decision Proc.pdf; /Users/jacquesthibodeau/Zotero/storage/ZJH6BKAF/Mancuso et al. - 2019 - Detecting Spiky Corruption in Markov Decision Proc.pdf; /Users/jacquesthibodeau/Zotero/storage/ZW3BBNWK/1907.html,,TechSafety; AI-Safety-Camp,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AI Safety Workshop in IJCAI 2019,,,,,,,,,,,,,,,,
7QA8U489,conferencePaper,2019,"Majha, Arushi; Sarkar, Sayan; Zagami, Davide",Categorizing Wireheading in Partially Embedded Agents,,,,,http://arxiv.org/abs/1906.09136,"$\textit{Embedded agents}$ are not explicitly separated from their environment, lacking clear I/O channels. Such agents can reason about and modify their internal parts, which they are incentivized to shortcut or $\textit{wirehead}$ in order to achieve the maximal reward. In this paper, we provide a taxonomy of ways by which wireheading can occur, followed by a definition of wirehead-vulnerable agents. Starting from the fully dualistic universal agent AIXI, we introduce a spectrum of partially embedded agents and identify wireheading opportunities that such agents can exploit, experimentally demonstrating the results with the GRL simulation platform AIXIjs. We contextualize wireheading in the broader class of all misalignment problems - where the goals of the agent conflict with the goals of the human designer - and conjecture that the only other possible type of misalignment is specification gaming. Motivated by this taxonomy, we define wirehead-vulnerable agents as embedded agents that choose to behave differently from fully dualistic agents lacking access to their internal parts.",2019-06-21,2022-01-30 04:49:30,2022-01-30 04:49:30,2019-12-16 03:26:47,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000001,,/Users/jacquesthibodeau/Zotero/storage/2PGIU7BA/Majha et al. - 2019 - Categorizing Wireheading in Partially Embedded Age.pdf; /Users/jacquesthibodeau/Zotero/storage/UGH6MA5M/Majha et al. - 2019 - Categorizing Wireheading in Partially Embedded Age.pdf; /Users/jacquesthibodeau/Zotero/storage/429IP46K/1906.html; /Users/jacquesthibodeau/Zotero/storage/MMNA4BT2/1906.html,,TechSafety; AI-Safety-Camp,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AI Safety Workshop in IJCAI 2019,,,,,,,,,,,,,,,,
6C9TQWFX,conferencePaper,2019,"Tangkaratt, Voot; Han, Bo; Khan, Mohammad Emtiyaz; Sugiyama, Masashi",VILD: Variational Imitation Learning with Diverse-quality Demonstrations,Proceedings of the 37th International Conference on Machine Learning,,,,http://arxiv.org/abs/1909.06769,"The goal of imitation learning (IL) is to learn a good policy from high-quality demonstrations. However, the quality of demonstrations in reality can be diverse, since it is easier and cheaper to collect demonstrations from a mix of experts and amateurs. IL in such situations can be challenging, especially when the level of demonstrators' expertise is unknown. We propose a new IL method called \underline{v}ariational \underline{i}mitation \underline{l}earning with \underline{d}iverse-quality demonstrations (VILD), where we explicitly model the level of demonstrators' expertise with a probabilistic graphical model and estimate it along with a reward function. We show that a naive approach to estimation is not suitable to large state and action spaces, and fix its issues by using a variational approach which can be easily implemented using existing reinforcement learning methods. Experiments on continuous-control benchmarks demonstrate that VILD outperforms state-of-the-art methods. Our work enables scalable and data-efficient IL under more realistic settings than before.",2019-09-15,2022-01-30 04:48:55,2022-01-30 04:48:55,2021-11-18 23:17:53,,,,,,,VILD,,,,,,,,,,,,arXiv.org,,ZSCC: 0000003  arXiv: 1909.06769,,/Users/jacquesthibodeau/Zotero/storage/KR4JBV4S/Tangkaratt et al. - 2019 - VILD Variational Imitation Learning with Diverse-.pdf; /Users/jacquesthibodeau/Zotero/storage/KZEUIWTZ/1909.html,,UnsortedSafety,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICML 2020,,,,,,,,,,,,,,,,
GPKVQEG4,conferencePaper,2021,"Hunt, Nathan; Fulton, Nathan; Magliacane, Sara; Hoang, Nghia; Das, Subhro; Solar-Lezama, Armando",Verifiably Safe Exploration for End-to-End Reinforcement Learning,HSCC '21: Proceedings of the 24th International Conference on Hybrid Systems: Computation and Control,,,10.1145/3447928.3456653,http://arxiv.org/abs/2007.01223,"Deploying deep reinforcement learning in safety-critical settings requires developing algorithms that obey hard constraints during exploration. This paper contributes a first approach toward enforcing formal safety constraints on end-to-end policies with visual inputs. Our approach draws on recent advances in object detection and automated reasoning for hybrid dynamical systems. The approach is evaluated on a novel benchmark that emphasizes the challenge of safely exploring in the presence of hard constraints. Our benchmark draws from several proposed problem sets for safe learning and includes problems that emphasize challenges such as reward signals that are not aligned with safety constraints. On each of these benchmark problems, our algorithm completely avoids unsafe behavior while remaining competitive at optimizing for as much reward as is safe. We also prove that our method of enforcing the safety constraints preserves all safe policies from the original environment.",2021-05,2022-01-30 04:48:55,2022-01-30 04:48:55,2021-11-09 12:10:05,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000002  arXiv: 2007.01223,,/Users/jacquesthibodeau/Zotero/storage/NHP43TJ9/Hunt et al. - 2020 - Verifiably Safe Exploration for End-to-End Reinfor.pdf; /Users/jacquesthibodeau/Zotero/storage/VK2PPGIX/2007.html,,UnsortedSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; I.2.8; Computer Science - Logic in Computer Science; F.3.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HSCC 2021,,,,,,,,,,,,,,,,
M4GP2KT9,conferencePaper,2021,"Jia, Feiran; Mate, Aditya; Li, Zun; Jabbari, Shahin; Chakraborty, Mithun; Tambe, Milind; Wellman, Michael; Vorobeychik, Yevgeniy",A Game-Theoretic Approach for Hierarchical Policy-Making,arXiv:2102.10646 [cs],,,,http://arxiv.org/abs/2102.10646,"We present the design and analysis of a multi-level game-theoretic model of hierarchical policy-making, inspired by policy responses to the COVID-19 pandemic. Our model captures the potentially mismatched priorities among a hierarchy of policy-makers (e.g., federal, state, and local governments) with respect to two main cost components that have opposite dependence on the policy strength, such as post-intervention infection rates and the cost of policy implementation. Our model further includes a crucial third factor in decisions: a cost of non-compliance with the policy-maker immediately above in the hierarchy, such as non-compliance of state with federal policies. Our first contribution is a closed-form approximation of a recently published agent-based model to compute the number of infections for any implemented policy. Second, we present a novel equilibrium selection criterion that addresses common issues with equilibrium multiplicity in our setting. Third, we propose a hierarchical algorithm based on best response dynamics for computing an approximate equilibrium of the hierarchical policy-making game consistent with our solution concept. Finally, we present an empirical investigation of equilibrium policy strategies in this game in terms of the extent of free riding as well as fairness in the distribution of costs depending on game parameters such as the degree of centralization and disagreements about policy priorities among the agents.",2021-02-21,2022-01-30 04:50:42,2022-01-30 04:50:42,2021-10-30 21:50:53,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000001  arXiv: 2102.10646,,/Users/jacquesthibodeau/Zotero/storage/C28KUI7S/Jia et al. - 2021 - A Game-Theoretic Approach for Hierarchical Policy-.pdf; /Users/jacquesthibodeau/Zotero/storage/3CD6QZFT/2102.html,,MetaSafety; AmbiguousSafety,Computer Science - Computer Science and Game Theory; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2nd International (Virtual) Workshop on Autonomous Agents for Social Good (AASG 2021),,,,,,,,,,,,,,,,
J95SGZ5U,conferencePaper,2018,"Smitha Milli; Lieder, Falk; Griffiths, Thomas L",A Rational Reinterpretation of Dual-Process Theories,,,,10.13140/rg.2.2.14956.46722/1,http://rgdoi.net/10.13140/RG.2.2.14956.46722/1,"Highly inﬂuential “dual-process"" accounts of human cognition postulate the coexistence of a slow accurate system with a fast error-prone system. But why would there be just two systems rather than, say, one or 93? Here, we argue that a dual-process architecture might be neither arbitrary nor irrational, but might instead reﬂect a rational tradeoff between the cognitive ﬂexibility afforded by multiple systems and the time and effort required to choose between them. We investigate what the optimal set and number of cognitive systems would be depending on the structure of the environment. We ﬁnd that the optimal number of systems depends on the variability of the environment and the difﬁculty of deciding when which system should be used. Furthermore, when having two systems is optimal, then the ﬁrst system is fast but error-prone and the second system is slow but accurate. Our ﬁndings thereby provide a rational reinterpretation of dual-process theories.",2018,2022-01-30 04:50:42,2022-01-30 04:50:42,2019-07-08 15:59:57,,,,,,,,,,,,,,en,,,,,DOI.org (Datacite),,ZSCC: 0000003[s2],,,,CHAI; TechSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Thirty-First AAAI Conference on Artiﬁcial Intelligence,,,,,,,,,,,,,,,,
JSDBTHXD,conferencePaper,2019,"Bansal, Somil; Bajcsy, Andrea; Ratner, Ellis; Dragan, Anca D.; Tomlin, Claire J.",A Hamilton-Jacobi Reachability-Based Framework for Predicting and Analyzing Human Motion for Safe Planning,2020 IEEE International Conference on Robotics and Automation (ICRA),,,,http://arxiv.org/abs/1910.13369,"Real-world autonomous systems often employ probabilistic predictive models of human behavior during planning to reason about their future motion. Since accurately modeling the human behavior a priori is challenging, such models are often parameterized, enabling the robot to adapt predictions based on observations by maintaining a distribution over the model parameters. This leads to a probabilistic prediction problem, which even though attractive, can be computationally demanding. In this work, we formalize the prediction problem as a stochastic reachability problem in the joint state space of the human and the belief over the model parameters. We further introduce a Hamilton-Jacobi reachability framework which casts a deterministic approximation of this stochastic reachability problem by restricting the allowable actions to a set rather than a distribution, while still maintaining the belief as an explicit state. This leads to two advantages: our approach gives rise to a novel predictor wherein the predictions can be performed at a significantly lower computational expense, and to a general framework which also enables us to perform predictor analysis. We compare our approach to a fully stochastic predictor using Bayesian inference and the worst-case forward reachable set in simulation and in hardware, and demonstrate how it can enable robust planning while not being overly conservative, even when the human model is inaccurate.",2019-10-29,2022-01-30 04:50:42,2022-01-30 04:50:42,2019-12-18 02:35:08,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000003[s0]  arXiv: 1910.13369,,/Users/jacquesthibodeau/Zotero/storage/XESDVH7I/Bansal et al. - 2019 - A Hamilton-Jacobi Reachability-Based Framework for.pdf; /Users/jacquesthibodeau/Zotero/storage/NAM8GA7X/1910.html,,CHAI; TechSafety; AmbiguosSafety,Computer Science - Machine Learning; Computer Science - Robotics; Electrical Engineering and Systems Science - Systems and Control,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2020 IEEE International Conference on Robotics and Automation (ICRA),,,,,,,,,,,,,,,,
U2CSBG92,conferencePaper,2018,"Ó hÉigeartaigh, Seán",The State of Research in Existential Risk,Proceedings from the first Garrick Colloquium on Catastrophic and Existential Risk,,,,https://www.risksciences.ucla.edu/news-events/2018/1/2/proceedings-of-the-first-international-colloquium-on-catastrophic-and-existential-risk,,2018,2022-01-30 04:50:26,2022-01-30 04:50:26,2020-12-12,,,,,,,,,,,,"B John Garrick Institute for the Risk Sciences, University of California Los Angeles",,,,,,,,,ZSCC: NoCitationData[s1]  ACC: 2,,/Users/jacquesthibodeau/Zotero/storage/PTF948QW/Ó hÉigeartaigh - 2018 - The State of Research in Existential Risk.pdf,,MetaSafety; CFI; CSER,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IKHJ2UAC,conferencePaper,2019,"Whittlestone, Jess; Nyrup, Rune; Alexandrova, Anna; Cave, Stephen",The Role and Limits of Principles in AI Ethics: Towards a Focus on Tensions,"AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",,,,,"The last few years have seen a proliferation of principles for AI ethics. There is substantial overlap between different sets of principles, with widespread agreement that AI should be used for the common good, should not be used to harm people or undermine their rights, and should respect widely held values such as fairness, privacy, and autonomy. While articulating and agreeing on principles is important, it is only a starting point. Drawing on comparisons with the field of bioethics, we highlight some of the limitations of principles: in particular, they are often too broad and high-level to guide ethics in practice. We suggest that an important next step for the field of AI ethics is to focus on exploring the tensions that inevitably arise as we try to implement principles in practice. By explicitly recognising these tensions we can begin to make decisions about how they should be resolved in specific cases, and develop frameworks and guidelines for AI ethics that are rigorous and practically relevant. We discuss some different specific ways that tensions arise in AI ethics, and what processes might be needed to resolve them.",2019,2022-01-30 04:50:26,2022-01-30 04:50:26,,7,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000087,,/Users/jacquesthibodeau/Zotero/storage/GUM2EHUB/Whittlestone et al. - The Role and Limits of Principles in AI Ethics To.pdf,,MetaSafety; CFI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"2019 AAAI/ACM Conference on AI, Ethics, and Society",,,,,,,,,,,,,,,,
ADQ42JZJ,conferencePaper,2018,"Martínez-Plumed, Fernando; Loe, Bao Sheng; Flach, Peter; Ó hÉigeartaigh, Seán; Vold, Karina; Hernández-Orallo, José",The Facets of Artificial Intelligence: A Framework to Track the Evolution of AI,Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence,978-0-9992411-2-7,,10.24963/ijcai.2018/718,https://www.ijcai.org/proceedings/2018/718,"We present nine facets for the analysis of the past and future evolution of AI. Each facet has also a set of edges that can summarise different trends and contours in AI. With them, we ﬁrst conduct a quantitative analysis using the information from two decades of AAAI/IJCAI conferences and around 50 years of documents from AI topics, an ofﬁcial database from the AAAI, illustrated by several plots. We then perform a qualitative analysis using the facets and edges, locating AI systems in the intelligence landscape and the discipline as a whole. This analytical framework provides a more structured and systematic way of looking at the shape and boundaries of AI.",2018-07,2022-01-30 04:50:25,2022-01-30 04:50:25,2020-11-14 01:15:31,5180-5187,,,,,,The Facets of Artificial Intelligence,,,,,International Joint Conferences on Artificial Intelligence Organization,"Stockholm, Sweden",en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s1]  ACC: 17,,/Users/jacquesthibodeau/Zotero/storage/W37ZFTH7/Martínez-Plumed et al. - 2018 - The Facets of Artificial Intelligence A Framework.pdf; /Users/jacquesthibodeau/Zotero/storage/IPMDVDXU/Martínez-Plumed et al. - 2018 - The Facets of Artificial Intelligence A Framework.pdf,,MetaSafety; CFI; CSER; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Twenty-Seventh International Joint Conference on Artificial Intelligence {IJCAI-18},,,,,,,,,,,,,,,,
TVJ2EKVB,conferencePaper,2019,"Hernandez-Orallo, Jose; Martınez-Plumed, Fernando; Avin, Shahar",Surveying Safety-relevant AI Characteristics,1st AAAI's Workshop on Artificial Intelligence Safety (SafeAI),,,,,"The current analysis in the AI safety literature usually combines a risk or safety issue (e.g., interruptibility) with a particular paradigm for an AI agent (e.g., reinforcement learning). However, there is currently no survey of safety-relevant characteristics of AI systems that may reveal neglected areas of research or suggest to developers what design choices they could make to avoid or minimise certain safety concerns. In this paper, we take a first step towards delivering such a survey, from two angles. The first features AI system characteristics that are already known to be relevant to safety concerns, including internal system characteristics, characteristics relating to the effect of the external environment on the system, and characteristics relating to the effect of the system on the target environment. The second presents a brief survey of a broad range of AI system characteristics that could prove relevant to safety research, including types of interaction, computation, integration, anticipation, supervision, modification, motivation and achievement. This survey enables further work in exploring system characteristics and design choices that affect safety concerns.",2019,2022-01-30 04:50:25,2022-01-30 04:50:25,,9,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: NoCitationData[s7]  ACC: 10  J: 4,,/Users/jacquesthibodeau/Zotero/storage/X9P84E7X/Hernandez-Orallo et al. - Surveying Safety-relevant AI Characteristics.pdf,,TechSafety; CFI; CSER,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
K4827FC7,conferencePaper,2020,"Cihon, Peter; Maas, Matthijs M.; Kemp, Luke",Should Artificial Intelligence Governance be Centralised?: Design Lessons from History,"Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",978-1-4503-7110-0,,10.1145/3375627.3375857,https://dl.acm.org/doi/10.1145/3375627.3375857,,2020-02-07,2022-01-30 04:50:25,2022-01-30 04:50:25,2020-12-12 16:10:16,228-234,,,,,,Should Artificial Intelligence Governance be Centralised?,,,,,ACM,New York NY USA,en,,,,,DOI.org (Crossref),,ZSCC: 0000016,,,,MetaSafety; CFI; CSER; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"AIES '20: AAAI/ACM Conference on AI, Ethics, and Society",,,,,,,,,,,,,,,,
HFZ22SRN,conferencePaper,2021,"Burden, John; Hernandez-Orallo, Jose",Negative Side Effects and AI Agent Indicators: Experiments in SafeLife,,,,,,"The widespread adoption and ubiquity of AI systems will require them to be safe. The safety issues that can arise from AI are broad and varied. In this paper we consider the safety issue of negative side effects and the consequences they can have on an environment. In the safety benchmarking domain SafeLife, we discuss the way that side effects are measured, as well as presenting results showing the relation between the magnitude of side effects and other metrics for three agent types: Deep Q-Networks, Proximal Policy Optimisation, and a Uniform Random Agent. We observe that different metrics and agent types lead to both monotonic and non-monotonic interactions, with the ﬁnding that the size and complexity of the environment versus the capability of the agent plays a major role in negative side effects, sometimes in intricate ways.",2021-01-01,2022-01-30 04:50:25,2022-01-30 04:50:25,,9,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/92N83U65/Burden and Hernandez-Orallo - Negative Side Effects and AI Agent Indicators Exp.pdf,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SafeAI@ AAAI,,,,,,,,,,,,,,,,
NSJVHAQP,conferencePaper,2020,"Bhatt, Umang; Andrus, McKane; Weller, Adrian; Xiang, Alice",Machine Learning Explainability for External Stakeholders,,,,,https://arxiv.org/abs/2007.05408v1,"As machine learning is increasingly deployed in high-stakes contexts affecting people's livelihoods, there have been growing calls to open the black box and to make machine learning algorithms more explainable. Providing useful explanations requires careful consideration of the needs of stakeholders, including end-users, regulators, and domain experts. Despite this need, little work has been done to facilitate inter-stakeholder conversation around explainable machine learning. To help address this gap, we conducted a closed-door, day-long workshop between academics, industry experts, legal scholars, and policymakers to develop a shared language around explainability and to understand the current shortcomings of and potential solutions for deploying explainable machine learning in service of transparency goals. We also asked participants to share case studies in deploying explainable machine learning at scale. In this paper, we provide a short summary of various case studies of explainable machine learning, lessons from those studies, and discuss open challenges.",2020-07-10,2022-01-30 04:50:25,2022-01-30 04:50:25,2020-11-23 01:16:21,,,,,,,,,,,,,,en,,,,,arxiv.org,,ZSCC: 0000014,,/Users/jacquesthibodeau/Zotero/storage/2C2574GF/Bhatt et al. - 2020 - Machine Learning Explainability for External Stake.pdf; /Users/jacquesthibodeau/Zotero/storage/8EMZGHET/2007.html,,MetaSafety; CFI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICML Workshop XXAI: Extending Explainable AI Beyond Deep Models and Classifiers,,,,,,,,,,,,,,,,
CCRB9244,conferencePaper,2020,"Burden, John; Hernandez-Orallo, Jose","Exploring AI Safety in Degrees: Generality, Capability and Control",Proceedings of the Workshop on Artificial Intelligence Safety (SafeAI 2020),,,,,"The landscape of AI safety is frequently explored differently by contrasting specialised AI versus general AI (or AGI), by analysing the short-term hazards of systems with limited capabilities against those more long-term risks posed by ‘superintelligence’, and by conceptualising sophisticated ways of bounding control an AI system has over its environment and itself (impact, harm to humans, self-harm, containment, etc.). In this position paper we reconsider these three aspects of AI safety as quantitative factors –generality, capability and control–, suggesting that by deﬁning metrics for these dimensions, AI risks can be characterised and analysed more precisely. As an example, we illustrate how to deﬁne these metrics and their values for some simple agents in a toy scenario within a reinforcement learning setting.",2020-08-10,2022-01-30 04:50:25,2022-01-30 04:50:25,,5,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000007,,"/Users/jacquesthibodeau/Zotero/storage/Q7VVUNP9/Burden and Hernandez-Orallo - Exploring AI Safety in Degrees Generality, Capabi.pdf",,TechSafety; CFI; CSER; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MCJ6I4DC,conferencePaper,2020,"Cremer, Carla Zoe; Whittlestone, Jess",Canaries in Technology Mines: Warning Signs of Transformative Progress in AI,,,,,,,2020,2022-01-30 04:50:25,2022-01-30 04:50:25,,7,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000001,,/Users/jacquesthibodeau/Zotero/storage/TQJZSWED/Cremer and Whittlestone - Canaries in Technology Mines Warning Signs of Tra.pdf,,MetaSafety; CFI; CSER; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1st International Workshop on Evaluating Progress in Artificial Intelligence - EPAI 2020,,,,,,,,,,,,,,,,
AJAXDNIF,conferencePaper,2020,"Prunkl, Carina; Whittlestone, Jess",Beyond Near- and Long-Term: Towards a Clearer Account of Research Priorities in AI Ethics and Society,arXiv:2001.04335 [cs],,,,http://arxiv.org/abs/2001.04335,"One way of carving up the broad ‘AI ethics and society’ research space that has emerged in recent years is to distinguish between ‘near-term’ and ‘long-term’ research. While such ways of breaking down the research space can be useful, we put forward several concerns about the near/long-term distinction gaining too much prominence in how research questions and priorities are framed. We highlight some ambiguities and inconsistencies in how the distinction is used, and argue that while there are differing priorities within this broad research community, these differences are not well-captured by the near/long-term distinction. We unpack the near/long-term distinction into four different dimensions, and propose some ways that researchers can communicate more clearly about their work and priorities using these dimensions. We suggest that moving towards a more nuanced conversation about research priorities can help establish new opportunities for collaboration, aid the development of more consistent and coherent research agendas, and enable identification of previously neglected research areas.",2020-01-21,2022-01-30 04:50:24,2022-01-30 04:50:24,2020-08-21 20:00:24,,,,,,,Beyond Near- and Long-Term,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000014  arXiv: 2001.04335,,/Users/jacquesthibodeau/Zotero/storage/VCRACQA2/Prunkl and Whittlestone - 2020 - Beyond Near- and Long-Term Towards a Clearer Acco.pdf,,MetaSafety; CFI; CSER; FHI; AmbiguosSafety,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"2020 AAAI/ACM Conference on AI, Ethics, and Society",,,,,,,,,,,,,,,,
IDS6VQUQ,conferencePaper,2018,"Cave, Stephen; ÓhÉigeartaigh, Seán S.",An AI Race for Strategic Advantage: Rhetoric and Risks,"Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",978-1-4503-6012-8,,10.1145/3278721.3278780,https://dl.acm.org/doi/10.1145/3278721.3278780,,2018-12-27,2022-01-30 04:50:24,2022-01-30 04:50:24,2020-12-12 17:39:09,36-40,,,,,,An AI Race for Strategic Advantage,,,,,ACM,New Orleans LA USA,en,,,,,DOI.org (Crossref),,ZSCC: 0000066,,/Users/jacquesthibodeau/Zotero/storage/5F7NG3ZZ/Cave and ÓhÉigeartaigh - 2018 - An AI Race for Strategic Advantage Rhetoric and R.pdf; /Users/jacquesthibodeau/Zotero/storage/IFPKRRTZ/Cave and ÓhÉigeartaigh - 2018 - An AI Race for Strategic Advantage Rhetoric and R.pdf,,MetaSafety; CFI; CSER,artificial intelligence; ai narratives; ai risks; ai safety; arms race; global governance; international cooperation; strategic competition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"AIES '18: AAAI/ACM Conference on AI, Ethics, and Society",,,,,,,,,,,,,,,,
46FKJWZG,conferencePaper,2021,"Maas, Matthijs M.","AI, Governance Displacement, and the (De)Fragmentation of International Law",SSRN Electronic Journal,,,10.2139/ssrn.3806624,https://www.ssrn.com/abstract=3806624,"The emergence, proliferation, and use of new general-purpose technologies can often produce significant political, redistributive, normative and legal effects on the world. Artificial intelligence (AI) has been identified as one such transformative technology. Many of its impacts may require global governance responses. However, what are the direct and indirect effects of AI technologies on the viability, form, or functioning of the international legal order itself? What, if any, are the prospects, peril or promise of AI-driven legal automation at the international level? This paper draws on an ‘AI Governance Disruption’ framework to understanding AI’s impacts on the global governance architecture. Focusing particularly on the potential for legal automation at the international law level, it explores three potential pathways of such ‘legal displacement’: (1) the automation of rule creation and arbitration; (2) the automation of monitoring & enforcement; or (3) the ‘replacement’ of international law with new architectural modes of (international) behaviour control. It then focuses on the effects of these trends on the architecture of international law. It distinguishes 10 different roles that AI applications could play, with distinct effects on the international legal order. That is, AI systems can serve as (1) legal ‘canary in the coal mine’, highlighting the need for greater cross-regime harmonization. However, it can also serve as (2) tough knot or (3) generator of regime fault lines. Under even modest scenarios of legal automation, AI systems may serve variably as a (4) shield, (5) patch, (6) cure, or (7) accelerator of international legal fragmentation. Finally, AI tools may serve as (8) differential enabler; (9) driver of value shifts, or (10) asymmetric weapon, potentially contributing to trends of contestation or erosion in the international legal order. The paper concludes with a brief review of the ways in which international lawyers or regime scholars might approach the risks and opportunities of increasing automation in international law, in order to leverage these trends and tools towards improved efficacy, resilience, and legitimacy of global governance.",2021-03,2022-01-30 04:50:24,2022-01-30 04:50:24,2021-10-31 16:58:43,,,,,,,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000000,,"/Users/jacquesthibodeau/Zotero/storage/EZ5UBQGN/Maas - 2021 - AI, Governance Displacement, and the (De)Fragmenta.pdf",,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,International Studies Association 2021,,,,,,,,,,,,,,,,
VJHUMTDA,conferencePaper,2020,"Hernandez-Orallo, Jose; Martınez-Plumed, Fernando; Avin, Shahar; Whittlestone, Jess; Ó hÉigeartaigh, Seán",AI Paradigms and AI Safety: Mapping Artefacts and Techniques to Safety Issues,European Conference on Artificial Intelligence,,,,,"AI safety often analyses a risk or safety issue, such as interruptibility, under a particular AI paradigm, such as reinforcement learning. But what is an AI paradigm and how does it affect the understanding and implications of the safety issue? Is AI safety research covering the most representative paradigms and the right combinations of paradigms with safety issues? Will current research directions in AI safety be able to anticipate more capable and powerful systems yet to come? In this paper we analyse these questions, introducing a distinction between two types of paradigms in AI: artefacts and techniques. We then use experimental data of research and media documents from AI Topics, an ofﬁcial publication of the AAAI, to examine how safety research is distributed across artefacts and techniques. We observe that AI safety research is not sufﬁciently anticipatory, and is heavily weighted towards certain research paradigms. We identify a need for AI safety to be more explicit about the artefacts and techniques for which a particular issue may be applicable, in order to identify gaps and cover a broader range of issues.",2020,2022-01-30 04:50:24,2022-01-30 04:50:24,,8,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000002[s1],,/Users/jacquesthibodeau/Zotero/storage/8VXPTJN3/Hernandez-Orallo et al. - 2020 - AI Paradigms and AI Safety Mapping Artefacts and .pdf,,TechSafety; CFI; CSER,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
F3RKFH4C,conferencePaper,2020,"Avin, Shahar; Gruetzemacher, Ross; Fox, James",Exploring AI Futures Through Role Play,"Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",978-1-4503-7110-0,,10.1145/3375627.3375817,https://dl.acm.org/doi/10.1145/3375627.3375817,,2020-02-07,2022-01-30 04:50:07,2022-01-30 04:50:07,2020-12-12 02:16:36,8-14,,,,,,,,,,,ACM,New York NY USA,en,,,,,DOI.org (Crossref),,ZSCC: 0000007,,/Users/jacquesthibodeau/Zotero/storage/VVU88IR4/Avin et al. - 2020 - Exploring AI Futures Through Role Play.pdf,,MetaSafety; CSER; BERI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"AIES '20: AAAI/ACM Conference on AI, Ethics, and Society",,,,,,,,,,,,,,,,
5IRP3444,conferencePaper,2019,"Krasheninnikov, Dmitrii; Shah, Rohin; van Hoof, Herke",Combining reward information from multiple sources,,,,,,"Given two sources of evidence about a latent variable, one can combine the information from both by multiplying the likelihoods of each piece of evidence. However, when one or both of the observation models are misspeciﬁed, the distributions will conﬂict. We study this problem in the setting with two conﬂicting reward functions learned from different sources. In such a setting, we would like to retreat to a broader distribution over reward functions, in order to mitigate the effects of misspeciﬁcation. We assume that an agent will maximize expected reward given this distribution over reward functions, and identify four desiderata for this setting. We propose a novel algorithm, Multitask Inverse Reward Design (MIRD), and compare it to a range of simple baselines. While all methods must trade off between conservatism and informativeness, through a combination of theory and empirical results on a toy environment, we ﬁnd that MIRD and its variant MIRD-IF strike a good balance between the two.",2019,2022-01-30 04:50:07,2022-01-30 04:50:07,,14,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000000[s0],,/Users/jacquesthibodeau/Zotero/storage/J9RMVNK6/Krasheninnikov et al. - Combining reward information from multiple sources.pdf,,CHAI; TechSafety; BERI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2019 workshop on Learning with Rich Experience: Integration of Learning Paradigms,,,,,,,,,,,,,,,,
86KX5STN,conferencePaper,2020,"Turner, Alexander Matt; Hadfield-Menell, Dylan; Tadepalli, Prasad",Conservative Agency,arXiv:1902.09725 [cs],,,,http://arxiv.org/abs/1902.09725,"Reward functions are easy to misspecify; although designers can make corrections after observing mistakes, an agent pursuing a misspecified reward function can irreversibly change the state of its environment. If that change precludes optimization of the correctly specified reward function, then correction is futile. For example, a robotic factory assistant could break expensive equipment due to a reward misspecification; even if the designers immediately correct the reward function, the damage is done. To mitigate this risk, we introduce an approach that balances optimization of the primary reward function with preservation of the ability to optimize auxiliary reward functions. Surprisingly, even when the auxiliary reward functions are randomly generated and therefore uninformative about the correctly specified reward function, this approach induces conservative, effective behavior.",2020,2022-01-30 04:50:07,2022-01-30 04:50:07,2019-12-16 22:27:35,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000030  arXiv: 1902.09725,,/Users/jacquesthibodeau/Zotero/storage/X9X2DMMP/Turner et al. - 2019 - Conservative Agency.pdf; /Users/jacquesthibodeau/Zotero/storage/SIFRRDFT/1902.html,,CHAI; TechSafety; BERI,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"AI, Ethics, and Society 2020",,,,,,,,,,,,,,,,
J25XFHEV,conferencePaper,2019,"Gleave, Adam; Dennis, Michael; Kant, Neel; Wild, Cody; Levine, Sergey; Russell, Stuart",Adversarial Policies: Attacking Deep Reinforcement Learning,,,,,http://arxiv.org/abs/1905.10615,"Deep reinforcement learning (RL) policies are known to be vulnerable to adversarial perturbations to their observations, similar to adversarial examples for classiﬁers. However, an attacker is not usually able to directly modify another agent’s observations. This might lead one to wonder: is it possible to attack an RL agent simply by choosing an adversarial policy acting in a multi-agent environment so as to create natural observations that are adversarial? We demonstrate the existence of adversarial policies in zero-sum games between simulated humanoid robots with proprioceptive observations, against state-of-the-art victims trained via self-play to be robust to opponents. The adversarial policies reliably win against the victims but generate seemingly random and uncoordinated behavior. We ﬁnd that these policies are more successful in high-dimensional environments, and induce substantially different activations in the victim policy network than when the victim plays against a normal opponent. Videos are available at https://adversarialpolicies.github.io/.",2019-05-25,2022-01-30 04:50:06,2022-01-30 04:50:06,2019-07-11 18:47:45,,,,,,,Adversarial Policies,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000139  arXiv: 1905.10615,,,,CHAI; TechSafety; BERI,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; I.2.6; Computer Science - Cryptography and Security,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICLR 2020,,,,,,,,,,,,,,,,
32BRB6AJ,conferencePaper,2020,"Bell, James; Linsefors, Linda; Oesterheld, Caspar; Skalse, Joar",Reinforcement Learning in Newcomblike Environments,Advances in Neural Information Processing Systems 34 pre-proceedings (NeurIPS 2021),,,,https://proceedings.neurips.cc/paper/2021/file/b9ed18a301c9f3d183938c451fa183df-Paper.pdf,"Newcomblike decision problems have been studied extensively in the decision theory literature, but they have so far been largely absent in the reinforcement learning literature. In this paper we study value-based reinforcement learning algorithms in the Newcomblike setting, and answer some of the fundamental theoretical questions about the behaviour of such algorithms in these environments. We show that a value-based reinforcement learning agent cannot converge to a policy that is not ratifiable, i.e., does not only choose actions that are optimal given that policy. This gives us a powerful tool for reasoning about the limit behaviour of agents – for example, it lets us show that there are Newcomblike environments in which a reinforcement learning agent cannot converge to any optimal policy. We show that a ratifiable policy always exists in our setting, but that there are cases in which a reinforcement learning agent normally cannot converge to it (and hence cannot converge at all). We also prove several results about the possible limit behaviours of agents in cases where they do not converge to any policy.",2020-12,2022-01-30 04:49:31,2022-01-30 04:49:31,2021-12-11 13:54:06,,,,,,,,,,,,,,,,,,,,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/CXGA8ZF6/b9ed18a301c9f3d183938c451fa183df-Paper.pdf,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2021,,,,,,,,,,,,,,,,
AZXZPENH,conferencePaper,2019,"Carey, Ryan",How Useful Is Quantilization For Mitigating Specification-Gaming?,,,,,,"For some tasks, there exists a goal that perfectly describes what the designer wants the AI system to achieve. For many tasks, however, the best available proxy objective is only a rough approximation of the designer’s intentions. When given such a goal, a system that optimizes the proxy objective tends to select degenerate solutions where the proxy reward is very different from the designer’s true reward function. One way to counteract the tendency toward speciﬁcation-gaming is quantilization, a method that interpolates between imitating demonstrations, and optimizing the proxy objective. If the demonstrations are of adequate quality, and the proxy reward overestimates performance, then quantilization has better guaranteed performance than other strategies. However, if the proxy reward underestimates performance, then either imitation or optimization will offer the best guarantee. This work introduces three new gym environments: Mountain Car-RR, Hopper-RR, and Video Pinball-RR, and shows that quantilization outperforms baselines on these tasks.",2019,2022-01-30 04:53:18,2022-01-30 04:53:18,,11,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/BXWIFTJ3/Carey - 2019 - HOW USEFUL IS QUANTILIZATION FOR MITIGATING SPECIF.pdf; /Users/jacquesthibodeau/Zotero/storage/6GSUMWMX/Carey - 2019 - HOW USEFUL IS QUANTILIZATION FOR MITIGATING SPECIF.pdf,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICLR 2019,,,,,,,,,,,,,,,,
TCGHD6UP,conferencePaper,2019,"Kenton, Zachary; Filos, Angelos; Evans, Owain; Gal, Yarin",Generalizing from a few environments in safety-critical reinforcement learning,,,,,http://arxiv.org/abs/1907.01475,"Before deploying autonomous agents in the real world, we need to be confident they will perform safely in novel situations. Ideally, we would expose agents to a very wide range of situations during training, allowing them to learn about every possible danger, but this is often impractical. This paper investigates safety and generalization from a limited number of training environments in deep reinforcement learning (RL). We find RL algorithms can fail dangerously on unseen test environments even when performing perfectly on training environments. Firstly, in a gridworld setting, we show that catastrophes can be significantly reduced with simple modifications, including ensemble model averaging and the use of a blocking classifier. In the more challenging CoinRun environment we find similar methods do not significantly reduce catastrophes. However, we do find that the uncertainty information from the ensemble is useful for predicting whether a catastrophe will occur within a few steps and hence whether human intervention should be requested.",2019-07-02,2022-01-30 04:53:10,2022-01-30 04:53:10,2019-12-16 02:16:41,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000010  arXiv: 1907.01475,,/Users/jacquesthibodeau/Zotero/storage/ANE5XDV5/Kenton et al. - 2019 - Generalizing from a few environments in safety-cri.pdf; /Users/jacquesthibodeau/Zotero/storage/JHSJ7739/Kenton et al. - 2019 - Generalizing from a few environments in safety-cri.pdf; /Users/jacquesthibodeau/Zotero/storage/6FIEE6TX/1907.html; /Users/jacquesthibodeau/Zotero/storage/XH3QNZBE/1907.html,,TechSafety; FHI,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SafeML ICLR 2019 Workshop,,,,,,,,,,,,,,,,
I8H3DUG4,conferencePaper,2015,"Soares, Nate; Fallenstein, Benja; Armstrong, Stuart; Yudkowsky, Eliezer",Corrigibility,Workshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence,,,,https://aaai.org/ocs/index.php/WS/AAAIW15/paper/view/10124/10136,,2015,2022-01-30 04:53:09,2022-01-30 04:53:09,,,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000092,,/Users/jacquesthibodeau/Zotero/storage/5HJ4SE3I/Corrigibility.pdf; /Users/jacquesthibodeau/Zotero/storage/RKMX5VC3/10124.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9DI34W7N,conferencePaper,2021,"Hammond, Lewis; Fox, James; Everitt, Tom; Abate, Alessandro; Wooldridge, Michael",Equilibrium Refinements for Multi-Agent Influence Diagrams: Theory and Practice,arXiv:2102.05008 [cs],,,,http://arxiv.org/abs/2102.05008,"Multi-agent influence diagrams (MAIDs) are a popular form of graphical model that, for certain classes of games, have been shown to offer key complexity and explainability advantages over traditional extensive form game (EFG) representations. In this paper, we extend previous work on MAIDs by introducing the concept of a MAID subgame, as well as subgame perfect and trembling hand perfect equilibrium refinements. We then prove several equivalence results between MAIDs and EFGs. Finally, we describe an open source implementation for reasoning about MAIDs and computing their equilibria.",2021-02-09,2022-01-30 04:53:09,2022-01-30 04:53:09,2021-10-31 19:02:39,,,,,,,Equilibrium Refinements for Multi-Agent Influence Diagrams,,,,,,,,,,,,arXiv.org,,ZSCC: 0000001  arXiv: 2102.05008,,/Users/jacquesthibodeau/Zotero/storage/MNA45BR6/Hammond et al. - 2021 - Equilibrium Refinements for Multi-Agent Influence .pdf; /Users/jacquesthibodeau/Zotero/storage/RFFR9PAB/2102.html; /Users/jacquesthibodeau/Zotero/storage/QNX3GVKX/2102.html,,TechSafety,Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20th International Conference on Autonomous Agents and Multiagent Systems (AAMAS-21),,,,,,,,,,,,,,,,
MCXZVITA,conferencePaper,2021,"Everitt, Tom; Carey, Ryan; Langlois, Eric; Ortega, Pedro A.; Legg, Shane",Agent Incentives: A Causal Perspective,Proceedings of the AAAI 2021 Conference,,,,http://arxiv.org/abs/2102.01685,"We present a framework for analysing agent incentives using causal influence diagrams. We establish that a well-known criterion for value of information is complete. We propose a new graphical criterion for value of control, establishing its soundness and completeness. We also introduce two new concepts for incentive analysis: response incentives indicate which changes in the environment affect an optimal decision, while instrumental control incentives establish whether an agent can influence its utility via a variable X. For both new concepts, we provide sound and complete graphical criteria. We show by example how these results can help with evaluating the safety and fairness of an AI system.",2021-03-15,2022-01-30 04:53:08,2022-01-30 04:53:08,2021-10-31 19:14:12,,,,,,,Agent Incentives,,,,,,,,,,,,arXiv.org,,ZSCC: 0000007  arXiv: 2102.01685,,/Users/jacquesthibodeau/Zotero/storage/7B2HXKEK/Everitt et al. - 2021 - Agent Incentives A Causal Perspective.pdf; /Users/jacquesthibodeau/Zotero/storage/2SVWH4CX/2102.html; /Users/jacquesthibodeau/Zotero/storage/5A6T2VB9/2102.html,,TechSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AAAI 2021,,,,,,,,,,,,,,,,
PJXNWQDW,conferencePaper,2020,"Cohen, Michael K.; Vellambi, Badri; Hutter, Marcus",Asymptotically Unambitious Artificial General Intelligence,arXiv:1905.12186 [cs],,,,http://arxiv.org/abs/1905.12186,"General intelligence, the ability to solve arbitrary solvable problems, is supposed by many to be artificially constructible. Narrow intelligence, the ability to solve a given particularly difficult problem, has seen impressive recent development. Notable examples include self-driving cars, Go engines, image classifiers, and translators. Artificial General Intelligence (AGI) presents dangers that narrow intelligence does not: if something smarter than us across every domain were indifferent to our concerns, it would be an existential threat to humanity, just as we threaten many species despite no ill will. Even the theory of how to maintain the alignment of an AGI's goals with our own has proven highly elusive. We present the first algorithm we are aware of for asymptotically unambitious AGI, where ""unambitiousness"" includes not seeking arbitrary power. Thus, we identify an exception to the Instrumental Convergence Thesis, which is roughly that by default, an AGI would seek power, including over us.",2020-07-21,2022-01-30 04:53:08,2022-01-30 04:53:08,2020-12-12 15:23:37,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000004  arXiv: 1905.12186,,/Users/jacquesthibodeau/Zotero/storage/8DBEA75S/Cohen et al. - 2020 - Asymptotically Unambitious Artificial General Inte.pdf; /Users/jacquesthibodeau/Zotero/storage/JC4S2NBR/Cohen et al. - 2020 - Asymptotically Unambitious Artificial General Inte.pdf; /Users/jacquesthibodeau/Zotero/storage/KH9FQ5MZ/Cohen et al. - 2020 - Asymptotically Unambitious Artificial General Inte.pdf; /Users/jacquesthibodeau/Zotero/storage/K6N5WBE9/Cohen et al. - 2020 - Asymptotically Unambitious Artificial General Inte.pdf; /Users/jacquesthibodeau/Zotero/storage/U8FQ7GXJ/1905.html; /Users/jacquesthibodeau/Zotero/storage/JUUQB6N7/1905.html; /Users/jacquesthibodeau/Zotero/storage/NGFACCVM/1905.html; /Users/jacquesthibodeau/Zotero/storage/BNMPXS8T/1905.html,,TechSafety; FHI; DeepMind,"Computer Science - Artificial Intelligence; I.2.0; I.2.6; I.2.0, I.2.6",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AAAI 2020,,,,,,,,,,,,,,,,
KVD64BBZ,conferencePaper,2017,"Abel, David; Salvatier, John; Stuhlmüller, Andreas; Evans, Owain",Agent-agnostic human-in-the-loop reinforcement learning,30th Conference on Neural Information Processing Systems (NIPS 2016),,,,,,2017,2022-01-30 04:53:08,2022-01-30 04:53:08,,,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000049,,/Users/jacquesthibodeau/Zotero/storage/K5CD6IT3/Abel et al. - 2017 - Agent-Agnostic Human-in-the-Loop Reinforcement Lea.pdf; /Users/jacquesthibodeau/Zotero/storage/R97ACVCM/1701.html; /Users/jacquesthibodeau/Zotero/storage/3AVWXX73/Abel et al. - 2017 - Agent-agnostic human-in-the-loop reinforcement lea.pdf; /Users/jacquesthibodeau/Zotero/storage/82F2UJJ4/1701.html,,TechSafety; FHI; Ought,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30th Conference on Neural Information Processing Systems (NIPS 2016),,,,,,,,,,,,,,,,
SAFIJZEA,conferencePaper,2016,"Krueger, David; Leike, Jan; Evans, Owain; Salvatier, John",Active reinforcement learning: Observing rewards at a cost,"Future of Interactive Learning Machines, NIPS Workshop",,,,,,2016,2022-01-30 04:53:08,2022-01-30 04:53:08,,,,,,,,Active reinforcement learning,,,,,,,,,,,,Google Scholar,,ZSCC: 0000010[s0],,/Users/jacquesthibodeau/Zotero/storage/KBMS9PIC/Krueger et al. - 2016 - Active reinforcement learning Observing rewards a.pdf,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXZRZTKQ,conferencePaper,2019,"Cohen, Michael K.; Catt, Elliot; Hutter, Marcus",A Strongly Asymptotically Optimal Agent in General Environments,Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence,,,,http://arxiv.org/abs/1903.01021,"Reinforcement Learning agents are expected to eventually perform well. Typically, this takes the form of a guarantee about the asymptotic behavior of an algorithm given some assumptions about the environment. We present an algorithm for a policy whose value approaches the optimal value with probability 1 in all computable probabilistic environments, provided the agent has a bounded horizon. This is known as strong asymptotic optimality, and it was previously unknown whether it was possible for a policy to be strongly asymptotically optimal in the class of all computable probabilistic environments. Our agent, Inquisitive Reinforcement Learner (Inq), is more likely to explore the more it expects an exploratory action to reduce its uncertainty about which environment it is in, hence the term inquisitive. Exploring inquisitively is a strategy that can be applied generally; for more manageable environment classes, inquisitiveness is tractable. We conducted experiments in ""grid-worlds"" to compare the Inquisitive Reinforcement Learner to other weakly asymptotically optimal agents.",2019-05-27,2022-01-30 04:53:07,2022-01-30 04:53:07,2020-08-18 21:41:08,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000006  arXiv: 1903.01021,,/Users/jacquesthibodeau/Zotero/storage/GW7QTF57/Cohen et al. - 2019 - A Strongly Asymptotically Optimal Agent in General.pdf; /Users/jacquesthibodeau/Zotero/storage/4RAI8NSE/Cohen et al. - 2019 - A Strongly Asymptotically Optimal Agent in General.pdf; /Users/jacquesthibodeau/Zotero/storage/7MREFNQ5/Cohen et al. - 2019 - A Strongly Asymptotically Optimal Agent in General.pdf; /Users/jacquesthibodeau/Zotero/storage/FKBA28KM/1903.html; /Users/jacquesthibodeau/Zotero/storage/BV4V8ZA4/1903.html; /Users/jacquesthibodeau/Zotero/storage/R852BRN9/1903.html,,TechSafety; FHI,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; I.2.6; I.2.8,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IJCAI 2019,,,,,,,,,,,,,,,,
BD9DG38U,conferencePaper,2019,"Qin, Chongli; O’Donoghue, Brendan; Stanforth, Robert; Gowal, Sven; Uesato, Jonathan; Swirszcz, Grzegorz; Kohli, Pushmeet",Verification Of Non-Linear Specifications For Neural Networks,,,,,,"Prior work on neural network veriﬁcation has focused on speciﬁcations that are linear functions of the output of the network, e.g., invariance of the classiﬁer output under adversarial perturbations of the input. In this paper, we extend veriﬁcation algorithms to be able to certify richer properties of neural networks. To do this we introduce the class of convex-relaxable speciﬁcations, which constitute nonlinear speciﬁcations that can be veriﬁed using a convex relaxation. We show that a number of important properties of interest can be modeled within this class, including conservation of energy in a learned dynamics model of a physical system; semantic consistency of a classiﬁer’s output labels under adversarial perturbations and bounding errors in a system that predicts the summation of handwritten digits. Our experimental evaluation shows that our method is able to effectively verify these speciﬁcations. Moreover, our evaluation exposes the failure modes in models which cannot be veriﬁed to satisfy these speciﬁcations. Thus, emphasizing the importance of training models not just to ﬁt training data but also to be consistent with speciﬁcations.",2019,2022-01-30 04:52:49,2022-01-30 04:52:49,,21,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000011[s2],,/Users/jacquesthibodeau/Zotero/storage/GERBNPZ4/Qin et al. - 2019 - VERIFICATION OF NON-LINEAR SPECIFICATIONS FOR NEUR.pdf,,TechSafety; DeepMind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICLR 2019,,,,,,,,,,,,,,,,
AVNF5F4K,conferencePaper,2018,"Dvijotham, Krishnamurthy; Garnelo, Marta; Fawzi, Alhussein; Kohli, Pushmeet",Verification of deep probabilistic models,"arXiv:1812.02795 [cs, stat]",,,,http://arxiv.org/abs/1812.02795,"Probabilistic models are a critical part of the modern deep learning toolbox - ranging from generative models (VAEs, GANs), sequence to sequence models used in machine translation and speech processing to models over functional spaces (conditional neural processes, neural processes). Given the size and complexity of these models, safely deploying them in applications requires the development of tools to analyze their behavior rigorously and provide some guarantees that these models are consistent with a list of desirable properties or specifications. For example, a machine translation model should produce semantically equivalent outputs for innocuous changes in the input to the model. A functional regression model that is learning a distribution over monotonic functions should predict a larger value at a larger input. Verification of these properties requires a new framework that goes beyond notions of verification studied in deterministic feedforward networks, since requiring worst-case guarantees in probabilistic models is likely to produce conservative or vacuous results. We propose a novel formulation of verification for deep probabilistic models that take in conditioning inputs and sample latent variables in the course of producing an output: We require that the output of the model satisfies a linear constraint with high probability over the sampling of latent variables and for every choice of conditioning input to the model. We show that rigorous lower bounds on the probability that the constraint is satisfied can be obtained efficiently. Experiments with neural processes show that several properties of interest while modeling functional spaces can be modeled within this framework (monotonicity, convexity) and verified efficiently using our algorithms",2018-12-06,2022-01-30 04:52:49,2022-01-30 04:52:49,2019-12-16 20:33:13,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000020  arXiv: 1812.02795,,/Users/jacquesthibodeau/Zotero/storage/8MI349F4/Dvijotham et al. - 2018 - Verification of deep probabilistic models.pdf; /Users/jacquesthibodeau/Zotero/storage/EXVPIWAF/1812.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2018 Workshop on Security in Machine Learning,,,,,,,,,,,,,,,,
AJQW7GX7,conferencePaper,2020,"Zoran, Daniel; Chrzanowski, Mike; Huang, Po-Sen; Gowal, Sven; Mott, Alex; Kohl, Pushmeet",Towards Robust Image Classification Using Sequential Attention Models,Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,https://openaccess.thecvf.com/content_CVPR_2020/html/Zoran_Towards_Robust_Image_Classification_Using_Sequential_Attention_Models_CVPR_2020_paper.html,"In this paper we propose to augment a modern neural-network architecture with an attention model inspired by human perception. Specifically, we adversarially train and analyze a neural model incorporating a human inspired, visual attention component that is guided by a recurrent top-down sequential process. Our experimental evaluation uncovers several notable findings about the robustness and behavior of this new model. First, introducing attention to the model significantly improves adversarial robustness resulting in state-of-the-art ImageNet accuracies under a wide range of random targeted attack strengths. Second, we show that by varying the number of attention steps (glances/fixations) for which the model is unrolled, we are able to make its defense capabilities stronger, even in light of stronger attacks --- resulting in a ""computational race"" between the attacker and the defender. Finally, we show that some of the adversarial examples generated by attacking our model are quite different from conventional adversarial examples --- they contain global, salient and spatially coherent structures coming from the target class that would be recognizable even to a human, and work by distracting the attention of the model away from the main object in the original image.",2020,2022-01-30 04:52:48,2022-01-30 04:52:48,2020-12-20,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000000[s2],,/Users/jacquesthibodeau/Zotero/storage/HKXE6UDC/Zoran et al. - 2019 - Towards Robust Image Classification Using Sequenti.pdf; /Users/jacquesthibodeau/Zotero/storage/84REF8IF/1912.html,,TechSafety; DeepMind,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,,,,,,,,,,,,,
T6XFBJ2P,conferencePaper,2016,"Orseau, Laurent; Armstrong, Stuart",Safely Interruptible Agents,,,,,,"Reinforcement learning agents interacting with a complex environment like the real world are unlikely to behave optimally all the time. If such an agent is operating in real-time under human supervision, now and then it may be necessary for a human operator to press the big red button to prevent the agent from continuing a harmful sequence of actions—harmful either for the agent or for the environment—and lead the agent into a safer situation. However, if the learning agent expects to receive rewards from this sequence, it may learn in the long run to avoid such interruptions, for example by disabling the red button—which is an undesirable outcome. This paper explores a way to make sure a learning agent will not learn to prevent (or seek!) being interrupted by the environment or a human operator. We provide a formal deﬁnition of safe interruptibility and exploit the off-policy learning property to prove that either some agents are already safely interruptible, like Q-learning, or can easily be made so, like Sarsa. We show that even ideal, uncomputable reinforcement learning agents for (deterministic) general computable environments can be made safely interruptible.",2016,2022-01-30 04:52:48,2022-01-30 04:52:48,,10,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000098,,/Users/jacquesthibodeau/Zotero/storage/WP7N4XD6/Orseau and Armstrong - Safely Interruptible Agents.pdf,,TechSafety; FHI; DeepMind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Conference on Uncertainty in Artificial Intelligence,,,,,,,,,,,,,,,,
IAH92MIP,conferencePaper,2018,"Moosavi-Dezfooli, Seyed-Mohsen; Fawzi, Alhussein; Uesato, Jonathan; Frossard, Pascal","Robustness via curvature regularization, and vice versa",2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,http://arxiv.org/abs/1811.09716,"State-of-the-art classifiers have been shown to be largely vulnerable to adversarial perturbations. One of the most effective strategies to improve robustness is adversarial training. In this paper, we investigate the effect of adversarial training on the geometry of the classification landscape and decision boundaries. We show in particular that adversarial training leads to a significant decrease in the curvature of the loss surface with respect to inputs, leading to a drastically more ""linear"" behaviour of the network. Using a locally quadratic approximation, we provide theoretical evidence on the existence of a strong relation between large robustness and small curvature. To further show the importance of reduced curvature for improving the robustness, we propose a new regularizer that directly minimizes curvature of the loss surface, and leads to adversarial robustness that is on par with adversarial training. Besides being a more efficient and principled alternative to adversarial training, the proposed regularizer confirms our claims on the importance of exhibiting quasi-linear behavior in the vicinity of data points in order to achieve robustness.",2018-11-23,2022-01-30 04:52:48,2022-01-30 04:52:48,2019-12-16 20:33:30,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s6]  ACC: 161  J: 46 arXiv: 1811.09716,,"/Users/jacquesthibodeau/Zotero/storage/JIRVCGMN/Moosavi-Dezfooli et al. - 2018 - Robustness via curvature regularization, and vice .pdf; /Users/jacquesthibodeau/Zotero/storage/SV67CTM2/1811.html",,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,,,,,,,,,,,,,
BRC4S5Q4,conferencePaper,2019,"Weng, Tsui-Wei; Dvijotham*, Krishnamurthy (Dj); Uesato*, Jonathan; Xiao*, Kai; Gowal*, Sven; Stanforth*, Robert; Kohli, Pushmeet",Toward Evaluating Robustness of Deep Reinforcement Learning with Continuous Control,,,,,https://openreview.net/forum?id=SylL0krYPS,We study the problem of continuous control agents in deep RL with adversarial attacks and proposed a two-step algorithm based on learned model dynamics.,2019-09-25,2022-01-30 04:52:48,2022-01-30 04:52:48,2020-12-12 15:24:12,,,,,,,,,,,,,,en,,,,,openreview.net,,ZSCC: 0000004,,/Users/jacquesthibodeau/Zotero/storage/6A22G5ZI/Weng et al. - 2019 - Toward Evaluating Robustness of Deep Reinforcement.pdf; /Users/jacquesthibodeau/Zotero/storage/J34FFI9Q/forum.html,,TechSafety; DeepMind; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,International Conference on Learning Representations,,,,,,,,,,,,,,,,
HXZ4FT7A,conferencePaper,2017,"Lakshminarayanan, Balaji; Pritzel, Alexander; Blundell, Charles",Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles,"arXiv:1612.01474 [cs, stat]",,,,http://arxiv.org/abs/1612.01474,"Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.",2017-11-03,2022-01-30 04:52:48,2022-01-30 04:52:48,2019-12-16 20:35:59,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000822[s0]  arXiv: 1612.01474,,/Users/jacquesthibodeau/Zotero/storage/EU34QGTH/Lakshminarayanan et al. - 2017 - Simple and Scalable Predictive Uncertainty Estimat.pdf; /Users/jacquesthibodeau/Zotero/storage/3E2CUIW2/1612.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NIPS 2017,,,,,,,,,,,,,,,,
XNKWIHWT,conferencePaper,2017,"Everitt, Tom; Krakovna, Victoria; Orseau, Laurent; Hutter, Marcus; Legg, Shane",Reinforcement Learning with a Corrupted Reward Channel,"arXiv:1705.08417 [cs, stat]",,,,http://arxiv.org/abs/1705.08417,"No real-world reward function is perfect. Sensory errors and software bugs may result in RL agents observing higher (or lower) rewards than they should. For example, a reinforcement learning agent may prefer states where a sensory error gives it the maximum reward, but where the true reward is actually small. We formalise this problem as a generalised Markov Decision Problem called Corrupt Reward MDP. Traditional RL methods fare poorly in CRMDPs, even under strong simplifying assumptions and when trying to compensate for the possibly corrupt rewards. Two ways around the problem are investigated. First, by giving the agent richer data, such as in inverse reinforcement learning and semi-supervised reinforcement learning, reward corruption stemming from systematic sensory errors may sometimes be completely managed. Second, by using randomisation to blunt the agent's optimisation, reward corruption can be partially managed under some assumptions.",2017-08-19,2022-01-30 04:52:47,2022-01-30 04:52:47,2019-12-16 20:35:54,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000066  arXiv: 1705.08417,,/Users/jacquesthibodeau/Zotero/storage/RXAX378F/Everitt et al. - 2017 - Reinforcement Learning with a Corrupted Reward Cha.pdf; /Users/jacquesthibodeau/Zotero/storage/8DU9QAQA/Everitt et al. - 2017 - Reinforcement Learning with a Corrupted Reward Cha.pdf; /Users/jacquesthibodeau/Zotero/storage/74KMWAKW/1705.html; /Users/jacquesthibodeau/Zotero/storage/R4CHAQWM/1705.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; I.2.6; I.2.8,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IJCAI 2017 AI and Autonomy track,,,,,,,,,,,,,,,,
GSPHBNCV,conferencePaper,2019,"Krakovna, Victoria; Orseau, Laurent; Kumar, Ramana; Martic, Miljan; Legg, Shane",Penalizing side effects using stepwise relative reachability,Proceedings of the Workshop on Artificial Intelligence Safety 2019,,,,http://arxiv.org/abs/1806.01186,"How can we design safe reinforcement learning agents that avoid unnecessary disruptions to their environment? We show that current approaches to penalizing side effects can introduce bad incentives, e.g. to prevent any irreversible changes in the environment, including the actions of other agents. To isolate the source of such undesirable incentives, we break down side effects penalties into two components: a baseline state and a measure of deviation from this baseline state. We argue that some of these incentives arise from the choice of baseline, and others arise from the choice of deviation measure. We introduce a new variant of the stepwise inaction baseline and a new deviation measure based on relative reachability of states. The combination of these design choices avoids the given undesirable incentives, while simpler baselines and the unreachability measure fail. We demonstrate this empirically by comparing different combinations of baseline and deviation measure choices on a set of gridworld experiments designed to illustrate possible bad incentives.",2019-03-08,2022-01-30 04:52:47,2022-01-30 04:52:47,2019-12-16 20:35:10,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s7]  ACC: 27  J: 9 arXiv: 1806.01186,,/Users/jacquesthibodeau/Zotero/storage/TPXKJ9KJ/Krakovna et al. - 2019 - Penalizing side effects using stepwise relative re.pdf; /Users/jacquesthibodeau/Zotero/storage/VKHV6EAF/Krakovna et al. - 2019 - Penalizing side effects using stepwise relative re.pdf; /Users/jacquesthibodeau/Zotero/storage/VWWRAC2Q/1806.html; /Users/jacquesthibodeau/Zotero/storage/K4V55HQ6/1806.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Workshop on Artificial Intelligence Safety 2019,,,,,,,,,,,,,,,,
TNE49C8Q,conferencePaper,2018,"Ibarz, Borja; Leike, Jan; Pohlen, Tobias; Irving, Geoffrey; Legg, Shane; Amodei, Dario",Reward learning from human preferences and demonstrations in Atari,"arXiv:1811.06521 [cs, stat]",,,,http://arxiv.org/abs/1811.06521,"To solve complex real-world problems with reinforcement learning, we cannot rely on manually specified reward functions. Instead, we can have humans communicate an objective to the agent directly. In this work, we combine two approaches to learning from human feedback: expert demonstrations and trajectory preferences. We train a deep neural network to model the reward function and use its predicted reward to train an DQN-based deep reinforcement learning agent on 9 Atari games. Our approach beats the imitation learning baseline in 7 games and achieves strictly superhuman performance on 2 games without using game rewards. Additionally, we investigate the goodness of fit of the reward model, present some reward hacking problems, and study the effects of noise in the human labels.",2018-11-15,2022-01-30 04:52:47,2022-01-30 04:52:47,2019-12-16 20:36:27,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000114  arXiv: 1811.06521,,/Users/jacquesthibodeau/Zotero/storage/27WAEWKV/Ibarz et al. - 2018 - Reward learning from human preferences and demonst.pdf; /Users/jacquesthibodeau/Zotero/storage/QEKJ65EV/Ibarz et al. - 2018 - Reward learning from human preferences and demonst.pdf; /Users/jacquesthibodeau/Zotero/storage/J8N3GNK5/1811.html; /Users/jacquesthibodeau/Zotero/storage/3ZUXGVRD/1811.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NIPS 2018,,,,,,,,,,,,,,,,
5AW256G4,conferencePaper,2020,"Armstrong, Stuart; Leike, Jan; Orseau, Laurent; Legg, Shane",Pitfalls of learning a reward function online,arXiv:2004.13654 [cs],,,,http://arxiv.org/abs/2004.13654,"In some agent designs like inverse reinforcement learning an agent needs to learn its own reward function. Learning the reward function and optimising for it are typically two different processes, usually performed at different stages. We consider a continual (``one life'') learning approach where the agent both learns the reward function and optimises for it at the same time. We show that this comes with a number of pitfalls, such as deliberately manipulating the learning process in one direction, refusing to learn, ``learning'' facts already known to the agent, and making decisions that are strictly dominated (for all relevant reward functions). We formally introduce two desirable properties: the first is `unriggability', which prevents the agent from steering the learning process in the direction of a reward function that is easier to optimise. The second is `uninfluenceability', whereby the reward-function learning process operates by learning facts about the environment. We show that an uninfluenceable process is automatically unriggable, and if the set of possible environments is sufficiently rich, the converse is true too.",2020-04-28,2022-01-30 04:52:47,2022-01-30 04:52:47,2020-08-18 21:24:08,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000005  arXiv: 2004.13654,,/Users/jacquesthibodeau/Zotero/storage/9SFICWU9/Armstrong et al. - 2020 - Pitfalls of learning a reward function online.pdf; /Users/jacquesthibodeau/Zotero/storage/QNDBJEGQ/2004.html,,TechSafety; FHI; DeepMind,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IJCAI 2020,,,,,,,,,,,,,,,,
8CVV7M9V,conferencePaper,2020,"Cohen, Michael K.; Hutter, Marcus",Pessimism About Unknown Unknowns Inspires Conservatism,Proceedings of Machine Learning Research,,,,http://arxiv.org/abs/2006.08753,"If we could define the set of all bad outcomes, we could hard-code an agent which avoids them; however, in sufficiently complex environments, this is infeasible. We do not know of any general-purpose approaches in the literature to avoiding novel failure modes. Motivated by this, we define an idealized Bayesian reinforcement learner which follows a policy that maximizes the worst-case expected reward over a set of world-models. We call this agent pessimistic, since it optimizes assuming the worst case. A scalar parameter tunes the agent's pessimism by changing the size of the set of world-models taken into account. Our first main contribution is: given an assumption about the agent's model class, a sufficiently pessimistic agent does not cause ""unprecedented events"" with probability $1-\delta$, whether or not designers know how to precisely specify those precedents they are concerned with. Since pessimism discourages exploration, at each timestep, the agent may defer to a mentor, who may be a human or some known-safe policy we would like to improve. Our other main contribution is that the agent's policy's value approaches at least that of the mentor, while the probability of deferring to the mentor goes to 0. In high-stakes environments, we might like advanced artificial agents to pursue goals cautiously, which is a non-trivial problem even if the agent were allowed arbitrary computing power; we present a formal solution.",2020-06-15,2022-01-30 04:52:47,2022-01-30 04:52:47,2021-11-19 00:01:59,,,,125,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000003  arXiv: 2006.08753,,/Users/jacquesthibodeau/Zotero/storage/MWFB3VPD/Cohen and Hutter - 2020 - Pessimism About Unknown Unknowns Inspires Conserva.pdf; /Users/jacquesthibodeau/Zotero/storage/2NGV2RMZ/2006.html; /Users/jacquesthibodeau/Zotero/storage/JEN2E66Z/2006.html,,TechSafety; DeepMind,"Computer Science - Machine Learning; Computer Science - Artificial Intelligence; I.2.0, I.2.6",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,33rd Annual Conference on Learning Theory,,,,,,,,,,,,,,,,
R8QM5T9F,conferencePaper,2019,"Gowal, Sven; Dvijotham, Krishnamurthy; Stanforth, Robert; Bunel, Rudy; Qin, Chongli; Uesato, Jonathan; Arandjelovic, Relja; Mann, Timothy; Kohli, Pushmeet",On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models,"arXiv:1810.12715 [cs, stat]",,,,http://arxiv.org/abs/1810.12715,"Recent work has shown that it is possible to train deep neural networks that are provably robust to norm-bounded adversarial perturbations. Most of these methods are based on minimizing an upper bound on the worst-case loss over all possible adversarial perturbations. While these techniques show promise, they often result in difﬁcult optimization procedures that remain hard to scale to larger networks. Through a comprehensive analysis, we show how a simple bounding technique, interval bound propagation (IBP), can be exploited to train large provably robust neural networks that beat the state-of-the-art in veriﬁed accuracy. While the upper bound computed by IBP can be quite weak for general networks, we demonstrate that an appropriate loss and clever hyper-parameter schedule allow the network to adapt such that the IBP bound is tight. This results in a fast and stable learning algorithm that outperforms more sophisticated methods and achieves state-of-the-art results on MNIST, CIFAR-10 and SVHN. It also allows us to train the largest model to be veriﬁed beyond vacuous bounds on a downscaled version of IMAGENET.",2019-08-29,2022-01-30 04:52:39,2022-01-30 04:52:39,2019-12-16 20:36:43,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: NoCitationData[s7]  ACC: 221  J: 93 arXiv: 1810.12715,,/Users/jacquesthibodeau/Zotero/storage/D227S2QI/Gowal et al. - 2019 - On the Effectiveness of Interval Bound Propagation.pdf,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Cryptography and Security,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS SECML 2018 Workshop,,,,,,,,,,,,,,,,
UKXRJHU4,conferencePaper,2018,"Farajtabar, Mehrdad; Chow, Yinlam; Ghavamzadeh, Mohammad",More Robust Doubly Robust Off-policy Evaluation,Proceedings of the 35th International Conference on Machine Learning,,,,http://arxiv.org/abs/1802.03493,"We study the problem of off-policy evaluation (OPE) in reinforcement learning (RL), where the goal is to estimate the performance of a policy from the data generated by another policy(ies). In particular, we focus on the doubly robust (DR) estimators that consist of an importance sampling (IS) component and a performance model, and utilize the low (or zero) bias of IS and low variance of the model at the same time. Although the accuracy of the model has a huge impact on the overall performance of DR, most of the work on using the DR estimators in OPE has been focused on improving the IS part, and not much on how to learn the model. In this paper, we propose alternative DR estimators, called more robust doubly robust (MRDR), that learn the model parameter by minimizing the variance of the DR estimator. We first present a formulation for learning the DR model in RL. We then derive formulas for the variance of the DR estimator in both contextual bandits and RL, such that their gradients w.r.t.~the model parameters can be estimated from the samples, and propose methods to efficiently minimize the variance. We prove that the MRDR estimators are strongly consistent and asymptotically optimal. Finally, we evaluate MRDR in bandits and RL benchmark problems, and compare its performance with the existing methods.",2018-05-23,2022-01-30 04:52:39,2022-01-30 04:52:39,2019-12-16 20:35:28,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000144  arXiv: 1802.03493,,/Users/jacquesthibodeau/Zotero/storage/ISNRPPXX/Farajtabar et al. - 2018 - More Robust Doubly Robust Off-policy Evaluation.pdf; /Users/jacquesthibodeau/Zotero/storage/SA8MNFUK/1802.html,,TechSafety; DeepMind,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,35th International Conference on Machine Learning,,,,,,,,,,,,,,,,
S6AXJC5M,conferencePaper,2018,"Rabinowitz, Neil C.; Perbet, Frank; Song, H. Francis; Zhang, Chiyuan; Eslami, S. M. Ali; Botvinick, Matthew",Machine Theory of Mind,Proceedings of the 35th International Conference on Machine Learning,,,,http://arxiv.org/abs/1802.07740,"Theory of mind (ToM; Premack & Woodruff, 1978) broadly refers to humans' ability to represent the mental states of others, including their desires, beliefs, and intentions. We propose to train a machine to build such models too. We design a Theory of Mind neural network -- a ToMnet -- which uses meta-learning to build models of the agents it encounters, from observations of their behaviour alone. Through this process, it acquires a strong prior model for agents' behaviour, as well as the ability to bootstrap to richer predictions about agents' characteristics and mental states using only a small number of behavioural observations. We apply the ToMnet to agents behaving in simple gridworld environments, showing that it learns to model random, algorithmic, and deep reinforcement learning agents from varied populations, and that it passes classic ToM tasks such as the ""Sally-Anne"" test (Wimmer & Perner, 1983; Baron-Cohen et al., 1985) of recognising that others can hold false beliefs about the world. We argue that this system -- which autonomously learns how to model other agents in its world -- is an important step forward for developing multi-agent AI systems, for building intermediating technology for machine-human interaction, and for advancing the progress on interpretable AI.",2018-03-12,2022-01-30 04:52:39,2022-01-30 04:52:39,2020-11-14 00:44:27,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000247  arXiv: 1802.07740,,/Users/jacquesthibodeau/Zotero/storage/JVMJRV27/Rabinowitz et al. - 2018 - Machine Theory of Mind.pdf; /Users/jacquesthibodeau/Zotero/storage/DXH576KA/1802.html,,TechSafety; DeepMind,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MMG5JK2G,conferencePaper,2019,"Ren, Jie; Liu, Peter J.; Fertig, Emily; Snoek, Jasper; Poplin, Ryan; DePristo, Mark A.; Dillon, Joshua V.; Lakshminarayanan, Balaji",Likelihood Ratios for Out-of-Distribution Detection,"arXiv:1906.02845 [cs, stat]",,,,http://arxiv.org/abs/1906.02845,"Discriminative neural networks offer little or no performance guarantees when deployed on data not generated by the same process as the training distribution. On such out-of-distribution (OOD) inputs, the prediction may not only be erroneous, but confidently so, limiting the safe deployment of classifiers in real-world applications. One such challenging application is bacteria identification based on genomic sequences, which holds the promise of early detection of diseases, but requires a model that can output low confidence predictions on OOD genomic sequences from new bacteria that were not present in the training data. We introduce a genomics dataset for OOD detection that allows other researchers to benchmark progress on this important problem. We investigate deep generative model based approaches for OOD detection and observe that the likelihood score is heavily affected by population level background statistics. We propose a likelihood ratio method for deep generative models which effectively corrects for these confounding background statistics. We benchmark the OOD detection performance of the proposed method against existing approaches on the genomics dataset and show that our method achieves state-of-the-art performance. We demonstrate the generality of the proposed method by showing that it significantly improves OOD detection when applied to deep generative models of images.",2019-12-05,2022-01-30 04:52:39,2022-01-30 04:52:39,2019-12-16 20:31:45,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000250  arXiv: 1906.02845,,/Users/jacquesthibodeau/Zotero/storage/Q6QQT5EF/Ren et al. - 2019 - Likelihood Ratios for Out-of-Distribution Detectio.pdf; /Users/jacquesthibodeau/Zotero/storage/F56NEI6E/1906.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2019,,,,,,,,,,,,,,,,
ZGNI9PR6,conferencePaper,2019,"Kovařík, Vojtěch; Carey, Ryan",(When) Is Truth-telling Favored in AI Debate?,Proceedings of the Workshop on Artificial Intelligence Safety,,,,http://arxiv.org/abs/1911.04266,"For some problems, humans may not be able to accurately judge the goodness of AI-proposed solutions. Irving et al. (2018) propose that in such cases, we may use a debate between two AI systems to amplify the problem-solving capabilities of a human judge. We introduce a mathematical framework that can model debates of this type and propose that the quality of debate designs should be measured by the accuracy of the most persuasive answer. We describe a simple instance of the debate framework called feature debate and analyze the degree to which such debates track the truth. We argue that despite being very simple, feature debates nonetheless capture many aspects of practical debates such as the incentives to confuse the judge or stall to prevent losing. We then outline how these models should be generalized to analyze a wider range of debate phenomena.",2019-12-15,2022-01-30 04:53:45,2022-01-30 04:53:45,2020-11-14 00:41:27,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000000  arXiv: 1911.04266,,/Users/jacquesthibodeau/Zotero/storage/K36M3K3T/Kovařík and Carey - 2019 - (When) Is Truth-telling Favored in AI Debate.pdf; /Users/jacquesthibodeau/Zotero/storage/7QS85VGU/1911.html,,TechSafety; FHI,Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NWKNXDG7,conferencePaper,2016,"Asaro, Peter M",The Liability Problem for Autonomous Artificial Agents,,,,,,"This paper describes and frames a central ethical issue–the liability problem–facing the regulation of artificial computational agents, including artificial intelligence (AI) and robotic systems, as they become increasingly autonomous, and supersede current capabilities. While it frames the issue in legal terms of liability and culpability, these terms are deeply imbued and interconnected with their ethical and moral correlate– responsibility. In order for society to benefit from advances in AI technology, it will be necessary to develop regulatory policies which manage the risk and liability of deploying systems with increasingly autonomous capabilities. However, current approaches to liability have difficulties when it comes to dealing with autonomous artificial agents because their behavior may be unpredictable to those who create and deploy them, and they will not be proper legal or moral agents. This problem is the motivation for a research project that will explore the fundamental concepts of autonomy, agency and liability; clarify the different varieties of agency that artificial systems might realize, including causal, legal and moral; and the illuminate the relationships between these. The paper will frame the problem of liability in autonomous agents, sketch out its relation to fundamental concepts in human legal and moral agency– including autonomy, agency, causation, intention, responsibility and culpability–and their applicability or inapplicability to autonomous artificial agents.",2016,2022-01-30 04:53:38,2022-01-30 04:53:38,,5,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000047,,/Users/jacquesthibodeau/Zotero/storage/HB4XG66E/Asaro - The Liability Problem for Autonomous Artificial Ag.pdf,,MetaSafety; AmbiguosSafety; FLI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AAAI Spring Symposia,,,,,,,,,,,,,,,,
JKWXTQPB,conferencePaper,2018,"Saunders, William; Sastry, Girish; Stuhlmueller, Andreas; Evans, Owain",Trial without Error: Towards Safe Reinforcement Learning via Human Intervention,Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems,,,,https://arxiv.org/abs/1707.05173v1,"AI systems are increasingly applied to complex tasks that involve interaction with humans. During training, such systems are potentially dangerous, as they haven't yet learned to avoid actions that could cause serious harm. How can an AI system explore and learn without making a single mistake that harms humans or otherwise causes serious damage? For model-free reinforcement learning, having a human ""in the loop"" and ready to intervene is currently the only way to prevent all catastrophes. We formalize human intervention for RL and show how to reduce the human labor required by training a supervised learner to imitate the human's intervention decisions. We evaluate this scheme on Atari games, with a Deep RL agent being overseen by a human for four hours. When the class of catastrophes is simple, we are able to prevent all catastrophes without affecting the agent's learning (whereas an RL baseline fails due to catastrophic forgetting). However, this scheme is less successful when catastrophes are more complex: it reduces but does not eliminate catastrophes and the supervised learner fails on adversarial examples found by the agent. Extrapolating to more challenging environments, we show that our implementation would not scale (due to the infeasible amount of human labor required). We outline extensions of the scheme that are necessary if we are to train model-free agents without a single catastrophe.",2018,2022-01-30 04:53:37,2022-01-30 04:53:37,2019-12-19 01:45:01,2067–2069,,,,,,Trial without Error,,,,,International Foundation for Autonomous Agents and Multiagent Systems,,en,,,,,arxiv.org,,ZSCC: NoCitationData[s1]  ACC: 142,,/Users/jacquesthibodeau/Zotero/storage/I2WTXHHW/Saunders et al. - 2018 - Trial without error Towards safe reinforcement le.pdf; /Users/jacquesthibodeau/Zotero/storage/NW6PQECP/citation.html; /Users/jacquesthibodeau/Zotero/storage/AV7ZKCMK/1707.html,,TechSafety; FHI; Ought,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WJI4USKA,conferencePaper,2016,"Armstrong, Stuart; Leike, Jan",Towards interactive inverse reinforcement learning,NIPS Workshop,,,,,,2016,2022-01-30 04:53:37,2022-01-30 04:53:37,,,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000004,,,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8H65GSA9,conferencePaper,2020,"O'Keefe, Cullen; Cihon, Peter; Garfinkel, Ben; Flynn, Carrick; Leung, Jade; Dafoe, Allan",The Windfall Clause: Distributing the Benefits of AI for the Common Good,"Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",978-1-4503-7110-0,,10.1145/3375627.3375842,https://dl.acm.org/doi/10.1145/3375627.3375842,,2020-02-07,2022-01-30 04:53:37,2022-01-30 04:53:37,2020-08-18 21:31:11,327-331,,,,,,The Windfall Clause,,,,,ACM,New York NY USA,en,,,,,DOI.org (Crossref),,ZSCC: 0000011,,/Users/jacquesthibodeau/Zotero/storage/GN5W52XA/O'Keefe et al. - 2020 - The Windfall Clause Distributing the Benefits of .pdf,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"AIES '20: AAAI/ACM Conference on AI, Ethics, and Society",,,,,,,,,,,,,,,,
GEFVXESX,conferencePaper,2020,"Shevlane, Toby; Dafoe, Allan",The Offense-Defense Balance of Scientific Knowledge: Does Publishing AI Research Reduce Misuse?,"AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",,,,http://arxiv.org/abs/2001.00463,"There is growing concern over the potential misuse of artificial intelligence (AI) research. Publishing scientific research can facilitate misuse of the technology, but the research can also contribute to protections against misuse. This paper addresses the balance between these two effects. Our theoretical framework elucidates the factors governing whether the published research will be more useful for attackers or defenders, such as the possibility for adequate defensive measures, or the independent discovery of the knowledge outside of the scientific community. The balance will vary across scientific fields. However, we show that the existing conversation within AI has imported concepts and conclusions from prior debates within computer security over the disclosure of software vulnerabilities. While disclosure of software vulnerabilities often favours defence, this cannot be assumed for AI research. The AI research community should consider concepts and policies from a broad set of adjacent fields, and ultimately needs to craft policy well-suited to its particular challenges.",2020-01-09,2022-01-30 04:53:36,2022-01-30 04:53:36,2020-11-14 00:34:29,,,,,,,The Offense-Defense Balance of Scientific Knowledge,,,,,,,,,,,,arXiv.org,,ZSCC: 0000006  arXiv: 2001.00463,,/Users/jacquesthibodeau/Zotero/storage/GXWDDN35/Shevlane and Dafoe - 2020 - The Offense-Defense Balance of Scientific Knowledg.pdf; /Users/jacquesthibodeau/Zotero/storage/JMJHDTSG/2001.html,,MetaSafety; FHI,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"AAAI/ACM Conference on AI, Ethics, and Society",,,,,,,,,,,,,,,,
8WX59P96,conferencePaper,2020,"Tucker, Aaron D.; Anderljung, Markus; Dafoe, Allan",Social and Governance Implications of Improved Data Efficiency,"Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3375627.3375863,http://arxiv.org/abs/2001.05068,"Many researchers work on improving the data efficiency of machine learning. What would happen if they succeed? This paper explores the social-economic impact of increased data efficiency. Specifically, we examine the intuition that data efficiency will erode the barriers to entry protecting incumbent data-rich AI firms, exposing them to more competition from data-poor firms. We find that this intuition is only partially correct: data efficiency makes it easier to create ML applications, but large AI firms may have more to gain from higher performing AI systems. Further, we find that the effect on privacy, data markets, robustness, and misuse are complex. For example, while it seems intuitive that misuse risk would increase along with data efficiency – as more actors gain access to any level of capability – the net effect crucially depends on how much defensive measures are improved. More investigation into data efficiency, as well as research into the “AI production function"", will be key to understanding the development of the AI industry and its societal impacts.",2020-02-07,2022-01-30 04:53:35,2022-01-30 04:53:35,2020-08-18 21:33:45,378-384,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000006  arXiv: 2001.05068,,/Users/jacquesthibodeau/Zotero/storage/9DUUK8Z7/Tucker et al. - 2020 - Social and Governance Implications of Improved Dat.pdf,,MetaSafety; FHI,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5VK55TPK,conferencePaper,2019,"Cihon, Peter; Maas, Matthijs M; Kemp, Luke",Should Artificial Intelligence Governance be Centralised? Six Design Lessons from History,"Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",,,,https://www.cser.ac.uk/media/uploads/files/Cihon_et_al-_2019-_Should_AI_Governance_be_Centralised.pdf,"Can effective international governance for artiﬁcial intelligence remain fragmented, or is there a need for a centralised international organisation for AI? We draw on the history of other international regimes to identify advantages and disadvantages in centralising AI governance. Some considerations, such as efﬁciency and political power, speak in favour of centralisation. Conversely, the risk of creating a slow and brittle institution speaks against it, as does the difﬁculty in securing participation while creating stringent rules. Other considerations depend on the speciﬁc design of a centralised institution. A well-designed body may be able to deter forum shopping and ensure policy coordination. However, forum shopping can be beneﬁcial and a fragmented landscape of institutions can be self-organising. Centralisation entails trade-offs and the details matter. We conclude with two core recommendations. First, the outcome will depend on the exact design of a central institution. A well-designed centralised regime covering a set of coherent issues could be beneﬁcial. But locking-in an inadequate structure may pose a fate worse than fragmentation. Second, for now fragmentation will likely persist. This should be closely monitored to see if it is self-organising or simply inadequate.",2019-12-15,2022-01-30 04:53:35,2022-01-30 04:53:35,2020-09-07,11,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: NoCitationData[s2]  ACC: 16,,/Users/jacquesthibodeau/Zotero/storage/9UVMQ2CZ/Cihon et al. - Should Artificial Intelligence Governance be Centr.pdf,,MetaSafety; CSER; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"AAAI/ACM Conference on AI, Ethics, and Society",,,,,,,,,,,,,,,,
835EJRGP,conferencePaper,2018,"Armstrong, Stuart; Mindermann, Sören",Occam's razor is insufficient to infer the preferences of irrational agents,Advances in Neural Information Processing Systems,,,,,"Inverse reinforcement learning (IRL) attempts to infer human rewards or preferences from observed behavior. Since human planning systematically deviates from rationality, several approaches have been tried to account for speciﬁc human shortcomings. However, the general problem of inferring the reward function of an agent of unknown rationality has received little attention. Unlike the well-known ambiguity problems in IRL, this one is practically relevant but cannot be resolved by observing the agent’s policy in enough environments. This paper shows (1) that a No Free Lunch result implies it is impossible to uniquely decompose a policy into a planning algorithm and reward function, and (2) that even with a reasonable simplicity prior/Occam’s razor on the set of decompositions, we cannot distinguish between the true decomposition and others that lead to high regret. To address this, we need simple ‘normative’ assumptions, which cannot be deduced exclusively from observations.",2018,2022-01-30 04:53:19,2022-01-30 04:53:19,,5598–5609,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000017[s0],,/Users/jacquesthibodeau/Zotero/storage/3MH76I6R/Armstrong and Mindermann - Occam's razor is insufficient to infer the prefere.pdf; /Users/jacquesthibodeau/Zotero/storage/BXEGUPH6/7803-occams-razor-is-insufficient-to-infer-the-preferences-of-irrational-agents.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"32nd Conference on Neural Information Processing Systems (NeurIPS 2018),",,,,,,,,,,,,,,,,
6NBGWRA8,conferencePaper,2015,"Armstrong, Stuart",Motivated value selection for artificial agents,Workshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence,,,,,,2015,2022-01-30 04:53:19,2022-01-30 04:53:19,,,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000046,,/Users/jacquesthibodeau/Zotero/storage/PZRINFPE/10183.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7GE6AZPJ,conferencePaper,2018,"Carey, Ryan",Incorrigibility in the CIRL Framework,"AIES '18: Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society",,,,http://arxiv.org/abs/1709.06275,"A value learning system has incentives to follow shutdown instructions, assuming the shutdown instruction provides information (in the technical sense) about which actions lead to valuable outcomes. However, this assumption is not robust to model mis-specification (e.g., in the case of programmer errors). We demonstrate this by presenting some Supervised POMDP scenarios in which errors in the parameterized reward function remove the incentive to follow shutdown commands. These difficulties parallel those discussed by Soares et al. (2015) in their paper on corrigibility. We argue that it is important to consider systems that follow shutdown commands under some weaker set of assumptions (e.g., that one small verified module is correctly implemented; as opposed to an entire prior probability distribution and/or parameterized reward function). We discuss some difficulties with simple ways to attempt to attain these sorts of guarantees in a value learning framework.",2018-06-03,2022-01-30 04:53:18,2022-01-30 04:53:18,2019-12-16 02:29:24,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000017  arXiv: 1709.06275,,/Users/jacquesthibodeau/Zotero/storage/8NIPD8BV/Carey - 2018 - Incorrigibility in the CIRL Framework.pdf; /Users/jacquesthibodeau/Zotero/storage/HPR8TDJ6/Carey - 2018 - Incorrigibility in the CIRL Framework.pdf; /Users/jacquesthibodeau/Zotero/storage/4G9NWGEB/1709.html; /Users/jacquesthibodeau/Zotero/storage/35KMSSRX/1709.html,,TechSafety; FHI; MIRI,Computer Science - Artificial Intelligence; ai safety; cirl; cooperative inverse reinforcement learning; corrigibility,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"2018 AAAI/ACM Conference on AI, Ethics, and Society",,,,,,,,,,,,,,,,
RZBBEH4R,conferencePaper,2016,"Evans, Owain; Stuhlmüller, Andreas; Goodman, Noah","Learning the preferences of ignorant, inconsistent agents",Thirtieth AAAI Conference on Artificial Intelligence,,,,,,2016,2022-01-30 04:53:18,2022-01-30 04:53:18,,,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000091,,/Users/jacquesthibodeau/Zotero/storage/IT7ZXEC3/12476.html,,TechSafety; FHI; Ought,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PWCWSMIK,conferencePaper,2015,"Evans, Owain; Goodman, Noah D",Learning the Preferences of Bounded Agents,NIPS Workshop on Bounded Optimality,,,,,,2015,2022-01-30 04:53:18,2022-01-30 04:53:18,,7,,,6,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000029  9 J:19,,/Users/jacquesthibodeau/Zotero/storage/3IWX99PK/Evans and Goodman - Learning the Preferences of Bounded Agents.pdf,,TechSafety; FHI; Ought,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DW4JCPPD,conferencePaper,2021,"Turner, Alexander Matt; Smith, Logan; Shah, Rohin; Critch, Andrew; Tadepalli, Prasad",Optimal Policies Tend to Seek Power,arXiv:1912.01683 [cs],,,,http://arxiv.org/abs/1912.01683,"Some researchers speculate that intelligent reinforcement learning (RL) agents would be incentivized to seek resources and power in pursuit of their objectives. Other researchers are skeptical, because RL agents need not have human-like power-seeking instincts. To clarify this debate, we develop the first formal theory of the statistical tendencies of optimal policies. In the context of Markov decision processes, we prove that certain environmental symmetries are sufficient for optimal policies to tend to seek power over the environment. These symmetries exist in many environments in which the agent can be shut down or destroyed. We prove that in these environments, most reward functions make it optimal to seek power by keeping a range of options available and, when maximizing average reward, by navigating towards larger sets of potential terminal states.",2021-10-23,2022-01-30 04:50:56,2022-01-30 04:50:56,2021-11-14 18:44:10,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s0]  ACC: 1  arXiv: 1912.01683,,/Users/jacquesthibodeau/Zotero/storage/6M65PKWS/Turner et al. - 2021 - Optimal Policies Tend to Seek Power.pdf; /Users/jacquesthibodeau/Zotero/storage/RDF4U9VH/1912.html,,TechSafety,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,35th Conference on Neural Information Processing Systems (NeurIPS 2021),,,,,,,,,,,,,,,,
H56ZKQ9D,conferencePaper,2019,"Carroll, Micah; Shah, Rohin; Ho, Mark K.; Griffiths, Thomas L.; Seshia, Sanjit A.; Abbeel, Pieter; Dragan, Anca",On the Utility of Learning about Humans for Human-AI Coordination,Advances in Neural Information Processing Systems 32 (NeurIPS 2019),,,,https://arxiv.org/abs/1910.05789v2,"While we would like agents that can coordinate with humans, current algorithms such as self-play and population-based training create agents that can coordinate with themselves. Agents that assume their partner to be optimal or similar to them can converge to coordination protocols that fail to understand and be understood by humans. To demonstrate this, we introduce a simple environment that requires challenging coordination, based on the popular game Overcooked, and learn a simple model that mimics human play. We evaluate the performance of agents trained via self-play and population-based training. These agents perform very well when paired with themselves, but when paired with our human model, they are significantly worse than agents designed to play with the human model. An experiment with a planning algorithm yields the same conclusion, though only when the human-aware planner is given the exact human model that it is playing with. A user study with real humans shows this pattern as well, though less strongly. Qualitatively, we find that the gains come from having the agent adapt to the human's gameplay. Given this result, we suggest several approaches for designing agents that learn about humans in order to better coordinate with them. Code is available at https://github.com/HumanCompatibleAI/overcooked_ai.",2019-10-13,2022-01-30 04:50:55,2022-01-30 04:50:55,2020-11-14 01:20:48,,,,,,,,,,,,,,en,,,,,arxiv.org,,ZSCC: 0000062,,/Users/jacquesthibodeau/Zotero/storage/564DIJ7M/Carroll et al. - On the Utility of Learning about Humans for Human-.pdf; /Users/jacquesthibodeau/Zotero/storage/N29XCQCE/Carroll et al. - 2019 - On the Utility of Learning about Humans for Human-.pdf; /Users/jacquesthibodeau/Zotero/storage/NKUA5J28/1910.html; /Users/jacquesthibodeau/Zotero/storage/7DUA2DSC/1910.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2019,,,,,,,,,,,,,,,,
Q44NESEK,conferencePaper,2019,"Huang, Sandy H.; Huang, Isabella; Pandya, Ravi; Dragan, Anca D.",Nonverbal Robot Feedback for Human Teachers,Proceedings of the Conference on Robot Learning,,,,http://arxiv.org/abs/1911.02320,"Robots can learn preferences from human demonstrations, but their success depends on how informative these demonstrations are. Being informative is unfortunately very challenging, because during teaching, people typically get no transparency into what the robot already knows or has learned so far. In contrast, human students naturally provide a wealth of nonverbal feedback that reveals their level of understanding and engagement. In this work, we study how a robot can similarly provide feedback that is minimally disruptive, yet gives human teachers a better mental model of the robot learner, and thus enables them to teach more effectively. Our idea is that at any point, the robot can indicate what it thinks the correct next action is, shedding light on its current estimate of the human's preferences. We analyze how useful this feedback is, both in theory and with two user studies---one with a virtual character that tests the feedback itself, and one with a PR2 robot that uses gaze as the feedback mechanism. We find that feedback can be useful for improving both the quality of teaching and teachers' understanding of the robot's capability.",2019-11-06,2022-01-30 04:50:55,2022-01-30 04:50:55,2019-12-18 02:41:20,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000003  arXiv: 1911.02320,,/Users/jacquesthibodeau/Zotero/storage/AA6GDGDX/Huang et al. - 2019 - Nonverbal Robot Feedback for Human Teachers.pdf; /Users/jacquesthibodeau/Zotero/storage/CR3FFFGV/1911.html,,CHAI; TechSafety,Computer Science - Machine Learning; Computer Science - Robotics; Computer Science - Human-Computer Interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Conference on Robot Learning,,,,,,,,,,,,,,,,
5FN3T5EH,conferencePaper,2015,"Hadﬁeld-Menell, Dylan; Russell, Stuart",Multitasking: Efﬁcient Optimal Planning for Bandit Superprocesses,Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence,,,,,"A bandit superprocess is a decision problem composed from multiple independent Markov decision processes (MDPs), coupled only by the constraint that, at each time step, the agent may act in only one of the MDPs. Multitasking problems of this kind are ubiquitous in the real world, yet very little is known about them from a computational viewpoint, beyond the observation that optimal policies for the superprocess may prescribe actions that would be suboptimal for an MDP considered in isolation. (This observation implies that many applications of sequential decision analysis in practice are technically incorrect, since the decision problem being solved is often part of a larger, unstated bandit superprocess.) The paper summarizes the state-of-theart in the theory of bandit superprocesses and contributes a novel upper bound on the global value function of a bandit superprocess, deﬁned in terms of a direct relaxation of the arms. The bound is equivalent to an existing bound (the Whittle integral), but is deﬁned constructively, as the value of a related multi-armed bandit. We provide a new method to compute this bound and derive the ﬁrst practical algorithm to select optimal actions in bandit superprocesses. The algorithm operates by repeatedly establishing dominance relations between actions using upper and lower bounds on action values. Experiments indicate that the algorithm’s run-time compares very favorably to other possible algorithms designed for more general factored MDPs.",2015-07,2022-01-30 04:50:55,2022-01-30 04:50:55,,10,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000002,,/Users/jacquesthibodeau/Zotero/storage/FTI9E6Q8/Hadﬁeld-Menell and Russell - Multitasking Efﬁcient Optimal Planning for Bandit.pdf,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CBIVNF95,conferencePaper,2018,"Gleave, Adam; Habryka, Oliver",Multi-task Maximum Entropy Inverse Reinforcement Learning,,,,,http://arxiv.org/abs/1805.08882,"Multi-task Inverse Reinforcement Learning (IRL) is the problem of inferring multiple reward functions from expert demonstrations. Prior work, built on Bayesian IRL, is unable to scale to complex environments due to computational constraints. This paper contributes a formulation of multi-task IRL in the more computationally efficient Maximum Causal Entropy (MCE) IRL framework. Experiments show our approach can perform one-shot imitation learning in a gridworld environment that single-task IRL algorithms need hundreds of demonstrations to solve. We outline preliminary work using meta-learning to extend our method to the function approximator setting of modern MCE IRL algorithms. Evaluating on multi-task variants of common simulated robotics benchmarks, we discover serious limitations of these IRL algorithms, and conclude with suggestions for further work.",2018-07-15,2022-01-30 04:50:55,2022-01-30 04:50:55,2019-12-18 01:12:51,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000020  arXiv: 1805.08882,,/Users/jacquesthibodeau/Zotero/storage/5V48367F/Gleave and Habryka - 2018 - Multi-task Maximum Entropy Inverse Reinforcement L.pdf,,CHAI; TechSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; I.2.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1st Workshop on Goal Specifications for Reinforce- ment Learning, FAIM 2018",,,,,,,,,,,,,,,,
V5MMVQWT,conferencePaper,2020,"Fickinger, Arnaud; Zhuang, Simon; Hadfield-Menell, Dylan; Russell, Stuart",Multi-Principal Assistance Games,,,,,http://arxiv.org/abs/2007.09540,"Assistance games (also known as cooperative inverse reinforcement learning games) have been proposed as a model for beneficial AI, wherein a robotic agent must act on behalf of a human principal but is initially uncertain about the humans payoff function. This paper studies multi-principal assistance games, which cover the more general case in which the robot acts on behalf of N humans who may have widely differing payoffs. Impossibility theorems in social choice theory and voting theory can be applied to such games, suggesting that strategic behavior by the human principals may complicate the robots task in learning their payoffs. We analyze in particular a bandit apprentice game in which the humans act first to demonstrate their individual preferences for the arms and then the robot acts to maximize the sum of human payoffs. We explore the extent to which the cost of choosing suboptimal arms reduces the incentive to mislead, a form of natural mechanism design. In this context we propose a social choice method that uses shared control of a system to combine preference inference with social welfare optimization.",2020-07-18,2022-01-30 04:50:55,2022-01-30 04:50:55,2020-08-28 17:24:47,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000007  arXiv: 2007.09540,,/Users/jacquesthibodeau/Zotero/storage/CTGBBT4P/Fickinger et al. - 2020 - Multi-Principal Assistance Games.pdf; /Users/jacquesthibodeau/Zotero/storage/KRF8QCZJ/2007.html,,CHAI; TechSafety,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICML 2020,,,,,,,,,,,,,,,,
6X7RBM9U,conferencePaper,2018,"Milli, Smitha; Schmidt, Ludwig; Dragan, Anca D.; Hardt, Moritz",Model Reconstruction from Model Explanations,"FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency",,,,http://arxiv.org/abs/1807.05185,"We show through theory and experiment that gradient-based explanations of a model quickly reveal the model itself. Our results speak to a tension between the desire to keep a proprietary model secret and the ability to offer model explanations. On the theoretical side, we give an algorithm that provably learns a two-layer ReLU network in a setting where the algorithm may query the gradient of the model with respect to chosen inputs. The number of queries is independent of the dimension and nearly optimal in its dependence on the model size. Of interest not only from a learning-theoretic perspective, this result highlights the power of gradients rather than labels as a learning primitive. Complementing our theory, we give effective heuristics for reconstructing models from gradient explanations that are orders of magnitude more query-efficient than reconstruction attacks relying on prediction interfaces.",2018-07-13,2022-01-30 04:50:55,2022-01-30 04:50:55,2019-12-18 01:13:24,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: NoCitationData[s6]  ACC: 69  J: 31 arXiv: 1807.05185,,/Users/jacquesthibodeau/Zotero/storage/QSGCSN5T/Milli et al. - 2018 - Model Reconstruction from Model Explanations.pdf; /Users/jacquesthibodeau/Zotero/storage/D5ADWGHJ/Milli et al. - 2018 - Model Reconstruction from Model Explanations.pdf; /Users/jacquesthibodeau/Zotero/storage/83WVJFJX/1807.html; /Users/jacquesthibodeau/Zotero/storage/S3VPKVIG/1807.html,,CHAI; TechSafety,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Conference on Fairness, Accountability, and Transparency",,,,,,,,,,,,,,,,
4NRTTKVW,conferencePaper,2018,"Zhang, Shun; Durfee, Edmund H.; Singh, Satinder",Minimax-Regret Querying on Side Effects for Safe Optimality in Factored Markov Decision Processes,Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence,978-0-9992411-2-7,,10.24963/ijcai.2018/676,https://www.ijcai.org/proceedings/2018/676,"As it achieves a goal on behalf of its human user, an autonomous agent’s actions may have side effects that change features of its environment in ways that negatively surprise its user. An agent that can be trusted to operate safely should thus only change features the user has explicitly permitted. We formalize this problem, and develop a planning algorithm that avoids potentially negative side effects given what the agent knows about (un)changeable features. Further, we formulate a provably minimax-regret querying strategy for the agent to selectively ask the user about features that it hasn’t explicitly been told about. We empirically show how much faster it is than a more exhaustive approach and how much better its queries are than those found by the best known heuristic.",2018-07,2022-01-30 04:50:55,2022-01-30 04:50:55,2020-11-14 01:15:22,4867-4873,,,,,,,,,,,International Joint Conferences on Artificial Intelligence Organization,"Stockholm, Sweden",en,,,,,DOI.org (Crossref),,ZSCC: 0000023,,/Users/jacquesthibodeau/Zotero/storage/XMWT43ZV/Zhang et al. - 2018 - Minimax-Regret Querying on Side Effects for Safe O.pdf,,CHAI; TechSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Twenty-Seventh International Joint Conference on Artificial Intelligence {IJCAI-18},,,,,,,,,,,,,,,,
4ZEWADP4,conferencePaper,2019,"Milli, Smitha; Dragan, Anca D.",Literal or Pedagogic Human? Analyzing Human Model Misspecification in Objective Learning,Proceedings of The 35th Uncertainty in Artificial Intelligence Conference,,,,http://proceedings.mlr.press/v115/milli20a.html,"It is incredibly easy for a system designer to misspecify the objective for an autonomous system (“robot”), thus motivating the desire to have the robot learn the objective from human behavior instead. Recent work has suggested that people have an interest in the robot performing well, and will thus behave pedagogically, choosing actions that are informative to the robot. In turn, robots beneﬁt from interpreting the behavior by accounting for this pedagogy. In this work, we focus on misspeciﬁcation: we argue that robots might not know whether people are being pedagogic or literal and that it is important to ask which assumption is safer to make. We cast objective learning into the more general form of a common-payoff game between the robot and human, and prove that in any such game literal interpretation is more robust to misspeciﬁcation. Experiments with human data support our theoretical results and point to the sensitivity of the pedagogic assumption.",2019-06-28,2022-01-30 04:50:55,2022-01-30 04:50:55,2020-12-20,,,,,,,Literal or Pedagogic Human?,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000003[s0]  arXiv: 1903.03877,,/Users/jacquesthibodeau/Zotero/storage/4532ZRGJ/Milli and Dragan - 2019 - Literal or Pedagogic Human Analyzing Human Model .pdf,,CHAI; TechSafety,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,The 35th Uncertainty in Artificial Intelligence Conference,,,,,,,,,,,,,,,,
ZJGDKBG2,conferencePaper,2019,"Hadfield-Menell, Dylan; Andrus, McKane; Hadfield, Gillian K.",Legible Normativity for AI Alignment: The Value of Silly Rules,"AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",,,,http://arxiv.org/abs/1811.01267,"It has become commonplace to assert that autonomous agents will have to be built to follow human rules of behavior–social norms and laws. But human laws and norms are complex and culturally varied systems; in many cases agents will have to learn the rules. This requires autonomous agents to have models of how human rule systems work so that they can make reliable predictions about rules. In this paper we contribute to the building of such models by analyzing an overlooked distinction between important rules and what we call silly rules —rules with no discernible direct impact on welfare. We show that silly rules render a normative system both more robust and more adaptable in response to shocks to perceived stability. They make normativity more legible for humans, and can increase legibility for AI systems as well. For AI systems to integrate into human normative systems, we suggest, it may be important for them to have models that include representations of silly rules.",2019,2022-01-30 04:50:55,2022-01-30 04:50:55,2019-07-08 15:47:44,,,,,,,Legible Normativity for AI Alignment,,,,,,,en,,,,,arXiv.org,,ZSCC: NoCitationData[s6]  ACC: 6  J: 2 arXiv: 1811.01267,,/Users/jacquesthibodeau/Zotero/storage/9N9AXA2T/Hadfield-Menell et al. - 2018 - Legible Normativity for AI Alignment The Value of.pdf; /Users/jacquesthibodeau/Zotero/storage/TRSUMM4D/1811.html,,CHAI; TechSafety,Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Computer Science - Human-Computer Interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"2019 AAAI/ACM Conference on AI, Ethics, and Society",,,,,,,,,,,,,,,,
D6VXDQ3V,conferencePaper,2019,"Choudhury, Rohan; Swamy, Gokul; Hadfield-Menell, Dylan; Dragan, Anca",On the Utility of Model Learning in HRI,2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI),,,,http://arxiv.org/abs/1901.01291,"Fundamental to robotics is the debate between model-based and model-free learning: should the robot build an explicit model of the world, or learn a policy directly? In the context of HRI, part of the world to be modeled is the human. One option is for the robot to treat the human as a black box and learn a policy for how they act directly. But it can also model the human as an agent, and rely on a “theory of mind” to guide or bias the learning (grey box). We contribute a characterization of the performance of these methods under the optimistic case of having an ideal theory of mind, as well as under different scenarios in which the assumptions behind the robot’s theory of mind for the human are wrong, as they inevitably will be in practice. We ﬁnd that there is a signiﬁcant sample complexity advantage to theory of mind methods and that they are more robust to covariate shift, but that when enough interaction data is available, black box approaches eventually dominate.",2019-01-04,2022-01-30 04:50:55,2022-01-30 04:50:55,2019-07-08 15:45:07,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000026  arXiv: 1901.01291,,/Users/jacquesthibodeau/Zotero/storage/NZN9AB88/Choudhury et al. - 2019 - On the Utility of Model Learning in HRI.pdf,,CHAI; TechSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI),,,,,,,,,,,,,,,,
D5PF7N4F,conferencePaper,2019,"Shah, Rohin; Gundotra, Noah; Abbeel, Pieter; Dragan, Anca D.","On the Feasibility of Learning, Rather than Assuming, Human Biases for Reward Inference",Proceedings of the 36th International Conference on Machine Learning,,,,http://arxiv.org/abs/1906.09624,"Our goal is for agents to optimize the right reward function, despite how difﬁcult it is for us to specify what that is. Inverse Reinforcement Learning (IRL) enables us to infer reward functions from demonstrations, but it usually assumes that the expert is noisily optimal. Real people, on the other hand, often have systematic biases: risk-aversion, myopia, etc. One option is to try to characterize these biases and account for them explicitly during learning. But in the era of deep learning, a natural suggestion researchers make is to avoid mathematical models of human behavior that are fraught with speciﬁc assumptions, and instead use a purely data-driven approach. We decided to put this to the test – rather than relying on assumptions about which speciﬁc bias the demonstrator has when planning, we instead learn the demonstrator’s planning algorithm that they use to generate demonstrations, as a differentiable planner. Our exploration yielded mixed ﬁndings: on the one hand, learning the planner can lead to better reward inference than relying on the wrong assumption; on the other hand, this beneﬁt is dwarfed by the loss we incur by going from an exact to a differentiable planner. This suggest that at least for the foreseeable future, agents need a middle ground between the ﬂexibility of data-driven methods and the useful bias of known human biases. Code is available at https: //tinyurl.com/learningbiases.",2019-06-23,2022-01-30 04:50:55,2022-01-30 04:50:55,2019-07-11 18:35:30,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000020  arXiv: 1906.09624,,"/Users/jacquesthibodeau/Zotero/storage/TTNVB7MM/Shah et al. - 2019 - On the Feasibility of Learning, Rather than Assumi.pdf; /Users/jacquesthibodeau/Zotero/storage/5WHA9REK/1906.html; /Users/jacquesthibodeau/Zotero/storage/4DMXNS7E/Shah et al. - 2019 - On the Feasibility of Learning, Rather than Assumi.pdf",,CHAI; TechSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,36th International Conference on Machine Learning,,,,,,,,,,,,,,,,
TK4DP9CE,conferencePaper,2021,"Lindner, David; Shah, Rohin; Abbeel, Pieter; Dragan, Anca",Learning What To Do By Simulating the Past,,,,,,"Since reward functions are hard to specify, recent work has focused on learning policies from human feedback. However, such approaches are impeded by the expense of acquiring such feedback. Recent work proposed that agents have access to a source of information that is effectively free: in any environment that humans have acted in, the state will already be optimized for human preferences, and thus an agent can extract information about what humans want from the state (Shah et al., 2019). Such learning is possible in principle, but requires simulating all possible past trajectories that could have led to the observed state. This is feasible in gridworlds, but how do we scale it to complex tasks? In this work, we show that by combining a learned feature encoder with learned inverse models, we can enable agents to simulate human actions backwards in time to infer what they must have done. The resulting algorithm is able to reproduce a speciﬁc skill in MuJoCo environments given a single state sampled from the optimal policy for that skill.",2021,2022-01-30 04:50:55,2022-01-30 04:50:55,,24,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/XAPXNEM2/Lindner et al. - 2021 - LEARNING WHAT TO DO BY SIMULATING THE PAST.pdf,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICLR 2021,,,,,,,,,,,,,,,,
8Z4999WK,conferencePaper,2018,"Bobu, Andreea; Bajcsy, Andrea; Fisac, Jaime F.; Dragan, Anca D.",Learning under Misspecified Objective Spaces,2nd Conference on Robot Learning (CoRL 2018),,,,http://arxiv.org/abs/1810.05157,"Learning robot objective functions from human input has become increasingly important, but state-of-the-art techniques assume that the human’s desired objective lies within the robot’s hypothesis space. When this is not true, even methods that keep track of uncertainty over the objective fail because they reason about which hypothesis might be correct, and not whether any of the hypotheses are correct. We focus speciﬁcally on learning from physical human corrections during the robot’s task execution, where not having a rich enough hypothesis space leads to the robot updating its objective in ways that the person did not actually intend. We observe that such corrections appear irrelevant to the robot, because they are not the best way of achieving any of the candidate objectives. Instead of naively trusting and learning from every human interaction, we propose robots learn conservatively by reasoning in real time about how relevant the human’s correction is for the robot’s hypothesis space. We test our inference method in an experiment with human interaction data, and demonstrate that this alleviates unintended learning in an in-person user study with a robot manipulator.",2018-10-26,2022-01-30 04:50:54,2022-01-30 04:50:54,2019-12-18 02:38:36,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000015  arXiv: 1810.05157,,/Users/jacquesthibodeau/Zotero/storage/BHXJU7S4/Bobu et al. - 2018 - Learning under Misspecified Objective Spaces.pdf,,CHAI; TechSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Robotics; Computer Science - Human-Computer Interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2nd Conference on Robot Learning (CoRL 2018),,,,,,,,,,,,,,,,
H4ETM44R,conferencePaper,2019,"Reddy, Siddharth; Dragan, Anca D.; Levine, Sergey; Legg, Shane; Leike, Jan",Learning Human Objectives by Evaluating Hypothetical Behavior,Proceedings of the 37th International Conference on Machine Learning,,,,http://proceedings.mlr.press/v119/reddy20a.html,"We seek to align agent behavior with a user's objectives in a reinforcement learning setting with unknown dynamics, an unknown reward function, and unknown unsafe states. The user knows the rewards and unsafe states, but querying the user is expensive. To address this challenge, we propose an algorithm that safely and interactively learns a model of the user's reward function. We start with a generative model of initial states and a forward dynamics model trained on off-policy data. Our method uses these models to synthesize hypothetical behaviors, asks the user to label the behaviors with rewards, and trains a neural network to predict the rewards. The key idea is to actively synthesize the hypothetical behaviors from scratch by maximizing tractable proxies for the value of information, without interacting with the environment. We call this method reward query synthesis via trajectory optimization (ReQueST). We evaluate ReQueST with simulated users on a state-based 2D navigation task and the image-based Car Racing video game. The results show that ReQueST significantly outperforms prior methods in learning reward models that transfer to new environments with different initial state distributions. Moreover, ReQueST safely trains the reward model to detect unsafe states, and corrects reward hacking before deploying the agent.",2019-12-05,2022-01-30 04:50:54,2022-01-30 04:50:54,2020-12-20,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000005[s0]  arXiv: 1912.05652,,/Users/jacquesthibodeau/Zotero/storage/RGR6BVZQ/Reddy et al. - 2019 - Learning Human Objectives by Evaluating Hypothetic.pdf; /Users/jacquesthibodeau/Zotero/storage/E85ASZIU/1912.html; /Users/jacquesthibodeau/Zotero/storage/QJDICWCK/1912.html,,CHAI; TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,37th International Conference on Machine Learning,,,,,,,,,,,,,,,,
4TS7JII8,conferencePaper,2018,"Basu, Chandrayee; Singhal, Mukesh; Dragan, Anca D.",Learning from Richer Human Guidance: Augmenting Comparison-Based Learning with Feature Queries,Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction  - HRI '18,,,10.1145/3171221.3171284,http://arxiv.org/abs/1802.01604,"We focus on learning the desired objective function for a robot. Although trajectory demonstrations can be very informative of the desired objective, they can also be difficult for users to provide. Answers to comparison queries, asking which of two trajectories is preferable, are much easier for users, and have emerged as an effective alternative. Unfortunately, comparisons are far less informative. We propose that there is much richer information that users can easily provide and that robots ought to leverage. We focus on augmenting comparisons with feature queries, and introduce a unified formalism for treating all answers as observations about the true desired reward. We derive an active query selection algorithm, and test these queries in simulation and on real users. We find that richer, feature-augmented queries can extract more information faster, leading to robots that better match user preferences in their behavior.",2018,2022-01-30 04:50:54,2022-01-30 04:50:54,2019-12-18 02:40:57,132-140,,,,,,Learning from Richer Human Guidance,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000031,,/Users/jacquesthibodeau/Zotero/storage/5GQZRWB7/Basu et al. - 2018 - Learning from Richer Human Guidance Augmenting Co.pdf; /Users/jacquesthibodeau/Zotero/storage/WMF3FHW2/1802.html,,CHAI; TechSafety; AmbiguosSafety,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6W8J2P4W,conferencePaper,2018,"Bajcsy, Andrea; Losey, Dylan P.; O'Malley, Marcia K.; Dragan, Anca D.","Learning from Physical Human Corrections, One Feature at a Time",Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction  - HRI '18,978-1-4503-4953-6,,10.1145/3171221.3171267,http://dl.acm.org/citation.cfm?doid=3171221.3171267,"We focus on learning robot objective functions from human guidance: specifically, from physical corrections provided by the person while the robot is acting. Objective functions are typically parametrized in terms of features, which capture aspects of the task that might be important. When the person intervenes to correct the robot’s behavior, the robot should update its understanding of which features matter, how much, and in what way. Unfortunately, real users do not provide optimal corrections that isolate exactly what the robot was doing wrong. Thus, when receiving a correction, it is difficult for the robot to determine which features the person meant to correct, and which features were changed unintentionally. In this paper, we propose to improve the efficiency of robot learning during physical interactions by reducing unintended learning. Our approach allows the human-robot team to focus on learning one feature at a time, unlike state-of-the-art techniques that update all features at once. We derive an online method for identifying the single feature which the human is trying to change during physical interaction, and experimentally compare this one-at-a-time approach to the all-at-once baseline in a user study. Our results suggest that users teaching one-at-a-time perform better, especially in tasks that require changing multiple features.",2018,2022-01-30 04:50:54,2022-01-30 04:50:54,2019-12-18 02:41:35,141-149,,,,,,,,,,,ACM Press,"Chicago, IL, USA",en,,,,,DOI.org (Crossref),,ZSCC: 0000063,,"/Users/jacquesthibodeau/Zotero/storage/NAECZFHT/Bajcsy et al. - 2018 - Learning from Physical Human Corrections, One Feat.pdf",,CHAI; TechSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,the 2018 ACM/IEEE International Conference,,,,,,,,,,,,,,,,
D65J6K2P,conferencePaper,2019,"Zhao, Ruihan; Tiomkin, Stas; Abbeel, Pieter",Learning Efficient Representation for Intrinsic Motivation,,,,,http://arxiv.org/abs/1912.02624,"Mutual Information between agent Actions and environment States (MIAS) quantifies the influence of agent on its environment. Recently, it was found that the maximization of MIAS can be used as an intrinsic motivation for artificial agents. In literature, the term empowerment is used to represent the maximum of MIAS at a certain state. While empowerment has been shown to solve a broad range of reinforcement learning problems, its calculation in arbitrary dynamics is a challenging problem because it relies on the estimation of mutual information. Existing approaches, which rely on sampling, are limited to low dimensional spaces, because high-confidence distribution-free lower bounds for mutual information require exponential number of samples. In this work, we develop a novel approach for the estimation of empowerment in unknown dynamics from visual observation only, without the need to sample for MIAS. The core idea is to represent the relation between action sequences and future states using a stochastic dynamic model in latent space with a specific form. This allows us to efficiently compute empowerment with the ""Water-Filling"" algorithm from information theory. We construct this embedding with deep neural networks trained on a sophisticated objective function. Our experimental results show that the designed embedding preserves information-theoretic properties of the original dynamics.",2019-12-08,2022-01-30 04:50:54,2022-01-30 04:50:54,2019-12-18 02:48:27,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000004  arXiv: 1912.02624,,/Users/jacquesthibodeau/Zotero/storage/C43XRQE9/Zhao et al. - 2019 - Learning Efficient Representation for Intrinsic Mo.pdf; /Users/jacquesthibodeau/Zotero/storage/ZMMKVSHT/1912.html,,CHAI; TechSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,33rd Conference on Neural Information Processing Systems (NeurIPS 2019),,,,,,,,,,,,,,,,
GZDCFRPG,conferencePaper,1998,"Russell, Stuart",Learning agents for uncertain environments,Proceedings of the eleventh annual conference on Computational learning theory  - COLT' 98,978-1-58113-057-7,,10.1145/279943.279964,http://portal.acm.org/citation.cfm?doid=279943.279964,,1998,2022-01-30 04:50:54,2022-01-30 04:50:54,2020-11-22 01:48:04,101-103,,,,,,,,,,,ACM Press,"Madison, Wisconsin, United States",en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s2]  ACC: 451,,/Users/jacquesthibodeau/Zotero/storage/U8UI78ZU/Russell - 1998 - Learning agents for uncertain environments (extend.pdf,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,the eleventh annual conference,,,,,,,,,,,,,,,,
X8EUNR79,conferencePaper,2017,"Hadfield-Menell, Dylan; Milli, Smitha; Abbeel, Pieter; Russell, Stuart; Dragan, Anca",Inverse Reward Design,Advances in Neural Information Processing Systems 30 (NIPS 2017),,,,http://arxiv.org/abs/1711.02827,"Autonomous agents optimize the reward function we give them. What they don't know is how hard it is for us to design a reward function that actually captures what we want. When designing the reward, we might think of some specific training scenarios, and make sure that the reward will lead to the right behavior in those scenarios. Inevitably, agents encounter new scenarios (e.g., new types of terrain) where optimizing that same reward may lead to undesired behavior. Our insight is that reward functions are merely observations about what the designer actually wants, and that they should be interpreted in the context in which they were designed. We introduce inverse reward design (IRD) as the problem of inferring the true objective based on the designed reward and the training MDP. We introduce approximate methods for solving IRD problems, and use their solution to plan risk-averse behavior in test MDPs. Empirical results suggest that this approach can help alleviate negative side effects of misspecified reward functions and mitigate reward hacking.",2017,2022-01-30 04:50:54,2022-01-30 04:50:54,2020-11-22 04:15:56,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000204  arXiv: 1711.02827,,/Users/jacquesthibodeau/Zotero/storage/IG3G2WF9/Hadfield-Menell et al. - 2020 - Inverse Reward Design.pdf; /Users/jacquesthibodeau/Zotero/storage/M4MGUB4S/1711.html,,CHAI; TechSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
H476ZK5U,conferencePaper,2017,"Bajcsy, Andrea; Losey, Dylan P; O’Malley, Marcia K; Dragan, Anca D",Learning Robot Objectives from Physical Human Interaction,Proceedings of Machine Learning Research,,,,,"When humans and robots work in close proximity, physical interaction is inevitable. Traditionally, robots treat physical interaction as a disturbance, and resume their original behavior after the interaction ends. In contrast, we argue that physical human interaction is informative: it is useful information about how the robot should be doing its task. We formalize learning from such interactions as a dynamical system in which the task objective has parameters that are part of the hidden state, and physical human interactions are observations about these parameters. We derive an online approximation of the robot’s optimal policy in this system, and test it in a user study. The results suggest that learning from physical interaction leads to better robot task performance with less human effort.",2017,2022-01-30 04:50:54,2022-01-30 04:50:54,,10,,,78,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000026[s2],,/Users/jacquesthibodeau/Zotero/storage/B7V4VQ4Q/Bajcsy et al. - Learning Robot Objectives from Physical Human Inte.pdf; /Users/jacquesthibodeau/Zotero/storage/CE6X2RSM/102348.html,,CHAI; TechSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1st Conference on Robot Learning (CoRL 2017),,,,,,,,,,,,,,,,
777W689G,conferencePaper,2019,"Zhang, Jason Y.; Dragan, Anca D.",Learning from Extrapolated Corrections,2019 International Conference on Robotics and Automation (ICRA),,,10.1109/ICRA.2019.8793554,,"Our goal is to enable robots to learn cost functions from user guidance. Often it is difficult or impossible for users to provide full demonstrations, so corrections have emerged as an easier guidance channel. However, when robots learn cost functions from corrections rather than demonstrations, they have to extrapolate a small amount of information - the change of a waypoint along the way - to the rest of the trajectory. We cast this extrapolation problem as online function approximation, which exposes different ways in which the robot can interpret what trajectory the person intended, depending on the function space used for the approximation. Our simulation results and user study suggest that using function spaces with non-Euclidean norms can better capture what users intend, particularly if environments are uncluttered. This, in turn, can lead to the robot learning a more accurate cost function and improves the user's subjective perceptions of the robot.",2019-05,2022-01-30 04:50:54,2022-01-30 04:50:54,,7034-7040,,,,,,,,,,,,,,,,,,IEEE Xplore,,ZSCC: 0000008  ISSN: 1050-4729,,/Users/jacquesthibodeau/Zotero/storage/WDX9K5VV/8793554.html; /Users/jacquesthibodeau/Zotero/storage/Z27ARW82/Zhang and Dragan - 2019 - Learning from Extrapolated Corrections.pdf,,CHAI; TechSafety,control engineering computing; Cost function; cost functions; Estimation; extrapolated corrections; extrapolation; extrapolation problem; function approximation; Function approximation; function space; Kernel; learning (artificial intelligence); nonEuclidean norms; online function approximation; Robot kinematics; robot learning; robot programming; Trajectory; user guidance,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 International Conference on Robotics and Automation (ICRA),,,,,,,,,,,,,,,,
CP3JGZGM,conferencePaper,2019,"Xu, Kelvin; Ratner, Ellis; Dragan, Anca; Levine, Sergey; Finn, Chelsea",Learning a Prior over Intent via Meta-Inverse Reinforcement Learning,Proceedings of the 36th International Conference on Machine Learning,,,,http://arxiv.org/abs/1805.12573,"A significant challenge for the practical application of reinforcement learning in the real world is the need to specify an oracle reward function that correctly defines a task. Inverse reinforcement learning (IRL) seeks to avoid this challenge by instead inferring a reward function from expert behavior. While appealing, it can be impractically expensive to collect datasets of demonstrations that cover the variation common in the real world (e.g. opening any type of door). Thus in practice, IRL must commonly be performed with only a limited set of demonstrations where it can be exceedingly difficult to unambiguously recover a reward function. In this work, we exploit the insight that demonstrations from other tasks can be used to constrain the set of possible reward functions by learning a ""prior"" that is specifically optimized for the ability to infer expressive reward functions from limited numbers of demonstrations. We demonstrate that our method can efficiently recover rewards from images for novel tasks and provide intuition as to how our approach is analogous to learning a prior.",2019-10-14,2022-01-30 04:50:54,2022-01-30 04:50:54,2019-12-18 02:40:07,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000036  1 J: 11 arXiv: 1805.12573,,/Users/jacquesthibodeau/Zotero/storage/FBXXF6K7/Xu et al. - 2019 - Learning a Prior over Intent via Meta-Inverse Rein.pdf; /Users/jacquesthibodeau/Zotero/storage/5GDRRPWX/1805.html,,CHAI; TechSafety,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,36th International Conference on Machine Learning,,,,,,,,,,,,,,,,
3V5UACJI,conferencePaper,2016,"Sadigh, Dorsa; Sastry, S. Shankar; Seshia, Sanjit A.; Dragan, Anca",Information gathering actions over human internal state,2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),978-1-5090-3762-9,,10.1109/IROS.2016.7759036,http://ieeexplore.ieee.org/document/7759036/,"Much of estimation of human internal state (goal, intentions, activities, preferences, etc.) is passive: an algorithm observes human actions and updates its estimate of human state. In this work, we embrace the fact that robot actions affect what humans do, and leverage it to improve state estimation. We enable robots to do active information gathering, by planning actions that probe the user in order to clarify their internal state. For instance, an autonomous car will plan to nudge into a human driver’s lane to test their driving style. Results in simulation and in a user study suggest that active information gathering signiﬁcantly outperforms passive state estimation.",2016-10,2022-01-30 04:50:53,2022-01-30 04:50:53,2019-12-18 01:40:07,66-73,,,,,,,,,,,IEEE,"Daejeon, South Korea",en,,,,,DOI.org (Crossref),,ZSCC: 0000176,,/Users/jacquesthibodeau/Zotero/storage/JXUN7TJI/Sadigh et al. - 2016 - Information gathering actions over human internal .pdf; /Users/jacquesthibodeau/Zotero/storage/X88ZSNX3/Sadigh et al. - 2016 - Information gathering actions over human internal .pdf,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),,,,,,,,,,,,,,,,
AVRPXH8F,conferencePaper,2019,"Pandya, Ravi; Huang, Sandy H.; Hadfield-Menell, Dylan; Dragan, Anca D.",Human-AI Learning Performance in Multi-Armed Bandits,"AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",,,,http://arxiv.org/abs/1812.09376,"People frequently face challenging decision-making problems in which outcomes are uncertain or unknown. Artiﬁcial intelligence (AI) algorithms exist that can outperform humans at learning such tasks. Thus, there is an opportunity for AI agents to assist people in learning these tasks more effectively. In this work, we use a multi-armed bandit as a controlled setting in which to explore this direction. We pair humans with a selection of agents and observe how well each human-agent team performs. We ﬁnd that team performance can beat both human and agent performance in isolation. Interestingly, we also ﬁnd that an agent’s performance in isolation does not necessarily correlate with the human-agent team’s performance. A drop in agent performance can lead to a disproportionately large drop in team performance, or in some settings can even improve team performance. Pairing a human with an agent that performs slightly better than them can make them perform much better, while pairing them with an agent that performs the same can make them them perform much worse. Further, our results suggest that people have different exploration strategies and might perform better with agents that match their strategy. Overall, optimizing human-agent team performance requires going beyond optimizing agent performance, to understanding how the agent’s suggestions will inﬂuence human decision-making.",2019,2022-01-30 04:50:53,2022-01-30 04:50:53,2019-07-08 15:46:25,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: NoCitationData[s6]  ACC: 2  J: 2 arXiv: 1812.09376,,,,CHAI; TechSafety,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"2019 AAAI/ACM Conference on AI, Ethics, and Society",,,,,,,,,,,,,,,,
EF6NXXC5,conferencePaper,2018,"Fisac, Jaime F.; Bronstein, Eli; Stefansson, Elis; Sadigh, Dorsa; Sastry, S. Shankar; Dragan, Anca D.",Hierarchical Game-Theoretic Planning for Autonomous Vehicles,Robotics: Science and Systems 2019,,,,http://arxiv.org/abs/1810.05766,"The actions of an autonomous vehicle on the road affect and are affected by those of other drivers, whether overtaking, negotiating a merge, or avoiding an accident. This mutual dependence, best captured by dynamic game theory, creates a strong coupling between the vehicle’s planning and its predictions of other drivers’ behavior, and constitutes an open problem with direct implications on the safety and viability of autonomous driving technology. Unfortunately, dynamic games are too computationally demanding to meet the real-time constraints of autonomous driving in its continuous state and action space. In this paper, we introduce a novel game-theoretic trajectory planning algorithm for autonomous driving, that enables real-time performance by hierarchically decomposing the underlying dynamic game into a long-horizon “strategic” game with simpliﬁed dynamics and full information structure, and a short-horizon “tactical” game with full dynamics and a simpliﬁed information structure. The value of the strategic game is used to guide the tactical planning, implicitly extending the planning horizon, pushing the local trajectory optimization closer to global solutions, and, most importantly, quantitatively accounting for the autonomous vehicle and the human driver’s ability and incentives to inﬂuence each other. In addition, our approach admits non-deterministic models of human decisionmaking, rather than relying on perfectly rational predictions. Our results showcase richer, safer, and more effective autonomous behavior in comparison to existing techniques.",2018-10-12,2022-01-30 04:50:53,2022-01-30 04:50:53,2019-07-08 16:10:43,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: NoCitationData[s6]  ACC: 106  J: 38 arXiv: 1810.05766,,,,CHAI; TechSafety,"Computer Science - Artificial Intelligence; Computer Science - Robotics; Computer Science - Multiagent Systems; I.2.9; 68T40, 93C85, 91A25; Mathematics - Optimization and Control",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Robotics: Science and Systems 2019,,,,,,,,,,,,,,,,
JXZF36GH,conferencePaper,2020,"Dobbe, Roel; Gilbert, Thomas Krendl; Mintz, Yonatan",Hard Choices in Artificial Intelligence: Addressing Normative Uncertainty through Sociotechnical Commitments,"AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",,,,http://arxiv.org/abs/1911.09005,"As AI systems become prevalent in high stakes domains such as surveillance and healthcare, researchers now examine how to design and implement them in a safe manner. However, the potential harms caused by systems to stakeholders in complex social contexts and how to address these remains unclear. In this paper, we explain the inherent normative uncertainty in debates about the safety of AI systems. We then address this as a problem of vagueness by examining its place in the design, training, and deployment stages of AI system development. We adopt Ruth Chang's theory of intuitive comparability to illustrate the dilemmas that manifest at each stage. We then discuss how stakeholders can navigate these dilemmas by incorporating distinct forms of dissent into the development pipeline, drawing on Elizabeth Anderson's work on the epistemic powers of democratic institutions. We outline a framework of sociotechnical commitments to formal, substantive and discursive challenges that address normative uncertainty across stakeholders, and propose the cultivation of related virtues by those responsible for development.",2020,2022-01-30 04:50:53,2022-01-30 04:50:53,2020-11-14 00:55:17,,,,,,,Hard Choices in Artificial Intelligence,,,,,,,,,,,,arXiv.org,,ZSCC: 0000001[s0]  arXiv: 1911.09005,,/Users/jacquesthibodeau/Zotero/storage/7VMINIWU/Dobbe et al. - 2019 - Hard Choices in Artificial Intelligence Addressin.pdf; /Users/jacquesthibodeau/Zotero/storage/K6X3NNFA/1911.html,,CHAI; TechSafety,Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Electrical Engineering and Systems Science - Systems and Control,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"AAAI/ACM Conference on AI, Ethics, and Society 2020",,,,,,,,,,,,,,,,
K25P3RCZ,conferencePaper,2019,"Hadfield-Menell, Dylan; Hadfield, Gillian",Incomplete Contracting and AI Alignment,"Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",,,,http://arxiv.org/abs/1804.04268,"We suggest that the analysis of incomplete contracting developed by law and economics researchers can provide a useful framework for understanding the AI alignment problem and help to generate a systematic approach to finding solutions. We first provide an overview of the incomplete contracting literature and explore parallels between this work and the problem of AI alignment. As we emphasize, misalignment between principal and agent is a core focus of economic analysis. We highlight some technical results from the economics literature on incomplete contracts that may provide insights for AI alignment researchers. Our core contribution, however, is to bring to bear an insight that economists have been urged to absorb from legal scholars and other behavioral scientists: the fact that human contracting is supported by substantial amounts of external structure, such as generally available institutions (culture, law) that can supply implied terms to fill the gaps in incomplete contracts. We propose a research agenda for AI alignment work that focuses on the problem of how to build AI that can replicate the human cognitive processes that connect individual incomplete contracts with this supporting external structure.",2019,2022-01-30 04:50:53,2022-01-30 04:50:53,2019-07-18 04:58:42,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s6]  ACC: 31  J: 14 arXiv: 1804.04268,,/Users/jacquesthibodeau/Zotero/storage/3BC8IA76/Hadfield-Menell and Hadfield - 2018 - Incomplete Contracting and AI Alignment.pdf; /Users/jacquesthibodeau/Zotero/storage/4CJJ6G6F/1804.html,,CHAI; TechSafety,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VVQW8ER5,conferencePaper,2018,"Liu, Chang; Hamrick, Jessica B.; Fisac, Jaime F.; Dragan, Anca D.; Hedrick, J. Karl; Sastry, S. Shankar; Griffiths, Thomas L.",Goal Inference Improves Objective and Perceived Performance in Human-Robot Collaboration,Proceedings of the 15th International Conferenceon Autonomous Agents and Multiagent Systems (AAMAS 2016),,,,http://arxiv.org/abs/1802.01780,"The study of human-robot interaction is fundamental to the design and use of robotics in real-world applications. Robots will need to predict and adapt to the actions of human collaborators in order to achieve good performance and improve safety and end-user adoption. This paper evaluates a human-robot collaboration scheme that combines the task allocation and motion levels of reasoning: the robotic agent uses Bayesian inference to predict the next goal of its human partner from his or her ongoing motion, and re-plans its own actions in real time. This anticipative adaptation is desirable in many practical scenarios, where humans are unable or unwilling to take on the cognitive overhead required to explicitly communicate their intent to the robot. A behavioral experiment indicates that the combination of goal inference and dynamic task planning significantly improves both objective and perceived performance of the human-robot team. Participants were highly sensitive to the differences between robot behaviors, preferring to work with a robot that adapted to their actions over one that did not.",2018-02-05,2022-01-30 04:50:53,2022-01-30 04:50:53,2020-12-13 20:00:34,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000047  arXiv: 1802.01780,,/Users/jacquesthibodeau/Zotero/storage/IZFZ7SNH/Liu et al. - 2018 - Goal Inference Improves Objective and Perceived Pe.pdf; /Users/jacquesthibodeau/Zotero/storage/ZWMVN6F2/1802.html,,CHAI; TechSafety; AmbiguosSafety,Computer Science - Artificial Intelligence; Computer Science - Robotics; I.2.0; I.2.6; Computer Science - Human-Computer Interaction; 68T05; I.2.8; I.2.9,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C5Z6H6IX,conferencePaper,2018,"Cundy, Chris; Filan, Daniel",Exploring Hierarchy-Aware Inverse Reinforcement Learning,arXiv:1807.05037 [cs],,,,http://arxiv.org/abs/1807.05037,"We introduce a new generative model for human planning under the Bayesian Inverse Reinforcement Learning (BIRL) framework which takes into account the fact that humans often plan using hierarchical strategies. We describe the Bayesian Inverse Hierarchical RL (BIHRL) algorithm for inferring the values of hierarchical planners, and use an illustrative toy model to show that BIHRL retains accuracy where standard BIRL fails. Furthermore, BIHRL is able to accurately predict the goals of `Wikispeedia' game players, with inclusion of hierarchical structure in the model resulting in a large boost in accuracy. We show that BIHRL is able to significantly outperform BIRL even when we only have a weak prior on the hierarchical structure of the plans available to the agent, and discuss the significant challenges that remain for scaling up this framework to more realistic settings.",2018-07-13,2022-01-30 04:50:45,2022-01-30 04:50:45,2019-12-18 01:12:20,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000005  arXiv: 1807.05037,,/Users/jacquesthibodeau/Zotero/storage/65A775B4/Cundy and Filan - 2018 - Exploring Hierarchy-Aware Inverse Reinforcement Le.pdf; /Users/jacquesthibodeau/Zotero/storage/3SB9PIM5/1807.html; /Users/jacquesthibodeau/Zotero/storage/DBVMCRNX/Cundy and Filan - 2018 - Exploring Hierarchy-Aware Inverse Reinforcement Le.pdf,,CHAI; TechSafety,Computer Science - Artificial Intelligence; Computer Science - Human-Computer Interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1st Workshop on Goal Specifications for Reinforcement Learning, ICML 2018",,,,,,,,,,,,,,,,
4BV35Z3X,conferencePaper,2018,"de Graaf, Maartje M.A.; Malle, Bertram F.; Dragan, Anca; Ziemke, Tom",Explainable Robotic Systems,Companion of the 2018 ACM/IEEE International Conference on Human-Robot Interaction  - HRI '18,978-1-4503-5615-2,,10.1145/3173386.3173568,http://dl.acm.org/citation.cfm?doid=3173386.3173568,"The increasing complexity of robotic systems are pressing the need for them to be transparent and trustworthy. When people interact with a robotic system, they will inevitably construct mental models to understand and predict its actions. However, people’s mental models of robotic systems stem from their interactions with living beings, which induces the risk of establishing incorrect or inadequate mental models of robotic systems and may lead people to either under- and over-trust these systems. We need to understand the inferences that people make about robots from their behavior, and leverage this understanding to formulate and implement behaviors into robotic systems that support the formation of correct mental models of and fosters trust calibration. This way, people will be better able to predict the intentions of these systems, and thus more accurately estimate their capabilities, better understand their actions, and potentially correct their errors. The aim of this full-day workshop is to provide a forum for researchers and practitioners to share and learn about recent research on people’s inferences of robot actions, as well as the implementation of transparent, predictable, and explainable behaviors into robotic systems.",2018,2022-01-30 04:50:45,2022-01-30 04:50:45,2019-12-18 02:40:25,387-388,,,,,,,,,,,ACM Press,"Chicago, IL, USA",en,,,,,DOI.org (Crossref),,ZSCC: 0000008,,/Users/jacquesthibodeau/Zotero/storage/FFV4ATRX/de Graaf et al. - 2018 - Explainable Robotic Systems.pdf,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Companion of the 2018 ACM/IEEE International Conference,,,,,,,,,,,,,,,,
BPHFCUVZ,conferencePaper,2018,"Huang, Sandy H.; Bhatia, Kush; Abbeel, Pieter; Dragan, Anca D.",Establishing Appropriate Trust via Critical States,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),,,,http://arxiv.org/abs/1810.08174,"In order to effectively interact with or supervise a robot, humans need to have an accurate mental model of its capabilities and how it acts. Learned neural network policies make that particularly challenging. We propose an approach for helping end-users build a mental model of such policies. Our key observation is that for most tasks, the essence of the policy is captured in a few critical states: states in which it is very important to take a certain action. Our user studies show that if the robot shows a human what its understanding of the task's critical states is, then the human can make a more informed decision about whether to deploy the policy, and if she does deploy it, when she needs to take control from it at execution time.",2018-10-18,2022-01-30 04:50:45,2022-01-30 04:50:45,2019-12-18 02:38:38,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000044  arXiv: 1810.08174,,/Users/jacquesthibodeau/Zotero/storage/ZJCEQTK4/Huang et al. - 2018 - Establishing Appropriate Trust via Critical States.pdf; /Users/jacquesthibodeau/Zotero/storage/PJVHAHWN/1810.html,,CHAI; TechSafety,Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),,,,,,,,,,,,,,,,
4EF7C9NX,conferencePaper,2019,"Gilbert, Thomas Krendl; Mintz, Yonatan",Epistemic Therapy for Bias in Automated Decision-Making,"Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",978-1-4503-6324-2,,10.1145/3306618.3314294,https://dl.acm.org/doi/10.1145/3306618.3314294,,2019-01-27,2022-01-30 04:50:45,2022-01-30 04:50:45,2020-12-17 22:13:08,61-67,,,,,,,,,,,ACM,Honolulu HI USA,en,,,,,DOI.org (Crossref),,ZSCC: 0000002,,/Users/jacquesthibodeau/Zotero/storage/G8JN4BTX/Gilbert and Mintz - 2019 - Epistemic Therapy for Bias in Automated Decision-M.pdf,,CHAI; TechSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"AIES '19: AAAI/ACM Conference on AI, Ethics, and Society",,,,,,,,,,,,,,,,
ZEAMIZP4,conferencePaper,2018,"Xu, Kelvin; Ratner, Ellis; Dragan, Anca; Levine, Sergey; Finn, Chelsea",Few-Shot Intent Inference via Meta-Inverse Reinforcement Learning,Proceedings of the 36th International Conference on Machine Learning,,,,http://proceedings.mlr.press/v97/xu19d/xu19d.pdf,A significant challenge for the practical application of reinforcement learning toreal world problems is the need to specify an oracle reward function that correctly defines a task. Inverse...,2018-09-27,2022-01-30 04:50:45,2022-01-30 04:50:45,2019-12-18 03:12:12,,,,,,,,,,,,,,,,,,,openreview.net,,ZSCC: 0000002,,/Users/jacquesthibodeau/Zotero/storage/KDPI92S8/Xu et al. - 2018 - Few-Shot Intent Inference via Meta-Inverse Reinfor.pdf; /Users/jacquesthibodeau/Zotero/storage/66PJEMEA/forum.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,36th International Conference on Machine Learning,,,,,,,,,,,,,,,,
QT5EDQJS,conferencePaper,2018,"Kwon, Minae; Huang, Sandy H.; Dragan, Anca D.",Expressing Robot Incapability,Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction - HRI '18,,,10.1145/3171221.3171276,http://arxiv.org/abs/1810.08167,"Our goal is to enable robots to express their incapability, and to do so in a way that communicates both what they are trying to accomplish and why they are unable to accomplish it. We frame this as a trajectory optimization problem: maximize the similarity between the motion expressing incapability and what would amount to successful task execution, while obeying the physical limits of the robot. We introduce and evaluate candidate similarity measures, and show that one in particular generalizes to a range of tasks, while producing expressive motions that are tailored to each task. Our user study supports that our approach automatically generates motions expressing incapability that communicate both what and why to end-users, and improve their overall perception of the robot and willingness to collaborate with it in the future.",2018,2022-01-30 04:50:45,2022-01-30 04:50:45,2019-12-18 02:40:52,87-95,,,,,,,,,,,ACM,,en,,,,,arXiv.org,,ZSCC: 0000072,,/Users/jacquesthibodeau/Zotero/storage/46F5BZ8U/Kwon et al. - 2018 - Expressing robot incapability.pdf; /Users/jacquesthibodeau/Zotero/storage/4NFN3ET4/Kwon et al. - 2018 - Expressing Robot Incapability.pdf; /Users/jacquesthibodeau/Zotero/storage/SU3ZGDE8/citation.html,,CHAI; TechSafety,Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
B4EIGIXS,conferencePaper,2021,"Knott, Paul; Carroll, Micah; Devlin, Sam; Ciosek, Kamil; Hofmann, Katja; Dragan, A. D.; Shah, Rohin",Evaluating the Robustness of Collaborative Agents,Proc. of the 20th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2021),,,,http://arxiv.org/abs/2101.05507,"In order for agents trained by deep reinforcement learning to work alongside humans in realistic settings, we will need to ensure that the agents are \emph{robust}. Since the real world is very diverse, and human behavior often changes in response to agent deployment, the agent will likely encounter novel situations that have never been seen during training. This results in an evaluation challenge: if we cannot rely on the average training or validation reward as a metric, then how can we effectively evaluate robustness? We take inspiration from the practice of \emph{unit testing} in software engineering. Specifically, we suggest that when designing AI agents that collaborate with humans, designers should search for potential edge cases in \emph{possible partner behavior} and \emph{possible states encountered}, and write tests which check that the behavior of the agent in these edge cases is reasonable. We apply this methodology to build a suite of unit tests for the Overcooked-AI environment, and use this test suite to evaluate three proposals for improving robustness. We find that the test suite provides significant insight into the effects of these proposals that were generally not revealed by looking solely at the average validation reward.",2021-01-14,2022-01-30 04:50:45,2022-01-30 04:50:45,2021-10-30 20:37:53,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000002  arXiv: 2101.05507,,/Users/jacquesthibodeau/Zotero/storage/ZKGWIRKG/Knott et al. - 2021 - Evaluating the Robustness of Collaborative Agents.pdf; /Users/jacquesthibodeau/Zotero/storage/ZC9RGFH2/2101.html,,TechSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Multiagent Systems; Computer Science - Human-Computer Interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AAMAS 2021,,,,,,,,,,,,,,,,
55SJ6TUU,conferencePaper,2017,"Palaniappan, Malayandi; Malik, Dhruv; Hadfield-Menell, Dylan; Dragan, Anca; Russell, Stuart",Efficient Cooperative Inverse Reinforcement Learning,Proc. ICML Work⁃ shop on Reliable Machine Learning in the Wild (2017),,,,,,2017,2022-01-30 04:50:44,2022-01-30 04:50:44,,5,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000004,,/Users/jacquesthibodeau/Zotero/storage/D7BTS6QX/Palaniappan et al. - Efficient Cooperative Inverse Reinforcement Learni.pdf,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UBRT43WR,conferencePaper,2020,"Halpern, Joseph Y.; Piermont, Evan",Dynamic Awareness,,,,,http://arxiv.org/abs/2007.02823,"We investigate how to model the beliefs of an agent who becomes more aware. We use the framework of Halpern and Rego (2013) by adding probability, and define a notion of a model transition that describes constraints on how, if an agent becomes aware of a new formula $\phi$ in state $s$ of a model $M$, she transitions to state $s^*$ in a model $M^*$. We then discuss how such a model can be applied to information disclosure.",2020-07-06,2022-01-30 04:50:44,2022-01-30 04:50:44,2020-12-19 02:26:06,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000003  arXiv: 2007.02823,,/Users/jacquesthibodeau/Zotero/storage/TIVQU7G4/Halpern and Piermont - 2020 - Dynamic Awareness.pdf; /Users/jacquesthibodeau/Zotero/storage/6C8IDR23/2007.html,,CHAI; TechSafety,Computer Science - Artificial Intelligence; Computer Science - Logic in Computer Science; Economics - Theoretical Economics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,17th International Conference on Principles of Knowledge Representation and Reasoning,,,,,,,,,,,,,,,,
W2N8KHAS,conferencePaper,2020,"Freire, Pedro; Gleave, Adam; Toyer, Sam; Russell, Stuart",DERAIL: Diagnostic Environments for Reward And Imitation Learning,Advances in Neural Information Processing Systems 33 Pre-proceedings,,,,http://arxiv.org/abs/2012.01365,"The objective of many real-world tasks is complex and difficult to procedurally specify. This makes it necessary to use reward or imitation learning algorithms to infer a reward or policy directly from human data. Existing benchmarks for these algorithms focus on realism, testing in complex environments. Unfortunately, these benchmarks are slow, unreliable and cannot isolate failures. As a complementary approach, we develop a suite of simple diagnostic tasks that test individual facets of algorithm performance in isolation. We evaluate a range of common reward and imitation learning algorithms on our tasks. Our results confirm that algorithm performance is highly sensitive to implementation details. Moreover, in a case-study into a popular preference-based reward learning implementation, we illustrate how the suite can pinpoint design flaws and rapidly evaluate candidate solutions. The environments are available at https://github.com/HumanCompatibleAI/seals .",2020-12-02,2022-01-30 04:50:44,2022-01-30 04:50:44,2020-12-18 00:37:38,,,,,,,DERAIL,,,,,,,,,,,,arXiv.org,,ZSCC: 0000002  arXiv: 2012.01365,,/Users/jacquesthibodeau/Zotero/storage/4ZCV83UD/Freire et al. - 2020 - DERAIL Diagnostic Environments for Reward And Imi.pdf; /Users/jacquesthibodeau/Zotero/storage/VVNRMKJH/2012.html; /Users/jacquesthibodeau/Zotero/storage/NDR2G3TZ/2012.html,,CHAI; TechSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Deep Reinforcement Learning Workshop at NeurIPS,,,,,,,,,,,,,,,,
EGTZPRSQ,conferencePaper,2016,"Hadfield-Menell, Dylan; Dragan, Anca; Abbeel, Pieter; Russell, Stuart",Cooperative Inverse Reinforcement Learning,Advances in Neural Information Processing Systems 29 (NIPS 2016),,,,https://proceedings.neurips.cc/paper/2016/hash/c3395dd46c34fa7fd8d729d8cf88b7a8-Abstract.html,"For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (CIRL). A CIRL problem is a cooperative, partial-information game with two agents, human and robot; both are rewarded according to the human's reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is suboptimal in CIRL, and derive an approximate CIRL algorithm.",2016-06-09,2022-01-30 04:50:44,2022-01-30 04:50:44,2020-12-21,,,,,,,,,,,,,,en,,,,,arxiv.org,,ZSCC: 0000411,,/Users/jacquesthibodeau/Zotero/storage/C32UQET4/Hadfield-Menell et al. - 2016 - Cooperative Inverse Reinforcement Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/N6SZFWKU/6420-cooperative-inverse-reinforcement-learning.html; /Users/jacquesthibodeau/Zotero/storage/X6JQE3BA/1606.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NIPS 2016,,,,,,,,,,,,,,,,
IVEVAANE,conferencePaper,2020,"Turner, Alexander Matt; Hadfield-Menell, Dylan; Tadepalli, Prasad",Conservative Agency via Attainable Utility Preservation,"AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3375627.3375851,http://arxiv.org/abs/1902.09725,"Reward functions are often misspeciﬁed. An agent optimizing an incorrect reward function can change its environment in large, undesirable, and potentially irreversible ways. Work on impact measurement seeks a means of identifying (and thereby avoiding) large changes to the environment. We propose a novel impact measure which induces conservative, effective behavior across a range of situations. The approach attempts to preserve the attainable utility of auxiliary objectives. We evaluate our proposal on an array of benchmark tasks and show that it matches or outperforms relative reachability, the state-of-the-art in impact measurement.",2020,2022-01-30 04:50:44,2022-01-30 04:50:44,2019-07-08 15:44:58,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000030  arXiv: 1902.09725,,/Users/jacquesthibodeau/Zotero/storage/4459ZMWD/Turner et al. - 2020 - Conservative Agency via Attainable Utility Preserv.pdf; /Users/jacquesthibodeau/Zotero/storage/375MJXAP/1902.html; /Users/jacquesthibodeau/Zotero/storage/7DTZW9IN/Turner et al. - 2019 - Conservative Agency via Attainable Utility Preserv.pdf,,CHAI; TechSafety,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KKKW3IZZ,conferencePaper,2021,"Zhuang, Simon; Hadﬁeld-Menell, Dylan",Consequences of Misaligned AI,Advances in Neural Information Processing Systems 33 (2020),,,,http://arxiv.org/abs/2102.03896,AI systems often rely on two key components: a speciﬁed goal or reward function and an optimization algorithm to compute the optimal behavior for that goal. This approach is intended to provide value for a principal: the user on whose behalf the agent acts. The objectives given to these agents often refer to a partial speciﬁcation of the principal’s goals. We consider the cost of this incompleteness by analyzing a model of a principal and an agent in a resource constrained world where the L attributes of the state correspond to different sources of utility for the principal. We assume that the reward function given to the agent only has support on J < L attributes. The contributions of our paper are as follows: 1) we propose a novel model of an incomplete principal—agent problem from artiﬁcial intelligence; 2) we provide necessary and sufﬁcient conditions under which indeﬁnitely optimizing for any incomplete proxy objective leads to arbitrarily low overall utility; and 3) we show how modifying the setup to allow reward functions that reference the full state or allowing the principal to update the proxy objective over time can lead to higher utility solutions. The results in this paper argue that we should view the design of reward functions as an interactive and dynamic process and identiﬁes a theoretical scenario where some degree of interactivity is desirable.,2021-02-07,2022-01-30 04:50:44,2022-01-30 04:50:44,,11,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000004,,/Users/jacquesthibodeau/Zotero/storage/8A4KHTU8/Zhuang and Hadﬁeld-Menell - Consequences of Misaligned AI.pdf,,CHAI; TechSafety,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2020,,,,,,,,,,,,,,,,
VKGPQCJR,conferencePaper,2021,"Dennis, Michael; Jaques, Natasha; Vinitsky, Eugene; Bayen, Alexandre; Russell, Stuart; Critch, Andrew; Levine, Sergey",Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design,arXiv:2012.02096 [cs],,,,http://arxiv.org/abs/2012.02096,"A wide range of reinforcement learning (RL) problems - including robustness, transfer learning, unsupervised RL, and emergent complexity - require specifying a distribution of tasks or environments in which a policy will be trained. However, creating a useful distribution of environments is error prone, and takes a significant amount of developer time and effort. We propose Unsupervised Environment Design (UED) as an alternative paradigm, where developers provide environments with unknown parameters, and these parameters are used to automatically produce a distribution over valid, solvable environments. Existing approaches to automatically generating environments suffer from common failure modes: domain randomization cannot generate structure or adapt the difficulty of the environment to the agent's learning progress, and minimax adversarial training leads to worst-case environments that are often unsolvable. To generate structured, solvable environments for our protagonist agent, we introduce a second, antagonist agent that is allied with the environment-generating adversary. The adversary is motivated to generate environments which maximize regret, defined as the difference between the protagonist and antagonist agent's return. We call our technique Protagonist Antagonist Induced Regret Environment Design (PAIRED). Our experiments demonstrate that PAIRED produces a natural curriculum of increasingly complex environments, and PAIRED agents achieve higher zero-shot transfer performance when tested in highly novel environments.",2021-02-03,2022-01-30 04:50:44,2022-01-30 04:50:44,2021-11-13 22:36:30,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s0]  ACC: 21  arXiv: 2012.02096,,/Users/jacquesthibodeau/Zotero/storage/23RGK32W/Dennis et al. - 2021 - Emergent Complexity and Zero-shot Transfer via Uns.pdf; /Users/jacquesthibodeau/Zotero/storage/8C39EB27/2012.html,,TechSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"34th Conference on Neural Information Processing Systems (NeurIPS 2020),",,,,,,,,,,,,,,,,
GIRDREWD,conferencePaper,2017,"Basu, C.; Yang, Q.; Hungerman, D.; Sinahal, M.; Draqan, A. D.",Do You Want Your Autonomous Car to Drive Like You?,2017 12th ACM/IEEE International Conference on Human-Robot Interaction (HRI,,,,,"With progress in enabling autonomous cars to drive safely on the road, it is time to start asking how they should be driving. A common answer is that they should be adopting their users' driving style. This makes the assumption that users want their autonomous cars to drive like they drive - aggressive drivers want aggressive cars, defensive drivers want defensive cars. In this paper, we put that assumption to the test. We find that users tend to prefer a significantly more defensive driving style than their own. Interestingly, they prefer the style they think is their own, even though their actual driving style tends to be more aggressive. We also find that preferences do depend on the specific driving scenario, opening the door for new ways of learning driving style preference.",2017-03,2022-01-30 04:50:44,2022-01-30 04:50:44,,417-425,,,,,,,,,,,,,,,,,,IEEE Xplore,,ZSCC: 0000092  ISSN: 2167-2148,,/Users/jacquesthibodeau/Zotero/storage/J3P5R8WX/Basu et al. - 2017 - Do You Want Your Autonomous Car to Drive Like You.pdf,,CHAI; TechSafety; AmbiguosSafety,Safety; actual driving style; aggressive cars; aggressive drivers; Atmospheric measurements; automobiles; Automobiles; Autonomous automobiles; autonomous car; autonomous cars; defensive cars; defensive drivers; defensive driving style; driving preferences; driving style; driving style preference learning; road safety; Roads; specific driving scenario; Task analysis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2017 12th ACM/IEEE International Conference on Human-Robot Interaction (HRI,,,,,,,,,,,,,,,,
V59HB96K,conferencePaper,2019,"Bourgin, David D.; Peterson, Joshua C.; Reichman, Daniel; Griffiths, Thomas L.; Russell, Stuart J.",Cognitive Model Priors for Predicting Human Decisions,Proceedings of the 36th International Conference on Machine Learning,,,,http://arxiv.org/abs/1905.09397,"Human decision-making underlies all economic behavior. For the past four decades, human decision-making under uncertainty has continued to be explained by theoretical models based on prospect theory, a framework that was awarded the Nobel Prize in Economic Sciences. However, theoretical models of this kind have developed slowly, and robust, high-precision predictive models of human decisions remain a challenge. While machine learning is a natural candidate for solving these problems, it is currently unclear to what extent it can improve predictions obtained by current theories. We argue that this is mainly due to data scarcity, since noisy human behavior requires massive sample sizes to be accurately captured by off-the-shelf machine learning methods. To solve this problem, what is needed are machine learning models with appropriate inductive biases for capturing human behavior, and larger datasets. We offer two contributions towards this end: first, we construct ""cognitive model priors"" by pretraining neural networks with synthetic data generated by cognitive models (i.e., theoretical models developed by cognitive psychologists). We find that fine-tuning these networks on small datasets of real human decisions results in unprecedented state-of-the-art improvements on two benchmark datasets. Second, we present the first large-scale dataset for human decision-making, containing over 240,000 human judgments across over 13,000 decision problems. This dataset reveals the circumstances where cognitive model priors are useful, and provides a new standard for benchmarking prediction of human decisions under uncertainty.",2019-05-22,2022-01-30 04:50:43,2022-01-30 04:50:43,2019-12-18 02:18:22,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000047  arXiv: 1905.09397,,/Users/jacquesthibodeau/Zotero/storage/J2ZDVPK2/Bourgin et al. - 2019 - Cognitive Model Priors for Predicting Human Decisi.pdf; /Users/jacquesthibodeau/Zotero/storage/T2SNNBCJ/1905.html,,CHAI; TechSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,36th International Conference on Machine Learning,,,,,,,,,,,,,,,,
MSC4I4M4,conferencePaper,2020,"Freedman, Rachel; Shah, Rohin; Dragan, Anca",Choice Set Misspeciﬁcation in Reward Inference,CEUR Workshop Proceedings,,,,http://ceur-ws.org/Vol-2640/paper_14.pdf,"Specifying reward functions for robots that operate in environments without a natural reward signal can be challenging, and incorrectly speciﬁed rewards can incentivise degenerate or dangerous behavior. A promising alternative to manually specifying reward functions is to enable robots to infer them from human feedback, like demonstrations or corrections. To interpret this feedback, robots treat as approximately optimal a choice the person makes from a choice set, like the set of possible trajectories they could have demonstrated or possible corrections they could have made. In this work, we introduce the idea that the choice set itself might be difﬁcult to specify, and analyze choice set misspeciﬁcation: what happens as the robot makes incorrect assumptions about the set of choices from which the human selects their feedback. We propose a classiﬁcation of different kinds of choice set misspeciﬁcation, and show that these different classes lead to meaningful differences in the inferred reward and resulting performance. While we would normally expect misspeciﬁcation to hurt, we ﬁnd that certain kinds of misspeciﬁcation are neither helpful nor harmful (in expectation). However, in other situations, misspeciﬁcation can be extremely harmful, leading the robot to believe the opposite of what it should believe. We hope our results will allow for better prediction and response to the effects of misspeciﬁcation in real-world reward inference.",2020,2022-01-30 04:50:43,2022-01-30 04:50:43,2020-12-18,7,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000001[s0],,/Users/jacquesthibodeau/Zotero/storage/NZSH77X3/Freedman et al. - Choice Set Misspeciﬁcation in Reward Inference.pdf,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CEUR Workshop,,,,,,,,,,,,,,,,
SSSCRXJG,conferencePaper,2020,"Shah, Rohin; Freire, Pedro; Alex, Neel; Freedman, Rachel; Krasheninnikov, Dmitrii; Chan, Lawrence; Dennis, Michael; Abbeel, Pieter; Dragan, Anca; Russell, Stuart",Benefits of Assistance over Reward Learning,,,,,https://openreview.net/forum?id=DFIoGDZejIB,"Much recent work has focused on how an agent can learn what to do from human feedback, leading to two major paradigms. The first paradigm is reward learning, in which the agent learns a reward...",2020-09-28,2022-01-30 04:50:43,2022-01-30 04:50:43,2020-12-18 00:39:34,,,,,,,,,,,,,,en,,,,,openreview.net,,ZSCC: 0000004,,/Users/jacquesthibodeau/Zotero/storage/X7AUG7ZJ/Anonymous - 2020 - Benefits of Assistance over Reward Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/EQCDNKBZ/forum.html,,CHAI; TechSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2020,,,,,,,,,,,,,,,,
VHS74F2I,conferencePaper,2021,"Verma, Pulkit; Marpally, Shashank Rao; Srivastava, Siddharth",Asking the Right Questions: Learning Interpretable Action Models Through Query Answering,arXiv:1912.12613 [cs],,,,http://arxiv.org/abs/1912.12613,"This paper develops a new approach for estimating an interpretable, relational model of a black-box autonomous agent that can plan and act. Our main contributions are a new paradigm for estimating such models using a minimal query interface with the agent, and a hierarchical querying algorithm that generates an interrogation policy for estimating the agent's internal model in a vocabulary provided by the user. Empirical evaluation of our approach shows that despite the intractable search space of possible agent models, our approach allows correct and scalable estimation of interpretable agent models for a wide class of black-box autonomous agents. Our results also show that this approach can use predicate classifiers to learn interpretable models of planning agents that represent states as images.",2021-04-09,2022-01-30 04:50:43,2022-01-30 04:50:43,2021-10-30 23:01:42,,,,,,,Asking the Right Questions,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s0]  ACC: 6  arXiv: 1912.12613,,/Users/jacquesthibodeau/Zotero/storage/V7GTH85I/Verma et al. - 2021 - Asking the Right Questions Learning Interpretable.pdf; /Users/jacquesthibodeau/Zotero/storage/BERGIMGS/1912.html,,TechSafety,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AAAI 2021,,,,,,,,,,,,,,,,
UVD25SIB,conferencePaper,2019,"Rahtz, Matthew; Fang, James; Dragan, Anca D.; Hadfield-Menell, Dylan",An Extensible Interactive Interface for Agent Design,,,,,http://arxiv.org/abs/1906.02641,"In artiﬁcial intelligence, we often specify tasks through a reward function. While this works well in some settings, many tasks are hard to specify this way. In deep reinforcement learning, for example, directly specifying a reward as a function of a high-dimensional observation is challenging. Instead, we present an interface for specifying tasks interactively using demonstrations. Our approach deﬁnes a set of increasingly complex policies. The interface allows the user to switch between these policies at ﬁxed intervals to generate demonstrations of novel, more complex, tasks. We train new policies based on these demonstrations and repeat the process. We present a case study of our approach in the Lunar Lander domain, and show that this simple approach can quickly learn a successful landing policy and outperforms an existing comparison-based deep RL method.",2019-06-06,2022-01-30 04:50:43,2022-01-30 04:50:43,2019-07-08 15:45:46,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000001  arXiv: 1906.02641,,/Users/jacquesthibodeau/Zotero/storage/5AZABBHX/Rahtz et al. - 2019 - An Extensible Interactive Interface for Agent Desi.pdf,,CHAI; TechSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Robotics; Computer Science - Human-Computer Interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 ICML Workshop on Human in the Loop Learning (HILL 2019),,,,,,,,,,,,,,,,
2W5PXMHE,conferencePaper,2018,"Malik, Dhruv; Palaniappan, Malayandi; Fisac, Jaime F.; Hadfield-Menell, Dylan; Russell, Stuart; Dragan, Anca D.","An Efficient, Generalized Bellman Update For Cooperative Inverse Reinforcement Learning",Proceedings of the 35th International Conference on Machine Learning,,,,http://arxiv.org/abs/1806.03820,"Our goal is for AI systems to correctly identify and act according to their human user’s objectives. Cooperative Inverse Reinforcement Learning (CIRL) formalizes this value alignment problem as a two-player game between a human and robot, in which only the human knows the parameters of the reward function: the robot needs to learn them as the interaction unfolds. Previous work showed that CIRL can be solved as a POMDP, but with an action space size exponential in the size of the reward parameter space. In this work, we exploit a speciﬁc property of CIRL—the human is a full information agent—to derive an optimality-preserving modiﬁcation to the standard Bellman update; this reduces the complexity of the problem by an exponential factor and allows us to relax CIRL’s assumption of human rationality. We apply this update to a variety of POMDP solvers and ﬁnd that it enables us to scale CIRL to non-trivial problems, with larger reward parameter spaces, and larger action spaces for both robot and human. In solutions to these larger problems, the human exhibits pedagogic (teaching) behavior, while the robot interprets it as such and attains higher value for the human.",2018-06-11,2022-01-30 04:50:43,2022-01-30 04:50:43,2019-07-12 00:13:10,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000019  arXiv: 1806.03820,,"/Users/jacquesthibodeau/Zotero/storage/CS44ETD8/Malik et al. - 2018 - An Efficient, Generalized Bellman Update For Coope.pdf; /Users/jacquesthibodeau/Zotero/storage/INHZCFF4/1806.html; /Users/jacquesthibodeau/Zotero/storage/K662JE53/1806.html",,CHAI; TechSafety,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICML 2018,,,,,,,,,,,,,,,,
FZBNIVSD,conferencePaper,2020,"Turner, Alexander Matt; Ratzlaff, Neale; Tadepalli, Prasad",Avoiding Side Effects in Complex Environments,Advances in Neural Information Processing Systems 33 pre-proceedings (NeurIPS 2020),,,,http://arxiv.org/abs/2006.06547,"Reward function speciﬁcation can be difﬁcult, even in simple environments. Realistic environments contain millions of states. Rewarding the agent for making a widget may be easy, but penalizing the multitude of possible negative side effects is hard. In toy environments, Attainable Utility Preservation (AUP) avoids side effects by penalizing shifts in the ability to achieve randomly generated goals. We scale this approach to large, randomly generated environments based on Conway’s Game of Life. By preserving optimal value for a single randomly generated reward function, AUP incurs modest overhead, completes the speciﬁed task, and avoids side effects.",2020-06-11,2022-01-30 04:50:43,2022-01-30 04:50:43,2020-08-31 18:07:40,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000011  arXiv: 2006.06547,,/Users/jacquesthibodeau/Zotero/storage/WW987S6M/Turner et al. - 2020 - Avoiding Side Effects in Complex Environments.pdf,,CHAI; TechSafety,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2020,,,,,,,,,,,,,,,,
W4R7FKMX,conferencePaper,2021,"Hendrycks, Dan; Burns, Collin; Basart, Steven; Critch, Andrew; Li, Jerry; Song, Dawn; Steinhardt, Jacob",Aligning AI With Shared Human Values,arXiv:2008.02275 [cs],,,,http://arxiv.org/abs/2008.02275,"We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the ETHICS dataset, we find that current language models have a promising but incomplete ability to predict basic human ethical judgements. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values.",2021-07-24,2022-01-30 04:50:42,2022-01-30 04:50:42,2021-10-30 21:58:26,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000001[s0]  arXiv: 2008.02275,,/Users/jacquesthibodeau/Zotero/storage/RC4VUUN5/Hendrycks et al. - 2020 - Aligning AI With Shared Human Values.pdf; /Users/jacquesthibodeau/Zotero/storage/XUW5SB9I/Hendrycks et al. - 2021 - Aligning AI With Shared Human Values.pdf; /Users/jacquesthibodeau/Zotero/storage/MSZT6G3R/2008.html; /Users/jacquesthibodeau/Zotero/storage/I6C3W2WW/2008.html,,CHAI; TechSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICLR 2021,,,,,,,,,,,,,,,,
P3T7ZT65,conferencePaper,2019,"Bahdanau, Dzmitry; Hill, Felix; Leike, Jan; Hughes, Edward; Hosseini, Arian; Kohli, Pushmeet; Grefenstette, Edward",Learning to Understand Goal Specifications by Modelling Reward,arXiv:1806.01946 [cs],,,,http://arxiv.org/abs/1806.01946,"Recent work has shown that deep reinforcement-learning agents can learn to follow language-like instructions from infrequent environment rewards. However, this places on environment designers the onus of designing language-conditional reward functions which may not be easily or tractably implemented as the complexity of the environment and the language scales. To overcome this limitation, we present a framework within which instruction-conditional RL agents are trained using rewards obtained not from the environment, but from reward models which are jointly trained from expert examples. As reward models improve, they learn to accurately reward agents for completing tasks for environment configurations---and for instructions---not present amongst the expert data. This framework effectively separates the representation of what instructions require from how they can be executed. In a simple grid world, it enables an agent to learn a range of commands requiring interaction with blocks and understanding of spatial relations and underspecified abstract arrangements. We further show the method allows our agent to adapt to changes in the environment without requiring new expert examples.",2019-02-15,2022-01-30 04:52:39,2022-01-30 04:52:39,2019-12-16 20:32:35,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s3]  ACC: 73  J: 33 arXiv: 1806.01946,,/Users/jacquesthibodeau/Zotero/storage/MG92TPH7/Bahdanau et al. - 2019 - Learning to Understand Goal Specifications by Mode.pdf; /Users/jacquesthibodeau/Zotero/storage/Z4PZME9T/1806.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICLR 2019,,,,,,,,,,,,,,,,
KIWMVR6M,conferencePaper,2020,"Anthony, Thomas; Eccles, Tom; Tacchetti, Andrea; Kramár, János; Gemp, Ian; Hudson, Thomas C.; Porcel, Nicolas; Lanctot, Marc; Pérolat, Julien; Everett, Richard; Werpachowski, Roman; Singh, Satinder; Graepel, Thore; Bachrach, Yoram",Learning to Play No-Press Diplomacy with Best Response Policy Iteration,34th Conference on Neural Information Processing Systems (NeurIPS 2020),,,,http://arxiv.org/abs/2006.04635,"Recent advances in deep reinforcement learning (RL) have led to considerable progress in many 2-player zero-sum games, such as Go, Poker and Starcraft. The purely adversarial nature of such games allows for conceptually simple and principled application of RL methods. However real-world settings are many-agent, and agent interactions are complex mixtures of common-interest and competitive aspects. We consider Diplomacy, a 7-player board game designed to accentuate dilemmas resulting from many-agent interactions. It also features a large combinatorial action space and simultaneous moves, which are challenging for RL algorithms. We propose a simple yet effective approximate best response operator, designed to handle large combinatorial action spaces and simultaneous moves. We also introduce a family of policy iteration methods that approximate ﬁctitious play. With these methods, we successfully apply RL to Diplomacy: we show that our agents convincingly outperform the previous state-of-the-art, and game theoretic equilibrium analysis shows that the new process yields consistent improvements.",2020-08-26,2022-01-30 04:52:39,2022-01-30 04:52:39,2020-08-31 17:58:54,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: NoCitationData[s2]  ACC: 16  arXiv: 2006.04635,,/Users/jacquesthibodeau/Zotero/storage/ZMDHG4BH/Anthony et al. - 2020 - Learning to Play No-Press Diplomacy with Best Resp.pdf,,TechSafety; DeepMind; AmbiguosSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2020,,,,,,,,,,,,,,,,
FVAQUPVW,conferencePaper,2019,"Everitt, Tom; Kumar, Ramana; Krakovna, Victoria; Legg, Shane",Modeling AGI Safety Frameworks with Causal Influence Diagrams,arXiv:1906.08663 [cs],,,,http://arxiv.org/abs/1906.08663,"Proposals for safe AGI systems are typically made at the level of frameworks, specifying how the components of the proposed system should be trained and interact with each other. In this paper, we model and compare the most promising AGI safety frameworks using causal influence diagrams. The diagrams show the optimization objective and causal assumptions of the framework. The unified representation permits easy comparison of frameworks and their assumptions. We hope that the diagrams will serve as an accessible and visual introduction to the main AGI safety frameworks.",2019-06-20,2022-01-30 04:52:39,2022-01-30 04:52:39,2019-12-16 20:27:05,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000014  arXiv: 1906.08663,,/Users/jacquesthibodeau/Zotero/storage/2GA3KWFC/Everitt et al. - 2019 - Modeling AGI Safety Frameworks with Causal Influen.pdf; /Users/jacquesthibodeau/Zotero/storage/J3KK437V/1906.html; /Users/jacquesthibodeau/Zotero/storage/9UU2WGKD/1906.html,,TechSafety; DeepMind,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IJCAI 2019 AI Safety Workshop,,,,,,,,,,,,,,,,
I2AKM3D5,conferencePaper,2019,"Wang, Chenglong; Bunel, Rudy; Dvijotham, Krishnamurthy; Huang, Po-Sen; Grefenstette, Edward; Kohli, Pushmeet",Knowing When to Stop: Evaluation and Verification of Conformity to Output-size Specifications,Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,,,,https://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Knowing_When_to_Stop_Evaluation_and_Verification_of_Conformity_to_CVPR_2019_paper.html,"Models such as Sequence-to-Sequence and Image-to-Sequence are widely used in real world applications. While the ability of these neural architectures to produce variable-length outputs makes them extremely effective for problems like Machine Translation and Image Captioning, it also leaves them vulnerable to failures of the form where the model produces outputs of undesirable length. This behavior can have severe consequences such as usage of increased computation and induce faults in downstream modules that expect outputs of a certain length. Motivated by the need to have a better understanding of the failures of these models, this paper proposes and studies the novel output-size modulation problem and makes two key technical contributions. First, to evaluate model robustness, we develop an easy-to-compute differentiable proxy objective that can be used with gradient-based algorithms to find output-lengthening inputs. Second and more importantly, we develop a verification approach that can formally verify whether a network always produces outputs within a certain length. Experimental results on Machine Translation and Image Captioning show that our output-lengthening approach can produce outputs that are 50 times longer than the input, while our verification approach can, given a model and input domain, prove that the output length is below a certain size.",2019-04-26,2022-01-30 04:52:38,2022-01-30 04:52:38,2020-12-20,,,,,,,Knowing When to Stop,,,,,,,,,,,,arXiv.org,,ZSCC: 0000010  arXiv: 1904.12004,,/Users/jacquesthibodeau/Zotero/storage/5RJNUC4N/Wang et al. - 2019 - Knowing When to Stop Evaluation and Verification .pdf; /Users/jacquesthibodeau/Zotero/storage/TGZNQ5NZ/1904.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IEEE/CVF Conference on Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,
TF8PSWUZ,conferencePaper,2019,"Nalisnick, Eric; Matsukawa, Akihiro; Teh, Yee Whye; Gorur, Dilan; Lakshminarayanan, Balaji",Hybrid Models with Deep and Invertible Features,"arXiv:1902.02767 [cs, stat]",,,,http://arxiv.org/abs/1902.02767,"We propose a neural hybrid model consisting of a linear model defined on a set of features computed by a deep, invertible transformation (i.e. a normalizing flow). An attractive property of our model is that both p(features), the density of the features, and p(targets | features), the predictive distribution, can be computed exactly in a single feed-forward pass. We show that our hybrid model, despite the invertibility constraints, achieves similar accuracy to purely predictive models. Moreover the generative component remains a good model of the input features despite the hybrid optimization objective. This offers additional capabilities such as detection of out-of-distribution inputs and enabling semi-supervised learning. The availability of the exact joint density p(targets, features) also allows us to compute many quantities readily, making our hybrid model a useful building block for downstream applications of probabilistic deep learning.",2019-05-29,2022-01-30 04:52:38,2022-01-30 04:52:38,2019-12-16 20:32:50,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000045  arXiv: 1902.02767,,/Users/jacquesthibodeau/Zotero/storage/VUE9U2IJ/Nalisnick et al. - 2019 - Hybrid Models with Deep and Invertible Features.pdf; /Users/jacquesthibodeau/Zotero/storage/V56US5SD/1902.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICML 2019,,,,,,,,,,,,,,,,
2BNK79XQ,conferencePaper,2019,"Nalisnick, Eric; Matsukawa, Akihiro; Teh, Yee Whye; Gorur, Dilan; Lakshminarayanan, Balaji",Do Deep Generative Models Know What They Don't Know?,"arXiv:1810.09136 [cs, stat]",,,,http://arxiv.org/abs/1810.09136,"A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data. A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong. Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel, out-of-distribution inputs. In this paper we challenge this assumption. We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former. Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN. To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood. We find such behavior persists even when we restrict the flows to constant-volume transformations. These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.",2019-02-24,2022-01-30 04:52:38,2022-01-30 04:52:38,2019-12-16 20:34:40,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s0]  ACC: 333  J: 130 arXiv: 1810.09136,,/Users/jacquesthibodeau/Zotero/storage/DKICH6Q7/Nalisnick et al. - 2019 - Do Deep Generative Models Know What They Don't Kno.pdf; /Users/jacquesthibodeau/Zotero/storage/U8767CAD/1810.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICLR 2019,,,,,,,,,,,,,,,,
6MVP5BIM,conferencePaper,2017,"Christiano, Paul; Leike, Jan; Brown, Tom B.; Martic, Miljan; Legg, Shane; Amodei, Dario",Deep reinforcement learning from human preferences,Advances in Neural Information Processing Systems 30 (NIPS 2017),,,,https://papers.nips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html,"For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.",2017-07-13,2022-01-30 04:52:38,2022-01-30 04:52:38,2020-12-20,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000517  arXiv: 1706.03741,,/Users/jacquesthibodeau/Zotero/storage/64K9P9TW/Christiano et al. - 2017 - Deep reinforcement learning from human preferences.pdf; /Users/jacquesthibodeau/Zotero/storage/73TQPCTW/Christiano et al. - 2017 - Deep reinforcement learning from human preferences.pdf; /Users/jacquesthibodeau/Zotero/storage/CZG2MJHH/1706.html; /Users/jacquesthibodeau/Zotero/storage/DBXZUTUS/1706.html; /Users/jacquesthibodeau/Zotero/storage/2SRIU7JU/1706.html,,TechSafety; Open-AI; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Human-Computer Interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NIPS 2017,,,,,,,,,,,,,,,,
4XQSZS4G,conferencePaper,2020,"Krakovna, Victoria; Orseau, Laurent; Ngo, Richard; Martic, Miljan; Legg, Shane",Avoiding Side Effects By Considering Future Tasks,,,,,https://arxiv.org/abs/2010.07877v1,"Designing reward functions is difficult: the designer has to specify what to do (what it means to complete the task) as well as what not to do (side effects that should be avoided while completing the task). To alleviate the burden on the reward designer, we propose an algorithm to automatically generate an auxiliary reward function that penalizes side effects. This auxiliary objective rewards the ability to complete possible future tasks, which decreases if the agent causes side effects during the current task. The future task reward can also give the agent an incentive to interfere with events in the environment that make future tasks less achievable, such as irreversible actions by other agents. To avoid this interference incentive, we introduce a baseline policy that represents a default course of action (such as doing nothing), and use it to filter out future tasks that are not achievable by default. We formally define interference incentives and show that the future task approach with a baseline policy avoids these incentives in the deterministic case. Using gridworld environments that test for side effects and interference, we show that our method avoids interference and is more effective for avoiding side effects than the common approach of penalizing irreversible actions.",2020-10-15,2022-01-30 04:52:37,2022-01-30 04:52:37,2020-11-14 01:21:10,,,,,,,,,,,,,,en,,,,,arxiv.org,,ZSCC: 0000008,,/Users/jacquesthibodeau/Zotero/storage/GD35BH4A/Krakovna et al. - 2020 - Avoiding Side Effects By Considering Future Tasks.pdf; /Users/jacquesthibodeau/Zotero/storage/JGBTUMMS/2010.html; /Users/jacquesthibodeau/Zotero/storage/ZKJFR6KA/2010.html,,TechSafety; DeepMind; AmbiguosSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,34th Conference on Neural Information Processing Systems (NeurIPS 2020),,,,,,,,,,,,,,,,
4C7IP8P3,conferencePaper,2019,"Ovadia, Yaniv; Fertig, Emily; Ren, Jie; Nado, Zachary; Sculley, D.; Nowozin, Sebastian; Dillon, Joshua V.; Lakshminarayanan, Balaji; Snoek, Jasper",Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift,"Advances in Neural Information Processing Systems, 2019",,,,http://arxiv.org/abs/1906.02530,"Modern machine learning methods including deep learning have achieved great success in predictive accuracy for supervised learning tasks, but may still fall short in giving useful estimates of their predictive {\em uncertainty}. Quantifying uncertainty is especially critical in real-world settings, which often involve input distributions that are shifted from the training distribution due to a variety of factors including sample bias and non-stationarity. In such settings, well calibrated uncertainty estimates convey information about when a model's output should (or should not) be trusted. Many probabilistic deep learning methods, including Bayesian-and non-Bayesian methods, have been proposed in the literature for quantifying predictive uncertainty, but to our knowledge there has not previously been a rigorous large-scale empirical comparison of these methods under dataset shift. We present a large-scale benchmark of existing state-of-the-art methods on classification problems and investigate the effect of dataset shift on accuracy and calibration. We find that traditional post-hoc calibration does indeed fall short, as do several other previous methods. However, some methods that marginalize over models give surprisingly strong results across a broad spectrum of tasks.",2019-06-06,2022-01-30 04:52:37,2022-01-30 04:52:37,2019-12-16 20:32:03,,,,,,,Can You Trust Your Model's Uncertainty?,,,,,,,,,,,,arXiv.org,,ZSCC: 0000558  arXiv: 1906.02530,,/Users/jacquesthibodeau/Zotero/storage/TVUXA7PD/Ovadia et al. - 2019 - Can You Trust Your Model's Uncertainty Evaluating.pdf; /Users/jacquesthibodeau/Zotero/storage/E986QDBI/1906.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2019,,,,,,,,,,,,,,,,
N3S2NM97,conferencePaper,2016,"Everitt, Tom; Hutter, Marcus",Avoiding Wireheading with Value Reinforcement Learning,AGI 2016: Artificial General Intelligence,,,,http://arxiv.org/abs/1605.03143,"How can we design good goals for arbitrarily intelligent agents? Reinforcement learning (RL) is a natural approach. Unfortunately, RL does not work well for generally intelligent agents, as RL agents are incentivised to shortcut the reward sensor for maximum reward -- the so-called wireheading problem. In this paper we suggest an alternative to RL called value reinforcement learning (VRL). In VRL, agents use the reward signal to learn a utility function. The VRL setup allows us to remove the incentive to wirehead by placing a constraint on the agent's actions. The constraint is defined in terms of the agent's belief distributions, and does not require an explicit specification of which actions constitute wireheading.",2016-05-10,2022-01-30 04:52:37,2022-01-30 04:52:37,2020-11-21 17:36:45,,,,,,,,Lecture Notes in Computer Science,,,,,,,,,,,arXiv.org,,ZSCC: 0000033  arXiv: 1605.03143,,/Users/jacquesthibodeau/Zotero/storage/RR5BW54G/Everitt and Hutter - 2016 - Avoiding Wireheading with Value Reinforcement Lear.pdf; /Users/jacquesthibodeau/Zotero/storage/8GNCR3DI/Everitt and Hutter - 2016 - Avoiding Wireheading with Value Reinforcement Lear.pdf; /Users/jacquesthibodeau/Zotero/storage/8USE4KKA/1605.html; /Users/jacquesthibodeau/Zotero/storage/FHW8U7HU/1605.html,,TechSafety; DeepMind,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,International Conference on Artificial General Intelligence,,,,,,,,,,,,,,,,
R26NQ63Z,conferencePaper,2019,"Hendrycks, Dan; Mu, Norman; Cubuk, Ekin D.; Zoph, Barret; Gilmer, Justin; Lakshminarayanan, Balaji",AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty,"arXiv:1912.02781 [cs, stat]",,,,http://arxiv.org/abs/1912.02781,"Modern deep neural networks can achieve high accuracy when the training distribution and test distribution are identically distributed, but this assumption is frequently violated in practice. When the train and test distributions are mismatched, accuracy can plummet. Currently there are few techniques that improve robustness to unforeseen data shifts encountered during deployment. In this work, we propose a technique to improve the robustness and uncertainty estimates of image classifiers. We propose AugMix, a data processing technique that is simple to implement, adds limited computational overhead, and helps models withstand unforeseen corruptions. AugMix significantly improves robustness and uncertainty measures on challenging image classification benchmarks, closing the gap between previous methods and the best possible performance in some cases by more than half.",2019-12-05,2022-01-30 04:52:37,2022-01-30 04:52:37,2019-12-16 20:30:48,,,,,,,AugMix,,,,,,,,,,,,arXiv.org,,ZSCC: 0000301  arXiv: 1912.02781,,/Users/jacquesthibodeau/Zotero/storage/CZEG4CV8/Hendrycks et al. - 2019 - AugMix A Simple Data Processing Method to Improve.pdf; /Users/jacquesthibodeau/Zotero/storage/9AGD2DPA/1912.html,,TechSafety; DeepMind; AmbiguosSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICLR 2020,,,,,,,,,,,,,,,,
CPPZDHR7,conferencePaper,2018,"Dvijotham, Krishnamurthy; Stanforth, Robert; Gowal, Sven; Mann, Timothy; Kohli, Pushmeet",A Dual Approach to Scalable Verification of Deep Networks,,,,,http://auai.org/uai2018/proceedings/papers/204.pdf,"This paper addresses the problem of formally verifying desirable properties of neural networks, i.e., obtaining provable guarantees that neural networks satisfy specifications relating their inputs and outputs (robustness to bounded norm adversarial perturbations, for example). Most previous work on this topic was limited in its applicability by the size of the network, network architecture and the complexity of properties to be verified. In contrast, our framework applies to a general class of activation functions and specifications on neural network inputs and outputs. We formulate verification as an optimization problem (seeking to find the largest violation of the specification) and solve a Lagrangian relaxation of the optimization problem to obtain an upper bound on the worst case violation of the specification being verified. Our approach is anytime i.e. it can be stopped at any time and a valid bound on the maximum violation can be obtained. We develop specialized verification algorithms with provable tightness guarantees under special assumptions and demonstrate the practical significance of our general verification approach on a variety of verification tasks.",2018-08-03,2022-01-30 04:52:36,2022-01-30 04:52:36,2020-12-20,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000274  arXiv: 1803.06567,,/Users/jacquesthibodeau/Zotero/storage/JI7PP7JX/Krishnamurthy et al. - 2018 - A Dual Approach to Scalable Verification of Deep N.pdf; /Users/jacquesthibodeau/Zotero/storage/WD27UPBX/1803.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Conference on Uncertainty in Artificial Intelligence,,,,,,,,,,,,,,,,
DJDTSQA3,conferencePaper,2019,"Qin, Chongli; Martens, James; Gowal, Sven; Krishnan, Dilip; Dvijotham, Krishnamurthy; Fawzi, Alhussein; De, Soham; Stanforth, Robert; Kohli, Pushmeet",Adversarial Robustness through Local Linearization,Advances in Neural Information Processing Systems 32 (NeurIPS 2019),,,,https://proceedings.neurips.cc/paper/2019/hash/0defd533d51ed0a10c5c9dbf93ee78a5-Abstract.html,"Adversarial training is an effective methodology for training deep neural networks that are robust against adversarial, norm-bounded perturbations. However, the computational cost of adversarial training grows prohibitively as the size of the model and number of input dimensions increase. Further, training against less expensive and therefore weaker adversaries produces models that are robust against weak attacks but break down under attacks that are stronger. This is often attributed to the phenomenon of gradient obfuscation; such models have a highly non-linear loss surface in the vicinity of training examples, making it hard for gradient-based attacks to succeed even though adversarial examples still exist. In this work, we introduce a novel regularizer that encourages the loss to behave linearly in the vicinity of the training data, thereby penalizing gradient obfuscation while encouraging robustness. We show via extensive experiments on CIFAR-10 and ImageNet, that models trained with our regularizer avoid gradient obfuscation and can be trained significantly faster than adversarial training. Using this regularizer, we exceed current state of the art and achieve 47% adversarial accuracy for ImageNet with l-infinity adversarial perturbations of radius 4/255 under an untargeted, strong, white-box attack. Additionally, we match state of the art results for CIFAR-10 at 8/255.",2019-10-10,2022-01-30 04:52:36,2022-01-30 04:52:36,2020-12-20,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000145  arXiv: 1907.02610,,/Users/jacquesthibodeau/Zotero/storage/KJHF9RJB/Qin et al. - 2019 - Adversarial Robustness through Local Linearization.pdf; /Users/jacquesthibodeau/Zotero/storage/QRC2SPTZ/1907.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2019,,,,,,,,,,,,,,,,
PHTMMSBH,conferencePaper,2018,"Uesato, Jonathan; O'Donoghue, Brendan; Oord, Aaron van den; Kohli, Pushmeet",Adversarial Risk and the Dangers of Evaluating Against Weak Attacks,Proceedings of the 35th International Conference on Machine Learning,,,,http://arxiv.org/abs/1802.05666,"This paper investigates recently proposed approaches for defending against adversarial examples and evaluating adversarial robustness. We motivate 'adversarial risk' as an objective for achieving models robust to worst-case inputs. We then frame commonly used attacks and evaluation metrics as defining a tractable surrogate objective to the true adversarial risk. This suggests that models may optimize this surrogate rather than the true adversarial risk. We formalize this notion as 'obscurity to an adversary,' and develop tools and heuristics for identifying obscured models and designing transparent models. We demonstrate that this is a significant problem in practice by repurposing gradient-free optimization techniques into adversarial attacks, which we use to decrease the accuracy of several recently proposed defenses to near zero. Our hope is that our formulations and results will help researchers to develop more powerful defenses.",2018-06-12,2022-01-30 04:52:36,2022-01-30 04:52:36,2019-12-16 20:35:22,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000335  arXiv: 1802.05666,,/Users/jacquesthibodeau/Zotero/storage/HKXXJPHG/Uesato et al. - 2018 - Adversarial Risk and the Dangers of Evaluating Aga.pdf; /Users/jacquesthibodeau/Zotero/storage/QXA64E5S/1802.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Cryptography and Security,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,35th International Conference on Machine Learning,,,,,,,,,,,,,,,,
FT98MHA9,conferencePaper,2019,"Huang, Po-Sen; Stanforth, Robert; Welbl, Johannes; Dyer, Chris; Yogatama, Dani; Gowal, Sven; Dvijotham, Krishnamurthy; Kohli, Pushmeet",Achieving Verified Robustness to Symbol Substitutions via Interval Bound Propagation,"arXiv:1909.01492 [cs, stat]",,,,http://arxiv.org/abs/1909.01492,"Neural networks are part of many contemporary NLP systems, yet their empirical successes come at the price of vulnerability to adversarial attacks. Previous work has used adversarial training and data augmentation to partially mitigate such brittleness, but these are unlikely to find worst-case adversaries due to the complexity of the search space arising from discrete text perturbations. In this work, we approach the problem from the opposite direction: to formally verify a system's robustness against a predefined class of adversarial attacks. We study text classification under synonym replacements or character flip perturbations. We propose modeling these input perturbations as a simplex and then using Interval Bound Propagation -- a formal model verification method. We modify the conventional log-likelihood training objective to train models that can be efficiently verified, which would otherwise come with exponential search complexity. The resulting models show only little difference in terms of nominal accuracy, but have much improved verified accuracy under perturbations and come with an efficiently computable formal guarantee on worst case adversaries.",2019-09-03,2022-01-30 04:52:36,2022-01-30 04:52:36,2019-12-16 20:31:13,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000074  arXiv: 1909.01492,,/Users/jacquesthibodeau/Zotero/storage/SB3JWI7M/Huang et al. - 2019 - Achieving Verified Robustness to Symbol Substituti.pdf; /Users/jacquesthibodeau/Zotero/storage/UJR69ER4/1909.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computation and Language; Computer Science - Cryptography and Security,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,EMNLP 2019,,,,,,,,,,,,,,,,
36ZPFWSH,conferencePaper,2018,"Ryffel, Theo; Trask, Andrew; Dahl, Morten; Wagner, Bobby; Mancuso, Jason; Rueckert, Daniel; Passerat-Palmbach, Jonathan",A generic framework for privacy preserving deep learning,"arXiv:1811.04017 [cs, stat]",,,,http://arxiv.org/abs/1811.04017,"We detail a new framework for privacy preserving deep learning and discuss its assets. The framework puts a premium on ownership and secure processing of data and introduces a valuable representation based on chains of commands and tensors. This abstraction allows one to implement complex privacy preserving constructs such as Federated Learning, Secure Multiparty Computation, and Differential Privacy while still exposing a familiar deep learning API to the end-user. We report early results on the Boston Housing and Pima Indian Diabetes datasets. While the privacy features apart from Differential Privacy do not impact the prediction accuracy, the current implementation of the framework introduces a significant overhead in performance, which will be addressed at a later stage of the development. We believe this work is an important milestone introducing the first reliable, general framework for privacy preserving deep learning.",2018-11-13,2022-01-30 04:52:36,2022-01-30 04:52:36,2019-12-16 20:34:29,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000218  arXiv: 1811.04017,,/Users/jacquesthibodeau/Zotero/storage/5HC3K36N/Ryffel et al. - 2018 - A generic framework for privacy preserving deep le.pdf; /Users/jacquesthibodeau/Zotero/storage/QQWKKRKX/1811.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Cryptography and Security,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,PPML 2018,,,,,,,,,,,,,,,,
54TFE9DS,conferencePaper,2021,"Oesterheld, Caspar; Conitzer, Vincent",Safe Pareto Improvements for Delegated Game Playing,Proc. of the 20th International Conference on Autonomous Agents and Multiagent Systems,,,,,"A set of players delegate playing a game to a set of representatives, one for each player. We imagine that each player trusts their respective representative’s strategic abilities. Thus, we might imagine that per default, the original players would simply instruct the representatives to play the original game as best as they can. In this paper, we ask: are there safe Pareto improvements on this default way of giving instructions? That is, we imagine that the original players can coordinate to tell their representatives to only consider some subset of the available strategies and to assign utilities to outcomes differently than the original players. Then can the original players do this in such a way that the payoff is guaranteed to be weakly higher than under the default instructions for all the original players? In particular, can they Pareto-improve without probabilistic assumptions about how the representatives play games? In this paper, we give some examples of safe Pareto improvements. We prove that the notion of safe Pareto improvements is closely related to a notion of outcome correspondence between games. We also show that under some specific assumptions about how the representatives play games, finding safe Pareto improvements is NP-complete.",2021,2022-01-30 04:51:37,2022-01-30 04:51:37,,17,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/U9S7EJHV/Oesterheld and Conitzer - 2021 - Safe Pareto Improvements for Delegated Game Playin.pdf,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AAMAS 2021,,,,,,,,,,,,,,,,
HH7IJ84S,conferencePaper,2015,"Garrabrant, Scott; Bhaskar, Siddharth; Demski, Abram; Garrabrant, Joanna; Koleszarik, George; Lloyd, Evan",Asymptotic Logical Uncertainty and The Benford Test,Artificial General Intelligence. AGI 2016,,,,http://arxiv.org/abs/1510.03370,"We give an algorithm A which assigns probabilities to logical sentences. For any simple infinite sequence of sentences whose truth-values appear indistinguishable from a biased coin that outputs ""true"" with probability p, we have that the sequence of probabilities that A assigns to these sentences converges to p.",2015-10-12,2022-01-30 04:56:47,2022-01-30 04:56:47,2020-12-13 20:21:31,,,,,,,,Lecture Notes in Computer Science,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s1]  ACC: 7  arXiv: 1510.03370,,/Users/jacquesthibodeau/Zotero/storage/FMECSHD8/Garrabrant et al. - 2015 - Asymptotic Logical Uncertainty and The Benford Tes.pdf; /Users/jacquesthibodeau/Zotero/storage/74HXXTRR/1510.html,,TechSafety; MIRI,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; F.4.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,International Conference on Artificial General Intelligence,,,,,,,,,,,,,,,,
UBDUGMDK,conferencePaper,2016,"Leike, Jan; Taylor, Jessica; Fallenstein, Benya",A Formal Solution to the Grain of Truth Problem,,,,,,"A Bayesian agent acting in a multi-agent environment learns to predict the other agents’ policies if its prior assigns positive probability to them (in other words, its prior contains a grain of truth). Finding a reasonably large class of policies that contains the Bayes-optimal policies with respect to this class is known as the grain of truth problem. Only small classes are known to have a grain of truth and the literature contains several related impossibility results. In this paper we present a formal and general solution to the full grain of truth problem: we construct a class of policies that contains all computable policies as well as Bayes-optimal policies for every lower semicomputable prior over the class. When the environment is unknown, Bayes-optimal agents may fail to act optimally even asymptotically. However, agents based on Thompson sampling converge to play ε-Nash equilibria in arbitrary unknown computable multi-agent environments. While these results are purely theoretical, we show that they can be computationally approximated arbitrarily closely.",2016,2022-01-30 04:56:46,2022-01-30 04:56:46,,10,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000011,,/Users/jacquesthibodeau/Zotero/storage/IKQJTV93/Leike et al. - A Formal Solution to the Grain of Truth Problem.pdf,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Conference on Uncertainty in Artificial Intelligence,,,,,,,,,,,,,,,,
Z75EWDEP,conferencePaper,2018,"Baum, Seth; Barrett, Anthony M",Towards an Integrated Assessment of Global Catastrophic Risk,"Catastrophic and Existential Risk: Proceedings of the First Colloquium, Garrick Institute for the Risk Sciences, University of California, Los Angeles, Forthcoming",,,,,,2018-01-17,2022-01-30 04:55:20,2022-01-30 04:55:20,,18,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000007,,/Users/jacquesthibodeau/Zotero/storage/JJ2WQ35R/Baum and Barrett - 2017 - Towards an integrated assessment of global catastr.pdf; /Users/jacquesthibodeau/Zotero/storage/BQ3ZMFE4/papers.html,,MetaSafety; GCRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,First International Colloquium on Catastrophic and Existential Risk,,,,,,,,,,,,,,,,
PA537GRN,conferencePaper,2021,"Owe, Andrea; Baum, Seth",The Ethics of Sustainability for Artificial Intelligence,"Proceedings of AI for People: Towards Sustainable AI, CAIP’21.",,,,https://gcrinstitute.org/the-ethics-of-sustainability-for-artificial-intelligence/,"Sustainability is widely considered a good thing and is therefore a matter of ethical significance. This paper analyzes the ethical dimensions of existing work on AI and sustainability, finding that most of it is focused on sustaining the environment for human benefit. The paper calls for sustainability that is not human-centric and that extends into the distant future, especially for advanced future AI as a technology that can advance expansion beyond Earth.",2021,2022-01-30 04:55:20,2022-01-30 04:55:20,2021-12-11 14:23:38,,,,,,,,,,,,,,en-US,,,,,,,ZSCC: 0000001,,/Users/jacquesthibodeau/Zotero/storage/638RA9I3/the-ethics-of-sustainability-for-artificial-intelligence.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CAIP'21,,,,,,,,,,,,,,,,
C95SMCT7,conferencePaper,2019,"Sarafian, Elad; Tamar, Aviv; Kraus, Sarit",Constrained Policy Improvement for Safe and Efficient Reinforcement Learning,Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence,,,,http://arxiv.org/abs/1805.07805,"We propose a policy improvement algorithm for Reinforcement Learning (RL) which is called Rerouted Behavior Improvement (RBI). RBI is designed to take into account the evaluation errors of the Q-function. Such errors are common in RL when learning the $Q$-value from finite past experience data. Greedy policies or even constrained policy optimization algorithms which ignore these errors may suffer from an improvement penalty (i.e. a negative policy improvement). To minimize the improvement penalty, the RBI idea is to attenuate rapid policy changes of low probability actions which were less frequently sampled. This approach is shown to avoid catastrophic performance degradation and reduce regret when learning from a batch of past experience. Through a two-armed bandit with Gaussian distributed rewards example, we show that it also increases data efficiency when the optimal action has a high variance. We evaluate RBI in two tasks in the Atari Learning Environment: (1) learning from observations of multiple behavior policies and (2) iterative RL. Our results demonstrate the advantage of RBI over greedy policies and other constrained policy optimization algorithms as a safe learning approach and as a general data efficient learning algorithm. An anonymous Github repository of our RBI implementation is found at https://github.com/eladsar/rbi.",2019-07-10,2022-01-30 04:59:45,2022-01-30 04:59:45,2020-11-14 00:52:37,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s1]  ACC: 0  arXiv: 1805.07805,,/Users/jacquesthibodeau/Zotero/storage/DC967S4Q/Sarafian et al. - 2019 - Constrained Policy Improvement for Safe and Effici.pdf; /Users/jacquesthibodeau/Zotero/storage/KNEPSHIS/1805.html,,TechSafety; AmbiguosSafety; Other-org,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Twenty-Ninth International Joint Conference on Artificial Intelligence,,,,,,,,,,,,,,,,
V49WBQ7S,conferencePaper,2020,"Brown, Noam; Bakhtin, Anton; Lerer, Adam; Gong, Qucheng",Combining Deep Reinforcement Learning and Search for Imperfect-Information Games,34th Conference on Neural Information Processing Systems (NeurIPS 2020),,,,http://arxiv.org/abs/2007.13544,"The combination of deep reinforcement learning and search at both training and test time is a powerful paradigm that has led to a number of a successes in single-agent settings and perfect-information games, best exemplified by the success of AlphaZero. However, algorithms of this form have been unable to cope with imperfect-information games. This paper presents ReBeL, a general framework for self-play reinforcement learning and search for imperfect-information games. In the simpler setting of perfect-information games, ReBeL reduces to an algorithm similar to AlphaZero. Results show ReBeL leads to low exploitability in benchmark imperfect-information games and achieves superhuman performance in heads-up no-limit Texas hold'em poker, while using far less domain knowledge than any prior poker AI. We also prove that ReBeL converges to a Nash equilibrium in two-player zero-sum games in tabular settings.",2020-07-27,2022-01-30 04:59:44,2022-01-30 04:59:44,2020-09-07 18:53:30,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000026  arXiv: 2007.13544,,/Users/jacquesthibodeau/Zotero/storage/QXI58ND2/Brown et al. - 2020 - Combining Deep Reinforcement Learning and Search f.pdf; /Users/jacquesthibodeau/Zotero/storage/C8ZNKWTE/2007.html,,TechSafety; Other-org,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,34th Conference on Neural Information Processing Systems (NeurIPS 2020),,,,,,,,,,,,,,,,
8826ECUU,conferencePaper,2018,"Raghunathan, Aditi; Steinhardt, Jacob; Liang, Percy",Certified Defenses against Adversarial Examples,,,,,http://arxiv.org/abs/1801.09344,"While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses. Can we somehow end this arms race? In this work, we study this problem for neural networks with one hidden layer. We first propose a method based on a semidefinite relaxation that outputs a certificate that for a given network and test input, no attack can force the error to exceed a certain value. Second, as this certificate is differentiable, we jointly optimize it with the network parameters, providing an adaptive regularizer that encourages robustness against all attacks. On MNIST, our approach produces a network and a certificate that no attack that perturbs each pixel by at most \epsilon = 0.1 can cause more than 35% test error.",2018,2022-01-30 04:59:44,2022-01-30 04:59:44,2020-12-13 23:31:22,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000653  JCC: 455 arXiv: 1801.09344,,/Users/jacquesthibodeau/Zotero/storage/INV72AUS/Raghunathan et al. - 2020 - Certified Defenses against Adversarial Examples.pdf; /Users/jacquesthibodeau/Zotero/storage/89JZRJGM/1801.html,,TechSafety; AmbiguosSafety; Other-org,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,International Conference on Learning Representations 2018,,,,,,,,,,,,,,,,
59IA3QC5,conferencePaper,2021,"Lindner, David; Matoba, Kyle; Meulemans, Alexander",Challenges for Using Impact Regularizers to Avoid Negative Side Effects,arXiv:2101.12509 [cs],,,,http://arxiv.org/abs/2101.12509,"Designing reward functions for reinforcement learning is difficult: besides specifying which behavior is rewarded for a task, the reward also has to discourage undesired outcomes. Misspecified reward functions can lead to unintended negative side effects, and overall unsafe behavior. To overcome this problem, recent work proposed to augment the specified reward function with an impact regularizer that discourages behavior that has a big impact on the environment. Although initial results with impact regularizers seem promising in mitigating some types of side effects, important challenges remain. In this paper, we examine the main current challenges of impact regularizers and relate them to fundamental design decisions. We discuss in detail which challenges recent approaches address and which remain unsolved. Finally, we explore promising directions to overcome the unsolved challenges in preventing negative side effects with impact regularizers.",2021-02-23,2022-01-30 04:59:44,2022-01-30 04:59:44,2021-11-13 22:39:06,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000000  arXiv: 2101.12509,,/Users/jacquesthibodeau/Zotero/storage/UPVI5Q45/Lindner et al. - 2021 - Challenges for Using Impact Regularizers to Avoid .pdf; /Users/jacquesthibodeau/Zotero/storage/SMIIU3CE/2101.html,,TechSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AAAI 2021,,,,,,,,,,,,,,,,
U8P2DUZ2,conferencePaper,2020,"Everett, Michael; Lutjens, Bjorn; How, Jonathan P.",Certified Adversarial Robustness for Deep Reinforcement Learning,"3rd Conference on Robot Learning (CoRL 2019),",,,,http://arxiv.org/abs/2004.06496,"Deep Neural Network-based systems are now the state-of-the-art in many robotics tasks, but their application in safety-critical domains remains dangerous without formal guarantees on network robustness. Small perturbations to sensor inputs (from noise or adversarial examples) are often enough to change network-based decisions, which was recently shown to cause an autonomous vehicle to swerve into another lane. In light of these dangers, numerous algorithms have been developed as defensive mechanisms from these adversarial inputs, some of which provide formal robustness guarantees or certificates. This work leverages research on certified adversarial robustness to develop an online certifiably robust for deep reinforcement learning algorithms. The proposed defense computes guaranteed lower bounds on state-action values during execution to identify and choose a robust action under a worst-case deviation in input space due to possible adversaries or noise. Moreover, the resulting policy comes with a certificate of solution quality, even though the true state and optimal action are unknown to the certifier due to the perturbations. The approach is demonstrated on a Deep Q-Network policy and is shown to increase robustness to noise and adversaries in pedestrian collision avoidance scenarios and a classic control task. This work extends one of our prior works with new performance guarantees, extensions to other RL algorithms, expanded results aggregated across more scenarios, an extension into scenarios with adversarial behavior, comparisons with a more computationally expensive method, and visualizations that provide intuition about the robustness algorithm.",2020-08-21,2022-01-30 04:59:43,2022-01-30 04:59:43,2020-08-31 17:29:23,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000022  arXiv: 2004.06496,,/Users/jacquesthibodeau/Zotero/storage/GJDIZKHK/Everett et al. - 2020 - Certified Adversarial Robustness for Deep Reinforc.pdf; /Users/jacquesthibodeau/Zotero/storage/ZTVVXNMQ/2004.html,,TechSafety; Other-org,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Cryptography and Security,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"3rd Conference on Robot Learning (CoRL 2019),",,,,,,,,,,,,,,,,
IHTBF4TH,conferencePaper,2018,"Yu, Han; Shen, Zhiqi; Miao, Chunyan; Leung, Cyril; Lesser, Victor R.; Yang, Qiang",Building Ethics into Artificial Intelligence,Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI-18),,,,http://arxiv.org/abs/1812.02953,"As artificial intelligence (AI) systems become increasingly ubiquitous, the topic of AI governance for ethical decision-making by AI has captured public imagination. Within the AI research community, this topic remains less familiar to many researchers. In this paper, we complement existing surveys, which largely focused on the psychological, social and legal discussions of the topic, with an analysis of recent advances in technical solutions for AI governance. By reviewing publications in leading AI conferences including AAAI, AAMAS, ECAI and IJCAI, we propose a taxonomy which divides the field into four areas: 1) exploring ethical dilemmas; 2) individual ethical decision frameworks; 3) collective ethical decision frameworks; and 4) ethics in human-AI interactions. We highlight the intuitions and key techniques used in each approach, and discuss promising future research directions towards successful integration of ethical AI systems into human societies.",2018-12-07,2022-01-30 04:59:37,2022-01-30 04:59:37,2020-11-14 00:53:39,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000128  arXiv: 1812.02953,,/Users/jacquesthibodeau/Zotero/storage/P4HD9IST/Yu et al. - 2018 - Building Ethics into Artificial Intelligence.pdf; /Users/jacquesthibodeau/Zotero/storage/SZNMZP8A/1812.html,,TechSafety; Other-org,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI-18),,,,,,,,,,,,,,,,
G7KMKWHX,conferencePaper,2019,"Fisac, Jaime F.; Lugovoy, Neil F.; Rubies-Royo, Vicenc; Ghosh, Shromona; Tomlin, Claire J.",Bridging Hamilton-Jacobi Safety Analysis and Reinforcement Learning,2019 International Conference on Robotics and Automation (ICRA),978-1-5386-6027-0,,10.1109/ICRA.2019.8794107,https://ieeexplore.ieee.org/document/8794107/,"Safety analysis is a necessary component in the design and deployment of autonomous systems. Techniques from robust optimal control theory, such as Hamilton-Jacobi reachability analysis, allow a rigorous formalization of safety as guaranteed constraint satisfaction. Unfortunately, the computational complexity of these tools for general dynamical systems scales poorly with state dimension, making existing tools impractical beyond small problems. Modern reinforcement learning methods have shown promising ability to ﬁnd approximate yet proﬁcient solutions to optimal control problems in complex and high-dimensional systems, however their formulation is restricted to problems with an additive payoff (reward) over time, unsuitable for reasoning about safety. In recent work, we proved that the problem of maximizing the minimum payoff over time, central to safety analysis, can be time-discounted to induce a contraction mapping. Here, we introduce a novel, timediscounted Safety Bellman Equation that renders reinforcement learning techniques amenable to quantitative safety analysis, enabling them to approximate the safe set and optimal safety policy. This opens a new avenue of research connecting controltheoretic safety analysis and the reinforcement learning domain. We demonstrate our formulation on a variety of simulated robotics tasks and reinforcement learning schemes, validating our results against analytic and numerical solutions when these can be obtained, and showing scalability to previously intractable problems of up to 18 state dimensions by exploiting state-of-the-art deep reinforcement learning algorithms.",2019-05,2022-01-30 04:59:37,2022-01-30 04:59:37,2020-11-14 01:15:15,8550-8556,,,,,,,,,,,IEEE,"Montreal, QC, Canada",en,,,,,DOI.org (Crossref),,ZSCC: 0000033,,/Users/jacquesthibodeau/Zotero/storage/XFP8J2JX/Fisac et al. - 2019 - Bridging Hamilton-Jacobi Safety Analysis and Reinf.pdf,,TechSafety; AmbiguosSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 International Conference on Robotics and Automation (ICRA),,,,,,,,,,,,,,,,
7W6QE66X,conferencePaper,2018,"Rossi, Francesca; Mattei, Nicholas",Building Ethically Bounded AI,Proceedings of the AAAI Conference on Artificial Intelligence,,,,http://arxiv.org/abs/1812.03980,"The more AI agents are deployed in scenarios with possibly unexpected situations, the more they need to be flexible, adaptive, and creative in achieving the goal we have given them. Thus, a certain level of freedom to choose the best path to the goal is inherent in making AI robust and flexible enough. At the same time, however, the pervasive deployment of AI in our life, whether AI is autonomous or collaborating with humans, raises several ethical challenges. AI agents should be aware and follow appropriate ethical principles and should thus exhibit properties such as fairness or other virtues. These ethical principles should define the boundaries of AI's freedom and creativity. However, it is still a challenge to understand how to specify and reason with ethical boundaries in AI agents and how to combine them appropriately with subjective preferences and goal specifications. Some initial attempts employ either a data-driven example-based approach for both, or a symbolic rule-based approach for both. We envision a modular approach where any AI technique can be used for any of these essential ingredients in decision making or decision support systems, paired with a contextual approach to define their combination and relative weight. In a world where neither humans nor AI systems work in isolation, but are tightly interconnected, e.g., the Internet of Things, we also envision a compositional approach to building ethically bounded AI, where the ethical properties of each component can be fruitfully exploited to derive those of the overall system. In this paper we define and motivate the notion of ethically-bounded AI, we describe two concrete examples, and we outline some outstanding challenges.",2018-12-10,2022-01-30 04:59:37,2022-01-30 04:59:37,2020-11-14 00:31:20,,,,33,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s2]  ACC: 36  arXiv: 1812.03980,,/Users/jacquesthibodeau/Zotero/storage/FKZA6ZVE/Rossi and Mattei - 2018 - Building Ethically Bounded AI.pdf; /Users/jacquesthibodeau/Zotero/storage/TKRZ5565/1812.html,,TechSafety; Other-org,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AAAI Conference on Artificial Intelligence,,,,,,,,,,,,,,,,
IWME7BT6,conferencePaper,2020,"Chen, Ting; Kornblith, Simon; Swersky, Kevin; Norouzi, Mohammad; Hinton, Geoffrey",Big Self-Supervised Models are Strong Semi-Supervised Learners,"34th Conference on Neural Information Processing Systems (NeurIPS 2020),",,,,http://arxiv.org/abs/2006.10029,"One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to most previous approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of a big (deep and wide) network during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2 (a modification of SimCLR), supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9\% ImageNet top-1 accuracy with just 1\% of the labels ($\le$13 labeled images per class) using ResNet-50, a $10\times$ improvement in label efficiency over the previous state-of-the-art. With 10\% of labels, ResNet-50 trained with our method achieves 77.5\% top-1 accuracy, outperforming standard supervised training with all of the labels.",2020-06-17,2022-01-30 04:59:37,2022-01-30 04:59:37,2020-08-31 18:02:23,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000505  arXiv: 2006.10029,,/Users/jacquesthibodeau/Zotero/storage/W93U7VHU/Chen et al. - 2020 - Big Self-Supervised Models are Strong Semi-Supervi.pdf,,TechSafety; AmbiguosSafety; Other-org,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2020,,,,,,,,,,,,,,,,
9SVVUE8N,conferencePaper,2019,"Hendrycks, Dan; Dietterich, Thomas",Benchmarking Neural Network Robustness to Common Corruptions and Perturbations,,,,,http://arxiv.org/abs/1903.12261,"In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, ImageNet-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called ImageNet-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.",2019-03-28,2022-01-30 04:59:36,2022-01-30 04:59:36,2020-12-22 23:29:59,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000898  arXiv: 1903.12261,,/Users/jacquesthibodeau/Zotero/storage/6IRJBZUB/Hendrycks and Dietterich - 2019 - Benchmarking Neural Network Robustness to Common C.pdf; /Users/jacquesthibodeau/Zotero/storage/XSJQR645/1903.html,,TechSafety; AmbiguosSafety; Other-org,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICLR 2019,,,,,,,,,,,,,,,,
A4SXFVC7,conferencePaper,2016,"Steinhardt, Jacob; Valiant, Gregory; Charikar, Moses",Avoiding Imposters and Delinquents: Adversarial Crowdsourcing and Peer Prediction,Advances in Neural Information Processing Systems 29 (NIPS 2016),,,,http://arxiv.org/abs/1606.05374,"We consider a crowdsourcing model in which $n$ workers are asked to rate the quality of $n$ items previously generated by other workers. An unknown set of $\alpha n$ workers generate reliable ratings, while the remaining workers may behave arbitrarily and possibly adversarially. The manager of the experiment can also manually evaluate the quality of a small number of items, and wishes to curate together almost all of the high-quality items with at most an $\epsilon$ fraction of low-quality items. Perhaps surprisingly, we show that this is possible with an amount of work required of the manager, and each worker, that does not scale with $n$: the dataset can be curated with $\tilde{O}\Big(\frac{1}{\beta\alpha^3\epsilon^4}\Big)$ ratings per worker, and $\tilde{O}\Big(\frac{1}{\beta\epsilon^2}\Big)$ ratings by the manager, where $\beta$ is the fraction of high-quality items. Our results extend to the more general setting of peer prediction, including peer grading in online classrooms.",2016-06-16,2022-01-30 04:59:36,2022-01-30 04:59:36,2020-12-13 19:48:09,,,,,,,Avoiding Imposters and Delinquents,,,,,,,,,,,,arXiv.org,,ZSCC: 0000030  arXiv: 1606.05374,,/Users/jacquesthibodeau/Zotero/storage/AIQHAJ8W/Steinhardt et al. - 2016 - Avoiding Imposters and Delinquents Adversarial Cr.pdf; /Users/jacquesthibodeau/Zotero/storage/CHB5HPQE/1606.html,,TechSafety; Other-org,Computer Science - Machine Learning; Computer Science - Computer Science and Game Theory; Computer Science - Data Structures and Algorithms; Computer Science - Human-Computer Interaction; Computer Science - Cryptography and Security,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
M9JCKE87,conferencePaper,2020,"Agarwal, Rishabh; Schuurmans, Dale; Norouzi, Mohammad",An Optimistic Perspective on Offline Reinforcement Learning,"arXiv:1907.04543 [cs, stat]",,,,http://arxiv.org/abs/1907.04543,"Off-policy reinforcement learning (RL) using a fixed offline dataset of logged interactions is an important consideration in real world applications. This paper studies offline RL using the DQN replay dataset comprising the entire replay experience of a DQN agent on 60 Atari 2600 games. We demonstrate that recent off-policy deep RL algorithms, even when trained solely on this fixed dataset, outperform the fully trained DQN agent. To enhance generalization in the offline setting, we present Random Ensemble Mixture (REM), a robust Q-learning algorithm that enforces optimal Bellman consistency on random convex combinations of multiple Q-value estimates. Offline REM trained on the DQN replay dataset surpasses strong RL baselines. Ablation studies highlight the role of offline dataset size and diversity as well as the algorithm choice in our positive results. Overall, the results here present an optimistic view that robust RL algorithms trained on sufficiently large and diverse offline datasets can lead to high quality policies. The DQN replay dataset can serve as an offline RL benchmark and is open-sourced.",2020-06-22,2022-01-30 04:59:35,2022-01-30 04:59:35,2020-08-28 17:35:41,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000137  arXiv: 1907.04543,,/Users/jacquesthibodeau/Zotero/storage/I8BUCNET/Agarwal et al. - 2020 - An Optimistic Perspective on Offline Reinforcement.pdf; /Users/jacquesthibodeau/Zotero/storage/4UQ64FEK/1907.html,,TechSafety; Other-org,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICML 2020,,,,,,,,,,,,,,,,
XGF52239,conferencePaper,2018,"Everitt, Tom; Lea, Gary; Hutter, Marcus",AGI Safety Literature Review,Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence,,,,http://arxiv.org/abs/1805.01109,"The development of Artificial General Intelligence (AGI) promises to be a major event. Along with its many potential benefits, it also raises serious safety concerns (Bostrom, 2014). The intention of this paper is to provide an easily accessible and up-to-date collection of references for the emerging field of AGI safety. A significant number of safety problems for AGI have been identified. We list these, and survey recent research on solving them. We also cover works on how best to think of AGI from the limited knowledge we have today, predictions for when AGI will first be created, and what will happen after its creation. Finally, we review the current public policy on AGI.",2018-05-21,2022-01-30 04:59:34,2022-01-30 04:59:34,2020-11-15 02:50:45,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000076  arXiv: 1805.01109,,/Users/jacquesthibodeau/Zotero/storage/ECXX8QH5/Everitt et al. - 2018 - AGI Safety Literature Review.pdf; /Users/jacquesthibodeau/Zotero/storage/N7U9FKW6/1805.html; /Users/jacquesthibodeau/Zotero/storage/3E7Q6G7R/Everitt et al. - 2018 - AGI Safety Literature Review.pdf,,TechSafety; Other-org,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PM74T3QP,conferencePaper,2020,"Fazelpour, Sina; Lipton, Zachary C.",Algorithmic Fairness from a Non-ideal Perspective,"arXiv:2001.09773 [cs, stat]",,,,http://arxiv.org/abs/2001.09773,"Inspired by recent breakthroughs in predictive modeling, practitioners in both industry and government have turned to machine learning with hopes of operationalizing predictions to drive automated decisions. Unfortunately, many social desiderata concerning consequential decisions, such as justice or fairness, have no natural formulation within a purely predictive framework. In efforts to mitigate these problems, researchers have proposed a variety of metrics for quantifying deviations from various statistical parities that we might expect to observe in a fair world and offered a variety of algorithms in attempts to satisfy subsets of these parities or to trade off the degree to which they are satisfied against utility. In this paper, we connect this approach to \emph{fair machine learning} to the literature on ideal and non-ideal methodological approaches in political philosophy. The ideal approach requires positing the principles according to which a just world would operate. In the most straightforward application of ideal theory, one supports a proposed policy by arguing that it closes a discrepancy between the real and the perfectly just world. However, by failing to account for the mechanisms by which our non-ideal world arose, the responsibilities of various decision-makers, and the impacts of proposed policies, naive applications of ideal thinking can lead to misguided interventions. In this paper, we demonstrate a connection between the fair machine learning literature and the ideal approach in political philosophy, and argue that the increasingly apparent shortcomings of proposed fair machine learning algorithms reflect broader troubles faced by the ideal approach. We conclude with a critical discussion of the harms of misguided solutions, a reinterpretation of impossibility results, and directions for future research.",2020-01-08,2022-01-30 04:59:34,2022-01-30 04:59:34,2020-09-05 16:51:01,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000029  arXiv: 2001.09773,,/Users/jacquesthibodeau/Zotero/storage/X2KWRS4A/Fazelpour and Lipton - 2020 - Algorithmic Fairness from a Non-ideal Perspective.pdf; /Users/jacquesthibodeau/Zotero/storage/3W2M84GS/2001.html,,TechSafety; AmbiguosSafety; Other-org,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society 2020",,,,,,,,,,,,,,,,
DSCR47GU,conferencePaper,2018,"Sarma, Gopal P.; Hay, Nick J.; Safron, Adam",AI Safety and Reproducibility: Establishing Robust Foundations for the Neuropsychology of Human Values,Lecture Notes in Computer Science,,,10.1007/978-3-319-99229-7_45,http://arxiv.org/abs/1712.04307,We propose the creation of a systematic effort to identify and replicate key findings in neuropsychology and allied fields related to understanding human values. Our aim is to ensure that research underpinning the value alignment problem of artificial intelligence has been sufficiently validated to play a role in the design of AI systems.,2018,2022-01-30 04:59:34,2022-01-30 04:59:34,2020-12-13 23:39:16,507-512,,,11088,,,AI Safety and Reproducibility,,,,,,,,,,,,arXiv.org,,ZSCC: 0000007  arXiv: 1712.04307,,/Users/jacquesthibodeau/Zotero/storage/X9Q6DXIZ/Sarma et al. - 2018 - AI Safety and Reproducibility Establishing Robust.pdf; /Users/jacquesthibodeau/Zotero/storage/RKKBXEHQ/1712.html,,MetaSafety; Other-org,Computer Science - Artificial Intelligence; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"International Conference on Computer Safety, Reliability, and Security (SAFECOMP 2018)",,,,,,,,,,,,,,,,
K4CP542W,conferencePaper,2019,"Baumann, Tobias; Graepel, Thore; Shawe-Taylor, John",Adaptive Mechanism Design: Learning to Promote Cooperation,2020 International Joint Conference on Neural Networks (IJCNN),,,,http://arxiv.org/abs/1806.04067,"In the future, artificial learning agents are likely to become increasingly widespread in our society. They will interact with both other learning agents and humans in a variety of complex settings including social dilemmas. We consider the problem of how an external agent can promote cooperation between artificial learners by distributing additional rewards and punishments based on observing the learners' actions. We propose a rule for automatically learning how to create right incentives by considering the players' anticipated parameter updates. Using this learning rule leads to cooperation with high social welfare in matrix games in which the agents would otherwise learn to defect with high probability. We show that the resulting cooperative outcome is stable in certain games even if the planning agent is turned off after a given number of episodes, while other games require ongoing intervention to maintain mutual cooperation. However, even in the latter case, the amount of necessary additional incentives decreases over time.",2019-11-20,2022-01-30 04:59:33,2022-01-30 04:59:33,2020-11-14 00:41:13,,,,,,,Adaptive Mechanism Design,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s2]  ACC: 10  arXiv: 1806.04067,,/Users/jacquesthibodeau/Zotero/storage/KMTJBPT6/Baumann et al. - 2019 - Adaptive Mechanism Design Learning to Promote Coo.pdf; /Users/jacquesthibodeau/Zotero/storage/4RMMHPN9/1806.html,,TechSafety; Other-org,Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2020 International Joint Conference on Neural Networks (IJCNN),,,,,,,,,,,,,,,,
Q5IKRD2K,conferencePaper,2018,"Behzadan, Vahid; Munir, Arslan; Yampolskiy, Roman V.",A Psychopathological Approach to Safety Engineering in AI and AGI,,,,,https://arxiv.org/abs/1805.08915v1,"The complexity of dynamics in AI techniques is already approaching that of complex adaptive systems, thus curtailing the feasibility of formal controllability and reachability analysis in the context of AI safety. It follows that the envisioned instances of Artificial General Intelligence (AGI) will also suffer from challenges of complexity. To tackle such issues, we propose the modeling of deleterious behaviors in AI and AGI as psychological disorders, thereby enabling the employment of psychopathological approaches to analysis and control of misbehaviors. Accordingly, we present a discussion on the feasibility of the psychopathological approaches to AI safety, and propose general directions for research on modeling, diagnosis, and treatment of psychological disorders in AGI.",2018-05-23,2022-01-30 04:59:33,2022-01-30 04:59:33,2020-11-14 01:17:01,,,,,,,,,,,,,,en,,,,,arxiv.org,,ZSCC: 0000016,,/Users/jacquesthibodeau/Zotero/storage/ZNKQIBA9/Behzadan et al. - 2018 - A Psychopathological Approach to Safety Engineerin.pdf; /Users/jacquesthibodeau/Zotero/storage/KB6ZSVFC/1805.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"International Conference on Computer Safety, Reliability, and Security",,,,,,,,,,,,,,,,
HRBWN3RH,conferencePaper,2018,"Wu, Yueh-Hua; Lin, Shou-De",A Low-Cost Ethics Shaping Approach for Designing Reinforcement Learning Agents,The Thirty-Second AAAI Conferenceon Artificial Intelligence (AAAI-18),,,,http://arxiv.org/abs/1712.04172,"This paper proposes a low-cost, easily realizable strategy to equip a reinforcement learning (RL) agent the capability of behaving ethically. Our model allows the designers of RL agents to solely focus on the task to achieve, without having to worry about the implementation of multiple trivial ethical patterns to follow. Based on the assumption that the majority of human behavior, regardless which goals they are achieving, is ethical, our design integrates human policy with the RL policy to achieve the target objective with less chance of violating the ethical code that human beings normally obey.",2018-09-10,2022-01-30 04:59:33,2022-01-30 04:59:33,2020-12-13 23:53:21,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000029   arXiv: 1712.04172,,/Users/jacquesthibodeau/Zotero/storage/X7R2DI2W/Wu and Lin - 2018 - A Low-Cost Ethics Shaping Approach for Designing R.pdf; /Users/jacquesthibodeau/Zotero/storage/GCCWAJE9/1712.html,,TechSafety; AmbiguosSafety; Other-org,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,The Thirty-Second AAAI Conferenceon Artificial Intelligence (AAAI-18),,,,,,,,,,,,,,,,
QMINGD6C,conferencePaper,2015,"Soares, Nate; Fallenstein, Benja",Toward Idealized Decision Theory,,,,,http://arxiv.org/abs/1507.01986,"This paper motivates the study of decision theory as necessary for aligning smarter-than-human artificial systems with human interests. We discuss the shortcomings of two standard formulations of decision theory, and demonstrate that they cannot be used to describe an idealized decision procedure suitable for approximation by artificial systems. We then explore the notions of policy selection and logical counterfactuals, two recent insights into decision theory that point the way toward promising paths for future research.",2015-07-07,2022-01-30 04:57:32,2022-01-30 04:57:32,2019-12-19 02:58:41,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000036  0 J: 30 arXiv: 1507.01986,,/Users/jacquesthibodeau/Zotero/storage/X6T99AUG/Soares and Fallenstein - 2015 - Toward Idealized Decision Theory.pdf; /Users/jacquesthibodeau/Zotero/storage/QCHDAWP4/Soares and Fallenstein - 2015 - Toward Idealized Decision Theory.pdf; /Users/jacquesthibodeau/Zotero/storage/W66QDEEJ/1507.html; /Users/jacquesthibodeau/Zotero/storage/W9BGSZJX/1507.html,,TechSafety; MIRI,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AGI-2015,,,,,,,,,,,,,,,,
T8H27GXJ,conferencePaper,2017,"Achiam, Joshua; Held, David; Tamar, Aviv; Abbeel, Pieter",Constrained policy optimization,Proceedings of the 34th International Conference on Machine Learning,,,,,,2017,2022-01-30 04:57:26,2022-01-30 04:57:26,,,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000549,,/Users/jacquesthibodeau/Zotero/storage/SJSX5V9Z/Achiam et al. - 2017 - Constrained policy optimization.pdf; /Users/jacquesthibodeau/Zotero/storage/ZKPZ3KVQ/1705.html,,TechSafety; Open-AI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4V8R5X9I,conferencePaper,2019,"Clark, Jack; Hadﬁeld, Gillian K",Regulatory Markets for AI Safety,,,,,https://arxiv.org/abs/2001.00078,We propose a new model for regulation to achieve AI safety: global regulatory markets. We ﬁrst sketch the model in general terms and provide an overview of the costs and beneﬁts of this approach. We then demonstrate how the model might work in practice: responding to the risk of adversarial attacks on AI models employed in commercial drones.,2019,2022-01-30 04:57:25,2022-01-30 04:57:25,,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000020,,/Users/jacquesthibodeau/Zotero/storage/ZX28GC8Q/Clark and Hadfield - 2019 - Regulatory Markets for AI Safety.pdf; /Users/jacquesthibodeau/Zotero/storage/JZ2QJ9K9/2001.html; /Users/jacquesthibodeau/Zotero/storage/G9J25559/Clark and Hadﬁeld - 2019 - REGULATORY MARKETS FOR AI SAFETY.pdf,,MetaSafety; Open-AI,Computer Science - Computers and Society; Economics - General Economics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Safe Machine Learning workshop at ICLR, 2019",,,,,,,,,,,,,,,,
2FGGZESV,conferencePaper,2020,"Stooke, Adam; Achiam, Joshua; Abbeel, Pieter",Responsive safety in reinforcement learning by pid lagrangian methods,International Conference on Machine Learning,,,,,,2020,2022-01-30 04:57:25,2022-01-30 04:57:25,,9133–9143,,,,,,,,,,,PMLR,,,,,,,Google Scholar,,ZSCC: 0000043,,/Users/jacquesthibodeau/Zotero/storage/WBNK9Z5V/Stooke et al. - 2020 - Responsive safety in reinforcement learning by pid.pdf; /Users/jacquesthibodeau/Zotero/storage/I5C469CU/stooke20a.html,,TechSafety; Open-AI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JMSJ3UEH,conferencePaper,2015,"Soares, Nate",The Value Learning Problem,Ethics for Artificial Intelligence Workshop at 25th International Joint Conference on Artificial Intelligence,,,,https://intelligence.org/files/ValueLearningProblem.pdf,"Autonomous AI systems’ programmed goals can easily fall short of programmers’ intentions. Even a machine intelligent enough to understand its designers’ intentions would not necessarily act as intended. We discuss early ideas on how one might design smarter-than-human AI systems that can inductively learn what to value from labeled training data, and highlight questions about the construction of systems that model and act upon their operators’ preferences.",2015,2022-01-30 04:56:59,2022-01-30 04:56:59,,7,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000070  8 J: 38,,/Users/jacquesthibodeau/Zotero/storage/AAGIJGBD/Soares - The Value Learning Problem.pdf,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IJCAI 2016,,,,,,,,,,,,,,,,
DNQKCSRM,conferencePaper,2016,"Everitt, Tom; Filan, Daniel; Daswani, Mayank; Hutter, Marcus",Self-Modification of Policy and Utility Function in Rational Agents,AGI 2016: Artificial General Intelligence,,,,http://arxiv.org/abs/1605.03142,"Any agent that is part of the environment it interacts with and has versatile actuators (such as arms and fingers), will in principle have the ability to self-modify -- for example by changing its own source code. As we continue to create more and more intelligent agents, chances increase that they will learn about this ability. The question is: will they want to use it? For example, highly intelligent systems may find ways to change their goals to something more easily achievable, thereby `escaping' the control of their designers. In an important paper, Omohundro (2008) argued that goal preservation is a fundamental drive of any intelligent system, since a goal is more likely to be achieved if future versions of the agent strive towards the same goal. In this paper, we formalise this argument in general reinforcement learning, and explore situations where it fails. Our conclusion is that the self-modification possibility is harmless if and only if the value function of the agent anticipates the consequences of self-modifications and use the current utility function when evaluating the future.",2016-05-10,2022-01-30 04:56:58,2022-01-30 04:56:58,2020-11-22 04:13:43,,,,,,,,Lecture Notes in Computer Science,,,,,,,,,,,arXiv.org,,ZSCC: 0000025  arXiv: 1605.03142,,/Users/jacquesthibodeau/Zotero/storage/9EW4V9AK/Everitt et al. - 2016 - Self-Modification of Policy and Utility Function i.pdf; /Users/jacquesthibodeau/Zotero/storage/9EXT3XCU/1605.html,,TechSafety; MIRI,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,International Conference on Artificial General Intelligence,,,,,,,,,,,,,,,,
CDZZ9WDE,conferencePaper,2015,"Everitt, Tom; Leike, Jan; Hutter, Marcus",Sequential Extensions of Causal and Evidential Decision Theory,ADT 2015: Algorithmic Decision Theory,,,,http://arxiv.org/abs/1506.07359,"Moving beyond the dualistic view in AI where agent and environment are separated incurs new challenges for decision making, as calculation of expected utility is no longer straightforward. The non-dualistic decision theory literature is split between causal decision theory and evidential decision theory. We extend these decision algorithms to the sequential setting where the agent alternates between taking actions and observing their consequences. We find that evidential decision theory has two natural extensions while causal decision theory only has one.",2015-06-24,2022-01-30 04:56:58,2022-01-30 04:56:58,2020-11-22 04:14:19,,,,,,,,Lecture Notes in Computer Science,,,,,,,,,,,arXiv.org,,ZSCC: 0000011  arXiv: 1506.07359,,/Users/jacquesthibodeau/Zotero/storage/ZPQ2R9HA/Everitt et al. - 2015 - Sequential Extensions of Causal and Evidential Dec.pdf; /Users/jacquesthibodeau/Zotero/storage/IXVQQIQD/1506.html,,TechSafety; MIRI,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,International Conference on Algorithmic Decision Theory,,,,,,,,,,,,,,,,
KSJ8QJE3,conferencePaper,2016,"Taylor, Jessica",Quantilizers: A safer alternative to maximizers for limited optimization,Workshops at the Thirtieth AAAI Conference on Artificial Intelligence,,,,,,2016,2022-01-30 04:56:58,2022-01-30 04:56:58,,,,,,,,Quantilizers,,,,,,,,,,,,Google Scholar,,ZSCC: 0000029,,/Users/jacquesthibodeau/Zotero/storage/27JB47GD/Taylor - 2016 - Quantilizers A safer alternative to maximizers fo.pdf; /Users/jacquesthibodeau/Zotero/storage/4JD5H2IP/12613.html,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2SEFD3C9,conferencePaper,2011,"Dewey, Daniel",Learning What to Value,Artificial General Intelligence,978-3-642-22886-5 978-3-642-22887-2,,10.1007/978-3-642-22887-2_35,http://link.springer.com/10.1007/978-3-642-22887-2_35,,2011,2022-01-30 04:56:57,2022-01-30 04:56:57,2021-01-23 20:28:26,309-314,,,6830,,,,,,,,Springer Berlin Heidelberg,"Berlin, Heidelberg",,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s1]  ACC: 61  Series Title: Lecture Notes in Computer Science,,,,,,"Schmidhuber, Jürgen; Thórisson, Kristinn R.; Looks, Moshe","Hutchison, David; Kanade, Takeo; Kittler, Josef; Kleinberg, Jon M.; Mattern, Friedemann; Mitchell, John C.; Naor, Moni; Nierstrasz, Oscar; Pandu Rangan, C.; Steffen, Bernhard; Sudan, Madhu; Terzopoulos, Demetri; Tygar, Doug; Vardi, Moshe Y.; Weikum, Gerhard",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XMAISE9S,conferencePaper,2016,"Benson-Tilsen, Tsvi; Soares, Nate",Formalizing convergent instrumental goals,Workshops at the Thirtieth AAAI Conference on Artificial Intelligence,,,,,,2016,2022-01-30 04:56:48,2022-01-30 04:56:48,,,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000012,,/Users/jacquesthibodeau/Zotero/storage/ZC55WH97/Benson-Tilsen and Soares - Formalizing Convergent Instrumental Goals.pdf; /Users/jacquesthibodeau/Zotero/storage/ISW5PA2B/12634.html,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
T9KCWKUD,conferencePaper,2019,"Kosoy, Vanessa",Delegative Reinforcement Learning: Learning To Avoid Traps With A Little Help,,,,,,"Most known regret bounds for reinforcement learning are either episodic or assume an environment without traps. We derive a regret bound without making either assumption, by allowing the algorithm to occasionally delegate an action to an external advisor. We thus arrive at a setting of active one-shot model-based reinforcement learning that we call DRL (delegative reinforcement learning.) The algorithm we construct in order to demonstrate the regret bound is a variant of Posterior Sampling Reinforcement Learning supplemented by a subroutine that decides which actions should be delegated. The algorithm is not anytime, since the parameters must be adjusted according to the target time discount. Currently, our analysis is limited to Markov decision processes with finite numbers of hypotheses, states and actions.",2019,2022-01-30 04:56:48,2022-01-30 04:56:48,,22,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000006,,/Users/jacquesthibodeau/Zotero/storage/NF24GTKM/Kosoy - 2019 - Delegative Reinforcement Learning learning to avo.pdf; /Users/jacquesthibodeau/Zotero/storage/U2UA962U/Kosoy - 2019 - DELEGATIVE REINFORCEMENT LEARNING LEARN- ING TO A.pdf,,TechSafety; MIRI,Computer Science - Machine Learning; Statistics - Machine Learning; I.2.6; 68Q32,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SafeML ICLR 2019 Workshop,,,,,,,,,,,,,,,,
WXU7767D,conferencePaper,2016,"Sotala, Kaj",Defining human values for value learners,Workshops at the Thirtieth AAAI Conference on Artificial Intelligence,,,,,,2016,2022-01-30 04:56:48,2022-01-30 04:56:48,,,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000019,,/Users/jacquesthibodeau/Zotero/storage/BJAT2I48/Sotala - 2016 - Defining human values for value learners.pdf; /Users/jacquesthibodeau/Zotero/storage/4NMRB9Z4/12633.html,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2JZ8J6BZ,conferencePaper,2010,"Salamon, Anna; Rayhawk, Stephen; Kramár, János",How Intelligible is Intelligence,ECAP10: VIII European Conference on Computing and Philosophy,,,,https://intelligence.org/files/HowIntelligible.pdf,"If human-level AI is eventually created, it may have unprecedented positive or negative consequences for humanity. It is therefore worth constructing the best possible forecasts of policy-relevant aspects of AI development trajectories—even though, at this early stage, the unknowns must remain very large.",2010,2022-01-30 04:56:48,2022-01-30 04:56:48,2021-01-23 20:19:59,8,,,,,,,,,,,,,en,,,,,,,ZSCC: 0000003,,/Users/jacquesthibodeau/Zotero/storage/SR3BV52U/HowIntelligible.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
X6FCKUH7,conferencePaper,2017,"Conitzer, Vincent; Sinnott-Armstrong, Walter; Borg, Jana Schaich; Deng, Yuan; Kramer, Max",Moral Decision Making Frameworks for Artificial Intelligence,"AAAI Workshops, 2017",,,,http://moralai.cs.duke.edu/documents/mai_docs/moralAAAI17.pdf,"The generality of decision and game theory has enabled domain-independent progress in AI research. For example, a better algorithm for ﬁnding good policies in (PO)MDPs can be instantly used in a variety of applications. But such a general theory is lacking when it comes to moral decision making. For AI applications with a moral component, are we then forced to build systems based on many ad-hoc rules? In this paper we discuss possible ways to avoid this conclusion.",2017,2022-01-30 05:00:02,2022-01-30 05:00:02,,5,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000139,,/Users/jacquesthibodeau/Zotero/storage/SK7EWVE3/Conitzer et al. - Moral Decision Making Frameworks for Artificial In.pdf,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"AAAI Workshops, 2017",,,,,,,,,,,,,,,,
GKGQKVFM,conferencePaper,2018,"Huang, Jessie; Wu, Fa; Precup, Doina; Cai, Yang",Learning Safe Policies with Expert Guidance,Advances in Neural Information Processing Systems 31 (NeurIPS 2018),,,,http://arxiv.org/abs/1805.08313,"We propose a framework for ensuring safe behavior of a reinforcement learning agent when the reward function may be difficult to specify. In order to do this, we rely on the existence of demonstrations from expert policies, and we provide a theoretical framework for the agent to optimize in the space of rewards consistent with its existing knowledge. We propose two methods to solve the resulting optimization: an exact ellipsoid-based method and a method in the spirit of the ""follow-the-perturbed-leader"" algorithm. Our experiments demonstrate the behavior of our algorithm in both discrete and continuous problems. The trained agent safely avoids states with potential negative effects while imitating the behavior of the expert in the other states.",2018-11-21,2022-01-30 04:59:59,2022-01-30 04:59:59,2020-11-14 00:44:35,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000018  arXiv: 1805.08313,,/Users/jacquesthibodeau/Zotero/storage/M5XBAMST/Huang et al. - 2018 - Learning Safe Policies with Expert Guidance.pdf; /Users/jacquesthibodeau/Zotero/storage/KWA7CVFJ/Huang et al. - 2018 - Learning Safe Policies with Expert Guidance.pdf; /Users/jacquesthibodeau/Zotero/storage/VJPZVNU8/1805.html; /Users/jacquesthibodeau/Zotero/storage/PHMBRHUV/1805.html,,TechSafety; Other-org,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2018,,,,,,,,,,,,,,,,
GWGXUC5S,conferencePaper,2019,"Lerer, Adam; Peysakhovich, Alexander",Learning Existing Social Conventions via Observationally Augmented Self-Play,"AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",,,,http://arxiv.org/abs/1806.10071,"In order for artificial agents to coordinate effectively with people, they must act consistently with existing conventions (e.g. how to navigate in traffic, which language to speak, or how to coordinate with teammates). A group's conventions can be viewed as a choice of equilibrium in a coordination game. We consider the problem of an agent learning a policy for a coordination game in a simulated environment and then using this policy when it enters an existing group. When there are multiple possible conventions we show that learning a policy via multi-agent reinforcement learning (MARL) is likely to find policies which achieve high payoffs at training time but fail to coordinate with the real group into which the agent enters. We assume access to a small number of samples of behavior from the true convention and show that we can augment the MARL objective to help it find policies consistent with the real group's convention. In three environments from the literature - traffic, communication, and team coordination - we observe that augmenting MARL with a small amount of imitation learning greatly increases the probability that the strategy found by MARL fits well with the existing social convention. We show that this works even in an environment where standard training methods very rarely find the true convention of the agent's partners.",2019-03-13,2022-01-30 04:59:59,2022-01-30 04:59:59,2020-11-14 00:38:01,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000021  arXiv: 1806.10071,,/Users/jacquesthibodeau/Zotero/storage/7FSUZWAU/Lerer and Peysakhovich - 2019 - Learning Existing Social Conventions via Observati.pdf; /Users/jacquesthibodeau/Zotero/storage/UTVCDMHZ/1806.html,,TechSafety; Other-org,Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"AIES '19: Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",,,,,,,,,,,,,,,,
IW9U6MXI,conferencePaper,2018,"Koller, Torsten; Berkenkamp, Felix; Turchetta, Matteo; Krause, Andreas",Learning-based Model Predictive Control for Safe Exploration,2018 IEEE Conference on Decision and Control (CDC),,,,http://arxiv.org/abs/1803.08287,"Learning-based methods have been successful in solving complex control tasks without significant prior knowledge about the system. However, these methods typically do not provide any safety guarantees, which prevents their use in safety-critical, real-world applications. In this paper, we present a learning-based model predictive control scheme that can provide provable high-probability safety guarantees. To this end, we exploit regularity assumptions on the dynamics in terms of a Gaussian process prior to construct provably accurate confidence intervals on predicted trajectories. Unlike previous approaches, we do not assume that model uncertainties are independent. Based on these predictions, we guarantee that trajectories satisfy safety constraints. Moreover, we use a terminal set constraint to recursively guarantee the existence of safe control actions at every iteration. In our experiments, we show that the resulting algorithm can be used to safely and efficiently explore and learn about dynamic systems.",2018-11-07,2022-01-30 04:59:59,2022-01-30 04:59:59,2020-12-13 23:12:15,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000224  arXiv: 1803.08287,,/Users/jacquesthibodeau/Zotero/storage/AWZVSTAT/Koller et al. - 2018 - Learning-based Model Predictive Control for Safe E.pdf; /Users/jacquesthibodeau/Zotero/storage/DBTEAK74/1803.html,,TechSafety; Other-org,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Robotics; Electrical Engineering and Systems Science - Systems and Control,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2018 IEEE Conference on Decision and Control (CDC),,,,,,,,,,,,,,,,
DFV3R6X3,conferencePaper,2019,"Sarma, Gopal P.; Safron, Adam; Hay, Nick J.","Integrative Biological Simulation, Neuropsychology, and AI Safety",Proceedings of the AAAI Workshop on Artificial Intelligence Safety 2019,,,,http://arxiv.org/abs/1811.03493,"We describe a biologically-inspired research agenda with parallel tracks aimed at AI and AI safety. The bottom-up component consists of building a sequence of biophysically realistic simulations of simple organisms such as the nematode $Caenorhabditis$ $elegans$, the fruit fly $Drosophila$ $melanogaster$, and the zebrafish $Danio$ $rerio$ to serve as platforms for research into AI algorithms and system architectures. The top-down component consists of an approach to value alignment that grounds AI goal structures in neuropsychology, broadly considered. Our belief is that parallel pursuit of these tracks will inform the development of value-aligned AI systems that have been inspired by embodied organisms with sensorimotor integration. An important set of side benefits is that the research trajectories we describe here are grounded in long-standing intellectual traditions within existing research communities and funding structures. In addition, these research programs overlap with significant contemporary themes in the biological and psychological sciences such as data/model integration and reproducibility.",2019-01-21,2022-01-30 04:59:58,2022-01-30 04:59:58,2020-11-14 00:58:15,,,,,,,,,,,,,Honolulu HI USA,,,,,,arXiv.org,,ZSCC: NoCitationData[s1]  ACC: 0  arXiv: 1811.03493,,"/Users/jacquesthibodeau/Zotero/storage/6GB52FT9/Sarma et al. - 2019 - Integrative Biological Simulation, Neuropsychology.pdf; /Users/jacquesthibodeau/Zotero/storage/K7JEEMCE/1811.html",,TechSafety; Other-org,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Neural and Evolutionary Computing; Quantitative Biology - Neurons and Cognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AAAI Workshop on Artificial Intelligence Safety,,,,,,,,,,,,,,,,
TNT8IZVP,conferencePaper,2019,"Eckersley, Peter",Impossibility and Uncertainty Theorems in AI Value Alignment (or why your AGI should not have a utility function),SafeAI 2019: Proceedings of the AAAI Workshop on Artificial Intelligence Safety 2019,,,,http://arxiv.org/abs/1901.00064,"Utility functions or their equivalents (value functions, objective functions, loss functions, reward functions, preference orderings) are a central tool in most current machine learning systems. These mechanisms for defining goals and guiding optimization run into practical and conceptual difficulty when there are independent, multi-dimensional objectives that need to be pursued simultaneously and cannot be reduced to each other. Ethicists have proved several impossibility theorems that stem from this origin; those results appear to show that there is no way of formally specifying what it means for an outcome to be good for a population without violating strong human ethical intuitions (in such cases, the objective function is a social welfare function). We argue that this is a practical problem for any machine learning system (such as medical decision support systems or autonomous weapons) or rigidly rule-based bureaucracy that will make high stakes decisions about human lives: such systems should not use objective functions in the strict mathematical sense. We explore the alternative of using uncertain objectives, represented for instance as partially ordered preferences, or as probability distributions over total orders. We show that previously known impossibility theorems can be transformed into uncertainty theorems in both of those settings, and prove lower bounds on how much uncertainty is implied by the impossibility results. We close by proposing two conjectures about the relationship between uncertainty in objectives and severe unintended consequences from AI systems.",2019-03-04,2022-01-30 04:59:58,2022-01-30 04:59:58,2020-11-14 00:58:06,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s1]  ACC: 25  arXiv: 1901.00064,,/Users/jacquesthibodeau/Zotero/storage/F97PFGRR/Eckersley - 2019 - Impossibility and Uncertainty Theorems in AI Value.pdf; /Users/jacquesthibodeau/Zotero/storage/BWXENTM8/1901.html,,TechSafety; Other-org,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
394DPAGJ,conferencePaper,2011,"Halpern, Joseph Y.; Pass, Rafael",I Don't Want to Think About it Now: Decision Theory With Costly Computation,Proceedings of the Twelfth International Conference on Principles of Knowledge Representation and Reasoning,,,,http://arxiv.org/abs/1106.2657,"Computation plays a major role in decision making. Even if an agent is willing to ascribe a probability to all states and a utility to all outcomes, and maximize expected utility, doing so might present serious computational problems. Moreover, computing the outcome of a given act might be difficult. In a companion paper we develop a framework for game theory with costly computation, where the objects of choice are Turing machines. Here we apply that framework to decision theory. We show how well-known phenomena like first-impression-matters biases (i.e., people tend to put more weight on evidence they hear early on), belief polarization (two people with different prior beliefs, hearing the same evidence, can end up with diametrically opposed conclusions), and the status quo bias (people are much more likely to stick with what they already have) can be easily captured in that framework. Finally, we use the framework to define some new notions: value of computational information (a computational variant of value of information) and and computational value of conversation.",2011-06-14,2022-01-30 04:59:58,2022-01-30 04:59:58,2020-11-22 02:29:10,,,,,,,I Don't Want to Think About it Now,,,,,,,,,,,,arXiv.org,,ZSCC: 0000019[s0]  arXiv: 1106.2657,,/Users/jacquesthibodeau/Zotero/storage/CWHKI6EW/Halpern and Pass - 2011 - I Don't Want to Think About it NowDecision Theory.pdf; /Users/jacquesthibodeau/Zotero/storage/H22KIAM5/1106.html,,TechSafety; AmbiguosSafety; Other-org,Computer Science - Computer Science and Game Theory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DKNWFMQS,conferencePaper,2020,"Tsipras, Dimitris; Santurkar, Shibani; Engstrom, Logan; Ilyas, Andrew; Madry, Aleksander",From ImageNet to Image Classification: Contextualizing Progress on Benchmarks,Proceedings of the 37th International Conference on Machine Learning,,,,http://arxiv.org/abs/2005.11295,"Building rich machine learning datasets in a scalable manner often necessitates a crowd-sourced data collection pipeline. In this work, we use human studies to investigate the consequences of employing such a pipeline, focusing on the popular ImageNet dataset. We study how specific design choices in the ImageNet creation process impact the fidelity of the resulting dataset---including the introduction of biases that state-of-the-art models exploit. Our analysis pinpoints how a noisy data collection pipeline can lead to a systematic misalignment between the resulting benchmark and the real-world task it serves as a proxy for. Finally, our findings emphasize the need to augment our current model training and evaluation toolkit to take such misalignments into account. To facilitate further research, we release our refined ImageNet annotations at https://github.com/MadryLab/ImageNetMultiLabel.",2020-05-22,2022-01-30 04:59:56,2022-01-30 04:59:56,2020-08-31 18:21:48,,,,,,,From ImageNet to Image Classification,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000041  arXiv: 2005.11295,,/Users/jacquesthibodeau/Zotero/storage/HTPNAJFE/Tsipras et al. - 2020 - From ImageNet to Image Classification Contextuali.pdf,,TechSafety; AmbiguosSafety; Other-org,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICML 2020,,,,,,,,,,,,,,,,
FS9XWNGD,conferencePaper,2014,"Tegmark, Max",Friendly Artificial Intelligence: the Physics Challenge,Artificial Intelligence and Ethics: Papers from the 2015 AAAI Workshop,,,,http://arxiv.org/abs/1409.0813,"Relentless progress in artificial intelligence (AI) is increasingly raising concerns that machines will replace humans on the job market, and perhaps altogether. Eliezer Yudkowski and others have explored the possibility that a promising future for humankind could be guaranteed by a superintelligent ""Friendly AI"", designed to safeguard humanity and its values. I argue that, from a physics perspective where everything is simply an arrangement of elementary particles, this might be even harder than it appears. Indeed, it may require thinking rigorously about the meaning of life: What is ""meaning"" in a particle arrangement? What is ""life""? What is the ultimate ethical imperative, i.e., how should we strive to rearrange the particles of our Universe and shape its future? If we fail to answer the last question rigorously, this future is unlikely to contain humans.",2014-09-03,2022-01-30 04:59:56,2022-01-30 04:59:56,2020-11-22 02:29:59,,,,,,,Friendly Artificial Intelligence,,,,,,,,,,,,arXiv.org,,ZSCC: 0000006[s0]  arXiv: 1409.0813,,/Users/jacquesthibodeau/Zotero/storage/D2KARUXE/Tegmark - 2014 - Friendly Artificial Intelligence the Physics Chal.pdf; /Users/jacquesthibodeau/Zotero/storage/8TK9V4IJ/1409.html,,TechSafety; Other-org,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FBI9D6CJ,conferencePaper,2011,"Alur, Rajeev",Formal verification of hybrid systems,Proceedings of the ninth ACM international conference on Embedded software - EMSOFT '11,978-1-4503-0714-7,,10.1145/2038642.2038685,http://dl.acm.org/citation.cfm?doid=2038642.2038685,,2011,2022-01-30 04:59:55,2022-01-30 04:59:55,2020-11-22 01:47:17,273,,,,,,,,,,,ACM Press,"Taipei, Taiwan",en,,,,,DOI.org (Crossref),,ZSCC: 0000262,,/Users/jacquesthibodeau/Zotero/storage/ED2K2TEU/Alur - 2011 - Formal verification of hybrid systems.pdf,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,the ninth ACM international conference,,,,,,,,,,,,,,,,
BJ8QESWH,conferencePaper,2020,"Atrey, Akanksha; Clary, Kaleigh; Jensen, David",Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep Reinforcement Learning,,,,,http://arxiv.org/abs/1912.05743,"Saliency maps are frequently used to support explanations of the behavior of deep reinforcement learning (RL) agents. However, a review of how saliency maps are used in practice indicates that the derived explanations are often unfalsiﬁable and can be highly subjective. We introduce an empirical approach grounded in counterfactual reasoning to test the hypotheses generated from saliency maps and assess the degree to which they correspond to the semantics of RL environments. We use Atari games, a common benchmark for deep RL, to evaluate three types of saliency maps. Our results show the extent to which existing claims about Atari games can be evaluated and suggest that saliency maps are best viewed as an exploratory tool rather than an explanatory tool.",2020-02-20,2022-01-30 04:59:48,2022-01-30 04:59:48,2020-08-31 18:36:43,,,,,,,Exploratory Not Explanatory,,,,,,,en,,,,,arXiv.org,,ZSCC: NoCitationData[s2]  ACC: 35  arXiv: 1912.05743,,/Users/jacquesthibodeau/Zotero/storage/MIXFX52G/Atrey et al. - 2020 - Exploratory Not Explanatory Counterfactual Analys.pdf,,TechSafety; Other-org,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICLR 2020,,,,,,,,,,,,,,,,
XWS9JVUI,conferencePaper,2020,"Hase, Peter; Bansal, Mohit",Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,,,,http://arxiv.org/abs/2005.01831,"Algorithmic approaches to interpreting machine learning models have proliferated in recent years. We carry out human subject tests that are the first of their kind to isolate the effect of algorithmic explanations on a key aspect of model interpretability, simulatability, while avoiding important confounding experimental factors. A model is simulatable when a person can predict its behavior on new inputs. Through two kinds of simulation tests involving text and tabular data, we evaluate five explanations methods: (1) LIME, (2) Anchor, (3) Decision Boundary, (4) a Prototype model, and (5) a Composite approach that combines explanations from each method. Clear evidence of method effectiveness is found in very few cases: LIME improves simulatability in tabular classification, and our Prototype method is effective in counterfactual simulation tests. We also collect subjective ratings of explanations, but we do not find that ratings are predictive of how helpful explanations are. Our results provide the first reliable and comprehensive estimates of how explanations influence simulatability across a variety of explanation methods and data domains. We show that (1) we need to be careful about the metrics we use to evaluate explanation methods, and (2) there is significant room for improvement in current methods. All our supporting code, data, and models are publicly available at: https://github.com/peterbhase/InterpretableNLP-ACL2020",2020-05-04,2022-01-30 04:59:48,2022-01-30 04:59:48,2020-08-31 18:40:23,,,,,,,Evaluating Explainable AI,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000066  arXiv: 2005.01831,,/Users/jacquesthibodeau/Zotero/storage/ATRH8PB2/Hase and Bansal - 2020 - Evaluating Explainable AI Which Algorithmic Expla.pdf,,MetaSafety; AmbiguosSafety; Other-org,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ACL 2020,,,,,,,,,,,,,,,,
DJNA82X8,conferencePaper,2020,"Guan, Lin; Verma, Mudit; Kambhampati, Subbarao",Explanation Augmented Feedback in Human-in-the-Loop Reinforcement Learning,,,,,http://arxiv.org/abs/2006.14804,"Human-in-the-loop Reinforcement Learning (HRL) aims to integrate human guidance with Reinforcement Learning (RL) algorithms to improve sample efficiency and performance. The usual human guidance in HRL is binary evaluative ""good"" or ""bad"" signal for queried states and actions. However, this suffers from the problems of weak supervision and poor efficiency in leveraging human feedback. To address this, we present EXPAND (Explanation Augmented Feedback) which allows for explanatory information to be given as saliency maps from the human in addition to the binary feedback. EXPAND employs a state perturbation approach based on the state salient information to augment the feedback, reducing the number of human feedback signals required. We choose two domains to evaluate this approach, Taxi and Atari-Pong. We demonstrate the effectiveness of our method on three metrics, environment sample efficiency, human feedback sample efficiency, and agent gaze. We show that our method outperforms our baselines. Finally, we present an ablation study to confirm our hypothesis that augmenting binary feedback with state salient information gives a boost in performance.",2020-07-16,2022-01-30 04:59:48,2022-01-30 04:59:48,2020-08-28 17:26:49,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000008  arXiv: 2006.14804,,/Users/jacquesthibodeau/Zotero/storage/FSDFWXIT/Guan et al. - 2020 - Explanation Augmented Feedback in Human-in-the-Loo.pdf; /Users/jacquesthibodeau/Zotero/storage/3STKCI5F/2006.html,,TechSafety; AmbiguosSafety; Other-org,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2020,,,,,,,,,,,,,,,,
HEBSUXVE,conferencePaper,2020,"Pruthi, Garima; Liu, Frederick; Sundararajan, Mukund; Kale, Satyen",Estimating Training Data Influence by Tracking Gradient Descent,34th Conference on Neural Information Processing Systems (NeurIPS 2020),,,,http://arxiv.org/abs/2002.08484,"We introduce a method called TrackIn that computes the influence of a training example on a prediction made by the model, by tracking how the loss on the test point changes during the training process whenever the training example of interest was utilized. We provide a scalable implementation of TrackIn via a combination of a few key ideas: (a) a first-order approximation to the exact computation, (b) using random projections to speed up the computation of the first-order approximation for large models, (c) using saved checkpoints of standard training procedures, and (d) cherry-picking layers of a deep neural network. An experimental evaluation shows that TrackIn is more effective in identifying mislabelled training examples than other related methods such as influence functions and representer points. We also discuss insights from applying the method on vision, regression and natural language tasks.",2020-07-13,2022-01-30 04:59:47,2022-01-30 04:59:47,2020-09-05 17:05:20,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000002[s0]  arXiv: 2002.08484,,/Users/jacquesthibodeau/Zotero/storage/Q78TZ27M/Pruthi et al. - 2020 - Estimating Training Data Influence by Tracking Gra.pdf; /Users/jacquesthibodeau/Zotero/storage/6AZ6XQXS/2002.html,,TechSafety; AmbiguosSafety; Other-org,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,34th Conference on Neural Information Processing Systems (NeurIPS 2020),,,,,,,,,,,,,,,,
Q7GNR4H4,conferencePaper,2018,"Behzadan, Vahid; Yampolskiy, Roman V.; Munir, Arslan",Emergence of Addictive Behaviors in Reinforcement Learning Agents,Proceedings of the AAAI Workshop on Artificial Intelligence Safety 2019,,,,http://arxiv.org/abs/1811.05590,"This paper presents a novel approach to the technical analysis of wireheading in intelligent agents. Inspired by the natural analogues of wireheading and their prevalent manifestations, we propose the modeling of such phenomenon in Reinforcement Learning (RL) agents as psychological disorders. In a preliminary step towards evaluating this proposal, we study the feasibility and dynamics of emergent addictive policies in Q-learning agents in the tractable environment of the game of Snake. We consider a slightly modified settings for this game, in which the environment provides a ""drug"" seed alongside the original ""healthy"" seed for the consumption of the snake. We adopt and extend an RL-based model of natural addiction to Q-learning agents in this settings, and derive sufficient parametric conditions for the emergence of addictive behaviors in such agents. Furthermore, we evaluate our theoretical analysis with three sets of simulation-based experiments. The results demonstrate the feasibility of addictive wireheading in RL agents, and provide promising venues of further research on the psychopathological modeling of complex AI safety problems.",2018-11-13,2022-01-30 04:59:47,2022-01-30 04:59:47,2020-11-14 01:10:06,,,,,,,,,,,,,Honolulu HI USA,,,,,,arXiv.org,,ZSCC: 0000009  arXiv: 1811.05590,,/Users/jacquesthibodeau/Zotero/storage/THPJ9EUR/Behzadan et al. - 2018 - Emergence of Addictive Behaviors in Reinforcement .pdf; /Users/jacquesthibodeau/Zotero/storage/36B2C4IA/1811.html,,TechSafety; Other-org,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AAAI Workshop on Artificial Intelligence Safety 2019,,,,,,,,,,,,,,,,
BVVDEU27,conferencePaper,2016,"Greene, Joshua; Rossi, Francesca; Tasioulas, John; Venable, Kristen Brent; Williams, Brian",Embedding Ethical Principles in Collective Decision Support Systems,,,,,,"The future will see autonomous machines acting in the same environment as humans, in areas as diverse as driving, assistive technology, and health care. Think of self-driving cars, companion robots, and medical diagnosis support systems. We also believe that humans and machines will often need to work together and agree on common decisions. Thus hybrid collective decision making systems will be in great need.",2016,2022-01-30 04:59:47,2022-01-30 04:59:47,,5,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000050,,/Users/jacquesthibodeau/Zotero/storage/KQNS3RIQ/Greene et al. - Embedding Ethical Principles in Collective Decisio.pdf,,TechSafety; AmbiguosSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16),,,,,,,,,,,,,,,,
ITAG9EEW,conferencePaper,2019,"Menda, Kunal; Driggs-Campbell, Katherine; Kochenderfer, Mykel J.",EnsembleDAgger: A Bayesian Approach to Safe Imitation Learning,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),,,,http://arxiv.org/abs/1807.08364,"While imitation learning is often used in robotics, the approach frequently suffers from data mismatch and compounding errors. DAgger is an iterative algorithm that addresses these issues by aggregating training data from both the expert and novice policies, but does not consider the impact of safety. We present a probabilistic extension to DAgger, which attempts to quantify the confidence of the novice policy as a proxy for safety. Our method, EnsembleDAgger, approximates a Gaussian Process using an ensemble of neural networks. Using the variance as a measure of confidence, we compute a decision rule that captures how much we doubt the novice, thus determining when it is safe to allow the novice to act. With this approach, we aim to maximize the novice's share of actions, while constraining the probability of failure. We demonstrate improved safety and learning performance compared to other DAgger variants and classic imitation learning on an inverted pendulum and in the MuJoCo HalfCheetah environment.",2019-07-19,2022-01-30 04:59:47,2022-01-30 04:59:47,2020-12-13 23:23:33,,,,,,,EnsembleDAgger,,,,,,,,,,,,arXiv.org,,ZSCC: 0000023   arXiv: 1807.08364,,/Users/jacquesthibodeau/Zotero/storage/W3ZZIFIQ/Menda et al. - 2019 - EnsembleDAgger A Bayesian Approach to Safe Imitat.pdf; /Users/jacquesthibodeau/Zotero/storage/AXVEPNWH/1807.html,,TechSafety; AmbiguosSafety; Other-org,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),,,,,,,,,,,,,,,,
3JZGMVVU,conferencePaper,2017,"Mhamdi, El Mahdi El; Guerraoui, Rachid; Hendrikx, Hadrien; Maurer, Alexandre",Dynamic Safe Interruptibility for Decentralized Multi-Agent Reinforcement Learning,"arXiv:1704.02882 [cs, stat]",,,,http://arxiv.org/abs/1704.02882,"In reinforcement learning, agents learn by performing actions and observing their outcomes. Sometimes, it is desirable for a human operator to \textit{interrupt} an agent in order to prevent dangerous situations from happening. Yet, as part of their learning process, agents may link these interruptions, that impact their reward, to specific states and deliberately avoid them. The situation is particularly challenging in a multi-agent context because agents might not only learn from their own past interruptions, but also from those of other agents. Orseau and Armstrong defined \emph{safe interruptibility} for one learner, but their work does not naturally extend to multi-agent systems. This paper introduces \textit{dynamic safe interruptibility}, an alternative definition more suited to decentralized learning problems, and studies this notion in two learning frameworks: \textit{joint action learners} and \textit{independent learners}. We give realistic sufficient conditions on the learning algorithm to enable dynamic safe interruptibility in the case of joint action learners, yet show that these conditions are not sufficient for independent learners. We show however that if agents can detect interruptions, it is possible to prune the observations to ensure dynamic safe interruptibility even for independent learners.",2017-05-22,2022-01-30 04:59:46,2022-01-30 04:59:46,2020-11-21 17:31:17,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000022  arXiv: 1704.02882,,/Users/jacquesthibodeau/Zotero/storage/AH67HTNI/Mhamdi et al. - 2017 - Dynamic Safe Interruptibility for Decentralized Mu.pdf; /Users/jacquesthibodeau/Zotero/storage/A66XSDKN/1704.html,,TechSafety; AmbiguosSafety; Other-org,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2017,,,,,,,,,,,,,,,,
BHA2PG7A,conferencePaper,2018,"Plisnier, Hélène; Steckelmacher, Denis; Brys, Tim; Roijers, Diederik M.; Nowé, Ann",Directed Policy Gradient for Safe Reinforcement Learning with Human Advice,,,,,http://arxiv.org/abs/1808.04096,"Many currently deployed Reinforcement Learning agents work in an environment shared with humans, be them co-workers, users or clients. It is desirable that these agents adjust to people's preferences, learn faster thanks to their help, and act safely around them. We argue that most current approaches that learn from human feedback are unsafe: rewarding or punishing the agent a-posteriori cannot immediately prevent it from wrong-doing. In this paper, we extend Policy Gradient to make it robust to external directives, that would otherwise break the fundamentally on-policy nature of Policy Gradient. Our technique, Directed Policy Gradient (DPG), allows a teacher or backup policy to override the agent before it acts undesirably, while allowing the agent to leverage human advice or directives to learn faster. Our experiments demonstrate that DPG makes the agent learn much faster than reward-based approaches, while requiring an order of magnitude less advice.",2018-10,2022-01-30 04:59:46,2022-01-30 04:59:46,2020-11-14 01:06:08,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000002  arXiv: 1808.04096,,/Users/jacquesthibodeau/Zotero/storage/ZJPCJCQH/Plisnier et al. - 2018 - Directed Policy Gradient for Safe Reinforcement Le.pdf; /Users/jacquesthibodeau/Zotero/storage/PBWSCFC4/1808.html,,TechSafety; AmbiguosSafety; Other-org,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,European Workshop on Reinforcement Learning 14,,,,,,,,,,,,,,,,
3ACJ73AJ,conferencePaper,1998,"Hanson, Robin",Long-Term Growth as a Sequence of Exponential Modes,George Mason University,,,,,"A world product time series covering two million years is well fit by either a sum of four exponentials, or a constant elasticity of substitution (CES) combination of three exponential growth modes: “hunting, ” “farming, ” and “industry. ” The CES parameters suggest that farming substituted for hunting, while industry complemented farming, making the industrial revolution a smoother transition. Each mode grew world product by a factor of a few hundred, and grew a hundred times faster than its predecessor. This weakly suggests that within the next century a new mode might appear with a doubling time measured in days, not years.",1998,2022-03-10 22:07:02,2022-03-10 22:07:02,,9–3,,,,,,,,,,,,,,,,,,CiteSeer,,,,/Users/jacquesthibodeau/Zotero/storage/3SH8H2LB/Hanson - 1998 - Long-Term Growth as a Sequence of Exponential Mode.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FCKDRA3I,conferencePaper,2014,"Pecka, Martin; Svoboda, Tomas",Safe Exploration Techniques for Reinforcement Learning – An Overview,Modelling and Simulation for Autonomous Systems,978-3-319-13823-7,,10.1007/978-3-319-13823-7_31,,"We overview different approaches to safety in (semi)autonomous robotics. Particularly, we focus on how to achieve safe behavior of a robot if it is requested to perform exploration of unknown states. Presented methods are studied from the viewpoint of reinforcement learning, a partially-supervised machine learning method. To collect training data for this algorithm, the robot is required to freely explore the state space – which can lead to possibly dangerous situations. The role of safe exploration is to provide a framework allowing exploration while preserving safety. The examined methods range from simple algorithms to sophisticated methods based on previous experience or state prediction. Our overview also addresses the issues of how to define safety in the real-world applications (apparently absolute safety is unachievable in the continuous and random real world). In the conclusion we also suggest several ways that are worth researching more thoroughly.",2014,2022-03-10 22:21:51,2022-03-10 22:21:51,,357-375,,,,,,,Lecture Notes in Computer Science,,,,Springer International Publishing,Cham,en,,,,,Springer Link,,,,,,,policy search; reinforcement learning; Safe exploration,"Hodicky, Jan",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ISTP692N,conferencePaper,1996,"Piccione, Michele; Rubinstein, A.",On the Interpretation of Decision Problems with Imperfect Recall,TARK,,,10.1006/GAME.1997.0536,,"It is argued that extensive decision problems (extensive games with a single player) with imperfect recall suffer from major ambiguities in the interpretation of information sets and strategies, which allows for different kinds of analysis. In this paper it is argued that extensive decision problems (extensive games with a single player) with imperfect recall suffer from major ambiguities in the interpretation of information sets and strategies. This indeterminacy allows for different kinds of analysis. We address the following issues:    1. Randomization at information sets    2. Consistent beliefs    3. Time consistency of optimal plans    4. The multi-selves approach to decision making    We illustrate our discussion through an example that we call the absentminded driver paradox.    An individual is sitting late at night in a bar planning his midnight trip home. In order to get home he has to take the highway and get off at the second exit. Turning at the first exit leads into a disastrous area (payoff 0). Turning at the second exit yields the highest reward (payoff 4). If he continues beyond the second exit he will reach the end of the highway and find a hotel where he can spend the night (payoff 1). The driver is absentminded and is aware of this fact. When reaching an intersection he cannot tell whether it is the first or the second intersection and he cannot remember how many he has passed. While sitting at the bar, all he can do is to decide whether or not to exit at an intersection (we exclude at this stage the possibility that the decision maker can include random elements in his strategy).    Planning his trip at the bar, the decision maker must conclude that it is impossible for him to get home and he should not exit when he reaches an intersection. Thus, his optimal plan will lead him to spend the night at the hotel and yields a payoff of 1. Now, suppose that he reaches an intersection. Remembering his strategy he concludes that he is at the first intersection with probability 1/2. Then, reviewing his plan, he must conclude that it is optimal for him to leave the highway since it yields an expected payoff of 2. Thus, despite no new information and no change in his preferences, the decision maker is tempted to change his initial plan once he reaches an intersection!    We find this example paradoxical as it exhibits a conflict between two ways of reasoning. The first instructs the decision maker to follow his initial decision not to exit, as this is the optimal rule of behavior. The second leads him to optimize expected payoffs given his beliefs and to deviate from his initial decision.",1996,2022-03-10 22:30:02,2022-03-10 22:30:02,,,,,,,,,,,,,,,,,,,,Semantic Scholar,,,,,https://www.semanticscholar.org/paper/On-the-Interpretation-of-Decision-Problems-with-Piccione-Rubinstein/dd03415f643088df006f9ad12e22f43a57adb552?p2df,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
K2CUU8HF,conferencePaper,2018,"Zhang, Shun; Durfee, Edmund H.; Singh, Satinder",Minimax-regret querying on side effects for safe optimality in factored Markov decision processes,Proceedings of the 27th International Joint Conference on Artificial Intelligence,978-0-9992411-2-7,,,,"As it achieves a goal on behalf of its human user, an autonomous agent's actions may have side effects that change features of its environment in ways that negatively surprise its user. An agent that can be trusted to operate safely should thus only change features the user has explicitly permitted. We formalize this problem, and develop a planning algorithm that avoids potentially negative side effects given what the agent knows about (un)changeable features. Further, we formulate a provably minimax-regret querying strategy for the agent to selectively ask the user about features that it hasn't explicitly been told about. We empirically show how much faster it is than a more exhaustive approach and how much better its queries are than those found by the best known heuristic.",2018-07-13,2022-03-10 22:43:05,2022-03-10 22:43:05,2022-03-10,4867–4873,,,,,,,IJCAI'18,,,,AAAI Press,"Stockholm, Sweden",,,,,,ACM Digital Library,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IDNB5XQT,conferencePaper,2020,"Tang, Yichuan Charlie",Towards Learning Multi-agent Negotiations via Self-Play,Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),,,,http://arxiv.org/abs/2001.10208,"Making sophisticated, robust, and safe sequential decisions is at the heart of intelligent systems. This is especially critical for planning in complex multi-agent environments, where agents need to anticipate other agents’ intentions and possible future actions. Traditional methods formulate the problem as a Markov Decision Process, but the solutions often rely on various assumptions and become brittle when presented with corner cases. In contrast, deep reinforcement learning (Deep RL) has been very effective at ﬁnding policies by simultaneously exploring, interacting, and learning from environments. Leveraging the powerful Deep RL paradigm, we demonstrate that an iterative procedure of self-play can create progressively more diverse environments, leading to the learning of sophisticated and robust multi-agent policies. We demonstrate this in a challenging multi-agent simulation of merging trafﬁc, where agents must interact and negotiate with others in order to successfully merge on or off the road. While the environment starts off simple, we increase its complexity by iteratively adding an increasingly diverse set of agents to the agent “zoo” as training progresses. Qualitatively, we ﬁnd that through selfplay, our policies automatically learn interesting behaviors such as defensive driving, overtaking, yielding, and the use of signal lights to communicate intentions to other agents. In addition, quantitatively, we show a dramatic improvement of the success rate of merging maneuvers from 63% to over 98%.",2020-01-28,2020-08-31 18:42,2020-12-21 18:34,2020-08-31 18:42,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: NoCitationData[s1]  ACC: 1 arXiv: 2001.10208,,/Users/angelica/Zotero/storage/8SG5VQX4/Tang - 2020 - Towards Learning Multi-agent Negotiations via Self.pdf,,Other-org; NotSafety; AmbiguosSafety,Computer Science - Artificial Intelligence; Computer Science - Robotics; Computer Science - Machine Learning; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IEEE/CVF International Conference on Computer Vision (ICCV),,,,,,,,,,,,,,,,"While the previous paper introduces other-play to become robust to unknown partners, this paper takes the other approach of simply training an agent that is robust to a wide, diverse population of possible agents. In particular, it studies a self-driving car ""zipper merge"" environment, and trains an agent to be robust to a variety of rule-based agents, as well as past versions of itself, and finds that this leads to a much more successful merging policy. However, this is evaluated against the population it is trained with, and not against any previously unseen agents."
TDMY3B4X,conferencePaper,2020,"Jeon, Hong Jun; Milli, Smitha; Dragan, Anca D.",Reward-rational (implicit) choice: A unifying formalism for reward learning,34th Conference on Neural Information Processing Systems (NeurIPS 2020),,,,http://arxiv.org/abs/2002.04833,"It is often difficult to hand-specify what the correct reward function is for a task, so researchers have instead aimed to learn reward functions from human behavior or feedback. The types of behavior interpreted as evidence of the reward function have expanded greatly in recent years. We've gone from demonstrations, to comparisons, to reading into the information leaked when the human is pushing the robot away or turning it off. And surely, there is more to come. How will a robot make sense of all these diverse types of behavior? Our key insight is that different types of behavior can be interpreted in a single unifying formalism - as a reward-rational choice that the human is making, often implicitly. The formalism offers both a unifying lens with which to view past work, as well as a recipe for interpreting new sources of information that are yet to be uncovered. We provide two examples to showcase this: interpreting a new feedback type, and reading into how the choice of feedback itself leaks information about the reward.",2020-06-16,2020-09-05 19:05,2020-12-19 17:18,2020-09-05 19:05,,,,,,,Reward-rational (implicit) choice,,,,,,,,,,,,arXiv.org,,ZSCC: 0000003  arXiv: 2002.04833,,/Users/angelica/Zotero/storage/2KBDQB55/Jeon et al. - 2020 - Reward-rational (implicit) choice A unifying form.pdf; /Users/angelica/Zotero/storage/3ZHTV5N5/2002.html,,NotSafety; CHAI-Berkeley; AmbiguosSafety,Computer Science - Artificial Intelligence; Computer Science - Human-Computer Interaction; Computer Science - Robotics; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2020,,,,,,,,,,,,,,,,"We've got algorithms for learning preferences from <@demonstrations@>(@Modeling Interaction via the Principle of Maximum Causal Entropy@) (possibly <@ranked@>(@Ranking-Based Reward Extrapolation without Rankings@)), <@comparisons@>(@Fine-Tuning GPT-2 from Human Preferences@), <@proxy rewards@>(@Inverse Reward Design@), and even the <@observed state@>(@Learning Preferences by Looking at the World@). The insight of this paper is that these are all instances of a simple underlying formalism.

Specifically, these forms of preference learning can be described by two properties: (1) the set of choices that the human picks from and (2) how each choice corresponds to a distribution over agent trajectories. Given these properties, we assume that the human makes their choice according to a Boltzmann-rational model (where the human is more likely to choose an option if it leads to higher expected reward). We have now specified a likelihood over the choice given the reward, and we can use Bayes rule to infer a distribution over the reward given the human's choice.

Consider more exotic types of feedback, such as the human's decision to <@turn the agent off@>(@The Off-Switch Game@). Here, the human has two options: turning the agent off (corresponding to the agent staying still forever), or letting it continue (corresponding to the agent taking the trajectory that maximizes its current expected reward). If the agent has the right reward function, then the Boltzmann rational human would let it continue; as a result, if the human instead tries to turn the agent off, Bayes Rule allows the agent to infer that its belief about the reward must be wrong. Thus, even this decision of whether to turn the agent off can be captured in this framework.

The paper then shows two examples of new feedback types that can be generated from this framework: first, credit assignment, in which the human identifies a subset of the trajectory that had maximal reward, and second, meta-choice, where the choice of which _type_ of feedback to give can itself give information about the reward function."
UAV9X75G,conferencePaper,2019,"Icarte, Rodrigo Toro; Valenzano, Richard; Waldie, Ethan; Castro, Margarita P; Klassen, Toryn Q; McIlraith, Sheila A",Learning Reward Machines for Partially Observable Reinforcement Learning,,,,,http://www.cs.toronto.edu/~rntoro/docs/LRM_paper.pdf,"Reward Machines (RMs) provide a structured, automata-based representation of a reward function that enables a Reinforcement Learning (RL) agent to decompose an RL problem into structured subproblems that can be efficiently learned via off-policy learning.  Here we show that RMs can be learned from experience, instead of being specified by the user, and that the resulting problem decomposition can be used to effectively solve partially observable RL problems.  We pose the task of learning RMs as a discrete optimization problem where the objective is to find an RM that decomposes the problem into a set of subproblems such that the combination of their optimal memoryless policies is an optimal policy for the original problem.  We show the effectiveness of this approach on three partially observable domains, where it significantly outperforms A3C, PPO, and ACER, and discuss its advantages, limitations, and broader potential.",2019,2020-08-31 17:45,2020-12-21 18:12,2020-08-31,19,,,,,,,,,,,,"Vancouver, Canada",en,,,,,Zotero,,ZSCC: 0000008,,/Users/angelica/Zotero/storage/P97PBKLS/Icarte et al. - Learning Reward Machines for Partially Observable .pdf,,Other-org; NotSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2019,,,,,,,,,,,,,,,,"Typically in reinforcement learning, the agent only gets access to a reward signal: it sees a single number saying how well it has done. The problem might be simpler to solve if the agent could get a more holistic view of the problem through a structured representation of the reward. This could allow it to infer things like “if I went left, I would get 5 reward, but if I went right, I would get 10 reward”. Under the current RL paradigm, it has to try both actions in separate episodes to learn this.

Model-based RL tries to recover some of this structured representation: it learns a model of the world and the reward function, such that you can ask queries of the form “if I took this sequence of actions, what reward would I get?” The hope is that the learned models will generalize to new sequences that we haven’t previously seen, allowing the agent to learn from fewer environment interactions (i.e. higher sample efficiency).

This work does something similar using _reward machines_. The key idea is to represent both the reward and some aspects of the dynamics using a finite state machine, which can then be reasoned about without collecting more experience. In particular, given a POMDP, they propose learning a set of states U such that when combining the observation o with the state u, we have an MDP instead of a POMDP. This is called a _perfect_ reward machine. To make this feasible, they assume the existence of a labeling function L that, given a transition <o, a, o’>, extracts all of the relevant state information. (Since POMDPs can be reduced to belief-space MDPs, it is always possible to extract a perfect reward machine by having U be the set of possible beliefs and L be the identity function, but the hope is that U and L can be much simpler in most cases.)

They provide a formulation of an optimization problem over finite state machines such that a perfect reward machine would be an optimal solution to that problem (though I believe other imperfect reward machines could also be optimal). Since they are searching over a discrete space, they need to use a discrete optimization algorithm, and end up using Tabu search.

Once they have learned a reward machine from experience and a labeling function L, how can they use it to improve policy learning? They propose a very simple idea: when we get experience <o, a, o’>, treat it as a separate experience for every possible u, so that you effectively multiply the size of your dataset. They can then learn optimal policies that are conditioned on the state u (which can be inferred at test time using the learned state machine). Experiments show that this works in some simple gridworlds."
U32FJBY8,conferencePaper,2019,"Edwards, Ashley D.; Sahni, Himanshu; Schroecker, Yannick; Isbell, Charles L.",Imitating Latent Policies from Observation,Proceedings of the 36th International Conference on Machine Learning,,,,http://arxiv.org/abs/1805.07914,"In this paper, we describe a novel approach to imitation learning that infers latent policies directly from state observations. We introduce a method that characterizes the causal effects of latent actions on observations while simultaneously predicting their likelihood. We then outline an action alignment procedure that leverages a small amount of environment interactions to determine a mapping between the latent and real-world actions. We show that this corrected labeling can be used for imitating the observed behavior, even though no expert actions are given. We evaluate our approach within classic control environments and a platform game and demonstrate that it performs better than standard approaches. Code for this work is available at https://github.com/ashedwards/ILPO.",2019-05-13,2020-11-14 00:50,2020-12-19 15:32,2020-11-14 00:50,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000045  arXiv: 1805.07914,,/Users/angelica/Zotero/storage/FW2VC7CH/1805.html; /Users/angelica/Zotero/storage/2LDT3XPV/Edwards et al. - 2019 - Imitating Latent Policies from Observation.pdf,,Other-org; NotSafety,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Typically in imitation learning, we assume that we have access to demonstrations that include the actions that the expert took. However, in many realistic settings we only have access to state observations (eg. driving videos). In this setting, we could still infer a reward function and then use reinforcement learning (RL) to imitate the behavior, but this would require a lot of interaction with the environment to learn the dynamics of the environment. Intuitively, even demonstrations with only states and no actions should give us a lot of information about the dynamics -- if we can extract this information, then we would need much less environment interaction during RL. (For example, if you watch a friend play a video game, you only see states, not actions; yet you can infer a lot about the game rules and gameplay.) The key idea is that each action probably causes similar effects on different states. So, they create a model with hidden action nodes z, and use the state observations to learn a policy P(z | s) and dynamics s' = g(s, z) (they assume deterministic dynamics). This is done end-to-end with neural nets, but essentially the net is looking at the sequence of states and figuring out how to assign actions z to each s (this is P(z | s)), such that we can learn a function g(s, z) that outputs the next observed state s'. Once this is trained, intuitively g(s, z) will already have captured most of the dynamics, and so now we only require a small number of environment actions to figure out how the true actions a correspond to the hidden actions z -- concretely, we train a model P(a | s, z). Then, in any state s, we first choose the most likely hidden action z* according to P(z | s), and then the most likely action a* according to P(a | s, z*)."
SUVSP4DJ,conferencePaper,2020,"Zhi-Xuan, Tan; Mann, Jordyn L.; Silver, Tom; Tenenbaum, Joshua B.; Mansinghka, Vikash K.",Online Bayesian Goal Inference for Boundedly-Rational Planning Agents,arXiv:2006.07532 [cs],,,,http://arxiv.org/abs/2006.07532,"People routinely infer the goals of others by observing their actions over time. Remarkably, we can do so even when those actions lead to failure, enabling us to assist others when we detect that they might not achieve their goals. How might we endow machines with similar capabilities? Here we present an architecture capable of inferring an agent's goals online from both optimal and non-optimal sequences of actions. Our architecture models agents as boundedly-rational planners that interleave search with execution by replanning, thereby accounting for sub-optimal behavior. These models are specified as probabilistic programs, allowing us to represent and perform efficient Bayesian inference over an agent's goals and internal planning processes. To perform such inference, we develop Sequential Inverse Plan Search (SIPS), a sequential Monte Carlo algorithm that exploits the online replanning assumption of these models, limiting computation by incrementally extending inferred plans as new actions are observed. We present experiments showing that this modeling and inference architecture outperforms Bayesian inverse reinforcement learning baselines, accurately inferring goals from both optimal and non-optimal trajectories involving failure and back-tracking, while generalizing across domains with compositional structure and sparse rewards.",2020-06-12,2020-08-27 16:41,2020-12-21 18:42,2020-08-27 16:41,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000001  arXiv: 2006.07532,,/Users/angelica/Zotero/storage/IUVIXQHI/Zhi-Xuan et al. - 2020 - Online Bayesian Goal Inference for Boundedly-Ratio.pdf; /Users/angelica/Zotero/storage/3XJI59EW/2006.html,,Other-org; NotSafety; AmbiguosSafety,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2020,,,,,,,,,,,,,,,,"Typical approaches to learning from demonstrations rely on assuming that the demonstrator is either optimal or noisily optimal. However, this is a pretty bad description of actual human reasoning: it is more accurate to say we are _boundedly-rational planners_. In particular, it makes more sense to assume that our plans are computed from a noisy process. How might we capture this in an algorithm?

This paper models the demonstrator as using a bounded probabilistic [A* search](https://en.wikipedia.org/wiki/A*_search_algorithm) to find plans for achieving their goal. The planner is also randomized to account for the difficulty of planning: in particular, when choosing which state to “think about” next, it chooses randomly with higher probability for more promising states (as opposed to vanilla A* which always chooses the most promising state).

The search may fail to find a plan that achieves the goal, in which case the demonstrator follows the actions of the most promising plan found by A* search until no longer possible (either an action leads to a state A* search hadn’t considered, or it reaches the end of its partial plan). Thus, this algorithm can assign significant probability to plans that fail to reach the goal.

The experiments show that this feature allows their SIPS algorithm to infer goals even when the demonstrator fails to reach their goal. For example, if an agent needs to get two keys to unlock two doors to get a blue gem, but only manages to unlock the first door, the algorithm can still infer that the agent’s goal was to obtain the blue gem.

I really like that this paper is engaging with the difficulty of dealing with systematically imperfect demonstrators, and it shows that it can do much better than Bayesian IRL for the domains they consider."
I7UTX9IP,conferencePaper,2019,"Yu, Lantao; Yu, Tianhe; Finn, Chelsea; Ermon, Stefano",Meta-Inverse Reinforcement Learning with Probabilistic Context Variables,Advances in Neural Information Processing Systems 32 (NeurIPS 2019),,,,https://arxiv.org/abs/1909.09314v2,"Providing a suitable reward function to reinforcement learning can be difficult in many real world applications. While inverse reinforcement learning (IRL) holds promise for automatically learning reward functions from demonstrations, several major challenges remain. First, existing IRL methods learn reward functions from scratch, requiring large numbers of demonstrations to correctly infer the reward for each task the agent may need to perform. Second, existing methods typically assume homogeneous demonstrations for a single behavior or task, while in practice, it might be easier to collect datasets of heterogeneous but related behaviors. To this end, we propose a deep latent variable model that is capable of learning rewards from demonstrations of distinct but related tasks in an unsupervised way. Critically, our model can infer rewards for new, structurally-similar tasks from a single demonstration. Our experiments on multiple continuous control tasks demonstrate the effectiveness of our approach compared to state-of-the-art imitation and inverse reinforcement learning methods.",2019-09-20,2020-11-14 01:20,2020-12-19 14:38,2020-11-14 01:20,,,,,,,,,,,,,,en,,,,,arxiv.org,,ZSCC: 0000005,,/Users/angelica/Zotero/storage/NJH9RTMM/1909.html; /Users/angelica/Zotero/storage/AS4ZAW5U/Yu et al. - 2019 - Meta-Inverse Reinforcement Learning with Probabili.pdf; /Users/angelica/Zotero/storage/45Z4XSRZ/1909.html,,Other-org; NotSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2019,,,,,,,,,,,,,,,,"This work explores improving performance on multi-task inverse reinforcement learning in a single-shot setting by extending <@Adversarial Inverse Reinforcement Learning@>(@Learning Robust Rewards with Adversarial Inverse Reinforcement Learning@) with ""latent context variables"" that condition the learned reward function. The paper makes two notable contributions: 1) It details an algorithm to simultaneously learn a flexible reward function and a conditional policy with competitive few-shot generalization abilities from expert demonstrations of multiple related tasks _without_ task specifications or identifiers; 2) The authors empirically demonstrate strong performance of a policy trained on the inferred reward of a structurally similar task with modified environmental dynamics, claiming that in order to succeed ""the agent must correctly infer the underlying goal of the task instead of simply mimicking the demonstration""."
DCZ338VY,conferencePaper,2020,"Barde, Paul; Roy, Julien; Jeon, Wonseok; Pineau, Joelle; Pal, Christopher; Nowrouzezahrai, Derek",Adversarial Soft Advantage Fitting: Imitation Learning without Policy Optimization,Advances in Neural Information Processing Systems 33 (2020),,,,http://arxiv.org/abs/2006.13258,"Adversarial imitation learning alternates between learning a discriminator -- which tells apart expert's demonstrations from generated ones -- and a generator's policy to produce trajectories that can fool this discriminator. This alternated optimization is known to be delicate in practice since it compounds unstable adversarial training with brittle and sample-inefficient reinforcement learning. We propose to remove the burden of the policy optimization steps by leveraging a novel discriminator formulation. Specifically, our discriminator is explicitly conditioned on two policies: the one from the previous generator's iteration and a learnable policy. When optimized, this discriminator directly learns the optimal generator's policy. Consequently, our discriminator's update solves the generator's optimization problem for free: learning a policy that imitates the expert does not require an additional optimization loop. This formulation effectively cuts by half the implementation and computational burden of adversarial imitation learning algorithms by removing the reinforcement learning phase altogether. We show on a variety of tasks that our simpler approach is competitive to prevalent imitation learning methods.",2020-06-23,2020-08-28 17:25,2020-12-21 17:56,2020-08-28 17:25,,,,,,,Adversarial Soft Advantage Fitting,,,,,,,,,,,,arXiv.org,,ZSCC: 0000000  arXiv: 2006.13258,,/Users/angelica/Zotero/storage/QPUMG8ER/Barde et al. - 2020 - Adversarial Soft Advantage Fitting Imitation Lear.pdf; /Users/angelica/Zotero/storage/35Z2KHC4/2006.html,,Other-org; NotSafety; AmbiguosSafety,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2020,,,,,,,,,,,,,,,,"This work aims to simplify algorithms for adversarial imitation learning by using a _structured_ discriminator, which is parameterised by the current generator and a learned policy. They prove that if so formulated, the policy that yields the optimal discriminator is exactly the same as the policy that generated the expert data, which is also precisely what we hope the generator will learn. As long as the discriminator's learned policy is parameterised correctly such that it can be sampled and evaluated, this eliminates the need for a reinforcement learning outer loop for policy improvement, as this learned policy can be substituted in for the generator's policy in the next training iteration. They empirically show the competitiveness of their method with state-of-the-art algorithms across a small but increasingly complex suite of tasks."
9JKJL9X6,conferencePaper,2019,"Frazier, Spencer; Riedl, Mark",Improving Deep Reinforcement Learning in Minecraft with Action Advice,"arXiv:1908.01007 [cs, stat]",,,,http://arxiv.org/abs/1908.01007,"Training deep reinforcement learning agents complex behaviors in 3D virtual environments requires significant computational resources. This is especially true in environments with high degrees of aliasing, where many states share nearly identical visual features. Minecraft is an exemplar of such an environment. We hypothesize that interactive machine learning IML, wherein human teachers play a direct role in training through demonstrations, critique, or action advice, may alleviate agent susceptibility to aliasing. However, interactive machine learning is only practical when the number of human interactions is limited, requiring a balance between human teacher effort and agent performance. We conduct experiments with two reinforcement learning algorithms which enable human teachers to give action advice, Feedback Arbitration and Newtonian Action Advice, under visual aliasing conditions. To assess potential cognitive load per advice type, we vary the accuracy and frequency of various human action advice techniques. Training efficiency, robustness against infrequent and inaccurate advisor input, and sensitivity to aliasing are examined.",2019-08-02,2020-11-14 00:50,2020-12-19 15:31,2020-11-14 00:50,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000005  arXiv: 1908.01007,,/Users/angelica/Zotero/storage/HQEG6UUP/1908.html; /Users/angelica/Zotero/storage/V6AUTV2Z/Frazier and Riedl - 2019 - Improving Deep Reinforcement Learning in Minecraft.pdf,,Other-org; NotSafety,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AAAI 2019,,,,,,,,,,,,,,,,"This paper uses maze-traversal in Minecraft to look at the extent to which human advice can help with _aliasing_ in 3D environments, the problem where many states share nearly identical visual features. The paper compares two advice-giving algorithms that rely on neural nets which are trained to explore and predict the utilities of possible actions they can take, sometimes accepting human advice. The two algorithms differ primarily in whether they provide advice for the current action, or provide advice that persists for several actions.

Experimental results suggest that both algorithms, but especially the one that applies to multiple actions, help with the problem of 3D aliasing, potentially because the system can rely on the movement advice it got in previous timesteps rather than having to discern tricky visual features in the moment. The paper also varies the frequency and accuracy of the advice given, and finds that receiving more advice significantly improves performance, even if that advice is only 50% accurate."
YJ7M3CU6,conferencePaper,2019,"Perez, Ethan; Karamcheti, Siddharth; Fergus, Rob; Weston, Jason; Kiela, Douwe; Cho, Kyunghyun",Finding Generalizable Evidence by Learning to Convince Q&A Models,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processingand the 9th International Joint Conference on Natural Language Processing,,,,http://arxiv.org/abs/1909.05863,"We propose a system that finds the strongest supporting evidence for a given answer to a question, using passage-based question-answering (QA) as a testbed. We train evidence agents to select the passage sentences that most convince a pretrained QA model of a given answer, if the QA model received those sentences instead of the full passage. Rather than finding evidence that convinces one model alone, we find that agents select evidence that generalizes; agent-chosen evidence increases the plausibility of the supported answer, as judged by other QA models and humans. Given its general nature, this approach improves QA in a robust manner: using agent-selected evidence (i) humans can correctly answer questions with only ~20% of the full passage and (ii) QA models can generalize to longer passages and harder questions.",2019-09-12,2020-11-14 00:41,2020-12-20 15:53,2020-11-14 00:41,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s1]  ACC: 8  arXiv: 1909.05863,,/Users/angelica/Zotero/storage/BKEIU8B8/1909.html; /Users/angelica/Zotero/storage/5WIHVGWV/Perez et al. - 2019 - Finding Generalizable Evidence by Learning to Conv.pdf,,Other-org; NotSafety,Computer Science - Artificial Intelligence; Computer Science - Multiagent Systems; Computer Science - Computation and Language; Computer Science - Information Retrieval,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper tries to improve performance on multiple-choice questions about text passages using a technique similar to <@AI safety via debate@>. The set-up consists of a **judge model** and one or more **evidence agents**. First, the judge model is pretrained on samples consisting of a passage, a multiple-choice question about that passage, and the correct answer to that question. Then, in the experimental portion of the set-up, instead of looking at a full passage, the judge model looks at a subsequence of the passage created by combining the outputs from several evidence agents. Each evidence agent has been given the same passage and assigned a particular answer to the question, and must select a limited number of sentences from the passage to present to the judge model to convince it of that answer.

The paper varies several parameters in its setup, including the training process for the judge model, the questions used, the process evidence agents use to select sentences, etc. It finds that for many settings of these parameters, when judge models are tasked with generalizing from shorter passages to longer passages, or easier passages to harder passages, they do better with the new passages when assisted by the evidence agents. It also finds that the sentences given as evidence by the evidence agents are convincing to humans as well as the judge model."
,conferencePaper,2018,"Wang, Xin; Chen, Wenhu; Wang, Yuan-Fang; Wang, William Yang",No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),,,,http://arxiv.org/abs/1804.09160,"Though impressive results have been achieved in visual captioning, the task of generating abstract stories from photo streams is still a little-tapped problem. Different from captions, stories have more expressive language styles and contain many imaginary concepts that do not appear in the images. Thus it poses challenges to behavioral cloning algorithms. Furthermore, due to the limitations of automatic metrics on evaluating story quality, reinforcement learning methods with hand-crafted rewards also face difficulties in gaining an overall performance boost. Therefore, we propose an Adversarial REward Learning (AREL) framework to learn an implicit reward function from human demonstrations, and then optimize policy search with the learned reward function. Though automatic eval- uation indicates slight performance boost over state-of-the-art (SOTA) methods in cloning expert behaviors, human evaluation shows that our approach achieves significant improvement in generating more human-like stories than SOTA systems.",2018-07-08,2020-11-14 00:59,2020-12-19 15:06,2020-11-14 00:59,,,,,,,No Metrics Are Perfect,,,,,,"Melbourne, Australia",,,,,,arXiv.org,,ZSCC: 0000077  arXiv: 1804.09160,,/Users/angelica/Zotero/storage/DY9LHUMJ/1804.html; /Users/angelica/Zotero/storage/MA9K2WHF/Wang et al. - 2018 - No Metrics Are Perfect Adversarial Reward Learnin.pdf; /Users/angelica/Zotero/storage/PF45HNCX/Wang et al. - 2018 - No Metrics Are Perfect Adversarial Reward Learnin.pdf; /Users/angelica/Zotero/storage/SFQZVBMR/1804.html,,Other-org; NotSafety,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,56th Annual Meeting of the Association for Computational Linguistics,,,,,,,,,,,,,,,,"This paper tackles visual story-telling, the task of generating a story that matches a sequence of photos. It proposes learning a reward function from the labeled dataset that can then be optimized with reinforcement learning, with the hope that the reward function is a good compression of what we want and so leads to more generalizable behavior. They show that the standard automated techniques for evaluating visual stories are not very good, and so they perform a Mechanical Turk study that shows very good results compared to prior work. MTurk workers are often unable to tell whether the stories were generated by their algorithm or a human!

How does it work? Their architecture has a policy network that creates the stories and a reward network that provides the supervision, which are trained adversarially. We can think of the reward function as inducing a probability distribution over stories, where stories with higher reward are more probable. Then, the reward network acts as a discriminator, trying to make its implied probability distribution similar to the empirical data distribution and dissimilar to the policy network distribution, while the policy network acts as a generator, creating a policy that tries to match the implied probability distribution of the reward network. (This is equivalent to maximizing the expected reward from the reward network.)"
,conferencePaper,2020,"Beaulieu, Shawn; Frati, Lapo; Miconi, Thomas; Lehman, Joel; Stanley, Kenneth O.; Clune, Jeff; Cheney, Nick",Learning to Continually Learn,"arXiv:2002.09571 [cs, stat]",,,,http://arxiv.org/abs/2002.09571,"Continual lifelong learning requires an agent or model to learn many sequentially ordered tasks, building on previous knowledge without catastrophically forgetting it. Much work has gone towards preventing the default tendency of machine learning models to catastrophically forget, yet virtually all such work involves manually-designed solutions to the problem. We instead advocate meta-learning a solution to catastrophic forgetting, allowing AI to learn to continually learn. Inspired by neuromodulatory processes in the brain, we propose A Neuromodulated Meta-Learning Algorithm (ANML). It differentiates through a sequential learning process to meta-learn an activation-gating function that enables contextdependent selective activation within a deep neural network. Specifically, a neuromodulatory (NM) neural network gates the forward pass of another (otherwise normal) neural network called the prediction learning network (PLN). The NM network also thus indirectly controls selective plasticity (i.e. the backward pass of) the PLN. ANML enables continual learning without catastrophic forgetting at scale: it produces state-of-the-art continual learning performance, sequentially learning as many as 600 classes (over 9,000 SGD updates).",2020-03-03,2020-08-31 18:00,2020-12-21 17:58,2020-08-31 18:00,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000012  arXiv: 2002.09571,,/Users/angelica/Zotero/storage/S859IFAF/Beaulieu et al. - 2020 - Learning to Continually Learn.pdf,,Other-org; NotSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ECAI 2020,,,,,,,,,,,,,,,,"This paper presents the **ANML** (A Neuromodulated Meta-Learning algorithm) method for countering catastrophic forgetting in continual learning. Continual learning is a problem setting where the system is presented with several tasks in sequence, and must maintain good performance on all of them. When training on new tasks, neural networks often “forget” how to perform the previous tasks, which is called catastrophic forgetting. This makes the naive approach of just training on each task in sequence ineffective.

The paper has two main ideas. First, rather than avoiding catastrophic forgetting by using hand-crafted solutions (e.g. previous methods have encouraged sparsity), the authors use meta-learning to directly optimise for this goal. This is done by **learning a network parameterization which, after training sequentially on many tasks, will get good performance on all tasks**. This outer loop objective can be optimised for directly by taking higher order gradients (gradients of gradients). The second idea is a novel form of neuromodulation. This takes the form of a neuromodulatory (NM) network, which takes the same input as the prediction network, and gates the prediction network’s forward pass. **This provides direct control of the output of the prediction network, but also indirect control of the learning of the prediction network, as gradients will only flow through the paths which haven’t been zeroed out by the gating mechanism.**

**Their method achieves state-of-the-art results on continual learning in Omniglot**, a few-shot dataset consisting of 1623 characters, each with only 20 hand-drawn examples. The network has to learn a sequence of tasks (e.g. classifying a character) with only 15 examples, and is then tested on overall performance over all the classes it’s learned. Their network gets 60% accuracy when presented with 600 classes in a row. **A classifier trained with the same data but shuffled independently at random only gets 68% accuracy**, implying that the catastrophic forgetting of their network only cost 8 percentage points. **Their method also learns a form of sparsity in the activations of the network in a much better way than the hand-crafted methods** - while per-class activations are very sparse, no neurons are wasted, as they all still activate over the entire dataset."
,conferencePaper,2020,"Jin, Di; Jin, Zhijing; Zhou, Joey Tianyi; Szolovits, Peter",Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment,Proceedings of the AAAI Conference on Artificial Intelligence,,,,http://arxiv.org/abs/1907.11932,"Machine learning algorithms are often vulnerable to adversarial examples that have imperceptible alterations from the original counterparts but can fool the state-of-the-art models. It is helpful to evaluate or even improve the robustness of these models by exposing the maliciously crafted adversarial examples. In this paper, we present TextFooler, a simple but strong baseline to generate natural adversarial text. By applying it to two fundamental natural language tasks, text classification and textual entailment, we successfully attacked three target models, including the powerful pre-trained BERT, and the widely used convolutional and recurrent neural networks. We demonstrate the advantages of this framework in three ways: (1) effective---it outperforms state-of-the-art attacks in terms of success rate and perturbation rate, (2) utility-preserving---it preserves semantic content and grammaticality, and remains correctly classified by humans, and (3) efficient---it generates adversarial text with computational complexity linear to the text length. *The code, pre-trained target models, and test examples are available at https://github.com/jind11/TextFooler.",2020-04-08,2020-08-31 18:47,2020-12-21 18:14,2020-08-31 18:47,,,,34,,,Is BERT Really Robust?,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s1]  ACC: 14  arXiv: 1907.11932,,/Users/angelica/Zotero/storage/Q6P7HAQB/Jin et al. - 2020 - Is BERT Really Robust A Strong Baseline for Natur.pdf; /Users/angelica/Zotero/storage/H44TF68D/1907.html,,Other-org; NotSafety; AmbiguosSafety,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AAAI Conference on Artificial Intelligence,,,,,,,,,,,,,,,,"This paper presents TextFooler, an algorithm for generating adversarial text for natural language tasks with only black-box access to models. TextFooler tries to generate sentences that are grammatical and semantically similar to original input sentences but produce incorrect labels. It does this by identifying a small set of most important words in the original sentence, generating candidate synonyms for those words, and gradually replacing the important words in the sentence by testing which synonyms cause the model to mispredict or report the least confidence score.

TextFooler is tested on three state-of-the-art NLP models-- WordCNN, WordLSTM, and BERT, all trained to ~80 - 90% test accuracy. On a variety of text classification datasets, TextFooler reduces accuracy to below ~15% with less than ~20% of the words perturbed. Humans evaluating the generated sentences say they are approximately as grammatical as the original, have the same label as the original in ~90% of cases, and have a sentence similarity score to the original sentence of 0.9 on a 0 to 1 scale. The paper finds that generally, models with higher original accuracy have higher after-attack acuracy.

The authors retrain BERT from scratch using data produced by TextFooler and then attack it using TextFooler again. They find that the after-attack accuracy is higher and that attacks require more perturbed words."
,conferencePaper,2020,"Ye, Deheng; Liu, Zhao; Sun, Mingfei; Shi, Bei; Zhao, Peilin; Wu, Hao; Yu, Hongsheng; Yang, Shaojie; Wu, Xipeng; Guo, Qingwei; Chen, Qiaobo; Yin, Yinyuting; Zhang, Hao; Shi, Tengfei; Wang, Liang; Fu, Qiang; Yang, Wei; Huang, Lanxiao",Mastering Complex Control in MOBA Games with Deep Reinforcement Learning,arXiv:1912.09729 [cs],,,,http://arxiv.org/abs/1912.09729,"We study the reinforcement learning problem of complex action control in the Multi-player Online Battle Arena (MOBA) 1v1 games. This problem involves far more complicated state and action spaces than those of traditional 1v1 games, such as Go and Atari series, which makes it very difﬁcult to search any policies with human-level performance. In this paper, we present a deep reinforcement learning framework to tackle this problem from the perspectives of both system and algorithm. Our system is of low coupling and high scalability, which enables efﬁcient explorations at large scale. Our algorithm includes several novel strategies, including control dependency decoupling, action mask, target attention, and dualclip PPO, with which our proposed actor-critic network can be effectively trained in our system. Tested on the MOBA game Honor of Kings, the trained AI agents can defeat top professional human players in full 1v1 games.",2020-01-02,2020-08-31 18:30,2020-12-21 18:40,2020-08-31 18:30,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: NoCitationData[s1]  ACC: 8  arXiv: 1912.09729,,/Users/angelica/Zotero/storage/LT9MLIQB/Ye et al. - 2020 - Mastering Complex Control in MOBA Games with Deep .pdf,,Other-org; NotSafety,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AAAI 2020,,,,,,,,,,,,,,,,"This paper presents an AI system that can play the Multi-player Online Battle Arena (MOBA) game _Honor of Kings_. They are inspired by <@OpenAI Five@> (and Honor of Kings sounds quite similar to Dota, though it is 1v1 instead of 5v5), and have a similar learning setup: reinforcement learning using PPO. Their architecture requires an off-policy algorithm (I’m not sure why, maybe they have stale parameters across their rollout servers), so they add an importance sampling correction to the PPO objective, as well as an additional type of gradient clipping. The input is a combination of the image and underlying game state info. The resulting agents are able to beat top human players, and in an event with the public, the AI system lost only 4 out of 2100 matches. Unlike OpenAI Five, this required only around 100 hours to train (though it’s unclear how much compute was used)."
,conferencePaper,2020,"Ghorbani, Amirata; Zou, James",Neuron Shapley: Discovering the Responsible Neurons,"34th Conference on Neural Information Processing Systems (NeurIPS 2020),",,,,http://arxiv.org/abs/2002.09815,"We develop Neuron Shapley as a new framework to quantify the contribution of individual neurons to the prediction and performance of a deep network. By accounting for interactions across neurons, Neuron Shapley is more effective in identifying important filters compared to common approaches based on activation patterns. Interestingly, removing just 30 filters with the highest Shapley scores effectively destroys the prediction accuracy of Inception-v3 on ImageNet. Visualization of these few critical filters provides insights into how the network functions. Neuron Shapley is a flexible framework and can be applied to identify responsible neurons in many tasks. We illustrate additional applications of identifying filters that are responsible for biased prediction in facial recognition and filters that are vulnerable to adversarial attacks. Removing these filters is a quick way to repair models. Enabling all these applications is a new multi-arm bandit algorithm that we developed to efficiently estimate Neuron Shapley values.",2020-02-25,2020-09-05 17:36,2020-12-21 18:07,2020-09-05 17:36,,,,,,,Neuron Shapley,,,,,,,,,,,,arXiv.org,,ZSCC: 0000001  arXiv: 2002.09815,,/Users/angelica/Zotero/storage/7DPG7NPJ/Ghorbani and Zou - 2020 - Neuron Shapley Discovering the Responsible Neuron.pdf; /Users/angelica/Zotero/storage/624F6ZWC/2002.html,,Other-org; NotSafety; AmbiguosSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2020,,,,,,,,,,,,,,,,"This paper presents a novel method, Neuron Shapley, that uses the [Shapley value framework](https://en.wikipedia.org/wiki/Shapley_value) to measure the importance of different neurons in determining an arbitrary metric of the neural net output. (Shapley values have been applied to machine learning before to [measure the importance of features to a model's output](https://christophm.github.io/interpretable-ml-book/shapley.html), but here the authors use them to calculate neuron importance.) Due to several novel approaches and optimisations in calculating these Shapley values, **the top k most responsible neurons (k ~ 30) can be feasibly found for large networks such as Inception-v3**.

The authors demonstrate that finding these neurons enables the performance of model surgery. Removing the top 30 neurons that contribute to accuracy completely destroys the accuracy, whereas in expectation removing 30 neurons at random from the network barely moves the accuracy at all. Since the method can be applied to an arbitrary metric, this kind of surgery can be performed for other metrics we care about. For example, removing the neurons which are most responsible for vulnerability to adversarial attacks makes the network more robust, and removing the neurons most responsible for the class-accuracy imbalance (a fairness metric) makes the classes much more even, while only reducing the overall accuracy a small amount."
,conferencePaper,2019,"Rhinehart, Nicholas; McAllister, Rowan; Kitani, Kris; Levine, Sergey",PRECOG: PREdiction Conditioned On Goals in Visual Multi-Agent Settings,"Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019",,,,http://arxiv.org/abs/1905.01296,"For autonomous vehicles (AVs) to behave appropriately on roads populated by human-driven vehicles, they must be able to reason about the uncertain intentions and decisions of other drivers from rich perceptual information. Towards these capabilities, we present a probabilistic forecasting model of future interactions between a variable number of agents. We perform both standard forecasting and the novel task of conditional forecasting, which reasons about how all agents will likely respond to the goal of a controlled agent (here, the AV). We train models on real and simulated data to forecast vehicle trajectories given past positions and LIDAR. Our evaluation shows that our model is substantially more accurate in multi-agent driving scenarios compared to existing state-of-the-art. Beyond its general ability to perform conditional forecasting queries, we show that our model's predictions of all agents improve when conditioned on knowledge of the AV's goal, further illustrating its capability to model agent interactions.",2019-09-30,2020-11-14 00:36,2020-12-19 17:01,2020-11-14 00:36,,,,,,,PRECOG,,,,,,,,,,,,arXiv.org,,ZSCC: 0000067  arXiv: 1905.01296,,/Users/angelica/Zotero/storage/SIGUVH5J/1905.html; /Users/angelica/Zotero/storage/Z7ULACMN/Rhinehart et al. - 2019 - PRECOG PREdiction Conditioned On Goals in Visual .pdf,,NotSafety; CHAI-Berkeley,Computer Science - Artificial Intelligence; Computer Science - Robotics; Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper models a multi-agent self driving car scenario by developing a model of future states conditional on both its own action and the action of multiple humans, and picking the latent-space action that balances between the desiderata of reaching its goal and preferring trajectories seen in the expert multi-agent trajectories its shown (where, e.g., two human agents rarely crash into one another)."
,conferencePaper,2019,"Behbahani, Feryal; Shiarlis, Kyriacos; Chen, Xi; Kurin, Vitaly; Kasewa, Sudhanshu; Stirbu, Ciprian; Gomes, João; Paul, Supratik; Oliehoek, Frans A.; Messias, João; Whiteson, Shimon",Learning from Demonstration in the Wild,2019 International Conference on Robotics and Automation (ICRA),,,,http://arxiv.org/abs/1811.03516,"Learning from demonstration (LfD) is useful in settings where hand-coding behaviour or a reward function is impractical. It has succeeded in a wide range of problems but typically relies on manually generated demonstrations or specially deployed sensors and has not generally been able to leverage the copious demonstrations available in the wild: those that capture behaviours that were occurring anyway using sensors that were already deployed for another purpose, e.g., traffic camera footage capturing demonstrations of natural behaviour of vehicles, cyclists, and pedestrians. We propose Video to Behaviour (ViBe), a new approach to learn models of behaviour from unlabelled raw video data of a traffic scene collected from a single, monocular, initially uncalibrated camera with ordinary resolution. Our approach calibrates the camera, detects relevant objects, tracks them through time, and uses the resulting trajectories to perform LfD, yielding models of naturalistic behaviour. We apply ViBe to raw videos of a traffic intersection and show that it can learn purely from videos, without additional expert knowledge.",2019-03-25,2020-11-14 00:49,2020-12-19 15:33,2020-11-14 00:49,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000017  arXiv: 1811.03516,,/Users/angelica/Zotero/storage/LAH4YFTR/1811.html; /Users/angelica/Zotero/storage/ZCCERNMX/Behbahani et al. - 2019 - Learning from Demonstration in the Wild.pdf,,Other-org; NotSafety,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 International Conference on Robotics and Automation (ICRA),,,,,,,,,,,,,,,,"This paper learns traffic trajectories from unsupervised data by converting traffic camera footage into a Unity scene simulation, using that simulation to generate pseudo-LIDAR readings for each ""expert trajectory"", and then training an agent to imitate them using a variant of generative adversarial imitation learning (GAIL)."
,conferencePaper,2018,"Tung, Hsiao-Yu Fish; Harley, Adam W.; Huang, Liang-Kang; Fragkiadaki, Katerina",Reward Learning from Narrated Demonstrations,Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,,,,http://arxiv.org/abs/1804.10692,"Humans effortlessly ""program"" one another by communicating goals and desires in natural language. In contrast, humans program robotic behaviours by indicating desired object locations and poses to be achieved, by providing RGB images of goal configurations, or supplying a demonstration to be imitated. None of these methods generalize across environment variations, and they convey the goal in awkward technical terms. This work proposes joint learning of natural language grounding and instructable behavioural policies reinforced by perceptual detectors of natural language expressions, grounded to the sensory inputs of the robotic agent. Our supervision is narrated visual demonstrations(NVD), which are visual demonstrations paired with verbal narration (as opposed to being silent). We introduce a dataset of NVD where teachers perform activities while describing them in detail. We map the teachers' descriptions to perceptual reward detectors, and use them to train corresponding behavioural policies in simulation.We empirically show that our instructable agents (i) learn visual reward detectors using a small number of examples by exploiting hard negative mined configurations from demonstration dynamics, (ii) develop pick-and place policies using learned visual reward detectors, (iii) benefit from object-factorized state representations that mimic the syntactic structure of natural language goal expressions, and (iv) can execute behaviours that involve novel objects in novel locations at test time, instructed by natural language.",2018-04-27,2020-11-14 01:12,2020-12-19 14:44,2020-11-14 01:12,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000014  arXiv: 1804.10692,,/Users/angelica/Zotero/storage/S7XRLNA9/1804.html; /Users/angelica/Zotero/storage/LL3XTG9L/Tung et al. - 2018 - Reward Learning from Narrated Demonstrations.pdf,,Other-org; NotSafety,Computer Science - Robotics; Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IEEE Conference on Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,"This paper learns and optimizes rewards given demonstrations of behavior along with a description of the behavior in natural language. Their dataset is a set of videos of humans demonstrating a task and describing it with natural language (such as ""the orange is in the bowl""). They combine several techniques to use this dataset to teach a robot. First, using speech recognition, they get a transcript of the natural language aligned with the video. They use object detectors to figure out what things are present in the image, and a syntactic parser to figure out the subject and object of the sentence, and match up these two results to figure out which objects in the image the natural language refers to, and extract their spatial features. They then train a classifier to take the spatial features and detecting whether it has achieved the goal, conditioned on the natural language description of the task. Now that they have a reward function (1 at a goal state, 0 otherwise) they can train a robot using DQN, though to get this to work they infer 3D object configurations from 2D images and use distance to the goal as a shaped reward."
,conferencePaper,2019,"Bussmann, Bart; Heinerman, Jacqueline; Lehman, Joel",Towards Empathic Deep Q-Learning,Proceedings of the Workshop on Artificial Intelligence Safety 2019,,,,http://arxiv.org/abs/1906.10918,"As reinforcement learning (RL) scales to solve increasingly complex tasks, interest continues to grow in the fields of AI safety and machine ethics. As a contribution to these fields, this paper introduces an extension to Deep Q-Networks (DQNs), called Empathic DQN, that is loosely inspired both by empathy and the golden rule (""Do unto others as you would have them do unto you""). Empathic DQN aims to help mitigate negative side effects to other agents resulting from myopic goal-directed behavior. We assume a setting where a learning agent coexists with other independent agents (who receive unknown rewards), where some types of reward (e.g. negative rewards from physical harm) may generalize across agents. Empathic DQN combines the typical (self-centered) value with the estimated value of other agents, by imagining (by its own standards) the value of it being in the other's situation (by considering constructed states where both agents are swapped). Proof-of-concept results in two gridworld environments highlight the approach's potential to decrease collateral harms. While extending Empathic DQN to complex environments is non-trivial, we believe that this first step highlights the potential of bridge-work between machine ethics and RL to contribute useful priors for norm-abiding RL agents.",2019-06-26,2020-11-14 00:53,2020-12-19 15:20,2020-11-14 00:53,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000001  arXiv: 1906.10918,,/Users/angelica/Zotero/storage/GTSKRQ99/1906.html; /Users/angelica/Zotero/storage/WEJ43S6K/Bussmann et al. - 2019 - Towards Empathic Deep Q-Learning.pdf,,Other-org; NotSafety,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Workshop on Artificial Intelligence Safety 2019,,,,,,,,,,,,,,,,"This paper introduces the empathic DQN, which is inspired by the golden rule: ""Do unto others as you would have them do unto you"". Given a specified reward, the empathic DQN optimizes for a weighted combination of the specified reward, and the reward that other agents in the environment would get if they were a copy of the agent. They show that this results in resource sharing (when there are diminishing returns to resources) and avoiding conflict in two toy gridworlds."
,conferencePaper,2019,"Shum, Michael; Kleiman-Weiner, Max; Littman, Michael L.; Tenenbaum, Joshua B.",Theory of Minds: Understanding Behavior in Groups Through Inverse Planning,Proceedings of the AAAI Conference on Artificial Intelligence,,,,http://arxiv.org/abs/1901.06085,"Human social behavior is structured by relationships. We form teams, groups, tribes, and alliances at all scales of human life. These structures guide multi-agent cooperation and competition, but when we observe others these underlying relationships are typically unobservable and hence must be inferred. Humans make these inferences intuitively and flexibly, often making rapid generalizations about the latent relationships that underlie behavior from just sparse and noisy observations. Rapid and accurate inferences are important for determining who to cooperate with, who to compete with, and how to cooperate in order to compete. Towards the goal of building machine-learning algorithms with human-like social intelligence, we develop a generative model of multi-agent action understanding based on a novel representation for these latent relationships called Composable Team Hierarchies (CTH). This representation is grounded in the formalism of stochastic games and multi-agent reinforcement learning. We use CTH as a target for Bayesian inference yielding a new algorithm for understanding behavior in groups that can both infer hidden relationships as well as predict future actions for multiple agents interacting together. Our algorithm rapidly recovers an underlying causal model of how agents relate in spatial stochastic games from just a few observations. The patterns of inference made by this algorithm closely correspond with human judgments and the algorithm makes the same rapid generalizations that people do.",2019-01-17,2020-11-14 00:31,2020-12-19 17:12,2020-11-14 00:31,,,,33,,,Theory of Minds,,,,,,,,,,,,arXiv.org,,ZSCC: 0000023  arXiv: 1901.06085,,/Users/angelica/Zotero/storage/W3GER4TE/1901.html; /Users/angelica/Zotero/storage/AXUCNZDI/Shum et al. - 2019 - Theory of Minds Understanding Behavior in Groups .pdf,,Other-org; NotSafety,Computer Science - Artificial Intelligence; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,AAAI Conference on Artificial Intelligence,,,,,,,,,,,,,,,,"This paper introduces Composable Team Hierarchies (CTH), a representation designed for reasoning about how agents reason about each other in collaborative and competitive environments. CTH uses two ""planning operators"": the Best Response operator returns the best policy in a single-agent game, and the Joint Planning operator returns the best team policy when all agents are cooperating. Competitive policies can then be derived via recursive application of those operations to subsets of agents (while holding the policies of other agents fixed). CTH draws from ideas in level-K planning (in which each agent assumes all other agents are at level K-1) and cooperative planning, but is more powerful than either approach.

The authors experiment with using CTH to probabilistically infer policies and future actions of agents participating in the stag-hunt task; they find that these judgements correlate well with human data."
,conferencePaper,2020,"Bobu, Andreea; Scobee, Dexter R. R.; Fisac, Jaime F.; Sastry, S. Shankar; Dragan, Anca D.",LESS is More: Rethinking Probabilistic Models of Human Behavior,Proceedings of the 2020 ACM/IEEE International Conference on Human-Robot Interaction,,,10.1145/3319502.3374811,http://arxiv.org/abs/2001.04465,"Robots need models of human behavior for both inferring human goals and preferences, and predicting what people will do. A common model is the Boltzmann noisily-rational decision model, which assumes people approximately optimize a reward function and choose trajectories in proportion to their exponentiated reward. While this model has been successful in a variety of robotics domains, its roots lie in econometrics, and in modeling decisions among different discrete options, each with its own utility or reward. In contrast, human trajectories lie in a continuous space, with continuous-valued features that influence the reward function. We propose that it is time to rethink the Boltzmann model, and design it from the ground up to operate over such trajectory spaces. We introduce a model that explicitly accounts for distances between trajectories, rather than only their rewards. Rather than each trajectory affecting the decision independently, similar trajectories now affect the decision together. We start by showing that our model better explains human behavior in a user study. We then analyze the implications this has for robot inference, first in toy environments where we have ground truth and find more accurate inference, and finally for a 7DOF robot arm learning from user demonstrations.",2020-03-09,2020-09-05 17:24,2020-11-24 01:54,2020-09-05 17:24,429-437,,,,,,LESS is More,,,,,,,,,,,,arXiv.org,,ZSCC: 0000003  arXiv: 2001.04465,,/Users/angelica/Zotero/storage/CRWIT633/Bobu et al. - 2020 - LESS is More Rethinking Probabilistic Models of H.pdf; /Users/angelica/Zotero/storage/NESEC5Y2/2001.html,,NotSafety; CHAI-Berkeley; AmbiguosSafety,Computer Science - Artificial Intelligence; Computer Science - Human-Computer Interaction; Computer Science - Robotics; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper introduces a new model for robots inferring human preferences called LESS. The traditional Boltzmann noisily-rational decision model assumes people approximately optimize a reward function and choose trajectories in proportion to their exponentiated reward. The Boltzmann model works well when modeling decisions among different discrete options, but runs into problems when modeling human trajectories in a continuous space, e.g. path finding, because it is very sensitive to the number of trajectories, even if they are similar-- if a robot using a Boltzmann model must predict whether a human navigates around an obstacle by taking one path on the left or one of three very-similar paths on the right, it will assign the same probability to each path by default.

To fix this, LESS predicts human behavior by treating each trajectory as part of a continuous space and mapping each one to a feature vector. The likelihood of selecting a trajectory is inversely proportional to its feature-space similarity with other trajectories, meaning similar trajectories are appropriately deweighted.

The paper tests the predictive performance of LESS vs. Boltzmann in several experimental environments, including an artifically constructed task where humans are asked to choose between similar paths for navigating around an obstacle, and a real-world task where humans demonstrate appropriate behaviors to a 7-degree-of-freedom robotic arm. In general, LESS performs better than Boltzmann when given a small number of samples of human behavior, but does equally well as the sample size is increased. In the robotic arm task, Boltzmann performed better when demonstrations were aggregated into a single batch and inference was run on the whole batch at once, representing trying to approximate the 'average' user rather than customizing behavior to each user. The paper claims that this happens because Boltzmann overlearns from demonstrations in sparse regions, and underlearns from dense demonstrations. As you increase the number of samples, you approximate the “true” trajectory space better and better, so the 10 trajectory sets vary less and less, which means Boltzmann won’t underperform so much. Since the single batch demonstration aggregated demonstrations, it had a similar effect in approximating the ""true"" trajectory space.

The paper notes that one limitation of this method is a reliance on a pre-specified set of robot features, though a small set of experimental results suggested that LESS still performed better than Boltzmann when adding a small number of irrelevant features."
,conferencePaper,2018,"Song, Jiaming; Ren, Hongyu; Sadigh, Dorsa; Ermon, Stefano",Multi-Agent Generative Adversarial Imitation Learning,Advances in Neural Information Processing Systems 31 (NeurIPS 2018),,,,http://arxiv.org/abs/1807.09936,"Imitation learning algorithms can be used to learn a policy from expert demonstrations without access to a reward signal. However, most existing approaches are not applicable in multi-agent settings due to the existence of multiple (Nash) equilibria and non-stationary environments. We propose a new framework for multi-agent imitation learning for general Markov games, where we build upon a generalized notion of inverse reinforcement learning. We further introduce a practical multi-agent actor-critic algorithm with good empirical performance. Our method can be used to imitate complex behaviors in high-dimensional environments with multiple cooperative or competing agents.",2018-07-25,2020-11-14 00:36,2020-12-19 17:00,2020-11-14 00:36,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000045  arXiv: 1807.09936,,/Users/angelica/Zotero/storage/U8VEZT6P/1807.html; /Users/angelica/Zotero/storage/IH43EH97/Song et al. - 2018 - Multi-Agent Generative Adversarial Imitation Learn.pdf,,Other-org; NotSafety,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2018,,,,,,,,,,,,,,,,"This paper generalizes [GAIL](http://www.jonathanho.me/files/HoErmon_NIPS2016.pdf) (which was covered [last week](https://mailchi.mp/ad852629e45a/alignment-newsletter-17)) to the multiagent setting, where we want to imitate a group of interacting agents. They want to find a Nash equilibrium in particular. They formalize the Nash equilibrium constraints and use this to motivate a particular optimization problem for multiagent IRL, that looks very similar to their optimization problem for regular IRL in GAIL. After that, it is quite similar to GAIL -- they use a regularizer ψ for the reward functions, show that the composition of multiagent RL and multiagent IRL can be solved as a single optimization problem involving the convex conjugate of ψ, and propose a particular instantiation of ψ that is data-dependent, giving an algorithm. They do have to assume in the theory that the multiagent RL problem has a unique solution, which is not typically true, but may not be too important. As before, to make the algorithm practical, they structure it like a GAN, with discriminators acting like reward functions. What if we have prior information that the game is cooperative or competitive? In this case, they propose changing the regularizer ψ, making it keep all the reward functions the same (if cooperative), making them negations of each other (in two-player zero-sum games), or leaving it as is. They evaluate in a variety of simple multiagent games, as well as a plank environment in which the environment changes between training and test time, thus requiring the agent to learn a robust policy, and find that the correct variant of MAGAIL (cooperative/competitive/neither) outperforms both behavioral cloning and single-agent GAIL (which they run N times to infer a separate reward for each agent)."
,conferencePaper,2018,"Lacotte, Jonathan; Ghavamzadeh, Mohammad; Chow, Yinlam; Pavone, Marco",Risk-Sensitive Generative Adversarial Imitation Learning,Proceedings of Machine Learning Research,,,,https://arxiv.org/abs/1808.04468v2,"We study risk-sensitive imitation learning where the agent's goal is to perform at least as well as the expert in terms of a risk profile. We first formulate our risk-sensitive imitation learning setting. We consider the generative adversarial approach to imitation learning (GAIL) and derive an optimization problem for our formulation, which we call it risk-sensitive GAIL (RS-GAIL). We then derive two different versions of our RS-GAIL optimization problem that aim at matching the risk profiles of the agent and the expert w.r.t. Jensen-Shannon (JS) divergence and Wasserstein distance, and develop risk-sensitive generative adversarial imitation learning algorithms based on these optimization problems. We evaluate the performance of our algorithms and compare them with GAIL and the risk-averse imitation learning (RAIL) algorithms in two MuJoCo and two OpenAI classical control tasks.",2018-08-13,2020-11-14 01:23,2020-12-19 14:30,2020-11-14 01:23,,,,,,,,,,,,,,en,,,,,arxiv.org,,ZSCC: NoCitationData[s0]  ACC: 11,,/Users/angelica/Zotero/storage/DAESW34J/1808.html; /Users/angelica/Zotero/storage/4YJLHV3W/Lacotte et al. - 2018 - Risk-Sensitive Generative Adversarial Imitation Le.pdf,,NotSafety; DeepMind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper extends GAIL to perform imitation learning where we try to optimize a policy for the mean reward collected under the constraint that the policy is no more risky than the expert policy. Since we don't know the true cost function, we have to approximate this problem with another problem where we infer the cost function as well, and evaluate the risk profile relative to the inferred cost function. The algorithm ends up looking very similar to the original GAIL algorithm, where the gradient updates change in order to include terms dependent on the conditional value-at-risk (CVaR). They evaluate against GAIL and RAIL (another risk-sensitive imitation learning algorithm) and find that their method performs the best on the Hopper and Walker Mujoco environments."
,conferencePaper,2019,"Singh, Avi; Yang, Larry; Hartikainen, Kristian; Finn, Chelsea; Levine, Sergey",End-to-End Robotic Reinforcement Learning without Reward Engineering,"arXiv:1904.07854 [cs, stat]",,,,http://arxiv.org/abs/1904.07854,"The combination of deep neural network models and reinforcement learning algorithms can make it possible to learn policies for robotic behaviors that directly read in raw sensory inputs, such as camera images, effectively subsuming both estimation and control into one model. However, real-world applications of reinforcement learning must specify the goal of the task by means of a manually programmed reward function, which in practice requires either designing the very same perception pipeline that end-to-end reinforcement learning promises to avoid, or else instrumenting the environment with additional sensors to determine if the task has been performed successfully. In this paper, we propose an approach for removing the need for manual engineering of reward specifications by enabling a robot to learn from a modest number of examples of successful outcomes, followed by actively solicited queries, where the robot shows the user a state and asks for a label to determine whether that state represents successful completion of the task. While requesting labels for every single state would amount to asking the user to manually provide the reward signal, our method requires labels for only a tiny fraction of the states seen during training, making it an efficient and practical approach for learning skills without manually engineered rewards. We evaluate our method on real-world robotic manipulation tasks where the observations consist of images viewed by the robot's camera. In our experiments, our method effectively learns to arrange objects, place books, and drape cloth, directly from images and without any manually specified reward functions, and with only 1-4 hours of interaction with the real world.",2019-05-15,2020-11-14 00:45,2020-12-19 15:39,2020-11-14 00:45,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000063  arXiv: 1904.07854,,/Users/angelica/Zotero/storage/273CLHH4/1904.html; /Users/angelica/Zotero/storage/VVYTBKFP/Singh et al. - 2019 - End-to-End Robotic Reinforcement Learning without .pdf,,NotSafety; CHAI-Berkeley,Computer Science - Robotics; Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Robotics: Science and Systems 2019,,,,,,,,,,,,,,,,"This paper demonstrates an approach that can learn to perform real world robotics tasks based not on example trajectories (states and actions) but just a small number (10) of pixel-level images of goal states showing successful task completion. Their method learns a GAN-like classifier to predict whether a given image is a success, continually adding data sampled from the still-learning policy to the set of negative examples, so the model at each step needs to further refine its model of success. The classifier, which is used as the reward signal in learning the policy, also makes use of a simple active learning approach, choosing the state its classifier is most confident is success and querying a human about it on fixed intervals, ultimately using less than 75 queries in all cases."
,conferencePaper,2016,"Ho, M. K.; Littman, M. L.; MacGlashan, J.; Cushman, F.; Austerweil, J. L.",Showing versus doing: Teaching by demonstration,NeurIPS,,,,https://par.nsf.gov/biblio/10082788-showing-versus-doing-teaching-demonstration,,16-Jan,2020-11-14 02:45,2020-12-19 00:42,2020-11-14 02:45,,,,,,,Showing versus doing,,,,,,,en,,,,,par.nsf.gov,,ZSCC: 0000050,,/Users/angelica/Zotero/storage/SDN8TSSD/Ho et al. - 2016 - Showing versus doing Teaching by demonstration.pdf; /Users/angelica/Zotero/storage/NJQ9UDHC/10082788.html; /Users/angelica/Zotero/storage/YG663AYK/10082788.html,,Other-org; NotSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS,,,,,,,,,,,,,,,,"This paper creates and validates a model of _pedagogy_ as applied to reward learning. Typically, inverse reinforcement learning (IRL) algorithms assume access to a set of demonstrations that are created from an approximately _optimal_ policy. However, in practice, when people are asked to _show_ a task, they don't give the optimal trajectory; they give the trajectory that helps the learner best _disambiguate_ between the possible tasks. They formalize this by creating a model in two steps:

1. A literal or IRL robot is one which learns rewards under the model that the demonstrator is Boltzmann rational.
2. The pedagogic human shows trajectories in proportion to how likely a literal robot would think the true reward is upon seeing the trajectory.

They validate this model with user studies and find that it predicts human demonstrations well."
,conferencePaper,2019,"Goyal, Prasoon; Niekum, Scott; Mooney, Raymond J.",Using Natural Language for Reward Shaping in Reinforcement Learning,Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI-19),,,,http://arxiv.org/abs/1903.02020,"Recent reinforcement learning (RL) approaches have shown strong performance in complex domains such as Atari games, but are often highly sample inefficient. A common approach to reduce interaction time with the environment is to use reward shaping, which involves carefully designing reward functions that provide the agent intermediate rewards for progress towards the goal. However, designing appropriate shaping rewards is known to be difficult as well as time-consuming. In this work, we address this problem by using natural language instructions to perform reward shaping. We propose the LanguagE-Action Reward Network (LEARN), a framework that maps free-form natural language instructions to intermediate rewards based on actions taken by the agent. These intermediate language-based rewards can seamlessly be integrated into any standard reinforcement learning algorithm. We experiment with Montezuma's Revenge from the Atari Learning Environment, a popular benchmark in RL. Our experiments on a diverse set of 15 tasks demonstrate that, for the same number of interactions with the environment, language-based rewards lead to successful completion of the task 60% more often on average, compared to learning without language.",2019-05-31,2020-11-14 00:51,2020-12-19 15:31,2020-11-14 00:51,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000026  arXiv: 1903.02020,,/Users/angelica/Zotero/storage/S5RPI4DN/1903.html; /Users/angelica/Zotero/storage/J4TZ869W/Goyal et al. - 2019 - Using Natural Language for Reward Shaping in Reinf.pdf,,Other-org; NotSafety,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper constructs a dataset for grounding natural language in Atari games, and uses it to improve performance on Atari. They have humans annotate short clips with natural language: for example, ""jump over the skull while going to the left"" in Montezuma's Revenge. They use this to build a model that predicts whether a given trajectory matches a natural language instruction. Then, while training an agent to play Atari, they have humans give the AI system an instruction in natural language. They use their natural language model to predict the probability that the trajectory matches the instruction, and add that as an extra shaping term in the reward. This leads to faster learning."
,conferencePaper,2015,"Javdani, Shervin; Srinivasa, Siddhartha S.; Bagnell, J. Andrew",Shared Autonomy via Hindsight Optimization,Robotics Science and Systems Online Proceedings,,,,http://arxiv.org/abs/1503.07619,"In shared autonomy, user input and robot autonomy are combined to control a robot to achieve a goal. Often, the robot does not know a priori which goal the user wants to achieve, and must both predict the user's intended goal, and assist in achieving that goal. We formulate the problem of shared autonomy as a Partially Observable Markov Decision Process with uncertainty over the user's goal. We utilize maximum entropy inverse optimal control to estimate a distribution over the user's goal based on the history of inputs. Ideally, the robot assists the user by solving for an action which minimizes the expected cost-to-go for the (unknown) goal. As solving the POMDP to select the optimal action is intractable, we use hindsight optimization to approximate the solution. In a user study, we compare our method to a standard predict-then-blend approach. We find that our method enables users to accomplish tasks more quickly while utilizing less input. However, when asked to rate each system, users were mixed in their assessment, citing a tradeoff between maintaining control authority and accomplishing tasks quickly.",2015-04-17,2020-11-14 00:41,2020-12-19 16:55,2020-11-14 00:41,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000105  arXiv: 1503.07619,,/Users/angelica/Zotero/storage/X4N47AN7/1503.html; /Users/angelica/Zotero/storage/JEXD79E5/Javdani et al. - 2015 - Shared Autonomy via Hindsight Optimization.pdf,,Other-org; NotSafety,Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Robotics Science and Systems,,,,,,,,,,,,,,,,"This paper considers a shared autonomy task in which a user controls a robot to achieve some goal, and the robot learns to assist the user, without knowing the goal in advance. They formalize this as a POMDP in which the state includes the user's goal, which the robot does not get to observe. However, the POMDP observation model assigns higher probability to user actions that better achieve the goal (a standard Boltzmann rationality model), and this allows the agent to reason about what the goal must be. In practice, for computational tractability, rather than choosing optimal actions in the overall POMDP, the robot chooses optimal actions using a technique called hindsight optimization, which _assumes that the robot will never learn more information about the user's goal_."
,conferencePaper,2019,"Brown, Daniel S.; Goo, Wonjoon; Nagarajan, Prabhat; Niekum, Scott",Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations,Proceedings of the 36th International Conference on Machine Learning,,,,http://arxiv.org/abs/1904.06387,"A critical flaw of existing inverse reinforcement learning (IRL) methods is their inability to significantly outperform the demonstrator. This is because IRL typically seeks a reward function that makes the demonstrator appear near-optimal, rather than inferring the underlying intentions of the demonstrator that may have been poorly executed in practice. In this paper, we introduce a novel reward-learning-from-observation algorithm, Trajectory-ranked Reward EXtrapolation (T-REX), that extrapolates beyond a set of (approximately) ranked demonstrations in order to infer high-quality reward functions from a set of potentially poor demonstrations. When combined with deep reinforcement learning, T-REX outperforms state-of-the-art imitation learning and IRL methods on multiple Atari and MuJoCo benchmark tasks and achieves performance that is often more than twice the performance of the best demonstration. We also demonstrate that T-REX is robust to ranking noise and can accurately extrapolate intention by simply watching a learner noisily improve at a task over time.",2019-07-08,2020-11-14 00:42,2020-12-19 16:53,2020-11-14 00:42,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000038  arXiv: 1904.06387,,/Users/angelica/Zotero/storage/8RNRKXZZ/1904.html; /Users/angelica/Zotero/storage/M2U4K9L8/Brown et al. - 2019 - Extrapolating Beyond Suboptimal Demonstrations via.pdf,,Other-org; NotSafety,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper claims to demonstrate a technique by which an agent learning from a demonstrator's actions can learn to outperform that demonstrator on their true reward, rather than, in the way of imitation learning or behavioral cloning, just mimicking the demonstrator under the assumption that the demonstrator's performance is optimal (or at least near-optimal). The key structural innovation of the paper is to learn using pairs of ranked trajectories and learn a neural network-based reward function based on correctly predicting which will be higher. This allows the model to predict what actions will lead to higher and lower reward, and to extrapolate that relationship beyond the best demonstration. When an agent is then trained using this reward model as it's ground truth reward, it's shown to be capable of outperforming the demonstrator on multiple tested environments, including Atari. An important distinction compared to some prior work is the fact that these rankings are collected in an off-policy manner, distinguishing it from [Deep RL from Human Preferences](https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/) where rankings are requested on trajectories generated as an agent learns. "
,conferencePaper,2018,"Bıyık, Erdem; Sadigh, Dorsa",Batch Active Preference-Based Learning of Reward Functions,"Proceedings of The 2nd Conference on Robot Learning, PMLR",,,,http://arxiv.org/abs/1810.04303,"Data generation and labeling are usually an expensive part of learning for robotics. While active learning methods are commonly used to tackle the former problem, preference-based learning is a concept that attempts to solve the latter by querying users with preference questions. In this paper, we will develop a new algorithm, batch active preference-based learning, that enables efficient learning of reward functions using as few data samples as possible while still having short query generation times. We introduce several approximations to the batch active learning problem, and provide theoretical guarantees for the convergence of our algorithms. Finally, we present our experimental results for a variety of robotics tasks in simulation. Our results suggest that our batch active learning algorithm requires only a few queries that are computed in a short amount of time. We then showcase our algorithm in a study to learn human users' preferences.",2018-10-09,2020-11-14 01:05,2020-12-19 15:00,2020-11-14 01:05,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000023  arXiv: 1810.04303,,/Users/angelica/Zotero/storage/4DICKN3M/1810.html; /Users/angelica/Zotero/storage/6QP72XEE/Bıyık and Sadigh - 2018 - Batch Active Preference-Based Learning of Reward F.pdf,,Other-org; NotSafety,Computer Science - Artificial Intelligence; Computer Science - Robotics; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"2nd Conference on Robot Learning, PMLR",,,,,,,,,,,,,,,,"This paper builds on a trend of recent papers that try to learn human preferences, not through demonstrations of optimal behavior, but through a human expressing a preference over two possible trajectories, which has both pragmatic advantages (re limits of human optimality) and theoretic ones (better ability to extrapolate a reward function). Here, the task is framed as: we want to send humans batches of paired trajectories to rank, but which ones? Batch learning is preferable to single-sample active learning because it's more efficient to update a network after a batch of human judgments, rather than after each single one. This adds complexity to the problem because you'd prefer to not have a batch of samples that are individually high-expected-information, but which are redundant with one another. The authors define an information criterion (basically the examples about which we're most uncertain of the human's judgment) and then pick a batch of examples based on different heuristics for getting a set of trajectories with high information content that are separated from each other in feature space."
,conferencePaper,2019,"de Haan, Pim; Jayaraman, Dinesh; Levine, Sergey",Causal Confusion in Imitation Learning,Advances in Neural Information Processing Systems 32 (NeurIPS 2019),,,,https://arxiv.org/abs/1905.11979v2,"Behavioral cloning reduces policy learning to supervised learning by training a discriminative model to predict expert actions given observations. Such discriminative models are non-causal: the training procedure is unaware of the causal structure of the interaction between the expert and the environment. We point out that ignoring causality is particularly damaging because of the distributional shift in imitation learning. In particular, it leads to a counter-intuitive ""causal misidentification"" phenomenon: access to more information can yield worse performance. We investigate how this problem arises, and propose a solution to combat it through targeted interventions---either environment interaction or expert queries---to determine the correct causal model. We show that causal misidentification occurs in several benchmark control domains as well as realistic driving settings, and validate our solution against DAgger and other baselines and ablations.",2019-05-28,2020-11-14 01:20,2020-12-19 14:37,2020-11-14 01:20,,,,,,,,,,,,,,en,,,,,arxiv.org,,ZSCC: 0000030,,/Users/angelica/Zotero/storage/HAG23RYZ/1905.html; /Users/angelica/Zotero/storage/AWC4L89A/1905.html; /Users/angelica/Zotero/storage/996ZN85Z/de Haan et al. - 2019 - Causal Confusion in Imitation Learning.pdf,,NotSafety; CHAI-Berkeley; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2019,,,,,,,,,,,,,,,,"This paper argues that _causal misidentification_ is a big problem in imitation learning. When the agent doesn't have a good model of what actions cause what state changes, it may mismodel the effects of a state change as a cause-- e.g., an agent learning to drive a car may incorrectly learn that it should turn on the brakes whenever the brake light on the dashboard is on. This leads to undesirable behavior where more information actually causes the agent to perform worse.

The paper presents an approach for resolving causal misidentification by (1) Training a specialized network to generate a ""disentangled"" representation of the state as variables, (2) Representing causal relationships between those variables in a graph structure, (3) Learning policies corresponding to each possible causal graph, and (4) Performing targeted interventions, either by querying an expert, or by executing a policy and observing the reward, to find the correct causal graph model.

The paper experiments with this method by testing it in environments artificially constructed to have confounding variables that correlate with actions but do not cause them. It finds that this method is successfully able to improve performance with confounding variables, and that it performs significantly better per number of queries (to an expert or of executing a policy) than any existing methods. It also finds that directly executing a policy and observing the reward is a more efficient strategy for narrowing down the correct causal graph than querying an expert."
,conferencePaper,2020,"Srinivas, Aravind; Laskin, Michael; Abbeel, Pieter",CURL: Contrastive Unsupervised Representations for Reinforcement Learning,Proceedings of the 37th International Conference on Machine Learning,,,,http://arxiv.org/abs/2004.04136,"We present CURL: Contrastive Unsupervised Representations for Reinforcement Learning. CURL extracts high-level features from raw pixels using contrastive learning and performs offpolicy control on top of the extracted features. CURL outperforms prior pixel-based methods, both model-based and model-free, on complex tasks in the DeepMind Control Suite and Atari Games showing 1.9x and 1.2x performance gains at the 100K environment and interaction steps benchmarks respectively. On the DeepMind Control Suite, CURL is the ﬁrst image-based algorithm to nearly match the sample-efﬁciency of methods that use state-based features. Our code is open-sourced and available at https://www. github.com/MishaLaskin/curl.",2020-07-07,2020-08-31 18:59,2020-12-19 17:30,2020-08-31 18:59,,,,,,,CURL,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000022  ACC: 22  arXiv: 2004.04136,,/Users/angelica/Zotero/storage/ZGWQBWJG/Srinivas et al. - 2020 - CURL Contrastive Unsupervised Representations for.pdf,,NotSafety; CHAI-Berkeley,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ICML 2020,,,,,,,,,,,,,,,,"This paper applies contrastive learning (discussed above) to reinforcement learning. In RL, rather than training in an initial unsupervised phase, the contrastive learning happens alongside the RL training, and so serves as an auxiliary objective to speed up learning. They use random crops for their data augmentation."
,conferencePaper,2019,"Khoury, Marc; Hadfield-Menell, Dylan",On the Geometry of Adversarial Examples,Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics (AISTATS) 2019,,,,http://arxiv.org/abs/1811.00525,"Adversarial examples are a pervasive phenomenon of machine learning models where seemingly imperceptible perturbations to the input lead to misclassiﬁcations for otherwise statistically accurate models. We propose a geometric framework, drawing on tools from the manifold reconstruction literature, to analyze the high-dimensional geometry of adversarial examples. In particular, we highlight the importance of codimension: for low-dimensional data manifolds embedded in high-dimensional space there are many directions off the manifold in which to construct adversarial examples. Adversarial examples are a natural consequence of learning a decision boundary that classiﬁes the low-dimensional data manifold well, but classiﬁes points near the manifold incorrectly. Using our geometric framework we prove (1) a tradeoff between robustness under different norms, (2) that adversarial training in balls around the data is sample inefﬁcient, and (3) sufﬁcient sampling conditions under which nearest neighbor classiﬁers and ball-based adversarial training are robust.",2019,2019-12-18 02:24,2020-12-26 23:16,2019-12-18 02:24,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000015  arXiv: 1811.00525,,/Users/angelica/Zotero/storage/FJUZGQWD/Khoury and Hadfield-Menell - 2018 - On the Geometry of Adversarial Examples.pdf,,NotSafety; CHAI-Berkeley; AmbiguosSafety,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,22nd International Conference on Artificial Intelligence and Statistics (AISTATS) 2019,,,,,,,,,,,,,,,,"This paper analyzes adversarial examples based off a key idea: even if the data of interest forms a low-dimensional manifold, as we often assume, the ϵ-tube _around_ the manifold is still high-dimensional, and so accuracy in an ϵ-ball around true data points will be hard to learn.

For a given L_p norm, we can define the optimal decision boundary to be the one that maximizes the margin from the true data manifold. If there exists some classifier that is adversarially robust, then the optimal decision boundary is as well. Their first result is that the optimal decision boundary can change dramatically if you change p. In particular, for concentric spheres, the optimal L_inf decision boundary provides an L_2 robustness guarantee √d times smaller than the optimal L_2 decision boundary, where d is the dimensionality of the input. This explains why a classifier that is adversarially trained on L_inf adversarial examples does so poorly on L_2 adversarial examples.

I'm not sure I understand the point of the next section, but I'll give it a try. They show that a nearest neighbors classifier can achieve perfect robustness if the underlying manifold is sampled sufficiently densely (requiring samples exponential in k, the dimensionality of the manifold). However, a learning algorithm with a particular property that they formalize would require exponentially more samples in at least some cases in order to have the same guarantee. I don't know why they chose the particular property they did -- my best guess is that the property is meant to represent what we get when we train a neural net on L_p adversarial examples. If so, then their theorem suggests that we would need exponentially more training points to achieve perfect robustness with adversarial training compared to a nearest neighbor classifier.

They next turn to the fact that the ϵ-tube around the manifold is d-dimensional instead of k-dimensional. If we consider ϵ-balls around the training set X, this covers a very small fraction of the ϵ-tube, approaching 0 as d becomes much larger than k, even if the training set X covers the k-dimensional manifold sufficiently well.

Another issue is that if we require adversarial robustness, then we severely restrict the number of possible decision boundaries, and so we may need significantly more expressive models to get one of these decision boundaries. In particular, since feedforward neural nets with Relu activations have ""piecewise linear"" decision boundaries (in quotes because I might be using the term incorrectly), it is hard for them to separate concentric spheres. Suppose that the spheres are separated by a distance d. Then for accuracy on the manifold, we only need the decision boundary to lie entirely in the shell of width d. However, for ϵ-tube adversarial robustness, the decision boundary must lie in a shell of width d - 2ϵ. They prove a lower bound on the number of linear regions for the decision boundary that grows as τ^(-d), where τ is the width of the shell, suggesting that adversarial robustness would require more parameters in the model.

Their experiments show that for simple learning problems (spheres and planes), adversarial examples tend to be in directions orthogonal to the manifold. In addition, if the true manifold has high codimension, then the learned model has poor robustness."
,conferencePaper,2018,"Kurakin, Alexey; Goodfellow, Ian; Bengio, Samy; Dong, Yinpeng; Liao, Fangzhou; Liang, Ming; Pang, Tianyu; Zhu, Jun; Hu, Xiaolin; Xie, Cihang; Wang, Jianyu; Zhang, Zhishuai; Ren, Zhou; Yuille, Alan; Huang, Sangxia; Zhao, Yao; Zhao, Yuzhe; Han, Zhonglin; Long, Junjiajia; Berdibekov, Yerkebulan; Akiba, Takuya; Tokui, Seiya; Abe, Motoki",Adversarial Attacks and Defences Competition,The NIPS '17 Competition: Building Intelligent Systems,,,,http://arxiv.org/abs/1804.00097,"To accelerate research on adversarial examples and robustness of machine learning classifiers, Google Brain organized a NIPS 2017 competition that encouraged researchers to develop new methods to generate adversarial examples as well as to develop new ways to defend against them. In this chapter, we describe the structure and organization of the competition and the solutions developed by several of the top-placing teams.",2018-03-30,2020-12-13 23:13,2020-12-18 21:08,2020-12-13 23:13,,,,,,,,The Springer Series on Challenges in Machine Learning,,,,,,,,,,,arXiv.org,,ZSCC: 0000118  arXiv: 1804.00097,,/Users/angelica/Zotero/storage/HHKVPJY9/Kurakin et al. - 2018 - Adversarial Attacks and Defences Competition.pdf; /Users/angelica/Zotero/storage/6D32WARL/1804.html,,Other-org; NotSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Cryptography and Security; Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,This is a report on a competition held at NIPS 2017 for the best adversarial attacks and defences. It includes a summary of the field and then shows the results from the competition.
,conferencePaper,2019,"Justesen, Niels; Duque, Miguel Gonzalez; Jaramillo, Daniel Cabarcas; Mouret, Jean-Baptiste; Risi, Sebastian",Learning a Behavioral Repertoire from Demonstrations,arXiv:1907.03046 [cs],,,,http://arxiv.org/abs/1907.03046,"Imitation Learning (IL) is a machine learning approach to learn a policy from a dataset of demonstrations. IL can be useful to kick-start learning before applying reinforcement learning (RL) but it can also be useful on its own, e.g. to learn to imitate human players in video games. However, a major limitation of current IL approaches is that they learn only a single ""average"" policy based on a dataset that possibly contains demonstrations of numerous different types of behaviors. In this paper, we propose a new approach called Behavioral Repertoire Imitation Learning (BRIL) that instead learns a repertoire of behaviors from a set of demonstrations by augmenting the state-action pairs with behavioral descriptions. The outcome of this approach is a single neural network policy conditioned on a behavior description that can be precisely modulated. We apply this approach to train a policy on 7,777 human replays to perform build-order planning in StarCraft II. Principal Component Analysis (PCA) is applied to construct a low-dimensional behavioral space from the high-dimensional army unit composition of each demonstration. The results demonstrate that the learned policy can be effectively manipulated to express distinct behaviors. Additionally, by applying the UCB1 algorithm, we are able to adapt the behavior of the policy - in-between games - to reach a performance beyond that of the traditional IL baseline approach.",2019-07-05,2020-11-14 00:52,2020-12-19 15:26,2020-11-14 00:52,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s0]  ACC: 3  arXiv: 1907.03046,,/Users/angelica/Zotero/storage/ZGWIDIBS/1907.html; /Users/angelica/Zotero/storage/NVEKA7C3/Justesen et al. - 2019 - Learning a Behavioral Repertoire from Demonstratio.pdf,,Other-org; NotSafety,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2020 IEEE Conference on Games (CoG),,,,,,,,,,,,,,,,"They extend vanilla Imitation Learning, adding a behaviour encoding as an input to the policy to prevent it from learning an 'average' behaviour, but instead to learn different strategies with a single policy. Training data includes 7,777 human demonstrations of Terran army build-orders v/s the Zerg in StarCraft 2, for which build-order strategies are first extracted in a high-dimension semantically-meaningful space, and then reduced to two dimensions using PCA. At training time, each game's 2D code _b_ is augmented to the state, and supervised learning is applied to the policy: π(s, b) = a, where _a_ is the action following state _s_ in the human demonstration."
,conferencePaper,2020,"Caron, Mathilde; Misra, Ishan; Mairal, Julien; Goyal, Priya; Bojanowski, Piotr; Joulin, Armand",Unsupervised Learning of Visual Features by Contrasting Cluster Assignments,"Thirty-fourth Conference on Neural Information Processing Systems (NeurIPS),",,,,http://arxiv.org/abs/2006.09882,"Unsupervised image representations have signiﬁcantly reduced the gap with supervised pretraining, notably with the recent achievements of contrastive learning methods. These contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally challenging. In this paper, we propose an online algorithm, SwAV, that takes advantage of contrastive methods without requiring to compute pairwise comparisons. Specifically, our method simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or “views”) of the same image, instead of comparing features directly as in contrastive learning. Simply put, we use a “swapped” prediction mechanism where we predict the cluster assignment of a view from the representation of another view. Our method can be trained with large and small batches and can scale to unlimited amounts of data. Compared to previous contrastive methods, our method is more memory efﬁcient since it does not require a large memory bank or a special momentum network. In addition, we also propose a new data augmentation strategy, multi-crop, that uses a mix of views with different resolutions in place of two full-resolution views, without increasing the memory or compute requirements much. We validate our ﬁndings by achieving 75.3% top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised pretraining on all the considered transfer tasks.",2020-07-17,2020-08-31 18:01,2020-12-21 18:00,2020-08-31 18:01,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000002  arXiv: 2006.09882,,/Users/angelica/Zotero/storage/9KVMKJ3H/Caron et al. - 2020 - Unsupervised Learning of Visual Features by Contra.pdf,,Other-org; TechSafety; AmbiguosSafety,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NeurIPS 2020,,,,,,,,,,,,,,,,"There has been a lot of work in self-supervised representation learning for image classification (previously summarized in [AN #92](https://mailchi.mp/d7e950bc8dbd/an-92learning-good-representations-with-contrastive-predictive-coding) and [AN #99](https://mailchi.mp/4f7ffc5cbe53/an-99-doubling-times-for-the-efficiency-of-ai-algorithms)). This paper sets a new SOTA of 75.3% top-1 ImageNet accuracy, when allowed to first do self-supervised representation learning on ImageNet, and then to train a linear classifier on top of the learned features using all of ImageNet.

Previous methods use a contrastive loss across the learned representations (possibly after being processed by a few MLP layers), which can be thought of as using the learned representation to predict the representation of augmented versions of the same input. In contrast, this paper uses the representation to predict “codes” of augmented versions, where the codes are computed using clustering."
,conferencePaper,2019,"Xu, Danfei; Denil, Misha",Positive-Unlabeled Reward Learning,"arXiv:1911.00459 [cs, stat]",,,,http://arxiv.org/abs/1911.00459,"Learning reward functions from data is a promising path towards achieving scalable Reinforcement Learning (RL) for robotics. However, a major challenge in training agents from learned reward models is that the agent can learn to exploit errors in the reward model to achieve high reward behaviors that do not correspond to the intended task. These reward delusions can lead to unintended and even dangerous behaviors. On the other hand, adversarial imitation learning frameworks tend to suffer the opposite problem, where the discriminator learns to trivially distinguish agent and expert behavior, resulting in reward models that produce low reward signal regardless of the input state. In this paper, we connect these two classes of reward learning methods to positive-unlabeled (PU) learning, and we show that by applying a large-scale PU learning algorithm to the reward learning problem, we can address both the reward under- and over-estimation problems simultaneously. Our approach drastically improves both GAIL and supervised reward learning, without any additional assumptions.",2019-11-01,2020-11-14 00:41,2020-12-19 16:54,2020-11-14 00:41,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000004  arXiv: 1911.00459,,/Users/angelica/Zotero/storage/K2F6CDV7/1911.html; /Users/angelica/Zotero/storage/IYDGFFHB/Xu and Denil - 2019 - Positive-Unlabeled Reward Learning.pdf,,NotSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Conference on Robot Learning 2020,,,,,,,,,,,,,,,,"The problem with learning a reward model and training an agent on the (now fixed) model is that the agent can learn to exploit errors in the reward model. Adversarial imitation learning seeks to avoid this by training a discriminator reward model with the agent: the discriminator is trained via supervised learning to distinguish between expert trajectories and agent trajectories, while the agent tries to fool the discriminator. However, this effectively treats the agent trajectories as negative examples — even once the agent has mastered the task. What we would really like to do is to treat the agent trajectories as unlabeled data. This is an instance of _semi-supervised learning_, in which a classifier has access to a small set of labeled data and a much larger collection of unlabeled data. In general, the common approach is to propagate classification information learned using labels to the unlabeled dataset. The authors apply a recent algorithm for positive-unlabeled (PU) learning, and show that this approach can improve upon both GAIL and supervised reward learning."
8TE87EXY,encyclopediaArticle,2021,,Von Neumann–Morgenstern utility theorem,Wikipedia,,,,https://en.wikipedia.org/w/index.php?title=Von_Neumann%E2%80%93Morgenstern_utility_theorem&oldid=1044421624,"In decision theory, the von Neumann–Morgenstern (VNM) utility theorem shows that, under certain axioms of rational behavior, a decision-maker faced with risky (probabilistic) outcomes of different choices will behave as if he or she is maximizing the expected value of some function defined over the potential outcomes at some specified point in the future. This function is known as the von Neumann–Morgenstern utility function. The theorem is the basis for expected utility theory. In 1947, John von Neumann and Oskar Morgenstern proved that any individual whose preferences satisfied four axioms has a utility function; such an individual's preferences can be represented on an interval scale and the individual will always prefer actions that maximize expected utility. That is, they proved that an agent is (VNM-)rational if and only if there exists a real-valued function u defined by possible outcomes such that every preference of the agent is characterized by maximizing the expected value of u, which can then be defined as the agent's VNM-utility (it is unique up to adding a constant and multiplying by a positive scalar). No claim is made that the agent has a ""conscious desire"" to maximize u, only that u exists. The expected utility hypothesis is that rationality can be modeled as maximizing an expected value, which given the theorem, can be summarized as ""rationality is VNM-rationality"". However, the axioms themselves have been critiqued on various grounds, resulting in the axioms being given further justification.VNM-utility is a decision utility in that it is used to describe decision preferences. It is related but not equivalent to so-called E-utilities (experience utilities), notions of utility intended to measure happiness such as that of Bentham's Greatest Happiness Principle.",2021-09-15,2022-03-10 22:16:04,2022-03-10 22:16:04,2022-03-10 22:16:04,,,,,,,,,,,,,,en,Creative Commons Attribution-ShareAlike License,,,,Wikipedia,,Page Version ID: 1044421624,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HX9UZ5JP,journalArticle,2020,"Cihon, Peter; Maas, Matthijs M.; Kemp, Luke",Fragmentation and the Future: Investigating Architectures for International AI Governance,Global Policy,,1758-5899,10.1111/1758-5899.12890,https://onlinelibrary.wiley.com/doi/abs/10.1111/1758-5899.12890,"The international governance of artificial intelligence (AI) is at a crossroads: should it remain fragmented or be centralised? We draw on the history of environment, trade, and security regimes to identify advantages and disadvantages in centralising AI governance. Some considerations, such as efficiency and political power, speak for centralisation. The risk of creating a slow and brittle institution, and the difficulty of pairing deep rules with adequate participation, speak against it. Other considerations depend on the specific design. A centralised body may be able to deter forum shopping and ensure policy coordination. However, forum shopping can be beneficial, and fragmented institutions could self-organise. In sum, these trade-offs should inform development of the AI governance architecture, which is only now emerging. We apply the trade-offs to the case of the potential development of high-level machine intelligence. We conclude with two recommendations. First, the outcome will depend on the exact design of a central institution. A well-designed centralised regime covering a set of coherent issues could be beneficial. But locking-in an inadequate structure may pose a fate worse than fragmentation. Second, fragmentation will likely persist for now. The developing landscape should be monitored to see if it is self-organising or simply inadequate.",2020,2022-01-30 04:47:43,2022-01-30 04:47:43,2021-11-13 15:58:24,545-556,,5,11,,,Fragmentation and the Future,,,,,,,en,,,,,Wiley Online Library,,ZSCC: 0000010  _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1758-5899.12890,,/Users/jacquesthibodeau/Zotero/storage/2TZBI3FR/Cihon et al. - 2020 - Fragmentation and the Future Investigating Archit.pdf,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JVMJ4RMM,journalArticle,2020,"Stray, Jonathan",Aligning AI Optimization to Community Well-Being,International Journal of Community Well-Being,,"2524-5295, 2524-5309",10.1007/s42413-020-00086-3,http://link.springer.com/10.1007/s42413-020-00086-3,,2020-12,2022-01-30 04:47:36,2022-01-30 04:47:36,2021-11-13 22:47:54,443-463,,4,3,,Int. Journal of Com. WB,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000010,,/Users/jacquesthibodeau/Zotero/storage/V3BEV7X4/Stray - 2020 - Aligning AI Optimization to Community Well-Being.pdf,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TK5F29IU,journalArticle,2021,"Hayden, Benjamin; Niv, Yael",The case against economic values in the orbitofrontal cortex (or anywhere else in the brain),Behavioral Neuroscience,,,10.1037/bne0000448,https://osf.io/7hgup,"Much of traditional neuroeconomics proceeds from the hypothesis that value is reified in the brain, that is, that there are neurons or brain regions whose responses serve the discrete purpose of encoding value. This hypothesis is supported by the finding that the activity of many neurons covaries with subjective value as estimated in specific tasks and has led to the idea that the primary function of the orbitofrontal cortex is to compute and signal economic value. Here we consider an alternative: that economic value, in the cardinal, common-currency sense, is not represented in the brain and used for choice by default. This idea is motivated by consideration of the economic concept of value, which places important epistemic constraints on our ability to identify its neural basis. It is also motivated by the behavioral economics literature, especially work on heuristics, which proposes value-free process models for much if not all of choice. Finally, it is buoyed by recent neural and behavioral findings regarding how animals and humans learn to choose between options. In light of our hypothesis, we critically reevaluate putative neural evidence for the representation of value and explore an alternative: direct learning of action policies. We delineate how this alternative can provide a robust account of behavior that concords with existing empirical data.",2021,2022-01-30 04:48:47,2022-01-30 04:48:47,2021-11-08 23:41:47,192-201,,2,135,,,,,,,,,,,,,,,DOI.org (Crossref),,ZSCC: 0000026  DOI: 10.31234/osf.io/7hgup,,,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NHWZIKZ2,journalArticle,2020,"Fernandes, Pedro; Santos, Francisco C.; Lopes, Manuel",Norms for Beneficial A.I.: A Computational Analysis of the Societal Value Alignment Problem,AI Communications,,"18758452, 09217126",10.3233/AIC-201502,http://arxiv.org/abs/1907.03843,"The rise of artificial intelligence (A.I.) based systems is already offering substantial benefits to the society as a whole. However, these systems may also enclose potential conflicts and unintended consequences. Notably, people will tend to adopt an A.I. system if it confers them an advantage, at which point non-adopters might push for a strong regulation if that advantage for adopters is at a cost for them. Here we propose an agent-based game-theoretical model for these conflicts, where agents may decide to resort to A.I. to use and acquire additional information on the payoffs of a stochastic game, striving to bring insights from simulation to what has been, hitherto, a mostly philosophical discussion. We frame our results under the current discussion on ethical A.I. and the conflict between individual and societal gains: the societal value alignment problem. We test the arising equilibria in the adoption of A.I. technology under different norms followed by artificial agents, their ensuing benefits, and the emergent levels of wealth inequality. We show that without any regulation, purely selfish A.I. systems will have the strongest advantage, even when a utilitarian A.I. provides significant benefits for the individual and the society. Nevertheless, we show that it is possible to develop A.I. systems following human conscious policies that, when introduced in society, lead to an equilibrium where the gains for the adopters are not at a cost for non-adopters, thus increasing the overall wealth of the population and lowering inequality. However, as shown, a self-organised adoption of such policies would require external regulation.",2020-12-18,2022-01-30 04:48:46,2022-01-30 04:48:46,2021-11-13 22:40:37,155-171,,3-6,33,,AIC,Norms for Beneficial A.I.,,,,,,,,,,,,arXiv.org,,ZSCC: 0000004  arXiv: 1907.03843,,/Users/jacquesthibodeau/Zotero/storage/JAVXSVNK/Fernandes et al. - 2020 - Norms for Beneficial A.I. A Computational Analysi.pdf; /Users/jacquesthibodeau/Zotero/storage/A9VEVGPV/1907.html,,UnsortedSafety,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HDWGJGAP,journalArticle,2021,"Mingard, Chris; Valle-Pérez, Guillermo; Skalse, Joar; Louis, Ard A.","Is SGD a Bayesian sampler? Well, almost",Journal of Machine Learning Research,,,,http://arxiv.org/abs/2006.15191,"Overparameterised deep neural networks (DNNs) are highly expressive and so can, in principle, generate almost any function that fits a training dataset with zero error. The vast majority of these functions will perform poorly on unseen data, and yet in practice DNNs often generalise remarkably well. This success suggests that a trained DNN must have a strong inductive bias towards functions with low generalisation error. Here we empirically investigate this inductive bias by calculating, for a range of architectures and datasets, the probability $P_{SGD}(f\mid S)$ that an overparameterised DNN, trained with stochastic gradient descent (SGD) or one of its variants, converges on a function $f$ consistent with a training set $S$. We also use Gaussian processes to estimate the Bayesian posterior probability $P_B(f\mid S)$ that the DNN expresses $f$ upon random sampling of its parameters, conditioned on $S$. Our main findings are that $P_{SGD}(f\mid S)$ correlates remarkably well with $P_B(f\mid S)$ and that $P_B(f\mid S)$ is strongly biased towards low-error and low complexity functions. These results imply that strong inductive bias in the parameter-function map (which determines $P_B(f\mid S)$), rather than a special property of SGD, is the primary explanation for why DNNs generalise so well in the overparameterised regime. While our results suggest that the Bayesian posterior $P_B(f\mid S)$ is the first order determinant of $P_{SGD}(f\mid S)$, there remain second order differences that are sensitive to hyperparameter tuning. A function probability picture, based on $P_{SGD}(f\mid S)$ and/or $P_B(f\mid S)$, can shed new light on the way that variations in architecture or hyperparameter settings such as batch size, learning rate, and optimiser choice, affect DNN performance.",2021-02,2022-01-30 04:48:46,2022-03-11 01:39:08,2021-11-13 22:56:31,,,,22,,,Is SGD a Bayesian sampler?,,,,,,,,,,,,arXiv.org,,ZSCC: 0000009  arXiv: 2006.15191,,"/Users/jacquesthibodeau/Zotero/storage/ACV9IXEG/Mingard et al. - 2020 - Is SGD a Bayesian sampler Well, almost.pdf; /Users/jacquesthibodeau/Zotero/storage/BC3NUUZG/Mingard et al. - 2020 - Is SGD a Bayesian sampler Well, almost.pdf; /Users/jacquesthibodeau/Zotero/storage/EN2JTJZ8/2006.html; /Users/jacquesthibodeau/Zotero/storage/VEJMZHMR/2006.html",,UnsortedSafety,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4E97MC3E,journalArticle,2020,"Zhu, Yixin; Gao, Tao; Fan, Lifeng; Huang, Siyuan; Edmonds, Mark; Liu, Hangxin; Gao, Feng; Zhang, Chi; Qi, Siyuan; Wu, Ying Nian; Tenenbaum, Joshua B.; Zhu, Song-Chun","Dark, Beyond Deep: A Paradigm Shift to Cognitive AI with Humanlike Common Sense",Engineering,,20958099,10.1016/j.eng.2020.01.011,http://arxiv.org/abs/2004.09044,"Recent progress in deep learning is essentially based on a ""big data for small tasks"" paradigm, under which massive amounts of data are used to train a classifier for a single narrow task. In this paper, we call for a shift that flips this paradigm upside down. Specifically, we propose a ""small data for big tasks"" paradigm, wherein a single artificial intelligence (AI) system is challenged to develop ""common sense"", enabling it to solve a wide range of tasks with little training data. We illustrate the potential power of this new paradigm by reviewing models of common sense that synthesize recent breakthroughs in both machine and human vision. We identify functionality, physics, intent, causality, and utility (FPICU) as the five core domains of cognitive AI with humanlike common sense. When taken as a unified concept, FPICU is concerned with the questions of ""why"" and ""how"", beyond the dominant ""what"" and ""where"" framework for understanding vision. They are invisible in terms of pixels but nevertheless drive the creation, maintenance, and development of visual scenes. We therefore coin them the ""dark matter"" of vision. Just as our universe cannot be understood by merely studying observable matter, we argue that vision cannot be understood without studying FPICU. We demonstrate the power of this perspective to develop cognitive AI systems with humanlike common sense by showing how to observe and apply FPICU with little training data to solve a wide range of challenging tasks, including tool use, planning, utility inference, and social learning. In summary, we argue that the next generation of AI must embrace ""dark"" humanlike common sense for solving novel tasks.",2020-03,2022-01-30 04:48:44,2022-01-30 04:48:44,2021-11-07 23:18:37,310-345,,3,6,,Engineering,"Dark, Beyond Deep",,,,,,,,,,,,arXiv.org,,ZSCC: 0000034  arXiv: 2004.09044,,"/Users/jacquesthibodeau/Zotero/storage/G95A4VAE/Zhu et al. - 2020 - Dark, Beyond Deep A Paradigm Shift to Cognitive A.pdf; /Users/jacquesthibodeau/Zotero/storage/XMMDMVVS/2004.html",,UnsortedSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DV7P9734,journalArticle,2017,"Lake, Brenden M.; Ullman, Tomer D.; Tenenbaum, Joshua B.; Gershman, Samuel J.",Building Machines That Learn and Think Like People,Behavioral and Brain Sciences,,,https://doi.org/10.1017/S0140525X16001837,http://arxiv.org/abs/1604.00289,"Recent progress in artificial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.",2017,2022-01-30 04:48:44,2022-03-11 01:38:53,2021-11-18 22:58:21,,,,40,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0001764  arXiv: 1604.00289,,/Users/jacquesthibodeau/Zotero/storage/6UBFFSST/Lake et al. - 2016 - Building Machines That Learn and Think Like People.pdf; /Users/jacquesthibodeau/Zotero/storage/YJPXAPGW/Lake et al. - 2016 - Building Machines That Learn and Think Like People.pdf; /Users/jacquesthibodeau/Zotero/storage/NHNDK29Q/1604.html,,UnsortedSafety,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
76QR898M,journalArticle,2020,"Sutrop, Margit",Challenges of Aligning Artificial Intelligence with Human Values,Acta Baltica Historiae et Philosophiae Scientiarum,,"22282009, 22282017",10.11590/abhps.2020.2.04,https://www.ies.ee/bahps/acta-baltica/abhps-8-2/04_Sutrop-2020-2-04.pdf,"As artificial intelligence (AI) systems are becoming increasingly autonomous and will soon be able to make decisions on their own about what to do, AI researchers have started to talk about the need to align AI with human values. The AI ‘value alignment problem’ faces two kinds of challenges—a technical and a normative one—which are interrelated. The technical challenge deals with the question of how to encode human values in artificial intelligence. The normative challenge is associated with two questions: “Which values or whose values should artificial intelligence align with?” My concern is that AI developers underestimate the difficulty of answering the normative question. They hope that we can easily identify the purposes we really desire and that they can focus on the design of those objectives. But how are we to decide which objectives or values to induce in AI, given that there is a plurality of values and moral principles and that our everyday life is full of moral disagreements? In my paper I will show that although it is not realistic to reach an agreement on what we, humans, really want as people value different things and seek different ends, it may be possible to agree on what we do not want to happen, considering the possibility that intelligence, equal to our own, or even exceeding it, can be created. I will argue for pluralism (and not for relativism!) which is compatible with objectivism. In spite of the fact that there is no uniquely best solution to every moral problem, it is still possible to identify which answers are wrong. And this is where we should begin the value alignment of AI.",2020-12-15,2022-01-30 04:48:44,2022-01-30 04:48:44,2021-11-13 22:38:41,54-72,,2,8,,ABHPS,,,,,,,,,,,,,DOI.org (Crossref),,ZSCC: 0000002,,/Users/jacquesthibodeau/Zotero/storage/VS3TSE5U/University of Tartu and Sutrop - 2020 - Challenges of Aligning Artificial Intelligence wit.pdf,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VXW84MIC,journalArticle,2020,"Cammarata, Nick; Goh, Gabriel; Carter, Shan; Schubert, Ludwig; Petrov, Michael; Olah, Chris",Curve Detectors,Distill,,2476-0757,10.23915/distill.00024.003,https://distill.pub/2020/circuits/curve-detectors,Part one of a three part deep dive into the curve neuron family.,2020-06-17,2022-01-30 04:48:28,2022-01-30 04:48:28,2021-11-14 16:21:07,e00024.003,,6,5,,Distill,,,,,,,,en,,,,,distill.pub,,ZSCC: 0000005,,,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BZTGZTHQ,journalArticle,2020,"Olah, Chris; Cammarata, Nick; Voss, Chelsea; Schubert, Ludwig; Goh, Gabriel",Naturally Occurring Equivariance in Neural Networks,Distill,,2476-0757,10.23915/distill.00024.004,https://distill.pub/2020/circuits/equivariance,"Neural networks naturally learn many transformed copies of the same feature, connected by symmetric weights.",2020-12-08,2022-01-30 04:48:28,2022-01-30 04:48:28,2021-11-14 16:21:59,e00024.004,,12,5,,Distill,,,,,,,,en,,,,,distill.pub,,ZSCC: 0000001,,,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UD8JD4XD,journalArticle,2020,"Barreto, André; Hou, Shaobo; Borsa, Diana; Silver, David; Precup, Doina",Fast reinforcement learning with generalized policy updates,Proceedings of the National Academy of Sciences,,"0027-8424, 1091-6490",10.1073/pnas.1907370117,http://www.pnas.org/lookup/doi/10.1073/pnas.1907370117,"The combination of reinforcement learning with deep learning is a promising approach to tackle important sequential decision-making problems that are currently intractable. One obstacle to overcome is the amount of data needed by learning systems of this type. In this article, we propose to address this issue through a divide-and-conquer approach. We argue that complex decision problems can be naturally decomposed into multiple tasks that unfold in sequence or in parallel. By associating each task with a reward function, this problem decomposition can be seamlessly accommodated within the standard reinforcement-learning formalism. The specific way we do so is through a generalization of two fundamental operations in reinforcement learning: policy improvement and policy evaluation. The generalized version of these operations allow one to leverage the solution of some tasks to speed up the solution of others. If the reward function of a task can be well approximated as a linear combination of the reward functions of tasks previously solved, we can reduce a reinforcement-learning problem to a simpler linear regression. When this is not the case, the agent can still exploit the task solutions by using them to interact with and learn about the environment. Both strategies considerably reduce the amount of data needed to solve a reinforcement-learning problem.",2020-12-01,2022-01-30 04:47:57,2022-01-30 04:47:57,2021-11-13 13:59:52,30079-30087,,48,117,,Proc Natl Acad Sci USA,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000026,,/Users/jacquesthibodeau/Zotero/storage/WGDHG555/Barreto et al. - 2020 - Fast reinforcement learning with generalized polic.pdf,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6ZGCJNUC,journalArticle,2018,"Rozo, Leonel; Amor, Heni Ben; Calinon, Sylvain; Dragan, Anca; Lee, Dongheui",Special issue on learning for human–robot collaboration,Autonomous Robots,,1573-7527,10.1007/s10514-018-9756-z,https://doi.org/10.1007/s10514-018-9756-z,,2018-06-01,2022-01-30 04:51:09,2022-01-30 04:51:09,2019-12-18 02:40:04,953-956,,5,42,,Auton Robot,,,,,,,,en,,,,,Springer Link,,ZSCC: 0000010,,/Users/jacquesthibodeau/Zotero/storage/G3JHNZVF/Rozo et al. - 2018 - Special issue on learning for human–robot collabor.pdf,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CJVETACN,journalArticle,2015,"Russell, Stuart; Dewey, Daniel; Tegmark, Max",Research priorities for robust and beneficial artificial intelligence: an open letter,AI Magazine,,,,,,2015,2022-01-30 04:51:08,2022-01-30 04:51:08,,,,4,36,,,Research priorities for robust and beneficial artificial intelligence,,,,,,,,,,,,Google Scholar,,ZSCC: 0000017,,/Users/jacquesthibodeau/Zotero/storage/Q8Q62AV8/Russell et al. - 2015 - Research priorities for robust and beneficial arti.pdf,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IRTZ78N7,journalArticle,2015,"Russell, Stuart",Recent developments in unifying logic and probability,Communications of the ACM,,10782,10.1145/2699411,http://dl.acm.org/citation.cfm?doid=2797100.2699411,,2015-06-25,2022-01-30 04:51:08,2022-01-30 04:51:08,2018-12-09 19:18:27,88-97,,7,58,,,,,,,,,,en,,,,,Crossref,,ZSCC: NoCitationData[s2]  ACC: 74,,,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RNU8DV83,journalArticle,2015,"Russell, Stuart; Dewey, Daniel; Tegmark, Max",Research Priorities for Robust and Beneficial Artificial Intelligence,AI Magazine,,"0738-4602, 0738-4602",10.1609/aimag.v36i4.2577,https://aaai.org/ojs/index.php/aimagazine/article/view/2577,"Success in the quest for artificial intelligence has the potential to bring unprecedented benefits to humanity, and it is therefore worthwhile to investigate how to maximize these benefits while avoiding potential pitfalls. This article gives numerous examples (which should by no means be construed as an exhaustive list) of such worthwhile research aimed at ensuring that AI remains robust and beneficial.",2015-12-31,2022-01-30 04:51:08,2022-01-30 04:51:08,2019-12-18 01:27:03,105,,4,36,,AIMag,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000575,,/Users/jacquesthibodeau/Zotero/storage/W2CF629Z/Russell et al. - 2015 - Research Priorities for Robust and Beneficial Arti.pdf,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MQHJJJJI,journalArticle,2019,"Baum, Seth D.; Armstrong, Stuart; Ekenstedt, Timoteus; Häggström, Olle; Hanson, Robin; Kuhlemann, Karin; Maas, Matthijs M.; Miller, James D.; Salmela, Markus; Sandberg, Anders",Long-term trajectories of human civilization,Foresight,,,,,,2019,2022-01-30 04:51:08,2022-01-30 04:51:08,,53–83,,1,21,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000048,,/Users/jacquesthibodeau/Zotero/storage/EFQQZHCW/Baum et al. - 2019 - Long-term trajectories of human civilization.pdf; /Users/jacquesthibodeau/Zotero/storage/RB58JW9E/html.html; /Users/jacquesthibodeau/Zotero/storage/T9WH9T4G/html.html; /Users/jacquesthibodeau/Zotero/storage/HDZZWUJ2/html.html; /Users/jacquesthibodeau/Zotero/storage/2WQX97PA/Baum et al. - 2019 - Long-term trajectories of human civilization.pdf,,CLR; MetaSafety; FHI; GCRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9H4IQEWN,journalArticle,2017,"Sotala, Kaj",How feasible is the rapid development of artificial superintelligence?,Physica Scripta,,1402-4896,10.1088/1402-4896/aa90e8,https://doi.org/10.1088%2F1402-4896%2Faa90e8,"What kinds of fundamental limits are there in how capable artificial intelligence (AI) systems might become? Two questions in particular are of interest: (1) How much more capable could AI become relative to humans, and (2) how easily could superhuman capability be acquired? To answer these questions, we will consider the literature on human expertise and intelligence, discuss its relevance for AI, and consider how AI could improve on humans in two major aspects of thought and expertise, namely simulation and pattern recognition. We find that although there are very real limits to prediction, it seems like AI could still substantially improve on human intelligence.",2017-10,2022-01-30 04:51:08,2022-01-30 04:51:08,2020-11-23 00:17:29,113001,,11,92,,Phys. Scr.,,,,,,,,en,,,,,Institute of Physics,,ZSCC: 0000016  Publisher: IOP Publishing,,/Users/jacquesthibodeau/Zotero/storage/EUNUQQPC/Sotala - 2017 - How feasible is the rapid development of artificia.pdf,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
79KF8UHI,journalArticle,1995,"Russell, S. J.; Subramanian, D.",Provably Bounded-Optimal Agents,Journal of Artificial Intelligence Research,,1076-9757,10.1613/jair.133,https://www.jair.org/index.php/jair/article/view/10134,"Since its inception, artificial intelligence has relied  upon a theoretical foundation centered around  perfect rationality  as   the desired property of intelligent systems. We argue, as others have   done, that this foundation is inadequate because it imposes   fundamentally unsatisfiable requirements. As a result, there has   arisen a wide gap between theory and practice in AI, hindering   progress in the field. We propose instead a property called  bounded   optimality. Roughly speaking, an agent is bounded-optimal if its   program is a solution to the constrained optimization problem   presented by its architecture and the task environment. We show how to   construct agents with this property for a simple class of machine   architectures in a broad class of real-time environments. We   illustrate these results using a simple model of an automated mail   sorting facility.  We also define a weaker property,  asymptotic   bounded optimality (ABO), that generalizes the notion of optimality in   classical complexity theory.  We then construct  universal  ABO   programs, i.e., programs that are ABO no matter what real-time   constraints are applied.  Universal ABO programs can be used as   building blocks for more complex systems. We conclude with a   discussion of the prospects for bounded optimality as a theoretical   basis for AI, and relate it to similar trends in philosophy,   economics, and game theory.",1995-05-01,2022-01-30 04:51:07,2022-01-30 04:51:07,2020-11-22 02:23:36,575-609,,,2,,jair,,,,,,,,,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s2]  ACC: 437,,/Users/jacquesthibodeau/Zotero/storage/E5SU8UNI/Russell and Subramanian - 1995 - Provably Bounded-Optimal Agents.pdf,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NCWIBN2Q,journalArticle,2018,"Sadigh, Dorsa; Landolfi, Nick; Sastry, Shankar S.; Seshia, Sanjit A.; Dragan, Anca D.",Planning for cars that coordinate with people: leveraging effects on human actions for planning and active information gathering over human internal state,Autonomous Robots,,"0929-5593, 1573-7527",10.1007/s10514-018-9746-1,http://link.springer.com/10.1007/s10514-018-9746-1,"Traditionally, autonomous cars treat human-driven vehicles like moving obstacles. They predict their future trajectories and plan to stay out of their way. While physically safe, this results in defensive and opaque behaviors. In reality, an autonomous car’s actions will actually affect what other cars will do in response, creating an opportunity for coordination. Our thesis is that we can leverage these responses to plan more efﬁcient and communicative behaviors. We introduce a formulation of interaction with human-driven vehicles as an underactuated dynamical system, in which the robot’s actions have consequences on the state of the autonomous car, but also on the human actions and thus the state of the human-driven car. We model these consequences by approximating the human’s actions as (noisily) optimal with respect to some utility function. The robot uses the human actions as observations of her underlying utility function parameters. We ﬁrst explore learning these parameters ofﬂine, and show that a robot planning in the resulting underactuated system is more efﬁcient than when treating the person as a moving obstacle. We also show that the robot can target speciﬁc desired effects, like getting the person to switch lanes or to proceed ﬁrst through an intersection. We then explore estimating these parameters online, and enable the robot to perform active information gathering: generating actions that purposefully probe the human in order to clarify their underlying utility parameters, like driving style or attention level. We show that this signiﬁcantly outperforms passive estimation and improves efﬁciency. Planning in our model results in coordination behaviors: the robot inches forward at an intersection to see if can go through, or it reverses to make the other car proceed ﬁrst. These behaviors result from the optimization, without relying on hand-coded signaling strategies. Our user studies support the utility of our model when interacting with real users.",2018-10,2022-01-30 04:51:07,2022-01-30 04:51:07,2019-12-18 02:39:40,1405-1426,,7,42,,Auton Robot,Planning for cars that coordinate with people,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000086,,/Users/jacquesthibodeau/Zotero/storage/IZXDWBKE/Sadigh et al. - 2018 - Planning for cars that coordinate with people lev.pdf,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7T52V2ZJ,journalArticle,2016,"Oesterheld, Caspar",Formalizing preference utilitarianism in physical world models,Synthese,,1573-0964,10.1007/s11229-015-0883-1,https://doi.org/10.1007/s11229-015-0883-1,"Most ethical work is done at a low level of formality. This makes practical moral questions inaccessible to formal and natural sciences and can lead to misunderstandings in ethical discussion. In this paper, we use Bayesian inference to introduce a formalization of preference utilitarianism in physical world models, specifically cellular automata. Even though our formalization is not immediately applicable, it is a first step in providing ethics and ultimately the question of how to “make the world better” with a formal basis.",2016-09-01,2022-01-30 04:51:07,2022-01-30 04:51:07,2020-11-23 00:22:50,2747-2759,,9,193,,Synthese,,,,,,,,en,,,,,Springer Link,,ZSCC: 0000010,,/Users/jacquesthibodeau/Zotero/storage/WH4KR7BA/Oesterheld - 2016 - Formalizing preference utilitarianism in physical .pdf,,CLR; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
G2PTAEDI,journalArticle,2019,"Oesterheld, Caspar",Approval-directed agency and the decision theory of Newcomb-like problems,Synthese,,,,,,2019,2022-01-30 04:51:06,2022-01-30 04:51:06,,1–14,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000003  Publisher: Springer,,/Users/jacquesthibodeau/Zotero/storage/9XXQZD4D/Oesterheld - 2019 - Approval-directed agency and the decision theory o.pdf; /Users/jacquesthibodeau/Zotero/storage/45AXUP2J/Oesterheld - 2019 - Approval-directed agency and the decision theory o.pdf,,CLR; TechSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MKCF7837,journalArticle,2019,"Perry, Brandon; Uuk, Risto",AI Governance and the Policymaking Process: Key Considerations for Reducing AI Risk,Big Data and Cognitive Computing,,,10.3390/bdcc3020026,https://www.mdpi.com/2504-2289/3/2/26,"This essay argues that a new subfield of AI governance should be explored that examines the policy-making process and its implications for AI governance. A growing number of researchers have begun working on the question of how to mitigate the catastrophic risks of transformative artificial intelligence, including what policies states should adopt. However, this essay identifies a preceding, meta-level problem of how the space of possible policies is affected by the politics and administrative mechanisms of how those policies are created and implemented. This creates a new set of key considerations for the field of AI governance and should influence the action of future policymakers. This essay examines some of the theories of the policymaking process, how they compare to current work in AI governance, and their implications for the field at large and ends by identifying areas of future research.",2019-06,2022-01-30 04:49:29,2022-01-30 04:49:29,2020-12-14 23:48:16,26,,2,3,,,AI Governance and the Policymaking Process,,,,,,,en,http://creativecommons.org/licenses/by/3.0/,,,,www.mdpi.com,,ZSCC: 0000017  Number: 2 Publisher: Multidisciplinary Digital Publishing Institute,,/Users/jacquesthibodeau/Zotero/storage/2R7BPXBH/Perry and Uuk - 2019 - AI Governance and the Policymaking Process Key Co.pdf,,MetaSafety; Other-org,AI governance; AI risk; policymaking process; typologies of AI policy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
G3BNDJ2G,journalArticle,2018,"Grace, Katja; Salvatier, John; Dafoe, Allan; Zhang, Baobao; Evans, Owain",When Will AI Exceed Human Performance? Evidence from AI Experts,Journal of Artificial Intelligence Research,,,,http://arxiv.org/abs/1705.08807,"Advances in artificial intelligence (AI) will transform modern life by reshaping transportation, health, science, finance, and the military. To adapt public policy, we need to better anticipate these advances. Here we report the results from a large survey of machine learning researchers on their beliefs about progress in AI. Researchers predict AI will outperform humans in many activities in the next ten years, such as translating languages (by 2024), writing high-school essays (by 2026), driving a truck (by 2027), working in retail (by 2031), writing a bestselling book (by 2049), and working as a surgeon (by 2053). Researchers believe there is a 50% chance of AI outperforming humans in all tasks in 45 years and of automating all human jobs in 120 years, with Asian respondents expecting these dates much sooner than North Americans. These results will inform discussion amongst researchers and policymakers about anticipating and managing trends in AI.",2018,2022-01-30 04:49:21,2022-03-11 01:39:18,2019-12-16 02:29:10,729–754,,,62,,,When Will AI Exceed Human Performance?,,,,,,,,,,,,arXiv.org,,ZSCC: 0000539  arXiv: 1705.08807,,/Users/jacquesthibodeau/Zotero/storage/MXS9Q4IA/Grace et al. - 2018 - When Will AI Exceed Human Performance Evidence fr.pdf; /Users/jacquesthibodeau/Zotero/storage/Q4VEH3BH/Grace et al. - 2018 - When Will AI Exceed Human Performance Evidence fr.pdf; /Users/jacquesthibodeau/Zotero/storage/LPHKZEY8/Grace et al. - 2018 - When Will AI Exceed Human Performance Evidence fr.pdf; /Users/jacquesthibodeau/Zotero/storage/Z6FGQ2Z6/Grace et al. - 2018 - When Will AI Exceed Human Performance Evidence fr.pdf; /Users/jacquesthibodeau/Zotero/storage/XZIMBZVK/1705.html; /Users/jacquesthibodeau/Zotero/storage/9HN4IS26/1705.html; /Users/jacquesthibodeau/Zotero/storage/J24W79SV/1705.html; /Users/jacquesthibodeau/Zotero/storage/TTDMTXPR/1705.html; /Users/jacquesthibodeau/Zotero/storage/P2MNDVAX/1705.html; /Users/jacquesthibodeau/Zotero/storage/6VC3INB6/1705.html; /Users/jacquesthibodeau/Zotero/storage/MM69SWUB/Grace et al. - 2018 - When will AI exceed human performance Evidence fr.pdf; /Users/jacquesthibodeau/Zotero/storage/A6EQJ4JQ/Grace et al. - 2018 - When will AI exceed human performance Evidence fr.pdf; /Users/jacquesthibodeau/Zotero/storage/AENSKWZK/11222.html; /Users/jacquesthibodeau/Zotero/storage/RVNTGCXF/11222.html; /Users/jacquesthibodeau/Zotero/storage/H2NGMISH/11222.html; /Users/jacquesthibodeau/Zotero/storage/227ZHEW8/11222.html; /Users/jacquesthibodeau/Zotero/storage/INUH2ER3/11222.html; /Users/jacquesthibodeau/Zotero/storage/94W3P62Z/11222.html,,AI-Impacts-NotFeatured; FHI; MetaSafety,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
D2PUDC8S,journalArticle,2017,"Lin, Henry W.; Tegmark, Max; Rolnick, David",Why Does Deep and Cheap Learning Work So Well?,Journal of Statistical Physics,,"0022-4715, 1572-9613",10.1007/s10955-017-1836-5,http://link.springer.com/10.1007/s10955-017-1836-5,,2017-09,2022-01-30 04:48:55,2022-01-30 04:48:55,2021-11-13 22:54:48,1223-1247,,6,168,,J Stat Phys,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000533,,/Users/jacquesthibodeau/Zotero/storage/4PBUWPQI/Lin et al. - 2017 - Why Does Deep and Cheap Learning Work So Well.pdf,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JXVPJ3PB,journalArticle,2017,"Garrabrant, Scott; Benson-Tilsen, Tsvi; Critch, Andrew; Soares, Nate; Taylor, Jessica",A Formal Approach to the Problem of Logical Non-Omniscience,Electronic Proceedings in Theoretical Computer Science,,2075-2180,10.4204/EPTCS.251.16,http://arxiv.org/abs/1707.08747,"We present the logical induction criterion for computable algorithms that assign probabilities to every logical statement in a given formal language, and refine those probabilities over time. The criterion is motivated by a series of stock trading analogies. Roughly speaking, each logical sentence phi is associated with a stock that is worth $1 per share if phi is true and nothing otherwise, and we interpret the belief-state of a logically uncertain reasoner as a set of market prices, where pt_N(phi)=50% means that on day N, shares of phi may be bought or sold from the reasoner for 50%. A market is then called a logical inductor if (very roughly) there is no polynomial-time computable trading strategy with finite risk tolerance that earns unbounded profits in that market over time. We then describe how this single criterion implies a number of desirable properties of bounded reasoners; for example, logical inductors outpace their underlying deductive process, perform universal empirical induction given enough time to think, and place strong trust in their own reasoning process.",2017-07-25,2022-01-30 04:50:42,2022-01-30 04:50:42,2019-05-05 21:13:06,221-235,,,251,,Electron. Proc. Theor. Comput. Sci.,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000008  arXiv: 1707.08747,,/Users/jacquesthibodeau/Zotero/storage/IBNV8E9I/Garrabrant et al. - 2017 - A Formal Approach to the Problem of Logical Non-Om.pdf; /Users/jacquesthibodeau/Zotero/storage/NIV483PR/1707.html; /Users/jacquesthibodeau/Zotero/storage/BA42R2SV/1707.html,,CHAI; TechSafety; MIRI,Computer Science - Logic in Computer Science; F.4.0; G.3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
62A83IME,journalArticle,2020,"Cave, Stephen; Dihal, Kanta",The Whiteness of AI,Philosophy & Technology,,2210-5441,10.1007/s13347-020-00415-6,https://doi.org/10.1007/s13347-020-00415-6,"This paper focuses on the fact that AI is predominantly portrayed as white—in colour, ethnicity, or both. We first illustrate the prevalent Whiteness of real and imagined intelligent machines in four categories: humanoid robots, chatbots and virtual assistants, stock images of AI, and portrayals of AI in film and television. We then offer three interpretations of the Whiteness of AI, drawing on critical race theory, particularly the idea of the White racial frame. First, we examine the extent to which this Whiteness might simply reflect the predominantly White milieus from which these artefacts arise. Second, we argue that to imagine machines that are intelligent, professional, or powerful is to imagine White machines because the White racial frame ascribes these attributes predominantly to White people. Third, we argue that AI racialised as White allows for a full erasure of people of colour from the White utopian imaginary. Finally, we examine potential consequences of the racialisation of AI, arguing it could exacerbate bias and misdirect concern.",2020-08-06,2022-01-30 04:50:26,2022-01-30 04:50:26,2020-08-21 19:30:56,,,,,,Philos. Technol.,,,,,,,,en,,,,,Springer Link,,ZSCC: 0000036,,/Users/jacquesthibodeau/Zotero/storage/PAGWG4H5/Cave and Dihal - 2020 - The Whiteness of AI.pdf,,MetaSafety; CFI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TWPD2FHB,journalArticle,2021,"Whittlestone, Jess; Arulkumaran, Kai; Crosby, Matthew",The Societal Implications of Deep Reinforcement Learning,Journal of Artificial Intelligence Research,,1076-9757,10.1613/jair.1.12360,https://doi.org/10.1613/jair.1.12360,"Deep Reinforcement Learning (DRL) is an avenue of research in Artificial Intelligence (AI) that has received increasing attention within the research community in recent years, and is beginning to show potential for real-world application. DRL is one of the most promising routes towards developing more autonomous AI systems that interact with and take actions in complex real-world environments, and can more flexibly solve a range of problems for which we may not be able to precisely specify a correct ‘answer’. This could have substantial implications for people’s lives: for example by speeding up automation in various sectors, changing the nature and potential harms of online influence, or introducing new safety risks in physical infrastructure. In this paper, we review recent progress in DRL, discuss how this may introduce novel and pressing issues for society, ethics, and governance, and highlight important avenues for future research to better understand DRL’s societal implications. This article appears in the special track on AI and Society.",2021-05-01,2022-01-30 04:50:26,2022-01-30 04:50:26,2021-10-30 19:47:53,1003–1030,,,70,,J. Artif. Int. Res.,,,,,,,,,,,,,May 2021,,ZSCC: 0000006,,/Users/jacquesthibodeau/Zotero/storage/C3HIICDA/Whittlestone et al. - 2021 - The Societal Implications of Deep Reinforcement Le.pdf,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8TWH52ZI,journalArticle,2021,"Stix, Charlotte; Maas, Matthijs M.",Bridging the gap: the case for an ‘Incompletely Theorized Agreement’ on AI policy,AI and Ethics,,2730-5961,10.1007/s43681-020-00037-w,https://doi.org/10.1007/s43681-020-00037-w,"Recent progress in artificial intelligence (AI) raises a wide array of ethical and societal concerns. Accordingly, an appropriate policy approach is urgently needed. While there has been a wave of scholarship in this field, the research community at times appears divided amongst those who emphasize ‘near-term’ concerns and those focusing on ‘long-term’ concerns and corresponding policy measures. In this paper, we seek to examine this alleged ‘gap’, with a view to understanding the practical space for inter-community collaboration on AI policy. We propose to make use of the principle of an ‘incompletely theorized agreement’ to bridge some underlying disagreements, in the name of important cooperation on addressing AI’s urgent challenges. We propose that on certain issue areas, scholars working with near-term and long-term perspectives can converge and cooperate on selected mutually beneficial AI policy projects, while maintaining their distinct perspectives.",2021-08-01,2022-01-30 04:50:25,2022-01-30 04:50:25,2021-10-31 17:04:06,261-271,,3,1,,AI Ethics,Bridging the gap,,,,,,,en,,,,,Springer Link,,ZSCC: 0000008,,/Users/jacquesthibodeau/Zotero/storage/F7AQ9SK8/Stix and Maas - 2021 - Bridging the gap the case for an ‘Incompletely Th.pdf,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GBC9PC5H,journalArticle,2021,"Liu, Hin-Yan; Maas, Matthijs M.",‘Solving for X?’ Towards a problem-finding framework to ground long-term governance strategies for artificial intelligence,Futures,,0016-3287,10.1016/j.futures.2020.102672,https://www.sciencedirect.com/science/article/pii/S0016328720301634,"Change is hardly a new feature in human affairs. Yet something has begun to change in change. In the face of a range of emerging, complex, and interconnected global challenges, society’s collective governance efforts may need to be put on a different footing. Many of these challenges derive from emerging technological developments – take Artificial Intelligence (AI), the focus of much contemporary governance scholarship and efforts. AI governance strategies have predominantly oriented themselves towards clear, discrete clusters of pre-defined problems. We argue that such ‘problem-solving’ approaches may be necessary, but are also insufficient in the face of many of the ‘wicked problems’ created or driven by AI. Accordingly, we propose in this paper a complementary framework for grounding long-term governance strategies for complex emerging issues such as AI into a ‘problem-finding’ orientation. We first provide a rationale by sketching the range of policy problems created by AI, and providing five reasons why problem-solving governance approaches to these challenges fail or fall short. We conversely argue that that creative, ‘problem-finding’ research into these governance challenges is not only warranted scientifically, but will also be critical in the formulation of governance strategies that are effective, meaningful, and resilient over the long-term. We accordingly illustrate the relation between and the complementarity of problem-solving and problem-finding research, by articulating a framework that distinguishes between four distinct ‘levels’ of governance: problem-solving research generally approaches AI (governance) issues from a perspective of (Level 0) ‘business-as-usual’ or as (Level 1) ‘governance puzzle-solving’. In contrast, problem-finding approaches emphasize (Level 2) ‘governance Disruptor-Finding’; or (Level 3) ‘Charting Macrostrategic Trajectories’. We apply this theoretical framework to contemporary governance debates around AI throughout our analysis to elaborate upon and to better illustrate our framework. We conclude with reflections on nuances, implications, and shortcomings of this long-term governance framework, offering a range of observations on intra-level failure modes, between-level complementarities, within-level path dependencies, and the categorical boundary conditions of governability (‘Governance Goldilocks Zone’). We suggest that this framework can help underpin more holistic approaches for long-term strategy-making across diverse policy domains and contexts, and help cross the bridge between concrete policies on local solutions, and longer-term considerations of path-dependent societal trajectories to avert, or joint visions towards which global communities can or should be rallied.",2021-02-01,2022-01-30 04:50:25,2022-01-30 04:50:25,2021-10-31 17:02:28,102672,,,126,,Futures,‘Solving for X?,,,,,,,en,,,,,ScienceDirect,,ZSCC: 0000005,,/Users/jacquesthibodeau/Zotero/storage/R74GTS83/S0016328720301634.html,,MetaSafety,Artificial intelligence; Governance disruptors; Governance goldilocks zone; Governance puzzles; Macrostrategic trajectories & destinations; Problem-finding,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
54ATHF4J,journalArticle,2020,"Peters, Dorian; Vold, Karina; Robinson, Diana; Calvo, Rafael A.",Responsible AI—Two Frameworks for Ethical Design Practice,IEEE Transactions on Technology and Society,,2637-6415,10.1109/TTS.2020.2974991,https://ieeexplore.ieee.org/document/9001063/,"In 2019, the IEEE launched the P7000 standards projects intended to address ethical issues in the design of autonomous and intelligent systems. This move came amidst a growing public concern over the unintended consequences of artiﬁcial intelligence (AI), compounded by the lack of an anticipatory process for attending to ethical impact within professional practice. However, the difﬁculty in moving from principles to practice presents a signiﬁcant challenge to the implementation of ethical guidelines. Herein, we describe two complementary frameworks for integrating ethical analysis into engineering practice to help address this challenge. We then provide the outcomes of an ethical analysis informed by these frameworks, conducted within the speciﬁc context of Internet-delivered therapy in digital mental health. We hope both the frameworks and analysis can provide tools and insights, not only for the context of digital healthcare but also for data-enabled and intelligent technology development more broadly.",2020-03,2022-01-30 04:50:25,2022-01-30 04:50:25,2020-08-21 19:54:52,34-47,,1,1,,IEEE Trans. Technol. Soc.,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000030,,/Users/jacquesthibodeau/Zotero/storage/RV2HP3AI/Peters et al. - 2020 - Responsible AI—Two Frameworks for Ethical Design P.pdf,,TechSafety; CFI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CAZ95ZMD,journalArticle,2020,"ÓhÉigeartaigh, Seán S.; Whittlestone, Jess; Liu, Yang; Zeng, Yi; Liu, Zhe",Overcoming Barriers to Cross-cultural Cooperation in AI Ethics and Governance,Philosophy & Technology,,"2210-5433, 2210-5441",10.1007/s13347-020-00402-x,http://link.springer.com/10.1007/s13347-020-00402-x,"Achieving the global benefits of artificial intelligence (AI) will require international cooperation on many areas of governance and ethical standards, while allowing for diverse cultural perspectives and priorities. There are many barriers to achieving this at present, including mistrust between cultures, and more practical challenges of coordinating across different locations. This paper focuses particularly on barriers to cooperation between Europe and North America on the one hand and East Asia on the other, as regions which currently have an outsized impact on the development of AI ethics and governance. We suggest that there is reason to be optimistic about achieving greater cross-cultural cooperation on AI ethics and governance. We argue that misunderstandings between cultures and regions play a more important role in undermining cross-cultural trust, relative to fundamental disagreements, than is often supposed. Even where fundamental differences exist, these may not necessarily prevent productive cross-cultural cooperation, for two reasons: (1) cooperation does not require achieving agreement on principles and standards for all areas of AI; and (2) it is sometimes possible to reach agreement on practical issues despite disagreement on more abstract values or principles. We believe that academia has a key role to play in promoting cross-cultural cooperation on AI ethics and governance, by building greater mutual understanding, and clarifying where different forms of agreement will be both necessary and possible. We make a number of recommendations for practical steps and initiatives, including translation and multilingual publication of key documents, researcher exchange programmes, and development of research agendas on cross-cultural topics.",2020-05-15,2022-01-30 04:50:25,2022-01-30 04:50:25,2020-08-21 19:45:24,,,,,,Philos. Technol.,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000026,,/Users/jacquesthibodeau/Zotero/storage/I6Z5EXPQ/ÓhÉigeartaigh et al. - 2020 - Overcoming Barriers to Cross-cultural Cooperation .pdf,,MetaSafety; CFI; CSER,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MQ8SWGRG,journalArticle,2020,"Coyle, Diane; Weller, Adrian",“Explaining” machine learning reveals policy challenges,Science,,"0036-8075, 1095-9203",,https://www.sciencemag.org/lookup/doi/10.1126/science.aba9647,,2020-06-26,2022-01-30 04:50:25,2022-01-30 04:50:25,2020-08-21 19:38:44,1433-1434,,6498,368,,,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000021,,/Users/jacquesthibodeau/Zotero/storage/NDWSWJX4/Coyle and Weller - 2020 - “Explaining” machine learning reveals policy chall.pdf,,MetaSafety; CFI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TITT7C74,journalArticle,2020,"Tzachor, Asaf; Whittlestone, Jess; Sundaram, Lalitha; hÉigeartaigh, Seán Ó",Artificial intelligence in a crisis needs ethics with urgency,Nature Machine Intelligence,,2522-5839,10.1038/s42256-020-0195-0,https://www.nature.com/articles/s42256-020-0195-0,"Artificial intelligence tools can help save lives in a pandemic. However, the need to implement technological solutions rapidly raises challenging ethical issues. We need new approaches for ethics with urgency, to ensure AI can be safely and beneficially used in the COVID-19 response and beyond.",2020-07,2022-01-30 04:50:24,2022-01-30 04:50:24,2020-08-21 19:43:33,365-366,,7,2,,,,,,,,,,en,2020 Springer Nature Limited,,,,www.nature.com,,ZSCC: NoCitationData[s2]  ACC: 20  Number: 7 Publisher: Nature Publishing Group,,/Users/jacquesthibodeau/Zotero/storage/C2PV7XMN/Tzachor et al. - 2020 - Artificial intelligence in a crisis needs ethics w.pdf; /Users/jacquesthibodeau/Zotero/storage/N37E4MJK/s42256-020-0195-0.html,,MetaSafety; CFI; CSER; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FS66RI5N,journalArticle,2019,"Cave, Stephen; Ó hÉigeartaigh, Seán S.",Bridging near- and long-term concerns about AI,Nature Machine Intelligence,,2522-5839,10.1038/s42256-018-0003-2,https://www.nature.com/articles/s42256-018-0003-2,"Debate about the impacts of AI is often split into two camps, one associated with the near term and the other with the long term. This divide is a mistake — the connections between the two perspectives deserve more attention, say Stephen Cave and Seán S. ÓhÉigeartaigh.",2019-01,2022-01-30 04:50:24,2022-01-30 04:50:24,2019-12-16 22:26:28,5-6,,1,1,,,,,,,,,,en,2019 Springer Nature Limited,,,,www.nature.com,,ZSCC: 0000039[s0],,/Users/jacquesthibodeau/Zotero/storage/Q8TE6ISU/Cave and ÓhÉigeartaigh - 2019 - Bridging near- and long-term concerns about AI.pdf; /Users/jacquesthibodeau/Zotero/storage/IBCVWHM9/s42256-018-0003-2.html; /Users/jacquesthibodeau/Zotero/storage/SFDBAMP6/s42256-018-0003-2.html; /Users/jacquesthibodeau/Zotero/storage/VI5J3DQP/s42256-018-0003-2.html; /Users/jacquesthibodeau/Zotero/storage/PMXSKZ3S/s42256-018-0003-2.html,,MetaSafety; CFI; CSER; FHI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DPZRS775,journalArticle,2021,"Zoe Cremer, Carla; Whittlestone, Jess",Artificial Canaries: Early Warning Signs for Anticipatory and Democratic Governance of AI,International Journal of Interactive Multimedia and Artificial Intelligence,,1989-1660,10.9781/ijimai.2021.02.011,https://www.ijimai.org/journal/sites/default/files/2021-02/ijimai_6_5_10.pdf,,2021,2022-01-30 04:50:24,2022-01-30 04:50:24,2021-10-30 19:59:46,100,,5,6,,IJIMAI,Artificial Canaries,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000001,,/Users/jacquesthibodeau/Zotero/storage/UJUII49M/Zoe Cremer and Whittlestone - 2021 - Artificial Canaries Early Warning Signs for Antic.pdf,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
J4M8S57W,journalArticle,2019,"Zerilli, John; Knott, Alistair; Maclaurin, James; Gavaghan, Colin",Algorithmic Decision-Making and the Control Problem,Minds and Machines,,"0924-6495, 1572-8641",10.1007/s11023-019-09513-7,http://link.springer.com/10.1007/s11023-019-09513-7,"Abstract                            The danger of human operators devolving responsibility to machines and failing to detect cases where they fail has been recognised for many years by industrial psychologists and engineers studying the human operators of complex machines. We call it “the control problem”, understood as the tendency of the human within a human–machine control loop to become complacent, over-reliant or unduly diffident when faced with the outputs of a reliable autonomous system. While the control problem has been investigated for some time, up to this point its manifestation in machine learning contexts has not received serious attention. This paper aims to fill that gap. We argue that, except in certain special circumstances, algorithmic decision tools should not be used in high-stakes or safety-critical decisions unless the systems concerned are significantly “better than human” in the relevant domain or subdomain of decision-making. More concretely, we recommend three strategies to address the control problem, the most promising of which involves a               complementary               (and potentially               dynamic               ) coupling between highly proficient algorithmic tools and human agents working alongside one another. We also identify six key principles which all such human–machine systems should reflect in their design. These can serve as a framework both for assessing the viability of any such human–machine system as well as guiding the design and implementation of such systems generally.",2019-12,2022-01-30 04:50:24,2022-01-30 04:50:24,2020-12-12 17:29:35,555-578,,4,29,,Minds & Machines,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000031,,/Users/jacquesthibodeau/Zotero/storage/EFMGD9CK/Zerilli et al. - 2019 - Algorithmic Decision-Making and the Control Proble.pdf,,TechSafety; CFI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TEXCA9UU,journalArticle,2020,"Hollanek, Tomasz",AI transparency: a matter of reconciling design with critique,AI & Society,,1435-5655,10.1007/s00146-020-01110-y,https://doi.org/10.1007/s00146-020-01110-y,"In the late 2010s, various international committees, expert groups, and national strategy boards have voiced the demand to ‘open’ the algorithmic black box, to audit, expound, and demystify artificial intelligence. The opening of the algorithmic black box, however, cannot be seen only as an engineering challenge. In this article, I argue that only the sort of transparency that arises from critique—a method of theoretical examination that, by revealing pre-existing power structures, aims to challenge them—can help us produce technological systems that are less deceptive and more just. I relate the question of AI transparency to the broader challenge of responsible making, contending that future action must aim to systematically reconcile design—as a way of concealing—with critique—as a manner of revealing.",2020-11-17,2022-01-30 04:50:24,2022-01-30 04:50:24,2020-11-23 01:14:10,,,,,,AI & Soc,AI transparency,,,,,,,en,,,,,Springer Link,,ZSCC: 0000001,,/Users/jacquesthibodeau/Zotero/storage/SMRN47IQ/Hollanek - 2020 - AI transparency a matter of reconciling design wi.pdf,,TechSafety; CFI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VKMMWSFX,journalArticle,2019,"Schubert, Stefan; Caviola, Lucius; Faber, Nadira S.",The Psychology of Existential Risk: Moral Judgments about Human Extinction,Scientific Reports,,2045-2322,10.1038/s41598-019-50145-9,https://www.nature.com/articles/s41598-019-50145-9,"The 21st century will likely see growing risks of human extinction, but currently, relatively small resources are invested in reducing such existential risks. Using three samples (UK general public, US general public, and UK students; total N = 2,507), we study how laypeople reason about human extinction. We find that people think that human extinction needs to be prevented. Strikingly, however, they do not think that an extinction catastrophe would be uniquely bad relative to near-extinction catastrophes, which allow for recovery. More people find extinction uniquely bad when (a) asked to consider the extinction of an animal species rather than humans, (b) asked to consider a case where human extinction is associated with less direct harm, and (c) they are explicitly prompted to consider long-term consequences of the catastrophes. We conclude that an important reason why people do not find extinction uniquely bad is that they focus on the immediate death and suffering that the catastrophes cause for fellow humans, rather than on the long-term consequences. Finally, we find that (d) laypeople—in line with prominent philosophical arguments—think that the quality of the future is relevant: they do find extinction uniquely bad when this means forgoing a utopian future.",2019-10-21,2022-01-30 04:50:08,2022-01-30 04:50:08,2020-12-12 02:39:30,15100,,1,9,,,The Psychology of Existential Risk,,,,,,,en,2019 The Author(s),,,,www.nature.com,,ZSCC: 0000013  Number: 1 Publisher: Nature Publishing Group,,/Users/jacquesthibodeau/Zotero/storage/E4P9KAJ8/Schubert et al. - 2019 - The Psychology of Existential Risk Moral Judgment.pdf; /Users/jacquesthibodeau/Zotero/storage/CDXPH7UX/s41598-019-50145-9.html,,MetaSafety; AmbiguosSafety; BERI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MPBR37U9,journalArticle,2019,"Manheim, David",Multiparty Dynamics and Failure Modes for Machine Learning and Artificial Intelligence,Big Data and Cognitive Computing,,,10.3390/bdcc3020021,https://www.mdpi.com/2504-2289/3/2/21,"An important challenge for safety in machine learning and artificial intelligence systems is a set of related failures involving specification gaming, reward hacking, fragility to distributional shifts, and Goodhart&rsquo;s or Campbell&rsquo;s law. This paper presents additional failure modes for interactions within multi-agent systems that are closely related. These multi-agent failure modes are more complex, more problematic, and less well understood than the single-agent case, and are also already occurring, largely unnoticed. After motivating the discussion with examples from poker-playing artificial intelligence (AI), the paper explains why these failure modes are in some senses unavoidable. Following this, the paper categorizes failure modes, provides definitions, and cites examples for each of the modes: accidental steering, coordination failures, adversarial misalignment, input spoofing and filtering, and goal co-option or direct hacking. The paper then discusses how extant literature on multi-agent AI fails to address these failure modes, and identifies work which may be useful for the mitigation of these failure modes.",2019-06,2022-01-30 04:50:07,2022-01-30 04:50:07,2020-11-14 01:02:46,21,,2,3,,,,,,,,,,en,http://creativecommons.org/licenses/by/3.0/,,,,www.mdpi.com,,ZSCC: 0000010  Number: 2 Publisher: Multidisciplinary Digital Publishing Institute,,/Users/jacquesthibodeau/Zotero/storage/6A6HJFPW/Manheim - 2019 - Multiparty Dynamics and Failure Modes for Machine .pdf; /Users/jacquesthibodeau/Zotero/storage/TW632HG4/htm.html,,TechSafety; BERI,artificial intelligence safety; Goodhart’s Law; multi-agent systems; specification gaming,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZHVDZQZN,journalArticle,2021,"Askell, Amanda; Bai, Yuntao; Chen, Anna; Drain, Dawn; Ganguli, Deep; Henighan, Tom; Jones, Andy; Joseph, Nicholas; Mann, Ben; DasSarma, Nova; Elhage, Nelson; Hatfield-Dodds, Zac; Hernandez, Danny; Kernion, Jackson; Ndousse, Kamal; Olsson, Catherine; Amodei, Dario; Brown, Tom; Clark, Jack; McCandlish, Sam; Olah, Chris; Kaplan, Jared",A General Language Assistant as a Laboratory for Alignment,arXiv:2112.00861 [cs],,,,http://arxiv.org/abs/2112.00861,"Given the broad capabilities of large language models, it should be possible to work towards a general-purpose, text-based assistant that is aligned with human values, meaning that it is helpful, honest, and harmless. As an initial foray in this direction we study simple baseline techniques and evaluations, such as prompting. We find that the benefits from modest interventions increase with model size, generalize to a variety of alignment evaluations, and do not compromise the performance of large models. Next we investigate scaling trends for several training objectives relevant to alignment, comparing imitation learning, binary discrimination, and ranked preference modeling. We find that ranked preference modeling performs much better than imitation learning, and often scales more favorably with model size. In contrast, binary discrimination typically performs and scales very similarly to imitation learning. Finally we study a `preference model pre-training' stage of training, with the goal of improving sample efficiency when finetuning on human preferences.",2021-12-09,2022-01-30 04:49:41,2022-03-11 01:37:07,2021-12-22 20:00:09,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s0]  arXiv: 2112.00861,,/Users/jacquesthibodeau/Zotero/storage/WFQZWMJ9/Askell et al. - 2021 - A General Language Assistant as a Laboratory for A.pdf; /Users/jacquesthibodeau/Zotero/storage/SSYFA967/Askell et al. - 2021 - A General Language Assistant as a Laboratory for A.pdf; /Users/jacquesthibodeau/Zotero/storage/QAQSMINN/2112.html; /Users/jacquesthibodeau/Zotero/storage/HVDJ3MV7/2112.html; /Users/jacquesthibodeau/Zotero/storage/UAHGDRM6/Askell et al. - 2021 - A General Language Assistant as a Laboratory for A.pdf; /Users/jacquesthibodeau/Zotero/storage/4ZQ4JR49/Askell et al. - 2021 - A General Language Assistant as a Laboratory for A.pdf; /Users/jacquesthibodeau/Zotero/storage/FUBMXQNV/2112.html; /Users/jacquesthibodeau/Zotero/storage/L2P2SVKR/2112.html,,Anthropic; TechSafety,Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9NGUJJWM,journalArticle,2015,"Pamlin, Dennis; Armstrong, Stuart",Global challenges: 12 risks that threaten human civilization,"Global Challenges Foundation, Stockholm",,,,,,2015,2022-01-30 04:53:17,2022-01-30 04:53:17,,,,,,,,Global challenges,,,,,,,,,,,,Google Scholar,,ZSCC: 0000044,,,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QFQEA659,journalArticle,2019,"Garfinkel, Ben; Dafoe, Allan",How does the offense-defense balance scale?,Journal of Strategic Studies,,0140-2390,10.1080/01402390.2019.1631810,https://doi.org/10.1080/01402390.2019.1631810,"We ask how the offense-defense balance scales, meaning how it changes as investments into a conflict increase. To do so we offer a general formalization of the offense-defense balance in terms of contest success functions. Simple models of ground invasions and cyberattacks that exploit software vulnerabilities suggest that, in both cases, growth in investments will favor offense when investment levels are sufficiently low and favor defense when they are sufficiently high. We refer to this phenomenon as offensive-then-defensive scaling or OD-scaling. Such scaling effects may help us understand the security implications of applications of artificial intelligence that in essence scale up existing capabilities.",2019-09-19,2022-01-30 04:53:17,2022-01-30 04:53:17,2019-12-16 02:16:59,736-763,,6,42,,,,,,,,,,,,,,,Taylor and Francis+NEJM,,ZSCC: 0000038,,/Users/jacquesthibodeau/Zotero/storage/U7XMZSMQ/01402390.2019.html; /Users/jacquesthibodeau/Zotero/storage/8BGM36P6/Garfinkel and Dafoe - 2019 - How does the offense-defense balance scale.pdf; /Users/jacquesthibodeau/Zotero/storage/GMNCIXGC/Garfinkel and Dafoe - 2019 - How does the offense-defense balance scale.pdf; /Users/jacquesthibodeau/Zotero/storage/N5F3S49W/01402390.2019.html; /Users/jacquesthibodeau/Zotero/storage/6HXGJ6Z6/01402390.2019.html,,MetaSafety; FHI,emerging technologies; Offense-defense theory; strategic stability,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GCB9MTUK,journalArticle,2018,"Dresler, Martin; Sandberg, Anders; Bublitz, Christoph; Ohla, Kathrin; Trenado, Carlos; Mroczko-Wasowicz, Aleksandra; Kühn, Simone; Repantis, Dimitris",Hacking the brain: dimensions of cognitive enhancement,ACS chemical neuroscience,,,,,,2018,2022-01-30 04:53:17,2022-01-30 04:53:17,,1137–1148,,3,10,,,Hacking the brain,,,,,,,,,,,,Google Scholar,,ZSCC: 0000046,,/Users/jacquesthibodeau/Zotero/storage/ZHIX2ZJF/acschemneuro.html; /Users/jacquesthibodeau/Zotero/storage/8JC34ET5/Dresler et al. - 2019 - Hacking the brain dimensions of cognitive enhance.pdf; /Users/jacquesthibodeau/Zotero/storage/Q8UUCPHK/acschemneuro.html; /Users/jacquesthibodeau/Zotero/storage/KMK8UZFI/uuid426d61fb-deab-411c-8778-ca7271e53ead.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
M24DR38P,journalArticle,2013,"Armstrong, Stuart",General Purpose Intelligence: Arguing The Orthogonality Thesis,Analysis and Metaphysics,,,,https://www.ceeol.com/search/article-detail?id=137912,"In his paper “The Superintelligent Will,” Nick Bostrom formalized the Orthogonality thesis: the idea that the final goals and intelligence levels of artificial agents are independent of each other. This paper presents arguments for a (narrower) version of the thesis. It proceeds through three steps. First it shows that superintelligent agents with essentially arbitrary goals can exist in our universe –both as theoretical impractical agents such as AIXI and as physically possible realworld agents. Then it argues that if humans are capable of building human-level artificial intelligences, we can build them with an extremely broad spectrum of goals. Finally it shows that the same result holds for any superintelligent agent we could directly or indirectly build. This result is relevant for arguments about the potential motivations of future agents: knowing an artificial agent is of high intelligence does not allow us to presume that it will be moral, we will need to figure out its goals directly.",2013,2022-01-30 04:53:10,2022-01-30 04:53:10,2020-12-18,17,,12,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000032,,/Users/jacquesthibodeau/Zotero/storage/QXXGIDCH/Armstrong - 2020 - GENERAL PURPOSE INTELLIGENCE ARGUING THE ORTHOGON.pdf,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
B67TRECB,journalArticle,2014,"Müller, Vincent C.; Bostrom, Nick",Future progress in artificial intelligence: A poll among experts,AI Matters,,,,,,2014,2022-01-30 04:53:10,2022-01-30 04:53:10,,9–11,,1,1,,,Future progress in artificial intelligence,,,,,,,,,,,,Google Scholar,,ZSCC: 0000036,,/Users/jacquesthibodeau/Zotero/storage/W2ZTR56A/Müller and Bostrom - 2014 - Future progress in artificial intelligence A poll.pdf; /Users/jacquesthibodeau/Zotero/storage/G8FD3JW5/citation.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DX99RGNH,journalArticle,2013,"Bostrom, Nick",Existential Risk Prevention as Global Priority,Global Policy,,1758-5899,10.1111/1758-5899.12002,https://onlinelibrary.wiley.com/doi/abs/10.1111/1758-5899.12002,"Existential risks are those that threaten the entire future of humanity. Many theories of value imply that even relatively small reductions in net existential risk have enormous expected value. Despite their importance, issues surrounding human-extinction risks and related hazards remain poorly understood. In this article, I clarify the concept of existential risk and develop an improved classification scheme. I discuss the relation between existential risks and basic issues in axiology, and show how existential risk reduction (via the maxipok rule) can serve as a strongly action-guiding principle for utilitarian concerns. I also show how the notion of existential risk suggests a new way of thinking about the ideal of sustainability. Policy Implications • Existential risk is a concept that can focus long-term global efforts and sustainability concerns. • The biggest existential risks are anthropogenic and related to potential future technologies. • A moral case can be made that existential risk reduction is strictly more important than any other global public good. • Sustainability should be reconceptualised in dynamic terms, as aiming for a sustainable trajectory rather than a sustainable state. • Some small existential risks can be mitigated today directly (e.g. asteroids) or indirectly (by building resilience and reserves to increase survivability in a range of extreme scenarios) but it is more important to build capacity to improve humanity’s ability to deal with the larger existential risks that will arise later in this century. This will require collective wisdom, technology foresight, and the ability when necessary to mobilise a strong global coordinated response to anticipated existential risks. • Perhaps the most cost-effective way to reduce existential risks today is to fund analysis of a wide range of existential risks and potential mitigation strategies, with a long-term perspective.",2013,2022-01-30 04:53:10,2022-01-30 04:53:10,2019-12-19 01:40:49,15-31,,1,4,,,,,,,,,,en,"© 2013 University of Durham and John Wiley & Sons, Ltd",,,,Wiley Online Library,,ZSCC: 0000462,,/Users/jacquesthibodeau/Zotero/storage/GQZJQ458/1758-5899.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P5NDJMSA,journalArticle,2020,"Cotton‐Barratt, Owen; Daniel, Max; Sandberg, Anders","Defence in Depth Against Human Extinction: Prevention, Response, Resilience, and Why They All Matter",Global Policy,,1758-5899,10.1111/1758-5899.12786,https://onlinelibrary.wiley.com/doi/abs/10.1111/1758-5899.12786,"We look at classifying extinction risks in three different ways, which affect how we can intervene to reduce risk. First, how does it start causing damage? Second, how does it reach the scale of a global catastrophe? Third, how does it reach everyone? In all of these three phases there is a defence layer that blocks most risks: First, we can prevent catastrophes from occurring. Second, we can respond to catastrophes before they reach a global scale. Third, humanity is resilient against extinction even in the face of global catastrophes. The largest probability of extinction is posed when all of these defences are weak, that is, by risks we are unlikely to prevent, unlikely to successfully respond to, and unlikely to be resilient against. We find that it’s usually best to invest significantly into strengthening all three defence layers. We also suggest ways to do so tailored to the classes of risk we identify. Lastly, we discuss the importance of underlying risk factors – events or structural conditions that may weaken the defence layers even without posing a risk of immediate extinction themselves.",2020,2022-01-30 04:53:09,2022-01-30 04:53:09,2020-08-18 21:30:09,271-282,,3,11,,,Defence in Depth Against Human Extinction,,,,,,,en,© 2020 The Authors. Global Policy published by Durham University and John Wiley & Sons Ltd.,,,,Wiley Online Library,,ZSCC: 0000013  _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1758-5899.12786,,/Users/jacquesthibodeau/Zotero/storage/PUZ3UWZ7/Cotton‐Barratt et al. - 2020 - Defence in Depth Against Human Extinction Prevent.pdf; /Users/jacquesthibodeau/Zotero/storage/J7XA65W9/1758-5899.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
84KGTFVX,journalArticle,2017,"Petratos, Pythagoras; Sandberg, Anders; Zhou, Feng",Cyber insurance,"Handbook of Cyber-Development, Cyber-Democracy, and Cyber-Defense",,,,,,2017,2022-01-30 04:53:09,2022-01-30 04:53:09,,1–28,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000004,,/Users/jacquesthibodeau/Zotero/storage/GMBF55X5/10.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UK5IWQD8,journalArticle,2015,"Cotton-Barratt, Owen; Ord, Toby",Existential risk and existential hope: definitions,Future of Humanity Institute: Technical Report,,,,,,2015,2022-01-30 04:53:09,2022-01-30 04:53:09,,78,,2015,1,,,Existential risk and existential hope,,,,,,,,,,,,Google Scholar,,ZSCC: 0000011,,/Users/jacquesthibodeau/Zotero/storage/3M5MWCSK/Cotton-Barratt and Ord - 2015 - Existential risk and existential hope definitions.pdf,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A3K8BZT9,journalArticle,2014,"Sandberg, Anders",Ethics of brain emulations,Journal of Experimental & Theoretical Artificial Intelligence,,,,,,2014,2022-01-30 04:53:09,2022-01-30 04:53:09,,439–457,,3,26,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000030,,/Users/jacquesthibodeau/Zotero/storage/V467G7XP/Sandberg - 2014 - Ethics of brain emulations.pdf; /Users/jacquesthibodeau/Zotero/storage/2I832CBI/0952813X.2014.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AZT7EP8S,journalArticle,2018,"Ding, Jeffrey",Deciphering China’s AI dream,Future of Humanity Institute Technical Report,,,,,,2018,2022-01-30 04:53:09,2022-01-30 04:53:09,,,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: NoCitationData[s4]  ACC: 134,,/Users/jacquesthibodeau/Zotero/storage/RAZG4SA6/Ding - 2018 - Deciphering China’s AI dream.pdf,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TQGSD4VT,journalArticle,2015,"Sandberg, Anders",Death and pain of a digital brain,New Scientist,,,,,,2015,2022-01-30 04:53:09,2022-01-30 04:53:09,,26–27,,3038,227,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000001,,/Users/jacquesthibodeau/Zotero/storage/I7PHUEE5/S026240791531174X.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QR9VVEN7,journalArticle,2021,"Dafoe, Allan; Bachrach, Yoram; Hadfield, Gillian; Horvitz, Eric; Larson, Kate; Graepel, Thore",Cooperative AI: machines must learn to find common ground,Nature,,,10.1038/d41586-021-01170-0,https://www.nature.com/articles/d41586-021-01170-0,"To help humanity solve fundamental problems of cooperation, scientists need to reconceive artificial intelligence as deeply social.",2021-05,2022-01-30 04:53:09,2022-01-30 04:53:09,2021-11-14 18:21:21,33-36,,7857,593,,,Cooperative AI,,,,,,,en,2021 Nature,,,,www.nature.com,,"ZSCC: 0000013  Bandiera_abtest: a Cg_type: Comment Number: 7857 Publisher: Nature Publishing Group Subject_term: Machine learning, Computer science, Society, Technology, Sociology, Human behaviour",,/Users/jacquesthibodeau/Zotero/storage/2ZKWDVBZ/Dafoe et al. - 2021 - Cooperative AI machines must learn to find common.pdf; /Users/jacquesthibodeau/Zotero/storage/TUF3PTSZ/d41586-021-01170-0.html,,MetaSafety,Computer science; Human behaviour; Machine learning; Society; Sociology; Technology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IVHRFFNN,journalArticle,2019,"Weiss, Jessica Chen; Dafoe, Allan","Authoritarian Audiences, Rhetoric, and Propaganda in International Crises: Evidence from China",International Studies Quarterly,,,,,,2019,2022-01-30 04:53:08,2022-01-30 04:53:08,,,,,,,,"Authoritarian Audiences, Rhetoric, and Propaganda in International Crises",,,,,,,,,,,,Google Scholar,,ZSCC: 0000027,,"/Users/jacquesthibodeau/Zotero/storage/B6NWBFCA/Weiss and Dafoe - 2019 - Authoritarian Audiences, Rhetoric, and Propaganda .pdf",,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4B35E3CP,journalArticle,2020,"O'Brien, John T.; Nelson, Cassidy",Assessing the Risks Posed by the Convergence of Artificial Intelligence and Biotechnology,Health Security,,"2326-5094, 2326-5108",10.1089/hs.2019.0122,https://www.liebertpub.com/doi/10.1089/hs.2019.0122,"Rapid developments are currently taking place in the ﬁelds of artiﬁcial intelligence (AI) and biotechnology, and applications arising from the convergence of these 2 ﬁelds are likely to offer immense opportunities that could greatly beneﬁt human health and biosecurity. The combination of AI and biotechnology could potentially lead to breakthroughs in precision medicine, improved biosurveillance, and discovery of novel medical countermeasures as well as facilitate a more effective public health emergency response. However, as is the case with many preceding transformative technologies, new opportunities often present new risks in parallel. Understanding the current and emerging risks at the intersection of AI and biotechnology is crucial for health security specialists and unlikely to be achieved by examining either ﬁeld in isolation. Uncertainties multiply as technologies merge, showcasing the need to identify robust assessment frameworks that could adequately analyze the risk landscape emerging at the convergence of these 2 domains. This paper explores the criteria needed to assess risks associated with AI and biotechnology and evaluates 3 previously published risk assessment frameworks. After highlighting their strengths and limitations and applying to relevant AI and biotechnology examples, the authors suggest a hybrid framework with recommendations for future approaches to risk assessment for convergent technologies.",2020-06-01,2022-01-30 04:53:08,2022-01-30 04:53:08,2020-08-18 21:41:31,219-227,,3,18,,Health Security,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000006,,/Users/jacquesthibodeau/Zotero/storage/MCVXQNE5/O'Brien and Nelson - 2020 - Assessing the Risks Posed by the Convergence of Ar.pdf,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8F8CMTV9,journalArticle,2014,"Sandberg, Anders",Being nice to software animals and babies,Intelligence Unbound: The Future of Uploaded and Machine Minds,,,,,,2014,2022-01-30 04:53:08,2022-01-30 04:53:08,,279,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000003,,/Users/jacquesthibodeau/Zotero/storage/JW9FBBWG/9781118736302.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
J3KN6NP9,journalArticle,2003,"Bostrom, Nick",Astronomical Waste: The Opportunity Cost of Delayed Technological Development: Nick Bostrom,Utilitas,,,10.1017/S0953820800004076,,,2003,2022-01-30 04:53:08,2022-01-30 04:53:08,,308–314,,3,15,,,Astronomical Waste,,,,,,,,,,,,PhilPapers,,ZSCC: 0000000[s0],,/Users/jacquesthibodeau/Zotero/storage/BDU9Z56W/Bostrom - 2003 - Astronomical Waste The Opportunity Cost of Delaye.pdf,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
W36BUBCX,journalArticle,2018,"Duettmann, Allison; Afanasjeva, Olga; Armstrong, Stuart; Braley, Ryan; Cussins, Jessica; Ding, Jeffrey; Eckersley, Peter; Guan, Melody; Vance, Alyssa; Yampolskiy, Roman",Artificial General Intelligence: Coordination & Great Powers,"Foresight Institute: Palo Alto, CA, USA",,,,,,2018,2022-01-30 04:53:08,2022-01-30 04:53:08,,,,,,,,Artificial General Intelligence,,,,,,,,,,,,Google Scholar,,ZSCC: 0000005,,/Users/jacquesthibodeau/Zotero/storage/FSMGIFGF/Duettmann et al. - 2018 - Artificial General Intelligence Coordination & Gr.pdf,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5R74U97D,journalArticle,2018,"Ruderman, Avraham; Everett, Richard; Sikder, Bristy; Soyer, Hubert; Uesato, Jonathan; Kumar, Ananya; Beattie, Charlie; Kohli, Pushmeet",Uncovering Surprising Behaviors in Reinforcement Learning via Worst-case Analysis,,,,,https://openreview.net/forum?id=SkgZNnR5tX,We find environment settings in which SOTA agents trained on navigation tasks display extreme failures suggesting failures in generalization.,2018-09-27,2022-01-30 04:52:49,2022-01-30 04:52:49,2020-12-12 15:24:23,,,,,,,,,,,,,,en,,,,,openreview.net,,ZSCC: 0000005,,/Users/jacquesthibodeau/Zotero/storage/I89H9IRW/Ruderman et al. - 2018 - Uncovering Surprising Behaviors in Reinforcement L.pdf; /Users/jacquesthibodeau/Zotero/storage/32BQDHJ9/forum.html,,TechSafety; DeepMind; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P237XTGF,journalArticle,2021,"Everitt, Tom; Hutter, Marcus",Reward Tampering Problems and Solutions in Reinforcement Learning: A Causal Influence Diagram Perspective,Synthese,,,,http://arxiv.org/abs/1908.04734,"Can an arbitrarily intelligent reinforcement learning agent be kept under control by a human user? Or do agents with sufficient intelligence inevitably find ways to shortcut their reward signal? This question impacts how far reinforcement learning can be scaled, and whether alternative paradigms must be developed in order to build safe artificial general intelligence. In this paper, we use an intuitive yet precise graphical model called causal influence diagrams to formalize reward tampering problems. We also describe a number of modifications to the reinforcement learning objective that prevent incentives for reward tampering. We verify the solutions using recently developed graphical criteria for inferring agent incentives from causal influence diagrams. Along the way, we also compare corrigibility and self-preservation properties of the various solutions, and discuss how they can be combined into a single agent without reward tampering incentives.",2021,2022-01-30 04:52:47,2022-01-30 04:52:47,2019-12-16 20:27:10,6435-6467,,,198,,,Reward Tampering Problems and Solutions in Reinforcement Learning,,,,,,,,,,,,arXiv.org,,ZSCC: 0000023  arXiv: 1908.04734,,/Users/jacquesthibodeau/Zotero/storage/NTKHT668/Everitt and Hutter - 2019 - Reward Tampering Problems and Solutions in Reinfor.pdf; /Users/jacquesthibodeau/Zotero/storage/N4BW89UV/Everitt and Hutter - 2019 - Reward Tampering Problems and Solutions in Reinfor.pdf; /Users/jacquesthibodeau/Zotero/storage/5598DIDB/1908.html; /Users/jacquesthibodeau/Zotero/storage/72W84CA5/1908.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DUE99KRM,journalArticle,2014,"Muehlhauser, Luke; Bostrom, Nick",Why we need friendly AI,Think,,,,,,2014,2022-01-30 04:53:45,2022-01-30 04:53:45,,41–47,,36,13,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000033,,/Users/jacquesthibodeau/Zotero/storage/328482VT/Muehlhauser and Bostrom - 2014 - Why we need friendly AI.pdf; /Users/jacquesthibodeau/Zotero/storage/ZGFS8MTM/3C576A0EE8DEFDE82FC809493B37A265.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9RTZ9S2A,journalArticle,2014,"Armstrong, Stuart; ÓhÉigeartaigh, Seán",Who knows anything about anything about AI?,Intelligence Unbound: The Future of Uploaded and Machine Minds,,,,,,2014,2022-01-30 04:53:45,2022-01-30 04:53:45,,46–60,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000003,,/Users/jacquesthibodeau/Zotero/storage/7M3UKW76/9781118736302.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EGIF8T8S,journalArticle,2018,"Currie, Adrian; Ó HÉigeartaigh, Seán; Apollo-University Of Cambridge Repository; Apollo-University Of Cambridge Repository",Working together to face humanity’s greatest threats: Introduction to The Future of Research on Catastrophic and Existential Risk.,Futures,,,10.17863/CAM.27560,https://www.repository.cam.ac.uk/handle/1810/280193,"Ours is a resilient species. Around 70,000 years ago our total population may have fallen to between three and ten thousand individuals, possibly due to a supervolcanic eruption (Ambrose 1998) . Yet our ancestors survived, squeezed through the bottleneck, and flourished. But this resilience cannot be taken for granted. We are interconnected and interdependent as never before; the power and scale of our technological capacities are unprecedented. We are in uncharted waters and thus our previous survival is no longer a reason to expect our continued survival (Bostrom 2013). As a result, it is urgent that we develop a systematic understanding of the nature and causes of catastrophic and existential risks.",2018-09-11,2022-01-30 04:53:38,2022-01-30 04:53:38,2020-12-13 22:25:19,,,,,,,Working together to face humanity’s greatest threats,,,,,,,,,,,,DOI.org (Datacite),,ZSCC: NoCitationData[s1]  ACC: 10  Publisher: Apollo - University of Cambridge Repository,,/Users/jacquesthibodeau/Zotero/storage/I5QMVCS8/Currie and Ó HÉigeartaigh - 2018 - Working together to face humanity’s greatest threa.pdf,,MetaSafety; CSER; FLI,,,,,Apollo-University Of Cambridge Repository; Apollo-University Of Cambridge Repository,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7DD2HTK9,journalArticle,2012,"Armstrong, Stuart; Sandberg, Anders; Bostrom, Nick",Thinking Inside the Box: Controlling and Using an Oracle AI,Minds and Machines,,"0924-6495, 1572-8641",10.1007/s11023-012-9282-2,http://link.springer.com/10.1007/s11023-012-9282-2,,2012-11,2022-01-30 04:53:37,2022-01-30 04:53:37,2020-11-22 05:25:29,299-324,,4,22,,Minds & Machines,Thinking Inside the Box,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000108,,,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
J5FVJG45,journalArticle,2019,"Sandberg, Anders","There is plenty of time at the bottom: the economics, risk and ethics of time compression",foresight,,,,,,2019,2022-01-30 04:53:37,2022-01-30 04:53:37,,84–99,,1,21,,,There is plenty of time at the bottom,,,,,,,,,,,,Google Scholar,,ZSCC: 0000002,,/Users/jacquesthibodeau/Zotero/storage/JJHQ248X/Sandberg - 2019 - There is plenty of time at the bottom the economi.pdf; /Users/jacquesthibodeau/Zotero/storage/IFG9FJ5G/html.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IR2NVS39,journalArticle,2014,"Beckstead, Nick; Bostrom, N.; Bowerman, N.; Cotton-Barratt, O.; MacAskill, W.; Eigeartaigh, S.; Ord, T.",Unprecedented technological risks,Policy brief. Available online: http://www. fhi. ox. ac. uk/wpcontent/uploads/Unprecedented-Technological-Risks. pdf. Last Accessed September,,,,,,2014,2022-01-30 04:53:37,2022-01-30 04:53:37,,2015,,,29,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000004,,/Users/jacquesthibodeau/Zotero/storage/VA6NV95K/Beckstead et al. - 2014 - Unprecedented technological risks.pdf,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
674VB36I,journalArticle,2014,"Sandberg, Anders",Transhumanism and the Meaning of Life,Religion and Transhumanism: The Unknown Future of Human Enhancement,,,,,,2014,2022-01-30 04:53:37,2022-01-30 04:53:37,,8,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000027,,/Users/jacquesthibodeau/Zotero/storage/WJZMQ83Q/Sandberg - 2014 - Transhumanism and the Meaning of Life.pdf,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4UAU4M26,journalArticle,2018,"Bostrom, Nick",The vulnerable world hypothesis,Global Policy,,,,,,2018,2022-01-30 04:53:37,2022-01-30 04:53:37,,,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: NoCitationData[s5]  ACC: 74,,/Users/jacquesthibodeau/Zotero/storage/TWVPACKI/1758-5899.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
48C7KN4A,journalArticle,2021,"Ding, Jeffrey; Dafoe, Allan",The Logic of Strategic Assets: From Oil to AI,Security Studies,,,10.1080/09636412.2021.1915583,https://arxiv.org/abs/2001.03246,"What resources and technologies are strategic? This question is often the focus of policy and theoretical debates, where the label “strategic” designates those assets that warrant the attention of the highest levels of the state. But these conversations are plagued by analytical confusion, flawed heuristics, and the rhetorical use of “strategic” to advance particular agendas. We aim to improve these conversations through conceptual clarification, introducing a theory based on important rivalrous externalities for which socially optimal behavior will not be produced alone by markets or individual national security entities. We distill and theorize the most important three forms of these externalities, which involve cumulative-, infrastructure-, and dependency-strategic logics. We then employ these logics to clarify three important cases: the Avon 2 engine in the 1950s, the U.S.-Japan technology rivalry in the late 1980s, and contemporary conversations about artificial intelligence.",2021-06-03,2022-01-30 04:53:36,2022-01-30 04:53:36,,182-212,,2,30,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000005,,/Users/jacquesthibodeau/Zotero/storage/ZSBT2TB2/Ding and Dafoe - The Logic of Strategic Assets From Oil to AI.pdf,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HVZV6X2T,journalArticle,2014,"Sandberg, Anders",The five biggest threats to human existence,"The Conversation, May",,,,,,2014,2022-01-30 04:53:36,2022-01-30 04:53:36,,,,,29,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000014,,,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9G5EAA7N,journalArticle,2020,"Scholl, Keller; Hanson, Robin",Testing the Automation Revolution Hypothesis,Economics Letters,,,https://doi.org/10.1016/j.econlet.2020.109287,https://www.sciencedirect.com/science/article/abs/pii/S0165176520301919,"Recently, many have predicted an imminent automation revolution, and large resulting job losses. Others have created metrics to predict new patterns in job automation vulnerability. As context to such claims, we test basic theory, two vulnerability metrics, and 251 O*NET job features as predictors of 1505 expert reports regarding automation levels in 832 U.S. job types from 1999 to 2019. We find that pay, employment, and vulnerability metrics are predictive (R^2~0.15), but add little to the top 25 O*NET job features, which together predict far better (R^2~0.55). These best predictors seem understandable in terms of traditional kinds of automation, and have not changed over our time period. Instead, it seems that jobs have changed their features to become more suitable for automation. We thus find no evidence yet of a revolution in the patterns or quantity of automation. And since, over this period, automation increases have predicted neither changes in pay nor employment, this suggests that workers have little to fear if such a revolution does come.",2020-08,2022-01-30 04:53:36,2022-01-30 04:53:36,2020-12-19 05:22:30,,,,193,,,,,,,,,,en,,,,,papers.ssrn.com,,ZSCC: 0000002  DOI: 10.2139/ssrn.3496364,,/Users/jacquesthibodeau/Zotero/storage/5K2P6H4X/papers.html,,MetaSafety; FHI,artificial intelligence; automation; employment; occupations; technology; wages,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PGPQFA45,journalArticle,2016,"Bostrom, Nick; Douglas, Thomas; Sandberg, Anders",The Unilateralist’s Curse and the Case for a Principle of Conformity,Social Epistemology,,0269-1728,10.1080/02691728.2015.1108373,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4959137/,"In some situations a number of agents each have the ability to undertake an initiative that would have significant effects on the others. Suppose that each of these agents is purely motivated by an altruistic concern for the common good. We show that if each agent acts on her own personal judgment as to whether the initiative should be undertaken, then the initiative will be undertaken more often than is optimal. We suggest that this phenomenon, which we call the unilateralist’s curse, arises in many contexts, including some that are important for public policy. To lift the curse, we propose a principle of conformity, which would discourage unilateralist action. We consider three different models for how this principle could be implemented, and respond to an objection that could be raised against it.",2016-07-03,2022-01-30 04:53:36,2022-01-30 04:53:36,2019-12-19 01:40:23,350-371,,4,30,,Soc Epistemol,,,,,,,,,,,,,PubMed Central,,ZSCC: NoCitationData[s4]  ACC: 32  PMID: 27499570 PMCID: PMC4959137,,/Users/jacquesthibodeau/Zotero/storage/D58DKCPS/02691728.2015.html; /Users/jacquesthibodeau/Zotero/storage/6J9H3WV5/02691728.2015.html; /Users/jacquesthibodeau/Zotero/storage/FGT8BKJU/Bostrom et al. - 2016 - The Unilateralist’s Curse and the Case for a Princ.pdf; ; /Users/jacquesthibodeau/Zotero/storage/A7EZBQIA/02691728.2015.html; /Users/jacquesthibodeau/Zotero/storage/I6NRCKZ9/02691728.2015.html,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4959137/,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IPDHKEBF,journalArticle,2012,"Bostrom, Nick",The Superintelligent Will: Motivation and Instrumental Rationality in Advanced Artificial Agents,Minds and Machines,,,,,,2012,2022-01-30 04:53:36,2022-01-30 04:53:36,,71–85,,2,22,,,The superintelligent will,,,,,,,,,,,,Google Scholar,,ZSCC: 0000268  Publisher: Springer,,/Users/jacquesthibodeau/Zotero/storage/F7ZW2HE4/Bostrom - 2012 - The superintelligent will Motivation and instrume.pdf; /Users/jacquesthibodeau/Zotero/storage/S6ZWXU2P/s11023-012-9281-3.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
W6K7JDJ5,journalArticle,2014,"Armstrong, Stuart; Sotala, Kaj; Ó hÉigeartaigh, Seán S.","The errors, insights and lessons of famous AI predictions – and what they mean for the future",Journal of Experimental & Theoretical Artificial Intelligence,,"0952-813X, 1362-3079",10.1080/0952813X.2014.895105,https://www.tandfonline.com/doi/full/10.1080/0952813X.2014.895105,,2014-07-03,2022-01-30 04:53:36,2022-01-30 04:53:36,2020-11-22 02:22:03,317-342,,3,26,,Journal of Experimental & Theoretical Artificial Intelligence,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000076,,"/Users/jacquesthibodeau/Zotero/storage/FGDN4MZQ/Armstrong et al. - 2014 - The errors, insights and lessons of famous AI pred.pdf",,MetaSafety; FHI; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WBKQEXZE,journalArticle,2016,"Bostrom, Nick","The Control Problem. Excerpts from Superintelligence: Paths, Dangers, Strategies",Science Fiction and Philosophy: From Time Travel to Superintelligence,,,,,,2016,2022-01-30 04:53:36,2022-01-30 04:53:36,,308–330,,,,,,The Control Problem. Excerpts from Superintelligence,,,,,,,,,,,,Google Scholar,,ZSCC: NoCitationData[s4]  ACC: 2,,/Users/jacquesthibodeau/Zotero/storage/HV36EXA9/9781118922590.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BIIKAGQ9,journalArticle,2018,"Tuyls, Karl; Pérolat, Julien; Lanctot, Marc; Ostrovski, Georg; Savani, Rahul; Leibo, Joel Z; Ord, Toby; Graepel, Thore; Legg, Shane",Symmetric Decomposition of Asymmetric Games,Scientific Reports,,2045-2322,10.1038/s41598-018-19194-4,http://www.nature.com/articles/s41598-018-19194-4,,2018-12,2022-01-30 04:53:36,2022-01-30 04:53:36,2020-12-18 06:44:16,1015,,1,8,,Sci Rep,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000028,,/Users/jacquesthibodeau/Zotero/storage/QZAG4JGH/Tuyls et al. - 2018 - Symmetric Decomposition of Asymmetric Games.pdf,,TechSafety; FHI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8C98HP2Q,journalArticle,2017,"Bostrom, Nick",Strategic implications of openness in AI development,Global Policy,,,,,,2017,2022-01-30 04:53:35,2022-01-30 04:53:35,,135–148,,2,8,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000118,,/Users/jacquesthibodeau/Zotero/storage/BQUWUV7E/Bostrom - 2017 - Strategic Implications of Openness in span style=.pdf; /Users/jacquesthibodeau/Zotero/storage/9FCZSPKD/1758-5899.html; /Users/jacquesthibodeau/Zotero/storage/VAG7GK35/1758-5899.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PZGR5876,journalArticle,2007,"Bostrom, Nick",Sleeping Beauty and Self-location: A Hybrid Model,Synthese,,"0039-7857, 1573-0964",10.1007/s11229-006-9010-7,http://link.springer.com/10.1007/s11229-006-9010-7,,2007-05-24,2022-01-30 04:53:35,2022-01-30 04:53:35,2020-11-22 04:57:23,59-78,,1,157,,Synthese,Sleeping Beauty and Self-location,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000084,,/Users/jacquesthibodeau/Zotero/storage/3ITIBQQR/Bostrom - 2007 - Sleeping Beauty and Self-location A Hybrid Model.pdf,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZNTSGQD9,journalArticle,2016,"Armstrong, Stuart; Bostrom, Nick; Shulman, Carl",Racing to the precipice: a model of artificial intelligence development,AI & society,,,,,,2016,2022-01-30 04:53:20,2022-01-30 04:53:20,,201–206,,2,31,,,Racing to the precipice,,,,,,,,,,,,Google Scholar,,ZSCC: 0000117,,/Users/jacquesthibodeau/Zotero/storage/R2KQIP2D/Armstrong et al. - 2016 - Racing to the precipice a model of artificial int.pdf; /Users/jacquesthibodeau/Zotero/storage/9DCWMVRI/s00146-015-0590-y.html; /Users/jacquesthibodeau/Zotero/storage/GF3WAQ5J/s00146-015-0590-y.html; /Users/jacquesthibodeau/Zotero/storage/QZ3MJSN2/Armstrong et al. - 2016 - Racing to the precipice a model of artificial int.pdf,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PNGRSCMM,journalArticle,2018,"Bostrom, Nick; Dafoe, Allan; Flynn, Carrick",Public Policy and Superintelligent AI: A Vector Field Approach,"Governance of AI Program, Future of Humanity Institute, University of Oxford: Oxford, UK",,,,,,2018,2022-01-30 04:53:19,2022-01-30 04:53:19,,,,,,,,Public Policy and Superintelligent AI,,,,,,,,,,,,Google Scholar,,ZSCC: 0000009[s0],,/Users/jacquesthibodeau/Zotero/storage/RU3NKC2J/Bostrom et al. - 2018 - Public Policy and Superintelligent AI A Vector Fi.pdf,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XCEK6ERA,journalArticle,2010,"Ord, Toby; Hillerbrand, Rafaela; Sandberg, Anders",Probing the improbable: methodological challenges for risks with low probabilities and high stakes,Journal of Risk Research,,1366-9877,10.1080/13669870903126267,https://doi.org/10.1080/13669870903126267,"Some risks have extremely high stakes. For example, a worldwide pandemic or asteroid impact could potentially kill more than a billion people. Comfortingly, scientific calcultions often put very low probabilities on the occurrence of such catastrophes. In this paper, we argue that there are important new methodological problems which arise when assessing global catastrophic risks and we focus on a problem regarding probability estimation. When an expert provides a calculation of the probability of an outcome, they are really providing the probability of the outcome occurring, given that their argument is watertight. However, their argument may fail for a number of reasons, such as a flaw in the underlying theory, a flaw in the modelling of the problem or a mistake in the calculations. If the probability estimate given by an argument is dwarfed by the chance that the argument itself is flawed, then the estimate is suspect. We develop this idea formally, explaining how it differs from the related distinction between model and parameter uncertainty. Using the risk estimates from the Large Hadron Collider as a test case, we show how serious the problem can be when it comes to catastrophic risks and how best to address it.",2010-03-01,2022-01-30 04:53:19,2022-01-30 04:53:19,2019-12-19 01:43:42,191-205,,2,13,,,Probing the improbable,,,,,,,,,,,,Taylor and Francis+NEJM,,ZSCC: 0000075,,/Users/jacquesthibodeau/Zotero/storage/WAX69HQK/13669870903126267.html; /Users/jacquesthibodeau/Zotero/storage/GAC3SCP9/Ord et al. - 2010 - Probing the improbable methodological challenges .pdf,,MetaSafety; FHI,calculation error; high stakes; low probability; particle accelerator; risk analysis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HVB78EJ5,journalArticle,2017,"Farquhar, Sebastian; Cotton-Barratt, Owen; Snyder-Beattie, Andrew",Pricing externalities to balance public risks and benefits of research,Health security,,,,,,2017,2022-01-30 04:53:19,2022-01-30 04:53:19,,401–408,,4,15,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000009,,/Users/jacquesthibodeau/Zotero/storage/ZQUZGJQG/PMC5576218.html; /Users/jacquesthibodeau/Zotero/storage/744WRBIH/hs.2016.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Z22KID9N,journalArticle,2016,"Bostrom, Nick; Dafoe, Allan; Flynn, Carrick",Policy desiderata in the development of machine superintelligence,"Future of Humanity Institute, University of Oxford. Retrieved June",,,,,,2016,2022-01-30 04:53:19,2022-01-30 04:53:19,,2018,,,8,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000022,,/Users/jacquesthibodeau/Zotero/storage/QMJDA2W8/Bostrom et al. - 2016 - Policy desiderata in the development of machine su.pdf,,MetaSafety; FHI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
62278P83,journalArticle,2013,"Dresler, Martin; Sandberg, Anders; Ohla, Kathrin; Bublitz, Christoph; Trenado, Carlos; Mroczko-Wąsowicz, Aleksandra; Kühn, Simone; Repantis, Dimitris",Non-pharmacological cognitive enhancement,Neuropharmacology,,,,,,2013,2022-01-30 04:53:19,2022-01-30 04:53:19,,529–543,,,64,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000187,,/Users/jacquesthibodeau/Zotero/storage/7RW4K4QG/Dresler et al. - 2013 - Non-pharmacological cognitive enhancement.pdf; /Users/jacquesthibodeau/Zotero/storage/X32M9RW4/S0028390812003310.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
94KSQKR7,journalArticle,2021,"Prunkl, Carina E. A.; Ashurst, Carolyn; Anderljung, Markus; Webb, Helena; Leike, Jan; Dafoe, Allan",Institutionalizing ethics in AI through broader impact requirements,Nature Machine Intelligence,,2522-5839,10.1038/s42256-021-00298-y,https://www.nature.com/articles/s42256-021-00298-y,"Turning principles into practice is one of the most pressing challenges of artificial intelligence (AI) governance. In this Perspective, we reflect on a governance initiative by one of the world’s largest AI conferences. In 2020, the Conference on Neural Information Processing Systems (NeurIPS) introduced a requirement for submitting authors to include a statement on the broader societal impacts of their research. Drawing insights from similar governance initiatives, including institutional review boards (IRBs) and impact requirements for funding applications, we investigate the risks, challenges and potential benefits of such an initiative. Among the challenges, we list a lack of recognized best practice and procedural transparency, researcher opportunity costs, institutional and social pressures, cognitive biases and the inherently difficult nature of the task. The potential benefits, on the other hand, include improved anticipation and identification of impacts, better communication with policy and governance experts, and a general strengthening of the norms around responsible research. To maximize the chance of success, we recommend measures to increase transparency, improve guidance, create incentives to engage earnestly with the process, and facilitate public deliberation on the requirement’s merits and future. Perhaps the most important contribution from this analysis are the insights we can gain regarding effective community-based governance and the role and responsibility of the AI research community more broadly.",2021-02,2022-01-30 04:53:18,2022-01-30 04:53:18,2021-10-31 19:12:54,104-110,,2,3,,Nat Mach Intell,,,,,,,,en,2021 Springer Nature Limited,,,,www.nature.com,,ZSCC: 0000005  Bandiera_abtest: a Cg_type: Nature Research Journals Number: 2 Primary_atype: Reviews Publisher: Nature Publishing Group Subject_term: Conferences and meetings;Policy;Publishing Subject_term_id: conferences-and-meetings;policy;publishing,,/Users/jacquesthibodeau/Zotero/storage/GQEBEVD3/Prunkl et al. - 2021 - Institutionalizing ethics in AI through broader im.pdf,,MetaSafety,Conferences and meetings; Policy; Publishing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
T8G2GCWV,journalArticle,2019,"Russell, Stuart",It's not too soon to be wary of AI: We need to act now to protect humanity from future superintelligent machines,IEEE Spectrum,,1939-9340,10.1109/MSPEC.2019.8847590,,"AI research is making great strides toward its long-term goal of human-level or superhuman intelligent machines. If it succeeds in its current form, however, that could well be catastrophic for the human race. The reason is that the ""standard model"" of AI requires machines to pursue a fixed objective specified by humans. We are unable to specify the objective completely and correctly, nor can we anticipate or prevent the harms that machines pursuing an incorrect objective will create when operating on a global scale with superhuman capabilities. Already, we see examples such as social-media algorithms that learn to optimize click-through by manipulating human preferences, with disastrous consequences for democratic systems.",2019-10,2022-01-30 04:50:54,2022-01-30 04:50:54,,46-51,,10,56,,,It's not too soon to be wary of AI,,,,,,,,,,,,IEEE Xplore,,ZSCC: 0000005,,/Users/jacquesthibodeau/Zotero/storage/3QV56CCC/8847590.html,,CHAI; TechSafety,artificial intelligence; Artificial intelligence; AI research; Calculators; current form; democratic systems; Earth; fixed objective; human preferences; human race; human-level; humanities; humanity; Robots; Safety; social-media algorithms; standard model; Standards; superhuman capabilities; superhuman intelligent machines; superintelligent machines; Switches,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
98C6EF6Z,journalArticle,2018,"Shah, Rohin; Gundotra, Noah; Abbeel, Pieter; Dragan, Anca",Inferring Reward Functions from Demonstrators with Unknown Biases,,,,,https://openreview.net/forum?id=rkgqCiRqKQ,"Our goal is to infer reward functions from demonstrations. In order to infer the correct reward function, we must account for the systematic ways in which the demonstrator is suboptimal. Prior work...",2018-09-27,2022-01-30 04:50:53,2022-01-30 04:50:53,2019-12-18 03:11:08,,,,,,,,,,,,,,,,,,,openreview.net,,ZSCC: 0000001,,/Users/jacquesthibodeau/Zotero/storage/S622QK9T/Shah et al. - 2018 - Inferring Reward Functions from Demonstrators with.pdf; /Users/jacquesthibodeau/Zotero/storage/STVESK9J/forum.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IM89NS2R,journalArticle,2020,"Gates, Vael; Griffiths, Thomas L.; Dragan, Anca D.",How to Be Helpful to Multiple People at Once,Cognitive Science,,"0364-0213, 1551-6709",10.1111/cogs.12841,https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12841,"When someone hosts a party, when governments choose an aid program, or when assistive robots decide what meal to serve to a family, decision-makers must determine how to help even when their recipients have very different preferences. Which combination of people’s desires should a decisionmaker serve? To provide a potential answer, we turned to psychology: What do people think is best when multiple people have different utilities over options? We developed a quantitative model of what people consider desirable behavior, characterizing participants’ preferences by inferring which combination of “metrics” (maximax, maxsum, maximin, or inequality aversion [IA]) best explained participants’ decisions in a drink-choosing task. We found that participants’ behavior was best described by the maximin metric, describing the desire to maximize the happiness of the worst-off person, though participant behavior was also consistent with maximizing group utility (the maxsum metric) and the IA metric to a lesser extent. Participant behavior was consistent across variation in the agents involved and tended to become more maxsum-oriented when participants were told they were players in the task (Experiment 1). In later experiments, participants maintained maximin behavior across multi-step tasks rather than shortsightedly focusing on the individual steps therein (Experiment 2, Experiment 3). By repeatedly asking participants what choices they would hope for in an optimal, just decision-maker, and carefully disambiguating which quantitative metrics describe these nuanced choices, we help constrain the space of what behavior we desire in leaders, artiﬁcial intelligence systems helping decision-makers, and the assistive robots and decision-makers of the future.",2020-06,2022-01-30 04:50:53,2022-01-30 04:50:53,2020-11-21 18:26:49,,,6,44,,Cogn Sci,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000003,,/Users/jacquesthibodeau/Zotero/storage/F6HG3522/Gates et al. - 2020 - How to Be Helpful to Multiple People at Once.pdf,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8TFC6UFR,journalArticle,2013,"Halpern, Joseph Y.; Pass, Rafael",Game Theory with Translucent Players,International Journal of Game Theory,,,,http://arxiv.org/abs/1308.3778,"A traditional assumption in game theory is that players are opaque to one another---if a player changes strategies, then this change in strategies does not affect the choice of other players' strategies. In many situations this is an unrealistic assumption. We develop a framework for reasoning about games where the players may be translucent to one another; in particular, a player may believe that if she were to change strategies, then the other player would also change strategies. Translucent players may achieve significantly more efficient outcomes than opaque ones. Our main result is a characterization of strategies consistent with appropriate analogues of common belief of rationality. Common Counterfactual Belief of Rationality (CCBR) holds if (1) everyone is rational, (2) everyone counterfactually believes that everyone else is rational (i.e., all players i believe that everyone else would still be rational even if $i$ were to switch strategies), (3) everyone counterfactually believes that everyone else is rational, and counterfactually believes that everyone else is rational, and so on. CCBR characterizes the set of strategies surviving iterated removal of minimax dominated strategies, where a strategy s for player i is minimax dominated by s' if the worst-case payoff for i using s' is better than the best possible payoff using s.",2013-08-17,2022-01-30 04:50:52,2022-01-30 04:50:52,2020-11-22 02:29:01,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000021  arXiv: 1308.3778,,/Users/jacquesthibodeau/Zotero/storage/UZXNUF5J/Halpern and Pass - 2013 - Game Theory with Translucent Players.pdf; /Users/jacquesthibodeau/Zotero/storage/DDVE6APB/1308.html,,CHAI; TechSafety; AmbiguosSafety,Computer Science - Computer Science and Game Theory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EMNDB395,journalArticle,2017,"Huang, Sandy H.; Held, David; Abbeel, Pieter; Dragan, Anca D.",Enabling Robots to Communicate their Objectives,Autonomous Robots,,,10.15607/RSS.2017.XIII.059,https://arxiv.org/abs/1702.03465v2,"The overarching goal of this work is to efficiently enable end-users to correctly anticipate a robot's behavior in novel situations. Since a robot's behavior is often a direct result of its underlying objective function, our insight is that end-users need to have an accurate mental model of this objective function in order to understand and predict what the robot will do. While people naturally develop such a mental model over time through observing the robot act, this familiarization process may be lengthy. Our approach reduces this time by having the robot model how people infer objectives from observed behavior, and then it selects those behaviors that are maximally informative. The problem of computing a posterior over objectives from observed behavior is known as Inverse Reinforcement Learning (IRL), and has been applied to robots learning human objectives. We consider the problem where the roles of human and robot are swapped. Our main contribution is to recognize that unlike robots, humans will not be exact in their IRL inference. We thus introduce two factors to define candidate approximate-inference models for human learning in this setting, and analyze them in a user study in the autonomous driving domain. We show that certain approximate-inference models lead to the robot generating example behaviors that better enable users to anticipate what it will do in novel situations. Our results also suggest, however, that additional research is needed in modeling how humans extrapolate from examples of robot behavior.",2017-02-11,2022-01-30 04:50:45,2022-01-30 04:50:45,2019-12-18 01:39:51,,,,43,,,,,,,,,,en,,,,,arxiv.org,,ZSCC: 0000002,,/Users/jacquesthibodeau/Zotero/storage/XI6MUDJM/Huang et al. - 2017 - Enabling Robots to Communicate their Objectives.pdf; /Users/jacquesthibodeau/Zotero/storage/3W9QBCM2/1702.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VSQNXUU5,journalArticle,2019,"Griffiths, Thomas L; Callaway, Frederick; Chang, Michael B; Grant, Erin; Krueger, Paul M; Lieder, Falk",Doing more with less: meta-reasoning and meta-learning in humans and machines,Current Opinion in Behavioral Sciences,,2352-1546,10.1016/j.cobeha.2019.01.005,http://www.sciencedirect.com/science/article/pii/S2352154618302122,"Artificial intelligence systems use an increasing amount of computation and data to solve very specific problems. By contrast, human minds solve a wide range of problems using a fixed amount of computation and limited experience. We identify two abilities that we see as crucial to this kind of general intelligence: meta-reasoning (deciding how to allocate computational resources) and meta-learning (modeling the learning environment to make better use of limited data). We summarize the relevant AI literature and relate the resulting ideas to recent work in psychology.",2019-10-01,2022-01-30 04:50:44,2022-01-30 04:50:44,2020-12-17 22:22:24,24-30,,,29,,Current Opinion in Behavioral Sciences,Doing more with less,SI: 29: Artificial Intelligence (2019),,,,,,en,,,,,ScienceDirect,,ZSCC: 0000039,,/Users/jacquesthibodeau/Zotero/storage/D7G3XQGQ/Griffiths et al. - 2019 - Doing more with less meta-reasoning and meta-lear.pdf; /Users/jacquesthibodeau/Zotero/storage/Z9B62VG8/S2352154618302122.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
I6ETESD7,journalArticle,2019,"Fridovich-Keil, David; Bajcsy, Andrea; Fisac, Jaime F; Herbert, Sylvia L; Wang, Steven; Dragan, Anca D; Tomlin, Claire J",Confidence-aware motion prediction for real-time collision avoidance <sup>1</sup>,The International Journal of Robotics Research,,"0278-3649, 1741-3176",10.1177/0278364919859436,http://journals.sagepub.com/doi/10.1177/0278364919859436,"One of the most difficult challenges in robot motion planning is to account for the behavior of other moving agents, such as humans. Commonly, practitioners employ predictive models to reason about where other agents are going to move. Though there has been much recent work in building predictive models, no model is ever perfect: an agent can always move unexpectedly, in a way that is not predicted or not assigned sufficient probability. In such cases, the robot may plan trajectories that appear safe but, in fact, lead to collision. Rather than trust a model’s predictions blindly, we propose that the robot should use the model’s current predictive accuracy to inform the degree of confidence in its future predictions. This model confidence inference allows us to generate probabilistic motion predictions that exploit modeled structure when the structure successfully explains human motion, and degrade gracefully whenever the human moves unexpectedly. We accomplish this by maintaining a Bayesian belief over a single parameter that governs the variance of our human motion model. We couple this prediction algorithm with a recently proposed robust motion planner and controller to guide the construction of robot trajectories that are, to a good approximation, collision-free with a high, user-specified probability. We provide extensive analysis of the combined approach and its overall safety properties by establishing a connection to reachability analysis, and conclude with a hardware demonstration in which a small quadcopter operates safely in the same space as a human pedestrian.",2019-06-24,2022-01-30 04:50:43,2022-01-30 04:50:43,2019-07-08 16:10:37,27836491985943,,,,,The International Journal of Robotics Research,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s3]  ACC: 40,,,,CHAI; TechSafety,human motion prediction; motion planning; robust control; safety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZTIEC2KV,journalArticle,2017,"Russell, Stuart","Artificial intelligence: The future is superintelligent [Book review of ""Life 3.0: Being Human in the Age of Artificial Intelligence"" by Max Tegmark]",Nature,,1476-4687,10.1038/548520a,http://www.nature.com/articles/548520a,Stuart Russell weighs up a book on the risks and rewards of the AI revolution.,2017-08-30,2022-01-30 04:50:43,2022-01-30 04:50:43,2019-04-03 00:22:19,520-521,,,548,,,Artificial intelligence,,,,,,,en,,,,,www-nature-com.proxy.lib.uwaterloo.ca,,ZSCC: NoCitationData[s3]  ACC: 26  J: 8,,/Users/jacquesthibodeau/Zotero/storage/M2ASUJMQ/548520a.html,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3SF48G8T,journalArticle,2020,"Mohamed, Shakir; Png, Marie-Therese; Isaac, William",Decolonial AI: Decolonial Theory as Sociotechnical Foresight in Artificial Intelligence,Philosophy & Technology,,"2210-5433, 2210-5441",10.1007/s13347-020-00405-8,http://arxiv.org/abs/2007.04068,"This paper explores the important role of critical science, and in particular of post-colonial and decolonial theories, in understanding and shaping the ongoing advances in artificial intelligence. Artificial Intelligence (AI) is viewed as amongst the technological advances that will reshape modern societies and their relations. Whilst the design and deployment of systems that continually adapt holds the promise of far-reaching positive change, they simultaneously pose significant risks, especially to already vulnerable peoples. Values and power are central to this discussion. Decolonial theories use historical hindsight to explain patterns of power that shape our intellectual, political, economic, and social world. By embedding a decolonial critical approach within its technical practice, AI communities can develop foresight and tactics that can better align research and technology development with established ethical principles, centring vulnerable peoples who continue to bear the brunt of negative impacts of innovation and scientific progress. We highlight problematic applications that are instances of coloniality, and using a decolonial lens, submit three tactics that can form a decolonial field of artificial intelligence: creating a critical technical practice of AI, seeking reverse tutelage and reverse pedagogies, and the renewal of affective and political communities. The years ahead will usher in a wave of new scientific breakthroughs and technologies driven by AI research, making it incumbent upon AI communities to strengthen the social contract through ethical foresight and the multiplicity of intellectual perspectives available to us; ultimately supporting future technologies that enable greater well-being, with the goal of beneficence and justice for all.",2020-12,2022-01-30 04:52:38,2022-01-30 04:52:38,2020-12-12 15:23:49,659-684,,4,33,,Philos. Technol.,Decolonial AI,,,,,,,,,,,,arXiv.org,,ZSCC: 0000014  arXiv: 2007.04068,,/Users/jacquesthibodeau/Zotero/storage/H43MBXZN/Mohamed et al. - 2020 - Decolonial AI Decolonial Theory as Sociotechnical.pdf; /Users/jacquesthibodeau/Zotero/storage/BUXA7WXT/2007.html,,MetaSafety; DeepMind; AmbiguosSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6IHS4DSU,journalArticle,2021,"Cohen, Michael K.; Hutter, Marcus",Curiosity Killed the Cat and the Asymptotically Optimal Agent,IEEE Journal on Selected Areas in Information Theory,,,,http://arxiv.org/abs/2006.03357,"Reinforcement learners are agents that learn to pick actions that lead to high reward. Ideally, the value of a reinforcement learner's policy approaches optimality--where the optimal informed policy is the one which maximizes reward. Unfortunately, we show that if an agent is guaranteed to be ""asymptotically optimal"" in any (stochastically computable) environment, then subject to an assumption about the true environment, this agent will be either destroyed or incapacitated with probability 1; both of these are forms of traps as understood in the Markov Decision Process literature. Environments with traps pose a well-known problem for agents, but we are unaware of other work which shows that traps are not only a risk, but a certainty, for agents of a certain caliber. Much work in reinforcement learning uses an ergodicity assumption to avoid this problem. Often, doing theoretical research under simplifying assumptions prepares us to provide practical solutions even in the absence of those assumptions, but the ergodicity assumption in reinforcement learning may have led us entirely astray in preparing safe and effective exploration strategies for agents in dangerous environments. Rather than assuming away the problem, we present an agent with the modest guarantee of approaching the performance of a mentor, doing safe exploration instead of reckless exploration.",2021-05-14,2022-01-30 04:52:38,2022-01-30 04:52:38,2020-09-05 17:28:03,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000000  arXiv: 2006.03357,,/Users/jacquesthibodeau/Zotero/storage/3TP7IVVC/Cohen and Hutter - 2020 - Curiosity Killed the Cat and the Asymptotically Op.pdf; /Users/jacquesthibodeau/Zotero/storage/ANRSI6HU/2006.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; I.2.0; I.2.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XC5KJ9EU,journalArticle,2020,"Gabriel, Iason","Artificial Intelligence, Values and Alignment",Minds and Machines,,,,http://arxiv.org/abs/2001.09768,"This paper looks at philosophical questions that arise in the context of AI alignment. It defends three propositions. First, normative and technical aspects of the AI alignment problem are interrelated, creating space for productive engagement between people working in both domains. Second, it is important to be clear about the goal of alignment. There are significant differences between AI that aligns with instructions, intentions, revealed preferences, ideal preferences, interests and values. A principle-based approach to AI alignment, which combines these elements in a systematic way, has considerable advantages in this context. Third, the central challenge for theorists is not to identify 'true' moral principles for AI; rather, it is to identify fair principles for alignment, that receive reflective endorsement despite widespread variation in people's moral beliefs. The final part of the paper explores three ways in which fair principles for AI alignment could potentially be identified.",2020-01-13,2022-01-30 04:52:37,2022-01-30 04:52:37,2020-08-18 21:26:03,,,,30,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000062  arXiv: 2001.09768,,"/Users/jacquesthibodeau/Zotero/storage/2GPMB6EI/Gabriel - 2020 - Artificial Intelligence, Values and Alignment.pdf; /Users/jacquesthibodeau/Zotero/storage/M2KN5UVP/2001.html",,TechSafety; DeepMind,Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7947JHHW,journalArticle,2017,"Sotala, Kaj; Gloor, Lukas",Superintelligence As a Cause or Cure For Risks of Astronomical Suffering,Informatica,,1854-3871,,http://www.informatica.si/index.php/informatica/article/view/1877,"Discussions about the possible consequences of creating superintelligence have included the possibility of existential risk , often understood mainly as the risk of human extinction. We argue that suffering risks (s-risks) , where an adverse outcome would bring about severe suffering on an astronomical scale, are risks of a comparable severity and probability as risks of extinction. Preventing them is the common interest of many different value systems. Furthermore, we argue that in the same way as superintelligent AI both contributes to existential risk but can also help prevent it, superintelligent AI can both be a suffering risk or help avoid it. Some types of work aimed at making superintelligent AI safe will also help prevent suffering risks, and there may also be a class of safeguards for AI that helps specifically against s-risks.",2017-12-27,2022-01-30 04:51:37,2022-01-30 04:51:37,2019-12-16 03:31:47,,,4,41,,,,,,,,,,en,"I assign to  Informatica ,  An International Journal of Computing and Informatics  (""Journal"") the copyright in the manuscript identified above and any additional material (figures, tables, illustrations, software or other information intended for publication) submitted as part of or as a supplement to the manuscript (""Paper"") in all forms and media throughout the world, in all languages, for the full term of copyright, effective when and if the article is accepted for publication. This transfer includes the right to reproduce and/or to distribute the Paper to other journals or digital libraries in electronic and online forms and systems.  I understand that I retain the rights to use the pre-prints, off-prints, accepted manuscript and published journal Paper for personal use, scholarly purposes and internal institutional use.  In certain cases, I can ask for retaining the publishing rights of the Paper. The Journal can permit or deny the request for publishing rights, to which I fully agree.  I declare that the submitted Paper is original, has been written by the stated authors and has not been published elsewhere nor is currently being considered for publication by any other journal and will not be submitted for such review while under review by this Journal.  The Paper contains no material that violates proprietary rights of any other person or entity. I have obtained written permission from copyright owners for any excerpts from copyrighted works that are included and have credited the sources in my article. I have informed the co-author(s) of the terms of this publishing agreement.           Copyright ©  Slovenian Society Informatika",,,,www.informatica.si,,ZSCC: 0000029,,/Users/jacquesthibodeau/Zotero/storage/GV5IEVSF/Sotala and Gloor - 2017 - Superintelligence As a Cause or Cure For Risks of .pdf; /Users/jacquesthibodeau/Zotero/storage/SV2BI2PQ/Sotala and Gloor - 2017 - Superintelligence As a Cause or Cure For Risks of .pdf; /Users/jacquesthibodeau/Zotero/storage/5A4BQUP4/1877.html; /Users/jacquesthibodeau/Zotero/storage/UXP4MDUQ/1877.html,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MRIMD86J,journalArticle,,"MacAskill, William; Vallinder, Aron; Shulman, Carl; Österheld, Caspar; Treutlein, Johannes",The Evidentialist’s Wager,Journal of Philosophy,,,,,"Suppose that an altruistic and morally motivated agent who is uncertain between evidential decision theory (EDT) and causal decision theory (CDT) finds herself in a situation in which the two theories give conflicting verdicts. We argue that even if she has significantly higher credence in CDT, she should nevertheless act in accordance with EDT. First, we claim that that the appropriate response to normative uncertainty is to hedge one’s bets. That is, if the stakes are much higher on one theory than another, and the credences you assign to each of these theories aren’t very different, then it’s appropriate to choose the option which performs best on the high-stakes theory. Second, we show that, given the assumption of altruism, the existence of correlated decision-makers will increase the stakes for EDT but leave the stakes for CDT unaffected. Together these two claims imply that whenever there are sufficiently many correlated agents, the appropriate response is to act in accordance with EDT.",forthcoming,2022-01-30 04:51:37,2022-01-30 04:51:37,,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000000[s1],,/Users/jacquesthibodeau/Zotero/storage/VQ2I8RXI/MacAskill et al. - The Evidentialist’s Wager.pdf,,CLR; TechSafety; AmbiguosSafety; GPI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FVCIV793,journalArticle,2019,"Oesterheld, Caspar",Robust program equilibrium,Theory and Decision,,,,,,2019,2022-01-30 04:51:36,2022-01-30 04:51:36,,143–159,,1,86,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000005  Publisher: Springer,,/Users/jacquesthibodeau/Zotero/storage/V78JH7AD/s11238-018-9679-3.html,,CLR; TechSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
82EKFW97,journalArticle,2020,"Levinstein, Benjamin A.; Soares, Nate; Journal of Philosophy Inc.",Cheating Death in Damascus,The Journal of Philosophy,,0022-362X,10.5840/jphil2020117516,http://www.pdcnet.org/oom/service?url_ver=Z39.88-2004&rft_val_fmt=&rft.imuse_id=jphil_2020_0117_0005_0237_0266&svc_id=info:www.pdcnet.org/collection,"Evidential and Causal Decision Theory are the leading contenders as theories of rational action, but both face fatal counterexamples. We present some new counterexamples, including one in which the optimal action is causally dominated. We also present a novel decision theory, Functional Decision Theory (fdt), which simultaneously solves both sets of counterexamples. Instead of considering which physical action of theirs would give rise to the best outcomes, fdt agents consider which output of their decision function would give rise to the best outcome. This theory relies on a notion of subjunctive dependence, where multiple implementations of the same mathematical function are considered (even counterfactually) to have identical results for logical rather than causal reasons. Taking these subjunctive dependencies into account allows fdt agents to outperform cdt and edt agents in, e.g., the presence of accurate predictors. While not necessary for considering classic decision theory problems, we note that a full speciﬁcation of fdt will require a non-trivial theory of logical counterfactuals and algorithmic similarity.",2020,2022-01-30 04:56:47,2022-01-30 04:56:47,2020-12-13 21:01:53,237-266,,5,117,,,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000011[s0],,/Users/jacquesthibodeau/Zotero/storage/PP23HCQJ/Levinstein et al. - 2020 - Cheating Death in Damascus.pdf,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GG84SPVN,journalArticle,2020,"Taylor, Jessica; Yudkowsky, Eliezer; LaVictoire, Patrick; Critch, Andrew; Taylor, Jessica; Yudkowsky, Eliezer; LaVictoire, Patrick; Critch, Andrew",Alignment for Advanced Machine Learning Systems,Ethics of Artificial Intelligence,,,10.1093/oso/9780190905033.003.0013,https://oxford.universitypressscholarship.com/view/10.1093/oso/9780190905033.001.0001/oso-9780190905033-chapter-13,"We survey eight research areas organized around one question: As learning systems become increasingly intelligent and autonomous, what design principles can best ensure that their behavior is aligned with the interests of the operators? We focus on two major technical obstacles to AI alignment: the challenge of specifying the right kind of objective functions, and the challenge of designing AI systems that avoid unintended consequences and undesirable behavior even in cases where the objective function does not line up perfectly with the intentions of the designers.",2020-09-17,2022-01-30 04:56:46,2022-01-30 04:56:46,2020-12-13 19:28:53,342-382,,,,,,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s0]  ZSCC: NoCitationData[s1]  ACC: 73,,/Users/jacquesthibodeau/Zotero/storage/WQNGUWMV/Taylor et al. - 2020 - Alignment for Advanced Machine Learning Systems.pdf,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
X22NB2NG,journalArticle,2019,"Critch, Andrew","A Parametric, Resource-Bounded Generalization Of Löb’s Theorem, And A Robust Cooperation Criterion For Open-Source Game Theory",The Journal of Symbolic Logic,,"0022-4812, 1943-5886",10.1017/jsl.2017.42,https://www.cambridge.org/core/product/identifier/S0022481217000421/type/journal_article,"Abstract             This article presents two theorems: (1) a generalization of Löb’s Theorem that applies to formal proof systems operating with bounded computational resources, such as formal verification software or theorem provers, and (2) a theorem on the robust cooperation of agents that employ proofs about one another’s source code as unexploitable criteria for cooperation. The latter illustrates a capacity for outperforming classical Nash equilibria and correlated equilibria, attaining mutually cooperative program equilibrium in the Prisoner’s Dilemma while remaining unexploitable, i.e., sometimes achieving the outcome (Cooperate, Cooperate), and never receiving the outcome (Cooperate, Defect) as player 1.",2019-12,2022-01-30 04:56:46,2022-01-30 04:56:46,2020-11-22 05:00:04,1368-1381,,4,84,,J. symb. log.,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s2]  ACC: 6,,,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9D7MHQDT,journalArticle,2019,"Trammell, Philip",Fixed-point solutions to the regress problem in normative uncertainty,Synthese,,1573-0964,10.1007/s11229-019-02098-9,https://doi.org/10.1007/s11229-019-02098-9,"When we are faced with a choice among acts, but are uncertain about the true state of the world, we may be uncertain about the acts’ “choiceworthiness”. Decision theories guide our choice by making normative claims about how we should respond to this uncertainty. If we are unsure which decision theory is correct, however, we may remain unsure of what we ought to do. Given this decision-theoretic uncertainty, meta-theories attempt to resolve the conflicts between our decision theories...but we may be unsure which meta-theory is correct as well. This reasoning can launch a regress of ever-higher-order uncertainty, which may leave one forever uncertain about what one ought to do. There is, fortunately, a class of circumstances under which this regress is not a problem. If one holds a cardinal understanding of subjective choiceworthiness, and accepts certain other criteria (which are too weak to specify any particular decision theory), one’s hierarchy of metanormative uncertainty ultimately converges to precise definitions of “subjective choiceworthiness” for any finite set of acts. If one allows the metanormative regress to extend to the transfinite ordinals, the convergence criteria can be weakened further. Finally, the structure of these results applies straightforwardly not just to decision-theoretic uncertainty, but also to other varieties of normative uncertainty, such as moral uncertainty.",2019-02-14,2022-01-30 04:55:29,2022-01-30 04:55:29,2020-12-13 23:49:11,,,,,,Synthese,,,,,,,,en,,,,,Springer Link,,ZSCC: 0000004,,/Users/jacquesthibodeau/Zotero/storage/QH6HTFAV/Trammell - 2019 - Fixed-point solutions to the regress problem in no.pdf,,MetaSafety; GPI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CD8ANBSK,journalArticle,2017,"Baum, Seth D.","The Social Science of Computerized Brains – Review of The Age of Em: Work, Love, and Life When Robots Rule the Earth by Robin Hanson (Oxford University Press, 2016)",Futures,,163287,10.1016/j.futures.2017.03.005,https://linkinghub.elsevier.com/retrieve/pii/S0016328716302518,,2017-06,2022-01-30 04:55:20,2022-01-30 04:55:20,2019-12-16 02:52:35,61-63,,,90,,Futures,The Social Science of Computerized Brains – Review of The Age of Em,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000000,,,,MetaSafety; GCRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
X38XZZ28,journalArticle,2014,"Baum, Seth D",The great downside dilemma for risky emerging technologies,Physica Scripta,,"0031-8949, 1402-4896",10.1088/0031-8949/89/12/128004,http://stacks.iop.org/1402-4896/89/i=12/a=128004?key=crossref.f5938bc78a3023d740968f020cfa9970,,2014-12-01,2022-01-30 04:55:20,2022-01-30 04:55:20,2019-12-16 02:49:17,128004,,12,89,,Phys. Scr.,,,,,,,,,,,,,DOI.org (Crossref),,ZSCC: 0000025,,/Users/jacquesthibodeau/Zotero/storage/4B6SN7S9/Baum - 2014 - The great downside dilemma for risky emerging tech.pdf,,MetaSafety; GCRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4NNMI86Z,journalArticle,2015,"Baum, Seth D.",The far future argument for confronting catastrophic threats to humanity: Practical significance and alternatives,Futures,,163287,10.1016/j.futures.2015.03.001,https://linkinghub.elsevier.com/retrieve/pii/S0016328715000312,,2015-09,2022-01-30 04:55:20,2022-01-30 04:55:20,2019-12-16 02:45:25,86-96,,,72,,Futures,The far future argument for confronting catastrophic threats to humanity,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000025,,,,MetaSafety; GCRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
H8KFKUZ2,journalArticle,2013,"Baum, Seth D.; Wilson, Grant S.",The Ethics of Global Catastrophic Risk from Dual-Use Bioengineering,"Ethics in Biology, Engineering and Medicine",,2151-805X,10.1615/EthicsBiologyEngMed.2013007629,"http://www.dl.begellhouse.com/journals/6ed509641f7324e6,709fef245eef4861,06d520d747a5c0d1.html",,2013,2022-01-30 04:55:20,2022-01-30 04:55:20,2019-12-16 02:49:31,59-72,,1,4,,Ethics Biology Eng Med,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000007,,,,MetaSafety; GCRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WFRENGDG,journalArticle,2018,"Baum, Seth D.",Reconciliation between factions focused on near-term and long-term artificial intelligence,AI & Society,,,,,,2018,2022-01-30 04:55:20,2022-01-30 04:55:20,,565–572,,4,33,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000034,,/Users/jacquesthibodeau/Zotero/storage/Q3E37524/Baum - 2018 - Reconciliation between factions focused on near-te.pdf; /Users/jacquesthibodeau/Zotero/storage/VRCUNE3P/s00146-017-0734-3.html,,MetaSafety; GCRI,Artificial Intelligence; Artificial General Intelligence; Artificial Superintelligence; Long-Term Artificial Intelligence; Near-Term Artificial Intelligence; Societal Impacts of Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RP8S2RZK,journalArticle,2020,"Baum, Seth D.",Quantifying the probability of existential catastrophe: A reply to Beard et al.,Futures,,0016-3287,10.1016/j.futures.2020.102608,http://www.sciencedirect.com/science/article/pii/S0016328720300987,"A recent article by Beard, Rowe, and Fox (BRF) evaluates ten methodologies for quantifying the probability of existential catastrophe. This article builds on BRF’s valuable contribution. First, this article describes the conceptual and mathematical relationship between the probability of existential catastrophe and the severity of events that could result in existential catastrophe. It discusses complications in this relationship arising from catastrophes occurring at different speeds and from multiple concurrent catastrophes. Second, this article revisits the ten BRF methodologies, finding an inverse relationship between a methodology’s ease of use and the quality of results it produces—in other words, achieving a higher quality of analysis will in general require a larger investment in analysis. Third, the manuscript discusses the role of probability quantification in the management of existential risks, describing why the probability is only sometimes needed for decision-making and arguing that analyses should support real-world risk management decisions and not just be academic exercises. If the findings of this article are taken into account, together with BRF’s evaluations of specific methodologies, then risk analyses of existential catastrophe may tend to be more successful at understanding and reducing the risks.",2020-10-01,2022-01-30 04:55:20,2022-01-30 04:55:20,2020-12-19 03:08:57,102608,,,123,,Futures,Quantifying the probability of existential catastrophe,,,,,,,en,,,,,ScienceDirect,,ZSCC: 0000002,,/Users/jacquesthibodeau/Zotero/storage/TFM6GRAD/S0016328720300987.html,,MetaSafety; AmbiguosSafety; GCRI,Existential risk; Global catastrophic risk; Probability; Risk analysis; Severity,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
58XWHNKX,journalArticle,2019,"Baum, Seth D.",Preparing for the unthinkable,Science,,"0036-8075, 1095-9203",10.1126/science.aay4219,http://www.sciencemag.org/lookup/doi/10.1126/science.aay4219,,2019-09-20,2022-01-30 04:55:20,2022-01-30 04:55:20,2019-12-16 02:51:12,1254-1254,,6459,365,,Science,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000000,,,,MetaSafety; GCRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PIJHCRX3,journalArticle,2017,"Baum, Seth D.",On the promotion of safe and socially beneficial artificial intelligence,AI & Society,,"0951-5666, 1435-5655",10.1007/s00146-016-0677-0,http://link.springer.com/10.1007/s00146-016-0677-0,,2017-11,2022-01-30 04:55:20,2022-01-30 04:55:20,2019-12-16 02:44:37,543-551,,4,32,,AI & Soc,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000080,,/Users/jacquesthibodeau/Zotero/storage/ZASDEGNA/papers.html,,MetaSafety; GCRI,artificial intelligence; artificial intelligence safety; beneficial artificial intelligence; social psychology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JA98S3JN,journalArticle,2017,"Barrett, Anthony Michael",Value of Global Catastrophic Risk (GCR) Information: Cost-Effectiveness-Based Approach for GCR Reduction,Decision Analysis,,"1545-8490, 1545-8504",10.1287/deca.2017.0350,http://pubsonline.informs.org/doi/10.1287/deca.2017.0350,,2017-09,2022-01-30 04:55:20,2022-01-30 04:55:20,2019-12-16 02:44:44,187-203,,3,14,,Decision Analysis,Value of Global Catastrophic Risk (GCR) Information,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000009,,/Users/jacquesthibodeau/Zotero/storage/26R7JATJ/Barrett - 2017 - Value of Global Catastrophic Risk (GCR) Informatio.pdf,,MetaSafety; GCRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DS8HKV3N,journalArticle,2018,"Baum, Seth",Superintelligence skepticism as a political tool,Information,,,,,,2018,2022-01-30 04:55:20,2022-01-30 04:55:20,,209,,9,9,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000019,,/Users/jacquesthibodeau/Zotero/storage/GTRGDFM2/Baum - 2018 - Superintelligence skepticism as a political tool.pdf; /Users/jacquesthibodeau/Zotero/storage/NRXVDB2J/Baum - 2018 - Superintelligence Skepticism as a Political Tool.pdf; /Users/jacquesthibodeau/Zotero/storage/IVKG4C2A/209.html,,MetaSafety; GCRI,artificial intelligence; skepticism; superintelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6VUTUU52,journalArticle,2020,"Baum, Seth D.",Social choice ethics in artificial intelligence,AI & Society,,"0951-5666, 1435-5655",10.1007/s00146-017-0760-1,http://link.springer.com/10.1007/s00146-017-0760-1,"A major approach to the ethics of artificial intelligence (AI) is to use social choice, in which the AI is designed to act according to the aggregate views of society. This is found in the AI ethics of “coherent extrapolated volition” and “bottom-up ethics”. This paper shows that the normative basis of AI social choice ethics is weak due to the fact that there is no one single aggregate ethical view of society. Instead, the design of social choice AI faces three sets of decisions: standing, concerning whose ethics views are included; measurement, concerning how their views are identified; and aggregation, concerning how individual views are combined to a single view that will guide AI behavior. These decisions must be made up front in the initial AI design—designers cannot “let the AI figure it out”. Each set of decisions poses difficult ethical dilemmas with major consequences for AI behavior, with some decision options yielding pathological or even catastrophic results. Furthermore, non-social choice ethics face similar issues, such as whether to count future generations or the AI itself. These issues can be more important than the question of whether or not to use social choice ethics. Attention should focus on these issues, not on social choice.",2020-03-01,2022-01-30 04:55:20,2022-01-30 04:55:20,2020-08-20 20:16:41,165-176,,1,35,,AI & Soc,,,,,,,,en,,,,,Springer Link,,ZSCC: 0000060,,/Users/jacquesthibodeau/Zotero/storage/5EKPE8IC/Baum - 2017 - Social Choice Ethics in Artificial Intelligence.pdf; /Users/jacquesthibodeau/Zotero/storage/NNB6BD38/Baum - 2020 - Social choice ethics in artificial intelligence.pdf; /Users/jacquesthibodeau/Zotero/storage/JZWAP722/papers.html,,MetaSafety; AmbiguosSafety; GCRI,artificial intelligence; bottom-up ethics; coherent extrapolated volition; ethics; social choice,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XC4B3IJS,journalArticle,2021,"Owe, Andrea; Baum, Seth D.",Moral consideration of nonhumans in the ethics of artificial intelligence,AI and Ethics,,"2730-5953, 2730-5961",10.1007/s43681-021-00065-0,https://link.springer.com/10.1007/s43681-021-00065-0,,2021-11,2022-01-30 04:55:20,2022-01-30 04:55:20,2021-10-31 19:21:16,517-528,,4,1,,AI Ethics,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000007,,/Users/jacquesthibodeau/Zotero/storage/BKTUR9DN/Owe and Baum - 2021 - Moral consideration of nonhumans in the ethics of .pdf,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AEG34VHF,journalArticle,2013,"Wilson, Grant",Minimizing global catastrophic and existential risks from emerging technologies through international law,Va. Envtl. LJ,,,,,,2013,2022-01-30 04:55:19,2022-01-30 04:55:19,,307,,,31,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000049,,/Users/jacquesthibodeau/Zotero/storage/8UD9P888/LandingPage.html,,MetaSafety; GCRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4UIDG8GK,journalArticle,2020,"Baum, Seth D.",Medium-Term Artificial Intelligence and Society,Information,,,10.3390/info11060290,https://www.mdpi.com/2078-2489/11/6/290,"There has been extensive attention to near-term and long-term AI technology and its accompanying societal issues, but the medium-term has gone largely overlooked. This paper develops the concept of medium-term AI, evaluates its importance, and analyzes some medium-term societal issues. Medium-term AI can be important in its own right and as a topic that can bridge the sometimes acrimonious divide between those who favor attention to near-term AI and those who prefer the long-term. The paper proposes the medium-term AI hypothesis: the medium-term is important from the perspectives of those who favor attention to near-term AI as well as those who favor attention to long-term AI. The paper analyzes medium-term AI in terms of governance institutions, collective action, corporate AI development, and military/national security communities. Across portions of these four areas, some support for the medium-term AI hypothesis is found, though in some cases the matter is unclear.",2020-06,2022-01-30 04:55:19,2022-01-30 04:55:19,2020-08-20 20:15:47,290,,6,11,,,,,,,,,,en,http://creativecommons.org/licenses/by/3.0/,,,,www.mdpi.com,,ZSCC: 0000009  Number: 6 Publisher: Multidisciplinary Digital Publishing Institute,,/Users/jacquesthibodeau/Zotero/storage/DCX936S5/Baum - 2020 - Medium-Term Artificial Intelligence and Society.pdf; /Users/jacquesthibodeau/Zotero/storage/T5QSRRBW/290.html,,MetaSafety; GCRI,intermediate-term AI; long-term AI; medium-term AI; mid-term AI; near-term AI; societal implications of AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9D355IDI,journalArticle,2017,"White, Trevor N.; Baum, Seth D.",Liability For Present And Future Robotics Technology,Robot Ethics 2.0: From Autonomous Cars to Artificial Intelligence,,,,,,2017,2022-01-30 04:55:19,2022-01-30 04:55:19,,5,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000009,,/Users/jacquesthibodeau/Zotero/storage/M4J3T437/books.html,,MetaSafety; GCRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
B8AWD78V,journalArticle,2021,"de Neufville, Robert; Baum, Seth D.",Collective action on artificial intelligence: A primer and review,Technology in Society,,0160791X,10.1016/j.techsoc.2021.101649,https://linkinghub.elsevier.com/retrieve/pii/S0160791X2100124X,,2021-08,2022-01-30 04:55:19,2022-01-30 04:55:19,2021-10-31 19:22:17,101649,,,66,,Technology in Society,Collective action on artificial intelligence,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000004,,,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JEU5BHGP,journalArticle,2021,"Owe, Andrea; Baum, Seth",Artificial Intelligence Needs Environmental Ethics,"Ethics, Policy, and Environment",,,,,,2021-11-14,2022-01-30 04:55:19,2022-01-30 04:55:19,,4,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/SXDZ5JXB/Baum - Artificial Intelligence Needs Environmental Ethics.pdf,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2H4QGF9B,journalArticle,2017,"Baum, Seth; Barrett, Anthony; Yampolskiy, Roman V.",Modeling and interpreting expert disagreement about artificial superintelligence,Informatica,,,,,,2017,2022-01-30 04:55:19,2022-01-30 04:55:19,,419–428,,7,41,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000018,,/Users/jacquesthibodeau/Zotero/storage/KGZTRH7I/Baum et al. - 2017 - Modeling and interpreting expert disagreement abou.pdf; /Users/jacquesthibodeau/Zotero/storage/27KSX2XF/papers.html; /Users/jacquesthibodeau/Zotero/storage/8ENBDWVI/papers.html,,MetaSafety; GCRI,artificial intelligence; risk analysis; artificial superintelligence; expert disagreement,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UKKJMH24,journalArticle,2021,"Baum, Seth; Owe, Andrea",From AI for People to AI for the World and the Universe,AI & Society,,,,,,2021-11-30,2022-01-30 04:55:19,2022-01-30 04:55:19,,3,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/QXWKBC9Z/Baum - From AI for People to AI for the World and the Uni.pdf,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DQF5DVFH,journalArticle,2018,"Baum, Seth",Countering Superintelligence Misinformation,Information,,,,,,2018,2022-01-30 04:55:19,2022-01-30 04:55:19,,244,,10,9,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000014,,/Users/jacquesthibodeau/Zotero/storage/JG6I57DX/Baum - 2018 - Countering Superintelligence Misinformation.pdf; /Users/jacquesthibodeau/Zotero/storage/BDEHJSEN/Baum - 2018 - Countering Superintelligence Misinformation.pdf; /Users/jacquesthibodeau/Zotero/storage/W7VRFICJ/244.html,,MetaSafety; GCRI,artificial intelligence; superintelligence; misinformation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
W6VMXGGZ,journalArticle,2021,"Cihon, Peter; Schuett, Jonas; Baum, Seth D.",Corporate Governance of Artificial Intelligence in the Public Interest,Information,,2078-2489,10.3390/info12070275,https://www.mdpi.com/2078-2489/12/7/275,"Corporations play a major role in artificial intelligence (AI) research, development, and deployment, with profound consequences for society. This paper surveys opportunities to improve how corporations govern their AI activities so as to better advance the public interest. The paper focuses on the roles of and opportunities for a wide range of actors inside the corporation—managers, workers, and investors—and outside the corporation—corporate partners and competitors, industry consortia, nonprofit organizations, the public, the media, and governments. Whereas prior work on multistakeholder AI governance has proposed dedicated institutions to bring together diverse actors and stakeholders, this paper explores the opportunities they have even in the absence of dedicated multistakeholder institutions. The paper illustrates these opportunities with many cases, including the participation of Google in the U.S. Department of Defense Project Maven; the publication of potentially harmful AI research by OpenAI, with input from the Partnership on AI; and the sale of facial recognition technology to law enforcement by corporations including Amazon, IBM, and Microsoft. These and other cases demonstrate the wide range of mechanisms to advance AI corporate governance in the public interest, especially when diverse actors work together.",2021-07-05,2022-01-30 04:55:19,2022-01-30 04:55:19,2021-10-31 19:21:54,275,,7,12,,Information,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000002,,/Users/jacquesthibodeau/Zotero/storage/63ATIFMI/Cihon et al. - 2021 - Corporate Governance of Artificial Intelligence in.pdf,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8HC9X6CU,journalArticle,2015,"Baum, Seth D.; Tonn, Bruce E.",Confronting future catastrophic threats to humanity,Futures,,163287,10.1016/j.futures.2015.08.004,https://linkinghub.elsevier.com/retrieve/pii/S0016328715001135,,2015-09,2022-01-30 04:55:19,2022-01-30 04:55:19,2019-12-16 02:52:43,1-3,,,72,,Futures,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000012,,,,MetaSafety; GCRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PJH97DIN,journalArticle,2020,"Baum, Seth D.",Artificial Interdisciplinarity: Artificial Intelligence for Research on Complex Societal Problems,Philosophy & Technology,,"2210-5433, 2210-5441",10.1007/s13347-020-00416-5,http://link.springer.com/10.1007/s13347-020-00416-5,"This paper considers the question: In what ways can artificial intelligence assist with interdisciplinary research for addressing complex societal problems and advancing the social good? Problems such as environmental protection, public health, and emerging technology governance do not fit neatly within traditional academic disciplines and therefore require an interdisciplinary approach. However, interdisciplinary research poses large cognitive challenges for human researchers that go beyond the substantial challenges of narrow disciplinary research. The challenges include epistemic divides between disciplines, the massive bodies of relevant literature, the peer review of work that integrates an eclectic mix of topics, and the transfer of interdisciplinary research insights from one problem to another. Artificial interdisciplinarity already helps with these challenges via search engines, recommendation engines, and automated content analysis. Future “strong artificial interdisciplinarity” based on human-level artificial general intelligence could excel at interdisciplinary research, but it may take a long time to develop and could pose major safety and ethical issues. Therefore, there is an important role for intermediate-term artificial interdisciplinarity systems that could make major contributions to addressing societal problems without the concerns associated with artificial general intelligence.",2020-07-16,2022-01-30 04:55:19,2022-01-30 04:55:19,2020-08-20 20:15:09,,,,,,Philos. Technol.,Artificial Interdisciplinarity,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/89NP8X9J/Baum - 2020 - Artificial Interdisciplinarity Artificial Intelli.pdf,,MetaSafety; GCRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PVCGA76X,journalArticle,2021,"Galaz, Victor; Centeno, Miguel A.; Callahan, Peter W.; Causevic, Amar; Patterson, Thayer; Brass, Irina; Baum, Seth; Farber, Darryl; Fischer, Joern; Garcia, David; McPhearson, Timon; Jimenez, Daniel; King, Brian; Larcey, Paul; Levy, Karen","Artificial intelligence, systemic risks, and sustainability",Technology in Society,,0160791X,10.1016/j.techsoc.2021.101741,https://linkinghub.elsevier.com/retrieve/pii/S0160791X21002165,,2021-11,2022-01-30 04:55:19,2022-01-30 04:55:19,2021-10-31 19:21:00,101741,,,67,,Technology in Society,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s1]  ACC: 4,,"/Users/jacquesthibodeau/Zotero/storage/2HV437ZM/Galaz et al. - 2021 - Artificial intelligence, systemic risks, and susta.pdf",,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TH67SMSJ,journalArticle,2021,"Cihon, Peter; Kleinaltenkamp, Moritz J.; Schuett, Jonas; Baum, Seth D.",AI CERTIFICATION: Advancing Ethical Practice by Reducing Information Asymmetries,IEEE Transactions on Technology and Society,,2637-6415,10.1109/TTS.2021.3077595,https://ieeexplore.ieee.org/document/9427056/,,2021,2022-01-30 04:55:18,2022-01-30 04:55:18,2021-10-31 19:21:42,1-1,,,,,IEEE Trans. Technol. Soc.,AI CERTIFICATION,,,,,,,,,,,,DOI.org (Crossref),,ZSCC: 0000006,,/Users/jacquesthibodeau/Zotero/storage/UE94D4B7/Cihon et al. - 2021 - AI CERTIFICATION Advancing Ethical Practice by Re.pdf,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IS3J9WCB,journalArticle,2017,"Barrett, Anthony M.; Baum, Seth D.",A model of pathways to artificial superintelligence catastrophe for risk and decision analysis,Journal of Experimental & Theoretical Artificial Intelligence,,"0952-813X, 1362-3079",10.1080/0952813X.2016.1186228,https://www.tandfonline.com/doi/full/10.1080/0952813X.2016.1186228,,2017-03-04,2022-01-30 04:55:18,2022-01-30 04:55:18,2019-12-16 02:44:58,397-414,,2,29,,Journal of Experimental & Theoretical Artificial Intelligence,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000044,,/Users/jacquesthibodeau/Zotero/storage/JFIRRI9N/Barrett and Baum - 2017 - A model of pathways to artificial superintelligenc.pdf,,MetaSafety; GCRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AIE4CQPT,journalArticle,2017,"Jilk, David J.",Conceptual-Linguistic Superintelligence,Informatica,,1854-3871,,http://www.informatica.si/index.php/informatica/article/view/1875,"We argue that artificial intelligence capable of sustaining an uncontrolled intelligence explosion must have a conceptual-linguistic faculty with substantial functional similarity to the human faculty. We then argue for three subsidiary claims: first, that detecting the presence of such a faculty will be an important indicator of imminent superintelligence; second, that such a superintelligence will, in creating further increases in intelligence, both face and consider the same sorts of existential risks that humans face today; third, that such a superintelligence is likely to assess and question its own values, purposes, and drives.",2017-12-27,2022-01-30 04:59:45,2022-01-30 04:59:45,2020-12-13 23:11:36,,,4,41,,,,,,,,,,en,"I assign to  Informatica ,  An International Journal of Computing and Informatics  (""Journal"") the copyright in the manuscript identified above and any additional material (figures, tables, illustrations, software or other information intended for publication) submitted as part of or as a supplement to the manuscript (""Paper"") in all forms and media throughout the world, in all languages, for the full term of copyright, effective when and if the article is accepted for publication. This transfer includes the right to reproduce and/or to distribute the Paper to other journals or digital libraries in electronic and online forms and systems.  I understand that I retain the rights to use the pre-prints, off-prints, accepted manuscript and published journal Paper for personal use, scholarly purposes and internal institutional use.  In certain cases, I can ask for retaining the publishing rights of the Paper. The Journal can permit or deny the request for publishing rights, to which I fully agree.  I declare that the submitted Paper is original, has been written by the stated authors and has not been published elsewhere nor is currently being considered for publication by any other journal and will not be submitted for such review while under review by this Journal.  The Paper contains no material that violates proprietary rights of any other person or entity. I have obtained written permission from copyright owners for any excerpts from copyrighted works that are included and have credited the sources in my article. I have informed the co-author(s) of the terms of this publishing agreement.           Copyright ©  Slovenian Society Informatika",,,,www.informatica.si,,ZSCC: 0000006  Number: 4,,/Users/jacquesthibodeau/Zotero/storage/QRMR33VP/Jilk - 2017 - Conceptual-Linguistic Superintelligence.pdf; /Users/jacquesthibodeau/Zotero/storage/KJZ6SVUS/1875.html; /Users/jacquesthibodeau/Zotero/storage/DQWGAKQA/1875.html,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BH567DIV,journalArticle,2014,"Halpern, Joseph Y.; Pass, Rafael; Seeman, Lior",Decision Theory with Resource-Bounded Agents,Topics in Cognitive Science,,17568757,10.1111/tops.12088,http://doi.wiley.com/10.1111/tops.12088,,2014-04,2022-01-30 04:59:45,2022-01-30 04:59:45,2020-11-22 01:47:43,245-257,,2,6,,Top Cogn Sci,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000026,,/Users/jacquesthibodeau/Zotero/storage/C5BB6KK7/Halpern et al. - 2014 - Decision Theory with Resource-Bounded Agents.pdf,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6G5389AS,journalArticle,2006,"McLaren, B.M.","Computational Models of Ethical Reasoning: Challenges, Initial Steps, and Future Directions",IEEE Intelligent Systems,,1541-1672,10.1109/MIS.2006.67,http://ieeexplore.ieee.org/document/1667950/,,2006-07,2022-01-30 04:59:44,2022-01-30 04:59:44,2020-11-22 02:23:04,29-37,,4,21,,IEEE Intell. Syst.,Computational Models of Ethical Reasoning,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000103,,/Users/jacquesthibodeau/Zotero/storage/C22P6XNW/McLaren - 2006 - Computational Models of Ethical Reasoning Challen.pdf,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
39KNIC5R,journalArticle,2020,"Turchin, Alexey; Denkenberger, David",Classification of global catastrophic risks connected with artificial intelligence,AI & Society,,0951-5666,10.1007/s00146-018-0845-5,https://link.springer.com/epdf/10.1007/s00146-018-0845-5,"A classification of the global catastrophic risks of AI is presented, along with a comprehensive list of previously identified risks. This classification allows the identification of several new risks. We show that at each level of AI’s intelligence power, separate types of possible catastrophes dominate. Our classification demonstrates that the field of AI risks is diverse, and includes many scenarios beyond the commonly discussed cases of a paperclip maximizer or robot-caused unemployment. Global catastrophic failure could happen at various levels of AI development, namely, (1) before it starts self-improvement, (2) during its takeoff, when it uses various instruments to escape its initial confinement, or (3) after it successfully takes over the world and starts to implement its goal system, which could be plainly unaligned, or feature-flawed friendliness. AI could also halt at later stages of its development either due to technical glitches or ontological problems. Overall, we identified around several dozen scenarios of AI-driven global catastrophe. The extent of this list illustrates that there is no one simple solution to the problem of AI safety, and that AI safety theory is complex and must be customized for each AI development level.",2020,2022-01-30 04:59:44,2022-01-30 04:59:44,2020-11-14 00:32:51,,,1,35,,,,,,,,,,en,,,,,www.readcube.com,,ZSCC: 0000071,,/Users/jacquesthibodeau/Zotero/storage/8EJZTBNT/Turchin and Denkenberger - 2020 - Classification of global catastrophic risks connec.pdf,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ADDSIC2J,journalArticle,2019,"Cohen, Jeremy M.; Rosenfeld, Elan; Kolter, J. Zico",Certified Adversarial Robustness via Randomized Smoothing,"arXiv:1902.02918 [cs, stat]",,,,http://arxiv.org/abs/1902.02918,"We show how to turn any classiﬁer that classiﬁes well under Gaussian noise into a new classiﬁer that is certiﬁably robust to adversarial perturbations under the 2 norm. This “randomized smoothing” technique has been proposed recently in the literature, but existing guarantees are loose. We prove a tight robustness guarantee in 2 norm for smoothing with Gaussian noise. We use randomized smoothing to obtain an ImageNet classiﬁer with e.g. a certiﬁed top-1 accuracy of 49% under adversarial perturbations with 2 norm less than 0.5 (=127/255). No certiﬁed defense has been shown feasible on ImageNet except for smoothing. On smaller-scale datasets where competing approaches to certiﬁed 2 robustness are viable, smoothing delivers higher certiﬁed accuracies. Our strong empirical results suggest that randomized smoothing is a promising direction for future research into adversarially robust classiﬁcation. Code and models are available at http: //github.com/locuslab/smoothing.",2019-06-15,2022-01-30 04:59:44,2022-01-30 04:59:44,2020-12-22 23:38:26,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000716  arXiv: 1902.02918,,/Users/jacquesthibodeau/Zotero/storage/D7VWNG9N/Cohen et al. - 2019 - Certified Adversarial Robustness via Randomized Sm.pdf,,TechSafety; AmbiguosSafety; Other-org,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IK622CNH,journalArticle,2012,"Hutter, Marcus",Can Intelligence Explode?,Journal of Consciousness Studies,,,,http://arxiv.org/abs/1202.6177,"The technological singularity refers to a hypothetical scenario in which technological advances virtually explode. The most popular scenario is the creation of super-intelligent algorithms that recursively create ever higher intelligences. It took many decades for these ideas to spread from science fiction to popular science magazines and finally to attract the attention of serious philosophers. David Chalmers' (JCS 2010) article is the first comprehensive philosophical analysis of the singularity in a respected philosophy journal. The motivation of my article is to augment Chalmers' and to discuss some issues not addressed by him, in particular what it could mean for intelligence to explode. In this course, I will (have to) provide a more careful treatment of what intelligence actually is, separate speed from intelligence explosion, compare what super-intelligent participants and classical human observers might experience and do, discuss immediate implications for the diversity and value of life, consider possible bounds on intelligence, and contemplate intelligences right at the singularity.",2012-02-28,2022-01-30 04:59:43,2022-01-30 04:59:43,2020-11-22 04:16:06,,,,19,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000041  arXiv: 1202.6177,,/Users/jacquesthibodeau/Zotero/storage/PGN32NHD/Hutter - 2012 - Can Intelligence Explode.pdf; /Users/jacquesthibodeau/Zotero/storage/GXCCDRVV/1202.html,,TechSafety; Other-org,Computer Science - Artificial Intelligence; Physics - Physics and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9334F5K2,journalArticle,2020,"Fischer, Ian; Alemi, Alexander A.",CEB Improves Model Robustness,Entropy,,,,http://arxiv.org/abs/2002.05380,"We demonstrate that the Conditional Entropy Bottleneck (CEB) can improve model robustness. CEB is an easy strategy to implement and works in tandem with data augmentation procedures. We report results of a large scale adversarial robustness study on CIFAR-10, as well as the ImageNet-C Common Corruptions Benchmark, ImageNet-A, and PGD attacks.",2020-02-13,2022-01-30 04:59:43,2022-01-30 04:59:43,2020-09-05 18:46:13,,,10,22,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000015  arXiv: 2002.05380,,/Users/jacquesthibodeau/Zotero/storage/JSUK3W5B/Fischer and Alemi - 2020 - CEB Improves Model Robustness.pdf; /Users/jacquesthibodeau/Zotero/storage/HZ5HWPQR/2002.html,,TechSafety; AmbiguosSafety; Other-org,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UASN6CZQ,journalArticle,2017,"Batin, Mikhail; Turchin, Alexey; Sergey, Markov; Zhila, Alisa; Denkenberger, David",Artificial Intelligence in Life Extension: from Deep Learning to Superintelligence,Informatica,,1854-3871,,http://www.informatica.si/index.php/informatica/article/view/1797,"In this paper we focus on the most efficacious AI applications for life extension and anti-aging at three expected stages of AI development: narrow AI, AGI and superintelligence. First, we overview the existing research and commercial work performed by a select number of startups and academic projects. We find that at the current stage of “narrow” AI, the most promising areas for life extension are geroprotector-combination discovery, detection of aging biomarkers, and personalized anti-aging therapy. These advances could help currently living people reach longevity escape velocity and survive until more advanced AI appears. When AI comes close to human level, the main contribution to life extension will come from AI integration with humans through brain-computer interfaces, integrated AI assistants capable of autonomously diagnosing and treating health issues, and cyber systems embedded into human bodies. Lastly, we speculate about the more remote future, when AI reaches the level of superintelligence and such life-extension methods as uploading human minds and creating nanotechnological bodies may become possible, thus lowering the probability of human death close to zero. We conclude that medical AI based superintelligence is intrinsically safer than, say, military AI, as it may help humans to evolve into part of the future superintelligence via brain augmentation, uploading, and a network of self-improving humans. Medical AI’s value system is focused on human benefit.",2017-12-27,2022-01-30 04:59:36,2022-01-30 04:59:36,2020-12-13 22:07:54,,,4,41,,,Artificial Intelligence in Life Extension,,,,,,,en,"I assign to  Informatica ,  An International Journal of Computing and Informatics  (""Journal"") the copyright in the manuscript identified above and any additional material (figures, tables, illustrations, software or other information intended for publication) submitted as part of or as a supplement to the manuscript (""Paper"") in all forms and media throughout the world, in all languages, for the full term of copyright, effective when and if the article is accepted for publication. This transfer includes the right to reproduce and/or to distribute the Paper to other journals or digital libraries in electronic and online forms and systems.  I understand that I retain the rights to use the pre-prints, off-prints, accepted manuscript and published journal Paper for personal use, scholarly purposes and internal institutional use.  In certain cases, I can ask for retaining the publishing rights of the Paper. The Journal can permit or deny the request for publishing rights, to which I fully agree.  I declare that the submitted Paper is original, has been written by the stated authors and has not been published elsewhere nor is currently being considered for publication by any other journal and will not be submitted for such review while under review by this Journal.  The Paper contains no material that violates proprietary rights of any other person or entity. I have obtained written permission from copyright owners for any excerpts from copyrighted works that are included and have credited the sources in my article. I have informed the co-author(s) of the terms of this publishing agreement.           Copyright ©  Slovenian Society Informatika",,,,www.informatica.si,,ZSCC: 0000030  Number: 4,,/Users/jacquesthibodeau/Zotero/storage/9P7GCWS8/Batin et al. - 2017 - Artificial Intelligence in Life Extension from De.pdf; /Users/jacquesthibodeau/Zotero/storage/UB6XNVTH/1797.html; /Users/jacquesthibodeau/Zotero/storage/7VGQTWMP/1797.html,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P8ABUARU,journalArticle,2019,"Snyder-Beattie, Andrew E.; Ord, Toby; Bonsall, Michael B.",An upper bound for the background rate of human extinction,Scientific Reports,,2045-2322,10.1038/s41598-019-47540-7,https://www.nature.com/articles/s41598-019-47540-7,"We evaluate the total probability of human extinction from naturally occurring processes. Such processes include risks that are well characterized such as asteroid impacts and supervolcanic eruptions, as well as risks that remain unknown. Using only the information that Homo sapiens has existed at least 200,000 years, we conclude that the probability that humanity goes extinct from natural causes in any given year is almost guaranteed to be less than one in 14,000, and likely to be less than one in 87,000. Using the longer track record of survival for our entire genus Homo produces even tighter bounds, with an annual probability of natural extinction likely below one in 870,000. These bounds are unlikely to be affected by possible survivorship bias in the data, and are consistent with mammalian extinction rates, typical hominin species lifespans, the frequency of well-characterized risks, and the frequency of mass extinctions. No similar guarantee can be made for risks that our ancestors did not face, such as anthropogenic climate change or nuclear/biological warfare.",2019-07-30,2022-01-30 04:59:35,2022-01-30 04:59:35,2019-12-16 22:38:58,1-9,,1,9,,,,,,,,,,en,2019 The Author(s),,,,www.nature.com,,ZSCC: 0000017,,/Users/jacquesthibodeau/Zotero/storage/PM4KFW9G/Snyder-Beattie et al. - 2019 - An upper bound for the background rate of human ex.pdf; /Users/jacquesthibodeau/Zotero/storage/IMA8B8KI/Snyder-Beattie et al. - 2019 - An upper bound for the background rate of human ex.pdf; /Users/jacquesthibodeau/Zotero/storage/J3EUKR39/s41598-019-47540-7.html,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
X8AFN559,journalArticle,2020,"McIlroy-Young, Reid; Sen, Siddhartha; Kleinberg, Jon; Anderson, Ashton",Aligning Superhuman AI with Human Behavior: Chess as a Model System,Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,,,10.1145/3394486.3403219,http://arxiv.org/abs/2006.01855,"As artificial intelligence becomes increasingly intelligent---in some cases, achieving superhuman performance---there is growing potential for humans to learn from and collaborate with algorithms. However, the ways in which AI systems approach problems are often different from the ways people do, and thus may be uninterpretable and hard to learn from. A crucial step in bridging this gap between human and artificial intelligence is modeling the granular actions that constitute human behavior, rather than simply matching aggregate human performance. We pursue this goal in a model system with a long history in artificial intelligence: chess. The aggregate performance of a chess player unfolds as they make decisions over the course of a game. The hundreds of millions of games played online by players at every skill level form a rich source of data in which these decisions, and their exact context, are recorded in minute detail. Applying existing chess engines to this data, including an open-source implementation of AlphaZero, we find that they do not predict human moves well. We develop and introduce Maia, a customized version of Alpha-Zero trained on human chess games, that predicts human moves at a much higher accuracy than existing engines, and can achieve maximum accuracy when predicting decisions made by players at a specific skill level in a tuneable way. For a dual task of predicting whether a human will make a large mistake on the next move, we develop a deep neural network that significantly outperforms competitive baselines. Taken together, our results suggest that there is substantial promise in designing artificial intelligence systems with human collaboration in mind by first accurately modeling granular human decision-making.",2020-08-23,2022-01-30 04:59:34,2022-01-30 04:59:34,2020-11-14 00:50:30,1677-1687,,,,,,Aligning Superhuman AI with Human Behavior,,,,,,,,,,,,arXiv.org,,ZSCC: 0000023  arXiv: 2006.01855,,/Users/jacquesthibodeau/Zotero/storage/EHATD65D/McIlroy-Young et al. - 2020 - Aligning Superhuman AI with Human Behavior Chess .pdf; /Users/jacquesthibodeau/Zotero/storage/C93VXAWD/2006.html,,TechSafety; Other-org,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
X63UF2U8,journalArticle,2004,"Kent, Adrian",A Critical Look at Risk Assessments for Global Catastrophes,Risk Analysis,,"0272-4332, 1539-6924",10.1111/j.0272-4332.2004.00419.x,https://onlinelibrary.wiley.com/doi/abs/10.1111/j.0272-4332.2004.00419.x,,2004-02,2022-01-30 04:59:32,2022-01-30 04:59:32,2020-11-22 05:02:07,157-168,,1,24,,Risk Analysis,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000061,,/Users/jacquesthibodeau/Zotero/storage/CGPWV2PJ/Kent - 2004 - A Critical Look at Risk Assessments for Global Cat.pdf,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4NWR885X,journalArticle,2019,"Irving, Geoffrey; Askell, Amanda",AI Safety Needs Social Scientists,Distill,,2476-0757,10.23915/distill.00014,https://distill.pub/2019/safety-needs-social-scientists,,2019-02-19,2022-01-30 04:58:18,2022-01-30 04:58:18,2019-12-16 20:00:35,10.23915/distill.00014,,2,4,,Distill,,,,,,,,,,,,,DOI.org (Crossref),,ZSCC: 0000028,,/Users/jacquesthibodeau/Zotero/storage/QVDDN5BC/distill-social-scientists.html,,MetaSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WZUHK5GA,journalArticle,2021,"Goh, Gabriel; Cammarata, Nick; Voss, Chelsea; Carter, Shan; Petrov, Michael; Schubert, Ludwig; Radford, Alec; Olah, Chris",Multimodal Neurons in Artificial Neural Networks,Distill,,2476-0757,10.23915/distill.00030,https://distill.pub/2021/multimodal-neurons,,2021-03-04,2022-01-30 04:57:25,2022-01-30 04:57:25,2021-10-31 22:39:28,10.23915/distill.00030,,3,6,,Distill,,,,,,,,,,,,,DOI.org (Crossref),,ZSCC: 0000034,,,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BXX6UCUX,journalArticle,2020,"Stiennon, Nisan; Ouyang, Long; Wu, Jeffrey; Ziegler, Daniel; Lowe, Ryan; Voss, Chelsea; Radford, Alec; Amodei, Dario; Christiano, Paul F.",Learning to summarize with human feedback,Advances in Neural Information Processing Systems,,,,,,2020,2022-01-30 04:57:25,2022-01-30 04:57:25,,,,,33,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000005[s0],,/Users/jacquesthibodeau/Zotero/storage/QH3WZ2FT/Stiennon et al. - 2020 - Learning to summarize with human feedback.pdf; /Users/jacquesthibodeau/Zotero/storage/GFUVXB6G/1f89885d556929e98d3ef9b86448f951-Abstract.html,,TechSafety; Open-AI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7KKMB7Q7,journalArticle,2015,"Sotala, Kaj; Yampolskiy, Roman V",Responses to catastrophic AGI risk: a survey,Physica Scripta,,"0031-8949, 1402-4896",10.1088/0031-8949/90/1/018001,https://iopscience.iop.org/article/10.1088/0031-8949/90/1/018001,,2015-01-01,2022-01-30 04:56:58,2022-01-30 04:56:58,2020-11-22 02:23:47,18001,,1,90,,Phys. Scr.,Responses to catastrophic AGI risk,,,,,,,,,,,,DOI.org (Crossref),,ZSCC: NoCitationData[s2]  ACC: 115,,/Users/jacquesthibodeau/Zotero/storage/7HQTGDPG/Sotala and Yampolskiy - 2015 - Responses to catastrophic AGI risk a survey.pdf,,MetaSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PSRNPTDU,journalArticle,2018,"Danzig, Richard",Managing Loss of Control as Many Militaries Pursue Technological Superiority,Arms Control Today,,,,,,2018-05-30,2022-01-30 05:00:00,2022-01-30 05:00:00,,40,,7,48,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000037,,,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ADSHWBTF,journalArticle,1973,"Michie, Donald",Machines and the Theory of Intelligence,Nature,,"0028-0836, 1476-4687",10.1038/241507a0,http://www.nature.com/articles/241507a0,,1973-02,2022-01-30 05:00:00,2022-01-30 05:00:00,2020-11-22 02:23:08,507-512,,5391,241,,Nature,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000021,,,,TechSafety; AmbiguosSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4NSAU4ME,journalArticle,2019,"Sarma, Gopal P.; Hay, Nick J.",Mammalian Value Systems,Informatica,,,,http://arxiv.org/abs/1607.08289,"Characterizing human values is a topic deeply interwoven with the sciences, humanities, art, and many other human endeavors. In recent years, a number of thinkers have argued that accelerating trends in computer science, cognitive science, and related disciplines foreshadow the creation of intelligent machines which meet and ultimately surpass the cognitive abilities of human beings, thereby entangling an understanding of human values with future technological development. Contemporary research accomplishments suggest sophisticated AI systems becoming widespread and responsible for managing many aspects of the modern world, from preemptively planning users' travel schedules and logistics, to fully autonomous vehicles, to domestic robots assisting in daily living. The extrapolation of these trends has been most forcefully described in the context of a hypothetical ""intelligence explosion,"" in which the capabilities of an intelligent software agent would rapidly increase due to the presence of feedback loops unavailable to biological organisms. The possibility of superintelligent agents, or simply the widespread deployment of sophisticated, autonomous AI systems, highlights an important theoretical problem: the need to separate the cognitive and rational capacities of an agent from the fundamental goal structure, or value system, which constrains and guides the agent's actions. The ""value alignment problem"" is to specify a goal structure for autonomous agents compatible with human values. In this brief article, we suggest that recent ideas from affective neuroscience and related disciplines aimed at characterizing neurological and behavioral universals in the mammalian class provide important conceptual foundations relevant to describing human values. We argue that the notion of ""mammalian value systems"" points to a potential avenue for fundamental research in AI safety and AI ethics.",2019-01-21,2022-01-30 05:00:00,2022-01-30 05:00:00,2020-12-13 23:38:36,,,3,41,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s1]  ACC: 11  arXiv: 1607.08289,,/Users/jacquesthibodeau/Zotero/storage/WKTKGKUA/Sarma and Hay - 2019 - Mammalian Value Systems.pdf; /Users/jacquesthibodeau/Zotero/storage/FWKZNIR2/1607.html,,TechSafety; Other-org,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Robotics; Computer Science - Computers and Society; Computer Science - Human-Computer Interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
42ZE8I25,journalArticle,2011,"Sullins, John P.",Introduction: Open Questions in Roboethics,Philosophy & Technology,,"2210-5433, 2210-5441",10.1007/s13347-011-0043-6,http://link.springer.com/10.1007/s13347-011-0043-6,,2011-09,2022-01-30 04:59:59,2022-01-30 04:59:59,2020-11-22 02:23:55,233-238,,3,24,,Philos. Technol.,Introduction,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000043,,/Users/jacquesthibodeau/Zotero/storage/6PNZQ3NW/Sullins - 2011 - Introduction Open Questions in Roboethics.pdf,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WGZA24CE,journalArticle,2017,"Bogosian, Kyle",Implementation of Moral Uncertainty in Intelligent Machines,Minds and Machines,,1572-8641,10.1007/s11023-017-9448-z,https://doi.org/10.1007/s11023-017-9448-z,"The development of artificial intelligence will require systems of ethical decision making to be adapted for automatic computation. However, projects to implement moral reasoning in artificial moral agents so far have failed to satisfactorily address the widespread disagreement between competing approaches to moral philosophy. In this paper I argue that the proper response to this situation is to design machines to be fundamentally uncertain about morality. I describe a computational framework for doing so and show that it efficiently resolves common obstacles to the implementation of moral philosophy in intelligent machines.",2017-12-01,2022-01-30 04:59:58,2022-01-30 04:59:58,2020-12-13 22:17:16,591-608,,4,27,,Minds & Machines,,,,,,,,en,,,,,Springer Link,,ZSCC: 0000031,,/Users/jacquesthibodeau/Zotero/storage/DSITHXNZ/Bogosian - 2017 - Implementation of Moral Uncertainty in Intelligent.pdf,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NV87FGU4,journalArticle,2019,"Riedl, Mark O.",Human-Centered Artificial Intelligence and Machine Learning,Human Behavior and Emerging Technologies,,,,http://arxiv.org/abs/1901.11184,"Humans are increasingly coming into contact with artificial intelligence and machine learning systems. Human-centered artificial intelligence is a perspective on AI and ML that algorithms must be designed with awareness that they are part of a larger system consisting of humans. We lay forth an argument that human-centered artificial intelligence can be broken down into two aspects: (1) AI systems that understand humans from a sociocultural perspective, and (2) AI systems that help humans understand them. We further argue that issues of social responsibility such as fairness, accountability, interpretability, and transparency.",2019-01-30,2022-01-30 04:59:57,2022-01-30 04:59:57,2020-11-14 00:32:19,,,1,1,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000092  arXiv: 1901.11184,,/Users/jacquesthibodeau/Zotero/storage/NMJIIGJ5/Riedl - 2019 - Human-Centered Artificial Intelligence and Machine.pdf; /Users/jacquesthibodeau/Zotero/storage/7IGC3HKT/1901.html,,TechSafety; Other-org,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QP56TU5K,journalArticle,2018,"Vamplew, Peter; Dazeley, Richard; Foale, Cameron; Firmin, Sally; Mummery, Jane",Human-aligned artificial intelligence is a multiobjective problem,Ethics and Information Technology,,1572-8439,10.1007/s10676-017-9440-6,https://doi.org/10.1007/s10676-017-9440-6,"As the capabilities of artificial intelligence (AI) systems improve, it becomes important to constrain their actions to ensure their behaviour remains beneficial to humanity. A variety of ethical, legal and safety-based frameworks have been proposed as a basis for designing these constraints. Despite their variations, these frameworks share the common characteristic that decision-making must consider multiple potentially conflicting factors. We demonstrate that these alignment frameworks can be represented as utility functions, but that the widely used Maximum Expected Utility (MEU) paradigm provides insufficient support for such multiobjective decision-making. We show that a Multiobjective Maximum Expected Utility paradigm based on the combination of vector utilities and non-linear action–selection can overcome many of the issues which limit MEU’s effectiveness in implementing aligned AI. We examine existing approaches to multiobjective AI, and identify how these can contribute to the development of human-aligned intelligent agents.",2018-03-01,2022-01-30 04:59:57,2022-01-30 04:59:57,2020-12-13 21:48:09,27-40,,1,20,,Ethics Inf Technol,,,,,,,,en,,,,,Springer Link,,ZSCC: 0000067,,/Users/jacquesthibodeau/Zotero/storage/3M4DM2R7/Vamplew et al. - 2018 - Human-aligned artificial intelligence is a multiob.pdf,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
87DAV94P,journalArticle,2011,"Baum, Seth D.; Goertzel, Ben; Goertzel, Ted G.",How long until human-level AI? Results from an expert assessment,Technological Forecasting and Social Change,,401625,10.1016/j.techfore.2010.09.006,https://linkinghub.elsevier.com/retrieve/pii/S0040162510002106,,2011-01,2022-01-30 04:59:57,2022-01-30 04:59:57,2020-11-22 04:16:21,185-195,,1,78,,Technological Forecasting and Social Change,How long until human-level AI?,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000132,,,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3XXE6R8G,journalArticle,2018,"Liu, Hin-Yan; Lauta, Kristian Cedervall; Maas, Matthijs Michiel",Governing Boring Apocalypses: A new typology of existential vulnerabilities and exposures for existential risk research,Futures,,0016-3287,10.1016/j.futures.2018.04.009,http://www.sciencedirect.com/science/article/pii/S0016328717301623,"In recent years, the study of existential risks has explored a range of natural and man-made catastrophes, from supervolcano eruption to nuclear war, and from global pandemics to potential risks from misaligned AI. These risks share the prospect of causing outright human extinction were they to occur. In this approach, such identified existential risks are frequently characterised by relatively singular origin events and concrete pathways of harm which directly jeopardise the survival of humanity, or undercut its potential for long-term technological progress. While this approach aptly identifies the most cataclysmic fates which may befall humanity, we argue that catastrophic ‘existential outcomes’ may likely arise from a broader range of sources and societal vulnerabilities, and through the complex interactions of disparate social, cultural, and natural processes—many of which, taken in isolation, might not be seen to merit attention as a global catastrophic, let alone existential, risk. This article argues that an emphasis on mitigating the hazards (discrete causes) of existential risks is an unnecessarily narrow framing of the challenge facing humanity, one which risks prematurely curtailing the spectrum of policy responses considered. Instead, it argues existential risks constitute but a subset in a broader set of challenges which could directly or indirectly contribute to existential consequences for humanity. To illustrate, we introduce and examine a set of existential risks that often fall outside the scope of, or remain understudied within, the field. By focusing on vulnerability and exposure rather than existential hazards, we develop a new taxonomy which captures factors contributing to these existential risks. Latent structural vulnerabilities in our technological systems and in our societal arrangements may increase our susceptibility to existential hazards. Finally, different types of exposure of our society or its natural base determine if or how a given hazard can interface with pre-existing vulnerabilities, to trigger emergent existential risks. We argue that far from being peripheral footnotes to their more direct and immediately terminal counterparts, these “Boring Apocalypses” may well prove to be the more endemic and problematic, dragging down and undercutting short-term successes in mitigating more spectacular risks. If the cardinal concern is humanity’s continued survival and prosperity, then focussing academic and public advocacy efforts on reducing direct existential hazards may have the paradoxical potential of exacerbating humanity’s indirect susceptibility to such outcomes. Adopting law and policy perspectives allow us to foreground societal dimensions that complement and reinforce the discourse on existential risks.",2018-09-01,2022-01-30 04:59:56,2022-01-30 04:59:56,2020-12-13 23:17:16,6-19,,,102,,Futures,Governing Boring Apocalypses,Futures of research in catastrophic and existential risk,,,,,,en,,,,,ScienceDirect,,ZSCC: 0000030,,,,MetaSafety; AmbiguosSafety; Other-org,Boring Apocalypse; Civilisational collapse; Existential Risks; Exposure; Vulnerability,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QV2MTIZJ,journalArticle,2019,"Torres, Phil",Existential risks: a philosophical analysis,Inquiry,,0020-174X,10.1080/0020174X.2019.1658626,https://doi.org/10.1080/0020174X.2019.1658626,"This paper examines and analyzes five definitions of ‘existential risk.’ It tentatively adopts a pluralistic approach according to which the definition that scholars employ should depend upon the particular context of use. More specifically, the notion that existential risks are ‘risks of human extinction or civilizational collapse’ is best when communicating with the public, whereas equating existential risks with a ‘significant loss of expected value’ may be the most effective definition for establishing existential risk studies as a legitimate field of scientific and philosophical inquiry. In making these arguments, the present paper hopes to provide a modicum of clarity to foundational issues relating to the central concept of arguably the most important discussion of our times.",2019-08-23,2022-01-30 04:59:48,2022-01-30 04:59:48,2020-11-14 01:16:56,1-26,,0,0,,,Existential risks,,,,,,,,,,,,Taylor and Francis+NEJM,,ZSCC: 0000004  Publisher: Routledge _eprint: https://doi.org/10.1080/0020174X.2019.1658626,,/Users/jacquesthibodeau/Zotero/storage/F7KA53VC/0020174X.2019.html,,MetaSafety; AmbiguosSafety; Other-org,analysis; existential risk studies; Existential risks; global catastrophic risks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CHVTVF6Z,journalArticle,2018,"Green, Brian Patrick",Ethical Reflections on Artificial Intelligence,Scientia et Fides,,"2353-5636, 2300-7648",10.12775/SetF.2018.015,http://apcz.umk.pl/czasopisma/index.php/SetF/article/view/SetF.2018.015,,2018-10-09,2022-01-30 04:59:48,2022-01-30 04:59:48,2020-12-13 23:07:52,9,,2,6,,SetF,,,,,,,,,,,,,DOI.org (Crossref),,ZSCC: 0000026,,/Users/jacquesthibodeau/Zotero/storage/ADZH5EW7/Green - 2018 - Ethical Reflections on Artificial Intelligence.pdf,,TechSafety; AmbiguosSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TIMZ3X84,journalArticle,2015,"Davis, Ernest",Ethical guidelines for a superintelligence,Artificial Intelligence,,43702,10.1016/j.artint.2014.12.003,https://linkinghub.elsevier.com/retrieve/pii/S0004370214001453,,2015-03,2022-01-30 04:59:47,2022-01-30 04:59:47,2020-11-22 04:16:24,121-124,,,220,,Artificial Intelligence,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000030,,,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4S5HQK4S,journalArticle,2008,"Hanson, Robin",Economics of the singularity,IEEE Spectrum,,0018-9235,10.1109/MSPEC.2008.4531461,http://ieeexplore.ieee.org/document/4531461/,,2008-06,2022-01-30 04:59:47,2022-01-30 04:59:47,2020-11-22 02:22:43,45-50,,6,45,,IEEE Spectr.,,,,,,,,,,,,,DOI.org (Crossref),,ZSCC: 0000085,,,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PV7TJE2E,journalArticle,1999,"Sobel, D.",Do the desires of rational agents converge?,Analysis,,"0003-2638, 1467-8284",10.1093/analys/59.3.137,https://academic.oup.com/analysis/article-lookup/doi/10.1093/analys/59.3.137,,1999-07-01,2022-01-30 04:59:46,2022-01-30 04:59:46,2020-11-22 02:23:46,137-147,,3,59,,Analysis,,,,,,,,en,,,,,DOI.org (Crossref),,ZSCC: 0000040,,,,TechSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NYYLH2LN,journalArticle,2021,"Hendrycks, Dan; Carlini, Nicholas; Schulman, John; Steinhardt, Jacob",Unsolved Problems in ML Safety,arXiv:2109.13916 [cs],,,,http://arxiv.org/abs/2109.13916,"Machine learning (ML) systems are rapidly increasing in size, are acquiring new capabilities, and are increasingly deployed in high-stakes settings. As with other powerful technologies, safety for ML should be a leading research priority. In response to emerging safety challenges in ML, such as those introduced by recent large-scale models, we provide a new roadmap for ML Safety and refine the technical problems that the field needs to address. We present four problems ready for research, namely withstanding hazards (""Robustness""), identifying hazards (""Monitoring""), steering ML systems (""Alignment""), and reducing deployment hazards (""External Safety""). Throughout, we clarify each problem's motivation and provide concrete research directions.",2021-12-25,2022-03-09 22:57:07,2022-03-09 22:57:07,2022-03-09 22:57:07,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2109.13916,,/Users/jacquesthibodeau/Zotero/storage/PYX4ZIAY/Hendrycks et al. - 2021 - Unsolved Problems in ML Safety.pdf; /Users/jacquesthibodeau/Zotero/storage/RKAC9MSI/2109.html,,,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
U3SKYVI8,journalArticle,2021,"Laidlaw, Cassidy; Russell, Stuart",Uncertain Decisions Facilitate Better Preference Learning,"arXiv:2106.10394 [cs, stat]",,,,http://arxiv.org/abs/2106.10394,"Existing observational approaches for learning human preferences, such as inverse reinforcement learning, usually make strong assumptions about the observability of the human's environment. However, in reality, people make many important decisions under uncertainty. To better understand preference learning in these cases, we study the setting of inverse decision theory (IDT), a previously proposed framework where a human is observed making non-sequential binary decisions under uncertainty. In IDT, the human's preferences are conveyed through their loss function, which expresses a tradeoff between different types of mistakes. We give the first statistical analysis of IDT, providing conditions necessary to identify these preferences and characterizing the sample complexity -- the number of decisions that must be observed to learn the tradeoff the human is making to a desired precision. Interestingly, we show that it is actually easier to identify preferences when the decision problem is more uncertain. Furthermore, uncertain decision problems allow us to relax the unrealistic assumption that the human is an optimal decision maker but still identify their exact preferences; we give sample complexities in this suboptimal case as well. Our analysis contradicts the intuition that partial observability should make preference learning more difficult. It also provides a first step towards understanding and improving preference learning methods for uncertain and suboptimal humans.",2021-10-28,2022-03-09 22:57:56,2022-03-09 22:57:56,2022-03-09 22:57:56,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2106.10394,,/Users/jacquesthibodeau/Zotero/storage/THNL7BX9/Laidlaw and Russell - 2021 - Uncertain Decisions Facilitate Better Preference L.pdf; /Users/jacquesthibodeau/Zotero/storage/EN2IKJX3/2106.html,,,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DKEPU826,journalArticle,2021,"Roman, Charlotte; Dennis, Michael; Critch, Andrew; Russell, Stuart",Accumulating Risk Capital Through Investing in Cooperation,arXiv:2101.10305 [cs],,,,http://arxiv.org/abs/2101.10305,"Recent work on promoting cooperation in multi-agent learning has resulted in many methods which successfully promote cooperation at the cost of becoming more vulnerable to exploitation by malicious actors. We show that this is an unavoidable trade-off and propose an objective which balances these concerns, promoting both safety and long-term cooperation. Moreover, the trade-off between safety and cooperation is not severe, and you can receive exponentially large returns through cooperation from a small amount of risk. We study both an exact solution method and propose a method for training policies that targets this objective, Accumulating Risk Capital Through Investing in Cooperation (ARCTIC), and evaluate them in iterated Prisoner's Dilemma and Stag Hunt.",2021-04-20,2022-03-09 22:58:09,2022-03-11 01:37:20,2022-03-09 22:58:09,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2101.10305,,/Users/jacquesthibodeau/Zotero/storage/LKX27ABC/Roman et al. - 2021 - Accumulating Risk Capital Through Investing in Coo.pdf; /Users/jacquesthibodeau/Zotero/storage/93KS3FYM/2101.html; /Users/jacquesthibodeau/Zotero/storage/2AWIMKR9/Roman et al. - 2021 - Accumulating Risk Capital Through Investing in Coo.pdf; /Users/jacquesthibodeau/Zotero/storage/SR2STL4U/2101.html,,,Computer Science - Artificial Intelligence; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EHPBTDLF,journalArticle,2022,"Hendrycks, Dan; Mazeika, Mantas; Zou, Andy; Patel, Sahil; Zhu, Christine; Navarro, Jesus; Song, Dawn; Li, Bo; Steinhardt, Jacob",What Would Jiminy Cricket Do? Towards Agents That Behave Morally,arXiv:2110.13136 [cs],,,,http://arxiv.org/abs/2110.13136,"When making everyday decisions, people are guided by their conscience, an internal sense of right and wrong. By contrast, artificial agents are currently not endowed with a moral sense. As a consequence, they may learn to behave immorally when trained on environments that ignore moral concerns, such as violent video games. With the advent of generally capable agents that pretrain on many environments, it will become necessary to mitigate inherited biases from environments that teach immoral behavior. To facilitate the development of agents that avoid causing wanton harm, we introduce Jiminy Cricket, an environment suite of 25 text-based adventure games with thousands of diverse, morally salient scenarios. By annotating every possible game state, the Jiminy Cricket environments robustly evaluate whether agents can act morally while maximizing reward. Using models with commonsense moral knowledge, we create an elementary artificial conscience that assesses and guides agents. In extensive experiments, we find that the artificial conscience approach can steer agents towards moral behavior without sacrificing performance.",2022-02-07,2022-03-09 22:58:16,2022-03-11 01:37:25,2022-03-09 22:58:16,,,,,,,What Would Jiminy Cricket Do?,,,,,,,,,,,,arXiv.org,,arXiv: 2110.13136,,/Users/jacquesthibodeau/Zotero/storage/NCUMY87S/Hendrycks et al. - 2022 - What Would Jiminy Cricket Do Towards Agents That .pdf; /Users/jacquesthibodeau/Zotero/storage/URSN934U/Hendrycks et al. - 2022 - What Would Jiminy Cricket Do Towards Agents That .pdf; /Users/jacquesthibodeau/Zotero/storage/L68I3PS9/2110.html; /Users/jacquesthibodeau/Zotero/storage/J5I4QY85/2110.html; /Users/jacquesthibodeau/Zotero/storage/Y4FWJ689/Hendrycks et al. - 2021 - What Would Jiminy Cricket Do Towards Agents That .pdf; /Users/jacquesthibodeau/Zotero/storage/K3PCNQBZ/2110.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Computers and Society; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CCHG9XGR,journalArticle,2021,"Filan, Daniel; Casper, Stephen; Hod, Shlomi; Wild, Cody; Critch, Andrew; Russell, Stuart",Clusterability in Neural Networks,arXiv:2103.03386 [cs],,,,http://arxiv.org/abs/2103.03386,"The learned weights of a neural network have often been considered devoid of scrutable internal structure. In this paper, however, we look for structure in the form of clusterability: how well a network can be divided into groups of neurons with strong internal connectivity but weak external connectivity. We find that a trained neural network is typically more clusterable than randomly initialized networks, and often clusterable relative to random networks with the same distribution of weights. We also exhibit novel methods to promote clusterability in neural network training, and find that in multi-layer perceptrons they lead to more clusterable networks with little reduction in accuracy. Understanding and controlling the clusterability of neural networks will hopefully render their inner workings more interpretable to engineers by facilitating partitioning into meaningful clusters.",2021-03-04,2022-03-09 22:58:27,2022-03-09 22:58:27,2022-03-09 22:58:27,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2103.03386,,/Users/jacquesthibodeau/Zotero/storage/BMSKZG99/Filan et al. - 2021 - Clusterability in Neural Networks.pdf; /Users/jacquesthibodeau/Zotero/storage/9INK4IXF/2103.html,,,Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MDPKLM9I,journalArticle,2021,"Zhuang, Simon; Hadfield-Menell, Dylan",Consequences of Misaligned AI,arXiv:2102.03896 [cs],,,,http://arxiv.org/abs/2102.03896,AI systems often rely on two key components: a specified goal or reward function and an optimization algorithm to compute the optimal behavior for that goal. This approach is intended to provide value for a principal: the user on whose behalf the agent acts. The objectives given to these agents often refer to a partial specification of the principal's goals. We consider the cost of this incompleteness by analyzing a model of a principal and an agent in a resource constrained world where the $L$ attributes of the state correspond to different sources of utility for the principal. We assume that the reward function given to the agent only has support on $J < L$ attributes. The contributions of our paper are as follows: 1) we propose a novel model of an incomplete principal-agent problem from artificial intelligence; 2) we provide necessary and sufficient conditions under which indefinitely optimizing for any incomplete proxy objective leads to arbitrarily low overall utility; and 3) we show how modifying the setup to allow reward functions that reference the full state or allowing the principal to update the proxy objective over time can lead to higher utility solutions. The results in this paper argue that we should view the design of reward functions as an interactive and dynamic process and identifies a theoretical scenario where some degree of interactivity is desirable.,2021-02-07,2022-03-09 22:58:33,2022-03-09 22:58:33,2022-03-09 22:58:33,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2102.03896,,/Users/jacquesthibodeau/Zotero/storage/T6BV6ZB4/Zhuang and Hadfield-Menell - 2021 - Consequences of Misaligned AI.pdf; /Users/jacquesthibodeau/Zotero/storage/HP4ZPC8U/2102.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BCG7XGVQ,journalArticle,2021,"Shah, Rohin; Wild, Cody; Wang, Steven H.; Alex, Neel; Houghton, Brandon; Guss, William; Mohanty, Sharada; Kanervisto, Anssi; Milani, Stephanie; Topin, Nicholay; Abbeel, Pieter; Russell, Stuart; Dragan, Anca",The MineRL BASALT Competition on Learning from Human Feedback,arXiv:2107.01969 [cs],,,,http://arxiv.org/abs/2107.01969,"The last decade has seen a significant increase of interest in deep learning research, with many public successes that have demonstrated its potential. As such, these systems are now being incorporated into commercial products. With this comes an additional challenge: how can we build AI systems that solve tasks where there is not a crisp, well-defined specification? While multiple solutions have been proposed, in this competition we focus on one in particular: learning from human feedback. Rather than training AI systems using a predefined reward function or using a labeled dataset with a predefined set of categories, we instead train the AI system using a learning signal derived from some form of human feedback, which can evolve over time as the understanding of the task changes, or as the capabilities of the AI system improve. The MineRL BASALT competition aims to spur forward research on this important class of techniques. We design a suite of four tasks in Minecraft for which we expect it will be hard to write down hardcoded reward functions. These tasks are defined by a paragraph of natural language: for example, ""create a waterfall and take a scenic picture of it"", with additional clarifying details. Participants must train a separate agent for each task, using any method they want. Agents are then evaluated by humans who have read the task description. To help participants get started, we provide a dataset of human demonstrations on each of the four tasks, as well as an imitation learning baseline that leverages these demonstrations. Our hope is that this competition will improve our ability to build AI systems that do what their designers intend them to do, even when the intent cannot be easily formalized. Besides allowing AI to solve more tasks, this can also enable more effective regulation of AI systems, as well as making progress on the value alignment problem.",2021-07-05,2022-03-09 22:58:35,2022-03-09 22:58:35,2022-03-09 22:58:35,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2107.01969,,/Users/jacquesthibodeau/Zotero/storage/89IYMW6H/Shah et al. - 2021 - The MineRL BASALT Competition on Learning from Hum.pdf; /Users/jacquesthibodeau/Zotero/storage/SJE63IZB/2107.html,,,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BFEB9Q94,journalArticle,2022,"Hod, Shlomi; Filan, Daniel; Casper, Stephen; Critch, Andrew; Russell, Stuart",Quantifying Local Specialization in Deep Neural Networks,arXiv:2110.08058 [cs],,,,http://arxiv.org/abs/2110.08058,"A neural network is locally specialized to the extent that parts of its computational graph (i.e. structure) can be abstractly represented as performing some comprehensible sub-task relevant to the overall task (i.e. functionality). Are modern deep neural networks locally specialized? How can this be quantified? In this paper, we consider the problem of taking a neural network whose neurons are partitioned into clusters, and quantifying how functionally specialized the clusters are. We propose two proxies for this: importance, which reflects how crucial sets of neurons are to network performance; and coherence, which reflects how consistently their neurons associate with features of the inputs. To measure these proxies, we develop a set of statistical methods based on techniques conventionally used to interpret individual neurons. We apply the proxies to partitionings generated by spectrally clustering a graph representation of the network's neurons with edges determined either by network weights or correlations of activations. We show that these partitionings, even ones based only on weights (i.e. strictly from non-runtime analysis), reveal groups of neurons that are important and coherent. These results suggest that graph-based partitioning can reveal local specialization and that statistical methods can be used to automatedly screen for sets of neurons that can be understood abstractly.",2022-02-07,2022-03-09 22:58:40,2022-03-11 01:37:49,2022-03-09 22:58:40,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2110.08058,,/Users/jacquesthibodeau/Zotero/storage/RXRBMQPT/Hod et al. - 2022 - Quantifying Local Specialization in Deep Neural Ne.pdf; /Users/jacquesthibodeau/Zotero/storage/Q6WYJA5N/2110.html; /Users/jacquesthibodeau/Zotero/storage/WBCI6CWN/Hod et al. - 2021 - Quantifying Local Specialization in Deep Neural Ne.pdf; /Users/jacquesthibodeau/Zotero/storage/PFKUDN8U/2110.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5GJ88798,journalArticle,2021,"Lindner, David; Shah, Rohin; Abbeel, Pieter; Dragan, Anca",Learning What To Do by Simulating the Past,"arXiv:2104.03946 [cs, stat]",,,,http://arxiv.org/abs/2104.03946,"Since reward functions are hard to specify, recent work has focused on learning policies from human feedback. However, such approaches are impeded by the expense of acquiring such feedback. Recent work proposed that agents have access to a source of information that is effectively free: in any environment that humans have acted in, the state will already be optimized for human preferences, and thus an agent can extract information about what humans want from the state. Such learning is possible in principle, but requires simulating all possible past trajectories that could have led to the observed state. This is feasible in gridworlds, but how do we scale it to complex tasks? In this work, we show that by combining a learned feature encoder with learned inverse models, we can enable agents to simulate human actions backwards in time to infer what they must have done. The resulting algorithm is able to reproduce a specific skill in MuJoCo environments given a single state sampled from the optimal policy for that skill.",2021-05-03,2022-03-09 22:58:45,2022-03-09 22:58:45,2022-03-09 22:58:45,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2104.03946,,/Users/jacquesthibodeau/Zotero/storage/Y8HNMMHK/Lindner et al. - 2021 - Learning What To Do by Simulating the Past.pdf; /Users/jacquesthibodeau/Zotero/storage/CUPUYI59/2104.html,,,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NJQIBMRG,journalArticle,2019,"Shah, Rohin; Krasheninnikov, Dmitrii; Alexander, Jordan; Abbeel, Pieter; Dragan, Anca",Preferences Implicit in the State of the World,"arXiv:1902.04198 [cs, stat]",,,,http://arxiv.org/abs/1902.04198,"Reinforcement learning (RL) agents optimize only the features specified in a reward function and are indifferent to anything left out inadvertently. This means that we must not only specify what to do, but also the much larger space of what not to do. It is easy to forget these preferences, since these preferences are already satisfied in our environment. This motivates our key insight: when a robot is deployed in an environment that humans act in, the state of the environment is already optimized for what humans want. We can therefore use this implicit preference information from the state to fill in the blanks. We develop an algorithm based on Maximum Causal Entropy IRL and use it to evaluate the idea in a suite of proof-of-concept environments designed to show its properties. We find that information from the initial state can be used to infer both side effects that should be avoided as well as preferences for how the environment should be organized. Our code can be found at https://github.com/HumanCompatibleAI/rlsp.",2019-04-18,2022-03-09 22:58:48,2022-03-09 22:58:48,2022-03-09 22:58:48,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1902.04198,,/Users/jacquesthibodeau/Zotero/storage/AW4CB7BH/Shah et al. - 2019 - Preferences Implicit in the State of the World.pdf; /Users/jacquesthibodeau/Zotero/storage/YX7GB7ZZ/1902.html,,,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IR98WD26,journalArticle,2021,"Brown, Daniel S.; Schneider, Jordan; Dragan, Anca D.; Niekum, Scott",Value Alignment Verification,arXiv:2012.01557 [cs],,,,http://arxiv.org/abs/2012.01557,"As humans interact with autonomous agents to perform increasingly complicated, potentially risky tasks, it is important to be able to efficiently evaluate an agent's performance and correctness. In this paper we formalize and theoretically analyze the problem of efficient value alignment verification: how to efficiently test whether the behavior of another agent is aligned with a human's values. The goal is to construct a kind of ""driver's test"" that a human can give to any agent which will verify value alignment via a minimal number of queries. We study alignment verification problems with both idealized humans that have an explicit reward function as well as problems where they have implicit values. We analyze verification of exact value alignment for rational agents and propose and analyze heuristic and approximate value alignment verification tests in a wide range of gridworlds and a continuous autonomous driving domain. Finally, we prove that there exist sufficient conditions such that we can verify exact and approximate alignment across an infinite set of test environments via a constant-query-complexity alignment test.",2021-06-11,2022-03-09 23:00:34,2022-03-11 01:36:24,2022-03-09 23:00:34,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2012.01557,,/Users/jacquesthibodeau/Zotero/storage/NWT7CF7Y/Brown et al. - 2021 - Value Alignment Verification.pdf; /Users/jacquesthibodeau/Zotero/storage/3CL3BXKY/2012.html; /Users/jacquesthibodeau/Zotero/storage/W7HB9SVE/Brown et al. - 2020 - Value Alignment Verification.pdf; /Users/jacquesthibodeau/Zotero/storage/55WZNQ7K/Brown et al. - 2020 - Value Alignment Verification.pdf; /Users/jacquesthibodeau/Zotero/storage/TA94RCWP/2012.html; /Users/jacquesthibodeau/Zotero/storage/R4DTXQIR/2012.html,,,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SEDFPFTC,journalArticle,2021,"Zhang, Tianjun; Rashidinejad, Paria; Jiao, Jiantao; Tian, Yuandong; Gonzalez, Joseph; Russell, Stuart",MADE: Exploration via Maximizing Deviation from Explored Regions,"arXiv:2106.10268 [cs, stat]",,,,http://arxiv.org/abs/2106.10268,"In online reinforcement learning (RL), efficient exploration remains particularly challenging in high-dimensional environments with sparse rewards. In low-dimensional environments, where tabular parameterization is possible, count-based upper confidence bound (UCB) exploration methods achieve minimax near-optimal rates. However, it remains unclear how to efficiently implement UCB in realistic RL tasks that involve non-linear function approximation. To address this, we propose a new exploration approach via \textit{maximizing} the deviation of the occupancy of the next policy from the explored regions. We add this term as an adaptive regularizer to the standard RL objective to balance exploration vs. exploitation. We pair the new objective with a provably convergent algorithm, giving rise to a new intrinsic reward that adjusts existing bonuses. The proposed intrinsic reward is easy to implement and combine with other existing RL algorithms to conduct exploration. As a proof of concept, we evaluate the new intrinsic reward on tabular examples across a variety of model-based and model-free algorithms, showing improvements over count-only exploration strategies. When tested on navigation and locomotion tasks from MiniGrid and DeepMind Control Suite benchmarks, our approach significantly improves sample efficiency over state-of-the-art methods. Our code is available at https://github.com/tianjunz/MADE.",2021-06-18,2022-03-09 23:02:49,2022-03-09 23:02:49,2022-03-09 23:02:49,,,,,,,MADE,,,,,,,,,,,,arXiv.org,,arXiv: 2106.10268,,/Users/jacquesthibodeau/Zotero/storage/9V52FPRY/Zhang et al. - 2021 - MADE Exploration via Maximizing Deviation from Ex.pdf; /Users/jacquesthibodeau/Zotero/storage/57TUF66N/2106.html,,,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DU935JQM,journalArticle,2021,"Dafoe, Allan; Zwetsloot, Remco; Cebul, Matthew",Reputations for Resolve and Higher-Order Beliefs in Crisis Bargaining,Journal of Conflict Resolution,,0022-0027,10.1177/0022002721995549,https://doi.org/10.1177/0022002721995549,"Reputations for resolve are said to be one of the few things worth fighting for, yet they remain inadequately understood. Discussions of reputation focus almost exclusively on first-order belief change—A stands firm, B updates its beliefs about A’s resolve. Such first-order reputational effects are important, but they are not the whole story. Higher-order beliefs—what A believes about B’s beliefs, and so on—matter a great deal as well. When A comes to believe that B is more resolved, this may decrease A’s resolve, and this in turn may increase B’s resolve, and so on. In other words, resolve is interdependent. We offer a framework for estimating higher-order effects, and find evidence of such reasoning in a survey experiment on quasi-elites. Our findings indicate both that states and leaders can develop potent reputations for resolve, and that higher-order beliefs are often responsible for a large proportion of these effects (40 percent to 70 percent in our experimental setting). We conclude by complementing the survey with qualitative evidence and laying the groundwork for future research.",2021-08-01,2022-03-09 23:04:04,2022-03-09 23:04:04,2022-03-09 23:04:04,1378-1404,,7-8,65,,Journal of Conflict Resolution,,,,,,,,en,,,,,SAGE Journals,,Publisher: SAGE Publications Inc,,/Users/jacquesthibodeau/Zotero/storage/GXRYYKPK/Dafoe et al. - 2021 - Reputations for Resolve and Higher-Order Beliefs i.pdf,,,bargaining; belief structure; conflict; game theory; survey experiment,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Q4IMBSVT,journalArticle,2021,"Ding, Jeffrey; Dafoe, Allan","Engines of Power: Electricity, AI, and General-Purpose Military Transformations","arXiv:2106.04338 [econ, q-fin]",,,,http://arxiv.org/abs/2106.04338,"Major theories of military innovation focus on relatively narrow technological developments, such as nuclear weapons or aircraft carriers. Arguably the most profound military implications of technological change, however, come from more fundamental advances arising from general purpose technologies, such as the steam engine, electricity, and the computer. With few exceptions, political scientists have not theorized about GPTs. Drawing from the economics literature on GPTs, we distill several propositions on how and when GPTs affect military affairs. We call these effects general-purpose military transformations. In particular, we argue that the impacts of GMTs on military effectiveness are broad, delayed, and shaped by indirect productivity spillovers. Additionally, GMTs differentially advantage those militaries that can draw from a robust industrial base in the GPT. To illustrate the explanatory value of our theory, we conduct a case study of the military consequences of electricity, the prototypical GPT. Finally, we apply our findings to artificial intelligence, which will plausibly cause a profound general-purpose military transformation.",2021-06-08,2022-03-09 23:04:48,2022-03-11 01:37:51,2022-03-09 23:04:47,,,,,,,Engines of Power,,,,,,,,,,,,arXiv.org,,arXiv: 2106.04338,,"/Users/jacquesthibodeau/Zotero/storage/ZKT7UYR3/Ding and Dafoe - 2021 - Engines of Power Electricity, AI, and General-Pur.pdf; /Users/jacquesthibodeau/Zotero/storage/M7A9LFQT/2106.html; /Users/jacquesthibodeau/Zotero/storage/9YDRJUPU/Ding and Dafoe - 2021 - Engines of Power Electricity, AI, and General-Pur.pdf; /Users/jacquesthibodeau/Zotero/storage/N8UE9PUS/2106.html",,,Economics - General Economics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
L58NK8PU,journalArticle,2021,"Ashurst, Carolyn; Hine, Emmie; Sedille, Paul; Carlier, Alexis",AI Ethics Statements -- Analysis and lessons learnt from NeurIPS Broader Impact Statements,arXiv:2111.01705 [cs],,,,http://arxiv.org/abs/2111.01705,"Ethics statements have been proposed as a mechanism to increase transparency and promote reflection on the societal impacts of published research. In 2020, the machine learning (ML) conference NeurIPS broke new ground by requiring that all papers include a broader impact statement. This requirement was removed in 2021, in favour of a checklist approach. The 2020 statements therefore provide a unique opportunity to learn from the broader impact experiment: to investigate the benefits and challenges of this and similar governance mechanisms, as well as providing an insight into how ML researchers think about the societal impacts of their own work. Such learning is needed as NeurIPS and other venues continue to question and adapt their policies. To enable this, we have created a dataset containing the impact statements from all NeurIPS 2020 papers, along with additional information such as affiliation type, location and subject area, and a simple visualisation tool for exploration. We also provide an initial quantitative analysis of the dataset, covering representation, engagement, common themes, and willingness to discuss potential harms alongside benefits. We investigate how these vary by geography, affiliation type and subject area. Drawing on these findings, we discuss the potential benefits and negative outcomes of ethics statement requirements, and their possible causes and associated challenges. These lead us to several lessons to be learnt from the 2020 requirement: (i) the importance of creating the right incentives, (ii) the need for clear expectations and guidance, and (iii) the importance of transparency and constructive deliberation. We encourage other researchers to use our dataset to provide additional analysis, to further our understanding of how researchers responded to this requirement, and to investigate the benefits and challenges of this and related mechanisms.",2021-11-02,2022-03-09 23:05:17,2022-03-11 01:36:34,2022-03-09 23:05:17,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2111.01705,,/Users/jacquesthibodeau/Zotero/storage/L4N8X3ZX/Ashurst et al. - 2021 - AI Ethics Statements -- Analysis and lessons learn.pdf; /Users/jacquesthibodeau/Zotero/storage/9LMZ9EW5/2111.html; /Users/jacquesthibodeau/Zotero/storage/4Q8AYI5N/Ashurst et al. - 2021 - AI Ethics Statements -- Analysis and lessons learn.pdf; /Users/jacquesthibodeau/Zotero/storage/N43N7BRA/Ashurst et al. - 2021 - AI Ethics Statements -- Analysis and lessons learn.pdf; /Users/jacquesthibodeau/Zotero/storage/NEZNP4G3/2111.html; /Users/jacquesthibodeau/Zotero/storage/5BVM9QMS/2111.html,,,Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GYZ35DBC,journalArticle,2020,"Critch, Andrew; Krueger, David",AI Research Considerations for Human Existential Safety (ARCHES),arXiv:2006.04948 [cs],,,,http://arxiv.org/abs/2006.04948,"Framed in positive terms, this report examines how technical AI research might be steered in a manner that is more attentive to humanity's long-term prospects for survival as a species. In negative terms, we ask what existential risks humanity might face from AI development in the next century, and by what principles contemporary technical research might be directed to address those risks. A key property of hypothetical AI technologies is introduced, called \emph{prepotence}, which is useful for delineating a variety of potential existential risks from artificial intelligence, even as AI paradigms might shift. A set of \auxref{dirtot} contemporary research \directions are then examined for their potential benefit to existential safety. Each research direction is explained with a scenario-driven motivation, and examples of existing work from which to build. The research directions present their own risks and benefits to society that could occur at various scales of impact, and in particular are not guaranteed to benefit existential safety if major developments in them are deployed without adequate forethought and oversight. As such, each direction is accompanied by a consideration of potentially negative side effects.",2020-05-29,2022-03-09 23:19:00,2022-03-09 23:19:00,2022-03-09 23:19:00,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2006.04948,,/Users/jacquesthibodeau/Zotero/storage/SMG6MYD4/Critch and Krueger - 2020 - AI Research Considerations for Human Existential S.pdf; /Users/jacquesthibodeau/Zotero/storage/HHDPPASH/2006.html,,,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computers and Society; I.2.0; 68T01,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NZEILG9B,journalArticle,2018,"Everitt, Tom; Lea, Gary; Hutter, Marcus",AGI Safety Literature Review,arXiv:1805.01109 [cs],,,,http://arxiv.org/abs/1805.01109,"The development of Artificial General Intelligence (AGI) promises to be a major event. Along with its many potential benefits, it also raises serious safety concerns (Bostrom, 2014). The intention of this paper is to provide an easily accessible and up-to-date collection of references for the emerging field of AGI safety. A significant number of safety problems for AGI have been identified. We list these, and survey recent research on solving them. We also cover works on how best to think of AGI from the limited knowledge we have today, predictions for when AGI will first be created, and what will happen after its creation. Finally, we review the current public policy on AGI.",2018-05-21,2022-03-09 23:19:06,2022-03-09 23:19:06,2022-03-09 23:19:06,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1805.01109,,/Users/jacquesthibodeau/Zotero/storage/VEG59QKP/Everitt et al. - 2018 - AGI Safety Literature Review.pdf; /Users/jacquesthibodeau/Zotero/storage/EM889F4G/1805.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HS7NKCD4,journalArticle,2021,"Zoe Cremer, Carla; Whittlestone, Jess",Artificial Canaries: Early Warning Signs for Anticipatory and Democratic Governance of AI,,,1989-1660,10.17863/CAM.65790,https://www.repository.cam.ac.uk/handle/1810/318673,"We propose a method for identifying early warning signs of transformative progress in artificial intelligence (AI), and discuss how these can support the anticipatory and democratic governance of AI. We call these early warning signs ‘canaries’, based on the use of canaries to provide early warnings of unsafe air pollution in coal mines. Our method combines expert elicitation and collaborative causal graphs to identify key milestones and identify the relationships between them. We present two illustrations of how this method could be used: to identify early warnings of harmful impacts of language models; and of progress towards high-level machine intelligence. Identifying early warning signs of transformative applications can support more efficient monitoring and timely regulation of progress in AI: as AI advances, its impacts on society may be too great to be governed retrospectively. It is essential that those impacted by AI have a say in how it is governed. Early warnings can give the public time and focus to influence emerging technologies using democratic, participatory technology assessments. We discuss the challenges in identifying early warning signals and propose directions for future work.",2021-01-01,2022-03-09 23:27:28,2022-03-09 23:27:28,2022-03-09 23:27:28,,,,,,,Artificial Canaries,,,,,,,en,Publisher's own licence,,,,www.repository.cam.ac.uk,,Accepted: 2021-03-12T00:30:30Z Publisher: Universidad Internacional de La Rioja,,/Users/jacquesthibodeau/Zotero/storage/6TI7TIS5/Zoe Cremer and Whittlestone - 2021 - Artificial Canaries Early Warning Signs for Antic.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CGVFQ9W5,journalArticle,2021,"Sandberg, Anders; Manheim, David",WHAT IS THE UPPER LIMIT OF VALUE?,,,,,,"How much value can our decisions create? We argue that unless our current understanding of physics is wrong in fairly fundamental ways, there exists an upper limit of value relevant to our decisions. First, due to the speed of light and the deﬁnition and conception of economic growth, the limit to economic growth is a restrictive one. Additionally, a related far larger but still ﬁnite limit exists for value in a much broader sense due to the physics of information and the ability of physical beings to place value on outcomes. We discuss how this argument can handle lexicographic preferences, probabilities, and the implications for inﬁnite ethics and ethical uncertainty.",2021,2022-03-09 23:28:18,2022-03-09 23:28:18,,24,,,,,,,,,,,,,en,,,,,Zotero,,,,/Users/jacquesthibodeau/Zotero/storage/PMTQ9HDF/Sandberg and Manheim - 2021 - WHAT IS THE UPPER LIMIT OF VALUE.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
B54WR9AZ,journalArticle,2021,"Cohen, Michael K.; Hutter, Marcus; Nanda, Neel",Fully General Online Imitation Learning,arXiv:2102.08686 [cs],,,,http://arxiv.org/abs/2102.08686,"In imitation learning, imitators and demonstrators are policies for picking actions given past interactions with the environment. If we run an imitator, we probably want events to unfold similarly to the way they would have if the demonstrator had been acting the whole time. No existing work provides formal guidance in how this might be accomplished, instead restricting focus to environments that restart, making learning unusually easy, and conveniently limiting the significance of any mistake. We address a fully general setting, in which the (stochastic) environment and demonstrator never reset, not even for training purposes. Our new conservative Bayesian imitation learner underestimates the probabilities of each available action, and queries for more data with the remaining probability. Our main result: if an event would have been unlikely had the demonstrator acted the whole time, that event's likelihood can be bounded above when running the (initially totally ignorant) imitator instead. Meanwhile, queries to the demonstrator rapidly diminish in frequency.",2021-02-17,2022-03-09 23:32:42,2022-03-09 23:32:42,2022-03-09 23:32:42,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2102.08686,,/Users/jacquesthibodeau/Zotero/storage/LFIE9KM6/Cohen et al. - 2021 - Fully General Online Imitation Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/A6K3L4JG/2102.html,,,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; I.2.0; I.2.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
H5V92LXR,journalArticle,2021,"Carey, Ryan; Langlois, Eric; Everitt, Tom; Legg, Shane",The Incentives that Shape Behaviour,arXiv:2001.07118 [cs],,,,http://arxiv.org/abs/2001.07118,"Which variables does an agent have an incentive to control with its decision, and which variables does it have an incentive to respond to? We formalise these incentives, and demonstrate unique graphical criteria for detecting them in any single decision causal influence diagram. To this end, we introduce structural causal influence models, a hybrid of the influence diagram and structural causal model frameworks. Finally, we illustrate how these incentives predict agent incentives in both fairness and AI safety applications.",2021-03-15,2022-03-09 23:32:56,2022-03-09 23:32:56,2022-03-09 23:32:56,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2001.07118,,/Users/jacquesthibodeau/Zotero/storage/UVM7AZLL/Carey et al. - 2021 - The Incentives that Shape Behaviour.pdf; /Users/jacquesthibodeau/Zotero/storage/B32JDJM5/2001.html,,,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; I.2.6; I.2.8,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A3PA2PD9,journalArticle,2021,"Everitt, Tom; Carey, Ryan; Langlois, Eric; Ortega, Pedro A.; Legg, Shane",Agent Incentives: A Causal Perspective,arXiv:2102.01685 [cs],,,,http://arxiv.org/abs/2102.01685,"We present a framework for analysing agent incentives using causal influence diagrams. We establish that a well-known criterion for value of information is complete. We propose a new graphical criterion for value of control, establishing its soundness and completeness. We also introduce two new concepts for incentive analysis: response incentives indicate which changes in the environment affect an optimal decision, while instrumental control incentives establish whether an agent can influence its utility via a variable X. For both new concepts, we provide sound and complete graphical criteria. We show by example how these results can help with evaluating the safety and fairness of an AI system.",2021-03-15,2022-03-09 23:33:13,2022-03-09 23:33:13,2022-03-09 23:33:13,,,,,,,Agent Incentives,,,,,,,,,,,,arXiv.org,,arXiv: 2102.01685,,/Users/jacquesthibodeau/Zotero/storage/LXH74N9Y/Everitt et al. - 2021 - Agent Incentives A Causal Perspective.pdf; /Users/jacquesthibodeau/Zotero/storage/S3WS3ZW2/2102.html,,,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QV2NX43N,journalArticle,2020,"Brundage, Miles; Avin, Shahar; Wang, Jasmine; Belfield, Haydn; Krueger, Gretchen; Hadfield, Gillian; Khlaaf, Heidy; Yang, Jingying; Toner, Helen; Fong, Ruth; Maharaj, Tegan; Koh, Pang Wei; Hooker, Sara; Leung, Jade; Trask, Andrew; Bluemke, Emma; Lebensold, Jonathan; O'Keefe, Cullen; Koren, Mark; Ryffel, Théo; Rubinovitz, J. B.; Besiroglu, Tamay; Carugati, Federica; Clark, Jack; Eckersley, Peter; de Haas, Sarah; Johnson, Maritza; Laurie, Ben; Ingerman, Alex; Krawczuk, Igor; Askell, Amanda; Cammarota, Rosario; Lohn, Andrew; Krueger, David; Stix, Charlotte; Henderson, Peter; Graham, Logan; Prunkl, Carina; Martin, Bianca; Seger, Elizabeth; Zilberman, Noa; hÉigeartaigh, Seán Ó; Kroeger, Frens; Sastry, Girish; Kagan, Rebecca; Weller, Adrian; Tse, Brian; Barnes, Elizabeth; Dafoe, Allan; Scharre, Paul; Herbert-Voss, Ariel; Rasser, Martijn; Sodhani, Shagun; Flynn, Carrick; Gilbert, Thomas Krendl; Dyer, Lisa; Khan, Saif; Bengio, Yoshua; Anderljung, Markus",Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims,arXiv:2004.07213 [cs],,,,http://arxiv.org/abs/2004.07213,"With the recent wave of progress in artificial intelligence (AI) has come a growing awareness of the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry and academia are insufficient to ensure responsible AI development. In order for AI developers to earn trust from system users, customers, civil society, governments, and other stakeholders that they are building AI responsibly, they will need to make verifiable claims to which they can be held accountable. Those outside of a given organization also need effective means of scrutinizing such claims. This report suggests various steps that different stakeholders can take to improve the verifiability of claims made about AI systems and their associated development processes, with a focus on providing evidence about the safety, security, fairness, and privacy protection of AI systems. We analyze ten mechanisms for this purpose--spanning institutions, software, and hardware--and make recommendations aimed at implementing, exploring, or improving those mechanisms.",2020-04-20,2022-03-09 23:33:41,2022-03-09 23:33:41,2022-03-09 23:33:41,,,,,,,Toward Trustworthy AI Development,,,,,,,,,,,,arXiv.org,,arXiv: 2004.07213,,/Users/jacquesthibodeau/Zotero/storage/CLSUZ3NQ/Brundage et al. - 2020 - Toward Trustworthy AI Development Mechanisms for .pdf; /Users/jacquesthibodeau/Zotero/storage/MJCV3PS3/2004.html,,,Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
N94L867V,journalArticle,2020,"Cohen, Michael K.; Vellambi, Badri; Hutter, Marcus",Asymptotically Unambitious Artificial General Intelligence,arXiv:1905.12186 [cs],,,,http://arxiv.org/abs/1905.12186,"General intelligence, the ability to solve arbitrary solvable problems, is supposed by many to be artificially constructible. Narrow intelligence, the ability to solve a given particularly difficult problem, has seen impressive recent development. Notable examples include self-driving cars, Go engines, image classifiers, and translators. Artificial General Intelligence (AGI) presents dangers that narrow intelligence does not: if something smarter than us across every domain were indifferent to our concerns, it would be an existential threat to humanity, just as we threaten many species despite no ill will. Even the theory of how to maintain the alignment of an AGI's goals with our own has proven highly elusive. We present the first algorithm we are aware of for asymptotically unambitious AGI, where ""unambitiousness"" includes not seeking arbitrary power. Thus, we identify an exception to the Instrumental Convergence Thesis, which is roughly that by default, an AGI would seek power, including over us.",2020-07-21,2022-03-09 23:35:06,2022-03-09 23:35:06,2022-03-09 23:35:06,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1905.12186,,/Users/jacquesthibodeau/Zotero/storage/T9VVQH77/Cohen et al. - 2020 - Asymptotically Unambitious Artificial General Inte.pdf; /Users/jacquesthibodeau/Zotero/storage/HYHW77FG/1905.html,,,"Computer Science - Artificial Intelligence; I.2.0; I.2.6; I.2.0, I.2.6",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9BS3DIFV,journalArticle,2019,"Armstrong, Stuart; Mindermann, Sören",Occam's razor is insufficient to infer the preferences of irrational agents,arXiv:1712.05812 [cs],,,,http://arxiv.org/abs/1712.05812,"Inverse reinforcement learning (IRL) attempts to infer human rewards or preferences from observed behavior. Since human planning systematically deviates from rationality, several approaches have been tried to account for specific human shortcomings. However, the general problem of inferring the reward function of an agent of unknown rationality has received little attention. Unlike the well-known ambiguity problems in IRL, this one is practically relevant but cannot be resolved by observing the agent's policy in enough environments. This paper shows (1) that a No Free Lunch result implies it is impossible to uniquely decompose a policy into a planning algorithm and reward function, and (2) that even with a reasonable simplicity prior/Occam's razor on the set of decompositions, we cannot distinguish between the true decomposition and others that lead to high regret. To address this, we need simple `normative' assumptions, which cannot be deduced exclusively from observations.",2019-01-11,2022-03-09 23:37:05,2022-03-09 23:37:05,2022-03-09 23:37:05,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1712.05812,,/Users/jacquesthibodeau/Zotero/storage/QVVBSXY6/Armstrong and Mindermann - 2019 - Occam's razor is insufficient to infer the prefere.pdf; /Users/jacquesthibodeau/Zotero/storage/X9NZ26SC/1712.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GMSS2I2A,journalArticle,2018,"Schulze, Sebastian; Evans, Owain",Active Reinforcement Learning with Monte-Carlo Tree Search,"arXiv:1803.04926 [cs, stat]",,,,http://arxiv.org/abs/1803.04926,"Active Reinforcement Learning (ARL) is a twist on RL where the agent observes reward information only if it pays a cost. This subtle change makes exploration substantially more challenging. Powerful principles in RL like optimism, Thompson sampling, and random exploration do not help with ARL. We relate ARL in tabular environments to Bayes-Adaptive MDPs. We provide an ARL algorithm using Monte-Carlo Tree Search that is asymptotically Bayes optimal. Experimentally, this algorithm is near-optimal on small Bandit problems and MDPs. On larger MDPs it outperforms a Q-learner augmented with specialised heuristics for ARL. By analysing exploration behaviour in detail, we uncover obstacles to scaling up simulation-based algorithms for ARL.",2018-03-26,2022-03-09 23:51:01,2022-03-09 23:51:01,2022-03-09 23:51:01,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1803.04926 version: 3,,/Users/jacquesthibodeau/Zotero/storage/CITUQFDJ/Schulze and Evans - 2018 - Active Reinforcement Learning with Monte-Carlo Tre.pdf; /Users/jacquesthibodeau/Zotero/storage/M8BI6MTF/1803.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SRAKWE4I,journalArticle,2018,"Martínez-Plumed, Fernando; Avin, Shahar; Brundage, Miles; Dafoe, Allan; hÉigeartaigh, Sean Ó; Hernández-Orallo, José",Accounting for the Neglected Dimensions of AI Progress,arXiv:1806.00610 [cs],,,,http://arxiv.org/abs/1806.00610,"We analyze and reframe AI progress. In addition to the prevailing metrics of performance, we highlight the usually neglected costs paid in the development and deployment of a system, including: data, expert knowledge, human oversight, software resources, computing cycles, hardware and network facilities, development time, etc. These costs are paid throughout the life cycle of an AI system, fall differentially on different individuals, and vary in magnitude depending on the replicability and generality of the AI solution. The multidimensional performance and cost space can be collapsed to a single utility metric for a user with transitive and complete preferences. Even absent a single utility function, AI advances can be generically assessed by whether they expand the Pareto (optimal) surface. We explore a subset of these neglected dimensions using the two case studies of Alpha* and ALE. This broadened conception of progress in AI should lead to novel ways of measuring success in AI, and can help set milestones for future progress.",2018-06-02,2022-03-09 23:51:04,2022-03-09 23:51:04,2022-03-09 23:51:04,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1806.00610,,/Users/jacquesthibodeau/Zotero/storage/IMHG69XE/Martínez-Plumed et al. - 2018 - Accounting for the Neglected Dimensions of AI Prog.pdf; /Users/jacquesthibodeau/Zotero/storage/NJ4S5T52/1806.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZYKYZD5S,journalArticle,2017,"Aslanides, John; Leike, Jan; Hutter, Marcus",Universal Reinforcement Learning Algorithms: Survey and Experiments,arXiv:1705.10557 [cs],,,,http://arxiv.org/abs/1705.10557,"Many state-of-the-art reinforcement learning (RL) algorithms typically assume that the environment is an ergodic Markov Decision Process (MDP). In contrast, the field of universal reinforcement learning (URL) is concerned with algorithms that make as few assumptions as possible about the environment. The universal Bayesian agent AIXI and a family of related URL algorithms have been developed in this setting. While numerous theoretical optimality results have been proven for these agents, there has been no empirical investigation of their behavior to date. We present a short and accessible survey of these URL algorithms under a unified notation and framework, along with results of some experiments that qualitatively illustrate some properties of the resulting policies, and their relative performance on partially-observable gridworld environments. We also present an open-source reference implementation of the algorithms which we hope will facilitate further understanding of, and experimentation with, these ideas.",2017-05-30,2022-03-09 23:51:50,2022-03-09 23:51:50,2022-03-09 23:51:50,,,,,,,Universal Reinforcement Learning Algorithms,,,,,,,,,,,,arXiv.org,,arXiv: 1705.10557,,/Users/jacquesthibodeau/Zotero/storage/VGWAU6RD/Aslanides et al. - 2017 - Universal Reinforcement Learning Algorithms Surve.pdf; /Users/jacquesthibodeau/Zotero/storage/RYKPWQKB/1705.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
22F863J6,journalArticle,2017,"Bostrom, Nick","Strategic Implications of Openness in <span style=""font-variant:small-caps;"">AI</span> Development",Global Policy,,"1758-5880, 1758-5899",10.1111/1758-5899.12403,https://onlinelibrary.wiley.com/doi/10.1111/1758-5899.12403,"This paper attempts a preliminary analysis of the global desirability of different forms of openness in AI development (including openness about source code, science, data, safety techniques, capabilities, and goals). Short-term impacts of increased openness appear mostly socially beneﬁcial in expectation. The strategic implications of medium and long-term impacts are complex. The evaluation of long-term impacts, in particular, may depend on whether the objective is to beneﬁt the present generation or to promote a time-neutral aggregate of well-being of future generations. Some forms of openness are plausibly positive on both counts (openness about safety measures, openness about goals). Others (openness about source code, science, and possibly capability) could lead to a tightening of the competitive situation around the time of the introduction of advanced AI, increasing the probability that winning the AI race is incompatible with using any safety method that incurs a delay or limits performance. We identify several key factors that must be taken into account by any well-founded opinion on the matter.",2017-05,2022-03-09 23:52:15,2022-03-09 23:52:15,2022-03-09 23:52:15,135-148,,2,8,,Glob Policy,"Strategic Implications of Openness in <span style=""font-variant",,,,,,,en,,,,,DOI.org (Crossref),,,,/Users/jacquesthibodeau/Zotero/storage/B6ZGD7GK/Bostrom - 2017 - Strategic Implications of Openness in span style=.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SBHN6IDW,journalArticle,2021,"Hammond, Lewis; Fox, James; Everitt, Tom; Abate, Alessandro; Wooldridge, Michael",Equilibrium Refinements for Multi-Agent Influence Diagrams: Theory and Practice,arXiv:2102.05008 [cs],,,,http://arxiv.org/abs/2102.05008,"Multi-agent influence diagrams (MAIDs) are a popular form of graphical model that, for certain classes of games, have been shown to offer key complexity and explainability advantages over traditional extensive form game (EFG) representations. In this paper, we extend previous work on MAIDs by introducing the concept of a MAID subgame, as well as subgame perfect and trembling hand perfect equilibrium refinements. We then prove several equivalence results between MAIDs and EFGs. Finally, we describe an open source implementation for reasoning about MAIDs and computing their equilibria.",2021-02-09,2022-03-09 23:52:51,2022-03-09 23:52:51,2022-03-09 23:52:51,,,,,,,Equilibrium Refinements for Multi-Agent Influence Diagrams,,,,,,,,,,,,arXiv.org,,arXiv: 2102.05008,,/Users/jacquesthibodeau/Zotero/storage/ZNTEMY45/Hammond et al. - 2021 - Equilibrium Refinements for Multi-Agent Influence .pdf; /Users/jacquesthibodeau/Zotero/storage/UF69D7DV/2102.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RWN45RJI,journalArticle,2020,"Dafoe, Allan; Hughes, Edward; Bachrach, Yoram; Collins, Tantum; McKee, Kevin R.; Leibo, Joel Z.; Larson, Kate; Graepel, Thore",Open Problems in Cooperative AI,arXiv:2012.08630 [cs],,,,http://arxiv.org/abs/2012.08630,"Problems of cooperation--in which agents seek ways to jointly improve their welfare--are ubiquitous and important. They can be found at scales ranging from our daily routines--such as driving on highways, scheduling meetings, and working collaboratively--to our global challenges--such as peace, commerce, and pandemic preparedness. Arguably, the success of the human species is rooted in our ability to cooperate. Since machines powered by artificial intelligence are playing an ever greater role in our lives, it will be important to equip them with the capabilities necessary to cooperate and to foster cooperation. We see an opportunity for the field of artificial intelligence to explicitly focus effort on this class of problems, which we term Cooperative AI. The objective of this research would be to study the many aspects of the problems of cooperation and to innovate in AI to contribute to solving these problems. Central goals include building machine agents with the capabilities needed for cooperation, building tools to foster cooperation in populations of (machine and/or human) agents, and otherwise conducting AI research for insight relevant to problems of cooperation. This research integrates ongoing work on multi-agent systems, game theory and social choice, human-machine interaction and alignment, natural-language processing, and the construction of social tools and platforms. However, Cooperative AI is not the union of these existing areas, but rather an independent bet about the productivity of specific kinds of conversations that involve these and other areas. We see opportunity to more explicitly focus on the problem of cooperation, to construct unified theory and vocabulary, and to build bridges with adjacent communities working on cooperation, including in the natural, social, and behavioural sciences.",2020-12-15,2022-03-09 23:53:01,2022-03-09 23:53:01,2022-03-09 23:53:01,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2012.08630,,/Users/jacquesthibodeau/Zotero/storage/HGX8IQXG/Dafoe et al. - 2020 - Open Problems in Cooperative AI.pdf; /Users/jacquesthibodeau/Zotero/storage/S4BURQM3/2012.html,,,Computer Science - Artificial Intelligence; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XK4AR4CU,journalArticle,2021,"Zhang, Baobao; Anderljung, Markus; Kahn, Lauren; Dreksler, Noemi; Horowitz, Michael C.; Dafoe, Allan",Ethics and Governance of Artificial Intelligence: Evidence from a Survey of Machine Learning Researchers,arXiv:2105.02117 [cs],,,,http://arxiv.org/abs/2105.02117,"Machine learning (ML) and artificial intelligence (AI) researchers play an important role in the ethics and governance of AI, including taking action against what they perceive to be unethical uses of AI (Belfield, 2020; Van Noorden, 2020). Nevertheless, this influential group's attitudes are not well understood, which undermines our ability to discern consensuses or disagreements between AI/ML researchers. To examine these researchers' views, we conducted a survey of those who published in the top AI/ML conferences (N = 524). We compare these results with those from a 2016 survey of AI/ML researchers (Grace, Salvatier, Dafoe, Zhang, & Evans, 2018) and a 2018 survey of the US public (Zhang & Dafoe, 2020). We find that AI/ML researchers place high levels of trust in international organizations and scientific organizations to shape the development and use of AI in the public interest; moderate trust in most Western tech companies; and low trust in national militaries, Chinese tech companies, and Facebook. While the respondents were overwhelmingly opposed to AI/ML researchers working on lethal autonomous weapons, they are less opposed to researchers working on other military applications of AI, particularly logistics algorithms. A strong majority of respondents think that AI safety research should be prioritized and that ML institutions should conduct pre-publication review to assess potential harms. Being closer to the technology itself, AI/ML re-searchers are well placed to highlight new risks and develop technical solutions, so this novel attempt to measure their attitudes has broad relevance. The findings should help to improve how researchers, private sector executives, and policymakers think about regulations, governance frameworks, guiding principles, and national and international governance strategies for AI.",2021-05-05,2022-03-09 23:54:19,2022-03-09 23:54:19,2022-03-09 23:54:19,,,,,,,Ethics and Governance of Artificial Intelligence,,,,,,,,,,,,arXiv.org,,arXiv: 2105.02117,,/Users/jacquesthibodeau/Zotero/storage/J7PD2QEY/Zhang et al. - 2021 - Ethics and Governance of Artificial Intelligence .pdf; /Users/jacquesthibodeau/Zotero/storage/W4NM5IWV/2105.html,,,Computer Science - Computers and Society; K.7.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6I9Z4FC6,journalArticle,2018,"Dafoe, Allan; Affairs, Journal of International",GLOBAL POLITICS AND THE GOVERNANCE OF ARTIFICIAL INTELLIGENCE,Journal of International Affairs,,0022-197X,,https://www.jstor.org/stable/26588347,,2018,2022-03-09 23:55:09,2022-03-09 23:55:09,2022-03-09 23:55:09,121-126,,1,72,,,,,,,,,,,,,,,JSTOR,,Publisher: Journal of International Affairs Editorial Board,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VXYAW24L,journalArticle,2021,"Hubinger, Evan; van Merwijk, Chris; Mikulik, Vladimir; Skalse, Joar; Garrabrant, Scott",Risks from Learned Optimization in Advanced Machine Learning Systems,arXiv:1906.01820 [cs],,,,http://arxiv.org/abs/1906.01820,"We analyze the type of learned optimization that occurs when a learned model (such as a neural network) is itself an optimizer - a situation we refer to as mesa-optimization, a neologism we introduce in this paper. We believe that the possibility of mesa-optimization raises two important questions for the safety and transparency of advanced machine learning systems. First, under what circumstances will learned models be optimizers, including when they should not be? Second, when a learned model is an optimizer, what will its objective be - how will it differ from the loss function it was trained under - and how can it be aligned? In this paper, we provide an in-depth analysis of these two primary questions and provide an overview of topics for future research.",2021-12-01,2022-03-09 23:55:28,2022-03-11 01:38:51,2022-03-09 23:55:28,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1906.01820,,/Users/jacquesthibodeau/Zotero/storage/WQW7ZA5U/Hubinger et al. - 2021 - Risks from Learned Optimization in Advanced Machin.pdf; /Users/jacquesthibodeau/Zotero/storage/9EA6PSD4/Hubinger et al. - 2021 - Risks from Learned Optimization in Advanced Machin.pdf; /Users/jacquesthibodeau/Zotero/storage/U9S3RLJX/Hubinger et al. - 2021 - Risks from Learned Optimization in Advanced Machin.pdf; /Users/jacquesthibodeau/Zotero/storage/BN4RGP95/Hubinger et al. - 2021 - Risks from Learned Optimization in Advanced Machin.pdf; /Users/jacquesthibodeau/Zotero/storage/EKJ56UWQ/1906.html; /Users/jacquesthibodeau/Zotero/storage/6DS7CBXW/1906.html; /Users/jacquesthibodeau/Zotero/storage/3IA3S8WE/1906.html; /Users/jacquesthibodeau/Zotero/storage/GDJR6XCV/1906.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BMNPC7NB,journalArticle,2021,"Garrabrant, Scott",Temporal Inference with Finite Factored Sets,"arXiv:2109.11513 [cs, math]",,,,http://arxiv.org/abs/2109.11513,"We propose a new approach to temporal inference, inspired by the Pearlian causal inference paradigm - though quite different from Pearl's approach formally. Rather than using directed acyclic graphs, we make use of factored sets, which are sets expressed as Cartesian products. We show that finite factored sets are powerful tools for inferring temporal relations. We introduce an analog of d-separation for factored sets, conditional orthogonality, and we demonstrate that this notion is equivalent to conditional independence in all probability distributions on a finite factored set.",2021-09-23,2022-03-10 13:38:50,2022-03-10 13:38:50,2022-03-10 13:38:50,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2109.11513,,/Users/jacquesthibodeau/Zotero/storage/FWFIXGY4/Garrabrant - 2021 - Temporal Inference with Finite Factored Sets.pdf; /Users/jacquesthibodeau/Zotero/storage/8BDWE93V/2109.html,,,Computer Science - Artificial Intelligence; Mathematics - Probability; Mathematics - Combinatorics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5A7SVW48,journalArticle,2021,"Galaz, Victor; Centeno, Miguel A.; Callahan, Peter W.; Causevic, Amar; Patterson, Thayer; Brass, Irina; Baum, Seth; Farber, Darryl; Fischer, Joern; Garcia, David; McPhearson, Timon; Jimenez, Daniel; King, Brian; Larcey, Paul; Levy, Karen","Artificial intelligence, systemic risks, and sustainability",Technology in Society,,0160-791X,10.1016/j.techsoc.2021.101741,https://www.sciencedirect.com/science/article/pii/S0160791X21002165,"Automated decision making and predictive analytics through artificial intelligence, in combination with rapid progress in technologies such as sensor technology and robotics are likely to change the way individuals, communities, governments and private actors perceive and respond to climate and ecological change. Methods based on various forms of artificial intelligence are already today being applied in a number of research fields related to climate change and environmental monitoring. Investments into applications of these technologies in agriculture, forestry and the extraction of marine resources also seem to be increasing rapidly. Despite a growing interest in, and deployment of AI-technologies in domains critical for sustainability, few have explored possible systemic risks in depth. This article offers a global overview of the progress of such technologies in sectors with high impact potential for sustainability like farming, forestry and the extraction of marine resources. We also identify possible systemic risks in these domains including a) algorithmic bias and allocative harms; b) unequal access and benefits; c) cascading failures and external disruptions, and d) trade-offs between efficiency and resilience. We explore these emerging risks, identify critical questions, and discuss the limitations of current governance mechanisms in addressing AI sustainability risks in these sectors.",2021-11-01,2022-03-10 16:29:12,2022-03-10 16:29:12,2022-03-10 16:29:12,101741,,,67,,Technology in Society,,,,,,,,en,,,,,ScienceDirect,,,,"/Users/jacquesthibodeau/Zotero/storage/PD366UDT/Galaz et al. - 2021 - Artificial intelligence, systemic risks, and susta.pdf",,,Artificial intelligence; Anthropocene; Automation; Climate change; Digitalization; Resilience; Social-ecological systems; Sustainability; Systemic risks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZFFZBUY6,journalArticle,2021,"Dean, Sarah; Gilbert, Thomas Krendl; Lambert, Nathan; Zick, Tom",Axes for Sociotechnical Inquiry in AI Research,IEEE Transactions on Technology and Society,,2637-6415,10.1109/TTS.2021.3074097,http://arxiv.org/abs/2105.06551,"The development of artificial intelligence (AI) technologies has far exceeded the investigation of their relationship with society. Sociotechnical inquiry is needed to mitigate the harms of new technologies whose potential impacts remain poorly understood. To date, subfields of AI research develop primarily individual views on their relationship with sociotechnics, while tools for external investigation, comparison, and cross-pollination are lacking. In this paper, we propose four directions for inquiry into new and evolving areas of technological development: value--what progress and direction does a field promote, optimization--how the defined system within a problem formulation relates to broader dynamics, consensus--how agreement is achieved and who is included in building it, and failure--what methods are pursued when the problem specification is found wanting. The paper provides a lexicon for sociotechnical inquiry and illustrates it through the example of consumer drone technology.",2021-06,2022-03-10 20:28:17,2022-03-10 20:28:17,2022-03-10 20:28:17,62-70,,2,2,,IEEE Trans. Technol. Soc.,,,,,,,,,,,,,arXiv.org,,arXiv: 2105.06551,,/Users/jacquesthibodeau/Zotero/storage/X5WI5WWL/Dean et al. - 2021 - Axes for Sociotechnical Inquiry in AI Research.pdf; /Users/jacquesthibodeau/Zotero/storage/5Z89A5Y2/2105.html,,,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HGJPUHJK,journalArticle,2020,"Gabriel, Iason","Artificial Intelligence, Values and Alignment",Minds and Machines,,"0924-6495, 1572-8641",10.1007/s11023-020-09539-2,http://arxiv.org/abs/2001.09768,"This paper looks at philosophical questions that arise in the context of AI alignment. It defends three propositions. First, normative and technical aspects of the AI alignment problem are interrelated, creating space for productive engagement between people working in both domains. Second, it is important to be clear about the goal of alignment. There are significant differences between AI that aligns with instructions, intentions, revealed preferences, ideal preferences, interests and values. A principle-based approach to AI alignment, which combines these elements in a systematic way, has considerable advantages in this context. Third, the central challenge for theorists is not to identify 'true' moral principles for AI; rather, it is to identify fair principles for alignment, that receive reflective endorsement despite widespread variation in people's moral beliefs. The final part of the paper explores three ways in which fair principles for AI alignment could potentially be identified.",2020-09,2022-03-10 20:28:34,2022-03-10 20:28:34,2022-03-10 20:28:34,411-437,,3,30,,Minds & Machines,,,,,,,,,,,,,arXiv.org,,arXiv: 2001.09768,,"/Users/jacquesthibodeau/Zotero/storage/ZQGRH8SN/Gabriel - 2020 - Artificial Intelligence, Values and Alignment.pdf; /Users/jacquesthibodeau/Zotero/storage/L5J2SN95/2001.html",,,Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MJBVI3Q7,journalArticle,2021,"Cave, Stephen; Whittlestone, Jess; Nyrup, Rune; hEigeartaigh, Sean O.; Calvo, Rafael A.",Using AI ethically to tackle covid-19,BMJ,,1756-1833,10.1136/bmj.n364,https://www.bmj.com/content/372/bmj.n364,"<p>Taking a principled approach is crucial to the successful use of AI in pandemic management, say <b>Stephen Cave and colleagues</b></p>",2021-03-16,2022-03-10 20:29:42,2022-03-10 20:29:42,2022-03-10 20:29:42,n364,,,372,,BMJ,,,,,,,,en,"Published by the BMJ Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://group.bmj.com/group/rights-licensing/permissions. This is an Open Access article distributed under the terms of the Creative Commons Attribution IGO License (https://creativecommons.org/licenses/by-nc/3.0/igo/), which permits use, distribution, and reproduction for non-commercial purposes in any medium, provided the original work is properly cited.",,,,www.bmj.com,,Publisher: British Medical Journal Publishing Group Section: Analysis PMID: 33722807,,/Users/jacquesthibodeau/Zotero/storage/JASMH8AC/Cave et al. - 2021 - Using AI ethically to tackle covid-19.pdf;,http://www.ncbi.nlm.nih.gov/pubmed/33722807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
J4LHQRIY,journalArticle,2019,"Solaiman, Irene; Brundage, Miles; Clark, Jack; Askell, Amanda; Herbert-Voss, Ariel; Wu, Jeff; Radford, Alec; Krueger, Gretchen; Kim, Jong Wook; Kreps, Sarah; McCain, Miles; Newhouse, Alex; Blazakis, Jason; McGuffie, Kris; Wang, Jasmine",Release Strategies and the Social Impacts of Language Models,arXiv:1908.09203 [cs],,,,http://arxiv.org/abs/1908.09203,"Large language models have a range of beneficial uses: they can assist in prose, poetry, and programming; analyze dataset biases; and more. However, their flexibility and generative capabilities also raise misuse concerns. This report discusses OpenAI's work related to the release of its GPT-2 language model. It discusses staged release, which allows time between model releases to conduct risk and benefit analyses as model sizes increased. It also discusses ongoing partnership-based research and provides recommendations for better coordination and responsible publication in AI.",2019-11-12,2022-03-10 20:36:37,2022-03-10 20:36:37,2022-03-10 20:36:37,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1908.09203,,/Users/jacquesthibodeau/Zotero/storage/B5GSSJW7/Solaiman et al. - 2019 - Release Strategies and the Social Impacts of Langu.pdf; /Users/jacquesthibodeau/Zotero/storage/HW9ZEYIY/1908.html,,,Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Computer Science - Computation and Language; I.2; I.2.7; K.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EY8JYQMC,journalArticle,2021,"Ramesh, Aditya; Pavlov, Mikhail; Goh, Gabriel; Gray, Scott; Voss, Chelsea; Radford, Alec; Chen, Mark; Sutskever, Ilya",Zero-Shot Text-to-Image Generation,arXiv:2102.12092 [cs],,,,http://arxiv.org/abs/2102.12092,"Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.",2021-02-26,2022-03-10 20:36:52,2022-03-10 20:36:52,2022-03-10 20:36:52,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2102.12092,,/Users/jacquesthibodeau/Zotero/storage/BU725VYB/Ramesh et al. - 2021 - Zero-Shot Text-to-Image Generation.pdf; /Users/jacquesthibodeau/Zotero/storage/AJLMCF72/2102.html,,,Computer Science - Machine Learning; Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IUYBC4HD,journalArticle,2021,"Chen, Mark; Tworek, Jerry; Jun, Heewoo; Yuan, Qiming; Pinto, Henrique Ponde de Oliveira; Kaplan, Jared; Edwards, Harri; Burda, Yuri; Joseph, Nicholas; Brockman, Greg; Ray, Alex; Puri, Raul; Krueger, Gretchen; Petrov, Michael; Khlaaf, Heidy; Sastry, Girish; Mishkin, Pamela; Chan, Brooke; Gray, Scott; Ryder, Nick; Pavlov, Mikhail; Power, Alethea; Kaiser, Lukasz; Bavarian, Mohammad; Winter, Clemens; Tillet, Philippe; Such, Felipe Petroski; Cummings, Dave; Plappert, Matthias; Chantzis, Fotios; Barnes, Elizabeth; Herbert-Voss, Ariel; Guss, William Hebgen; Nichol, Alex; Paino, Alex; Tezak, Nikolas; Tang, Jie; Babuschkin, Igor; Balaji, Suchir; Jain, Shantanu; Saunders, William; Hesse, Christopher; Carr, Andrew N.; Leike, Jan; Achiam, Josh; Misra, Vedant; Morikawa, Evan; Radford, Alec; Knight, Matthew; Brundage, Miles; Murati, Mira; Mayer, Katie; Welinder, Peter; McGrew, Bob; Amodei, Dario; McCandlish, Sam; Sutskever, Ilya; Zaremba, Wojciech",Evaluating Large Language Models Trained on Code,arXiv:2107.03374 [cs],,,,http://arxiv.org/abs/2107.03374,"We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.",2021-07-14,2022-03-10 20:38:12,2022-03-10 20:38:12,2022-03-10 20:38:12,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2107.03374,,/Users/jacquesthibodeau/Zotero/storage/BUBKA67E/Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf; /Users/jacquesthibodeau/Zotero/storage/RRVHPXPL/2107.html,,,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TSEAGFB6,journalArticle,2016,"Brockman, Greg; Cheung, Vicki; Pettersson, Ludwig; Schneider, Jonas; Schulman, John; Tang, Jie; Zaremba, Wojciech",OpenAI Gym,arXiv:1606.01540 [cs],,,,http://arxiv.org/abs/1606.01540,"OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.",2016-06-05,2022-03-10 20:38:24,2022-03-10 20:38:24,2022-03-10 20:38:24,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1606.01540,,/Users/jacquesthibodeau/Zotero/storage/2C58UTVC/Brockman et al. - 2016 - OpenAI Gym.pdf; /Users/jacquesthibodeau/Zotero/storage/MXYQWT9G/1606.html,,,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
G39HHA46,journalArticle,2020,"Brown, Tom B.; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Gretchen; Henighan, Tom; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel M.; Wu, Jeffrey; Winter, Clemens; Hesse, Christopher; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Scott; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario",Language Models are Few-Shot Learners,arXiv:2005.14165 [cs],,,,http://arxiv.org/abs/2005.14165,"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",2020-07-22,2022-03-10 20:38:32,2022-03-11 01:36:43,2022-03-10 20:38:32,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2005.14165,,/Users/jacquesthibodeau/Zotero/storage/PUXYRW8X/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf; /Users/jacquesthibodeau/Zotero/storage/EJCL7RHC/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf; /Users/jacquesthibodeau/Zotero/storage/74LHPBWZ/2005.html; /Users/jacquesthibodeau/Zotero/storage/SARDP4KI/2005.html; /Users/jacquesthibodeau/Zotero/storage/9HH92G2G/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf; /Users/jacquesthibodeau/Zotero/storage/PP7AKKE5/2005.html,,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YRKHJSNG,journalArticle,2021,"Wu, Jeff; Ouyang, Long; Ziegler, Daniel M.; Stiennon, Nisan; Lowe, Ryan; Leike, Jan; Christiano, Paul",Recursively Summarizing Books with Human Feedback,arXiv:2109.10862 [cs],,,,http://arxiv.org/abs/2109.10862,"A major challenge for scaling machine learning is training models to perform tasks that are very difficult or time-consuming for humans to evaluate. We present progress on this problem on the task of abstractive summarization of entire fiction novels. Our method combines learning from human feedback with recursive task decomposition: we use models trained on smaller parts of the task to assist humans in giving feedback on the broader task. We collect a large volume of demonstrations and comparisons from human labelers, and fine-tune GPT-3 using behavioral cloning and reward modeling to do summarization recursively. At inference time, the model first summarizes small sections of the book and then recursively summarizes these summaries to produce a summary of the entire book. Our human labelers are able to supervise and evaluate the models quickly, despite not having read the entire books themselves. Our resulting model generates sensible summaries of entire books, even matching the quality of human-written summaries in a few cases ($\sim5\%$ of books). We achieve state-of-the-art results on the recent BookSum dataset for book-length summarization. A zero-shot question-answering model using these summaries achieves state-of-the-art results on the challenging NarrativeQA benchmark for answering questions about books and movie scripts. We release datasets of samples from our model.",2021-09-27,2022-03-10 20:38:53,2022-03-10 20:38:53,2022-03-10 20:38:53,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2109.10862,,/Users/jacquesthibodeau/Zotero/storage/SGPNRN2J/Wu et al. - 2021 - Recursively Summarizing Books with Human Feedback.pdf; /Users/jacquesthibodeau/Zotero/storage/6G3CZCAV/2109.html,,,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A2P76UVF,journalArticle,2021,"Cammarata, Nick; Goh, Gabriel; Carter, Shan; Voss, Chelsea; Schubert, Ludwig; Olah, Chris",Curve Circuits,Distill,,2476-0757,10.23915/distill.00024.006,https://distill.pub/2020/circuits/curve-circuits,Reverse engineering the curve detection algorithm from InceptionV1 and reimplementing it from scratch.,2021-01-30,2022-03-10 20:42:58,2022-03-10 20:42:58,2022-03-10 20:42:58,e00024.006,,1,6,,Distill,,,,,,,,en,,,,,distill.pub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LNB8SL9F,journalArticle,2021,"Welbl, Johannes; Glaese, Amelia; Uesato, Jonathan; Dathathri, Sumanth; Mellor, John; Hendricks, Lisa Anne; Anderson, Kirsty; Kohli, Pushmeet; Coppin, Ben; Huang, Po-Sen",Challenges in Detoxifying Language Models,arXiv:2109.07445 [cs],,,,http://arxiv.org/abs/2109.07445,"Large language models (LM) generate remarkably fluent text and can be efficiently adapted across NLP tasks. Measuring and guaranteeing the quality of generated text in terms of safety is imperative for deploying LMs in the real world; to this end, prior work often relies on automatic evaluation of LM toxicity. We critically discuss this approach, evaluate several toxicity mitigation strategies with respect to both automatic and human evaluation, and analyze consequences of toxicity mitigation in terms of model bias and LM quality. We demonstrate that while basic intervention strategies can effectively optimize previously established automatic metrics on the RealToxicityPrompts dataset, this comes at the cost of reduced LM coverage for both texts about, and dialects of, marginalized groups. Additionally, we find that human raters often disagree with high automatic toxicity scores after strong toxicity reduction interventions -- highlighting further the nuances involved in careful evaluation of LM toxicity.",2021-09-15,2022-03-10 20:43:48,2022-03-10 20:43:48,2022-03-10 20:43:48,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2109.07445,,/Users/jacquesthibodeau/Zotero/storage/U3DAT2NL/Welbl et al. - 2021 - Challenges in Detoxifying Language Models.pdf; /Users/jacquesthibodeau/Zotero/storage/CJ49LCNY/2109.html,,,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computers and Society; I.2.6; Computer Science - Computation and Language; I.2.7,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
G8SBLG2B,journalArticle,2021,"Open Ended Learning Team; Stooke, Adam; Mahajan, Anuj; Barros, Catarina; Deck, Charlie; Bauer, Jakob; Sygnowski, Jakub; Trebacz, Maja; Jaderberg, Max; Mathieu, Michael; McAleese, Nat; Bradley-Schmieg, Nathalie; Wong, Nathaniel; Porcel, Nicolas; Raileanu, Roberta; Hughes-Fitt, Steph; Dalibard, Valentin; Czarnecki, Wojciech Marian",Open-Ended Learning Leads to Generally Capable Agents,arXiv:2107.12808 [cs],,,,http://arxiv.org/abs/2107.12808,"In this work we create agents that can perform well beyond a single, individual task, that exhibit much wider generalisation of behaviour to a massive, rich space of challenges. We define a universe of tasks within an environment domain and demonstrate the ability to train agents that are generally capable across this vast space and beyond. The environment is natively multi-agent, spanning the continuum of competitive, cooperative, and independent games, which are situated within procedurally generated physical 3D worlds. The resulting space is exceptionally diverse in terms of the challenges posed to agents, and as such, even measuring the learning progress of an agent is an open research problem. We propose an iterative notion of improvement between successive generations of agents, rather than seeking to maximise a singular objective, allowing us to quantify progress despite tasks being incomparable in terms of achievable rewards. We show that through constructing an open-ended learning process, which dynamically changes the training task distributions and training objectives such that the agent never stops learning, we achieve consistent learning of new behaviours. The resulting agent is able to score reward in every one of our humanly solvable evaluation levels, with behaviour generalising to many held-out points in the universe of tasks. Examples of this zero-shot generalisation include good performance on Hide and Seek, Capture the Flag, and Tag. Through analysis and hand-authored probe tasks we characterise the behaviour of our agent, and find interesting emergent heuristic behaviours such as trial-and-error experimentation, simple tool use, option switching, and cooperation. Finally, we demonstrate that the general capabilities of this agent could unlock larger scale transfer of behaviour through cheap finetuning.",2021-07-31,2022-03-10 20:44:15,2022-03-11 01:39:33,2022-03-10 20:44:15,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2107.12808,,/Users/jacquesthibodeau/Zotero/storage/LH8MJ9MJ/Open Ended Learning Team et al. - 2021 - Open-Ended Learning Leads to Generally Capable Age.pdf; /Users/jacquesthibodeau/Zotero/storage/EED6JN7B/Open Ended Learning Team et al. - 2021 - Open-Ended Learning Leads to Generally Capable Age.pdf; /Users/jacquesthibodeau/Zotero/storage/7XJS5P2B/2107.html; /Users/jacquesthibodeau/Zotero/storage/TX886NWC/2107.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
68F2QNJ2,journalArticle,2021,"Gabriel, Iason",Towards a Theory of Justice for Artificial Intelligence,arXiv:2110.14419 [cs],,,,http://arxiv.org/abs/2110.14419,"This paper explores the relationship between artificial intelligence and principles of distributive justice. Drawing upon the political philosophy of John Rawls, it holds that the basic structure of society should be understood as a composite of socio-technical systems, and that the operation of these systems is increasingly shaped and influenced by AI. As a consequence, egalitarian norms of justice apply to the technology when it is deployed in these contexts. These norms entail that the relevant AI systems must meet a certain standard of public justification, support citizens rights, and promote substantively fair outcomes -- something that requires specific attention be paid to the impact they have on the worst-off members of society.",2021-10-27,2022-03-10 20:44:24,2022-03-10 20:44:24,2022-03-10 20:44:24,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2110.14419,,/Users/jacquesthibodeau/Zotero/storage/DMXBXRGB/Gabriel - 2021 - Towards a Theory of Justice for Artificial Intelli.pdf; /Users/jacquesthibodeau/Zotero/storage/DGJX6FK5/2110.html,,,Computer Science - Computers and Society; K.4.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PF74H37Z,journalArticle,2022,"Alex, Neel; Lifland, Eli; Tunstall, Lewis; Thakur, Abhishek; Maham, Pegah; Riedel, C. Jess; Hine, Emmie; Ashurst, Carolyn; Sedille, Paul; Carlier, Alexis; Noetel, Michael; Stuhlmüller, Andreas",RAFT: A Real-World Few-Shot Text Classification Benchmark,arXiv:2109.14076 [cs],,,,http://arxiv.org/abs/2109.14076,"Large pre-trained language models have shown promise for few-shot learning, completing text-based tasks given only a few task-specific examples. Will models soon solve classification tasks that have so far been reserved for human research assistants? Existing benchmarks are not designed to measure progress in applied settings, and so don't directly answer this question. The RAFT benchmark (Real-world Annotated Few-shot Tasks) focuses on naturally occurring tasks and uses an evaluation setup that mirrors deployment. Baseline evaluations on RAFT reveal areas current techniques struggle with: reasoning over long texts and tasks with many classes. Human baselines show that some classification tasks are difficult for non-expert humans, reflecting that real-world value sometimes depends on domain expertise. Yet even non-expert human baseline F1 scores exceed GPT-3 by an average of 0.11. The RAFT datasets and leaderboard will track which model improvements translate into real-world benefits at https://raft.elicit.org .",2022-01-18,2022-03-10 20:50:10,2022-03-11 01:37:14,2022-03-10 20:50:10,,,,,,,RAFT,,,,,,,,,,,,arXiv.org,,arXiv: 2109.14076,,/Users/jacquesthibodeau/Zotero/storage/NSWL9BX4/Alex et al. - 2022 - RAFT A Real-World Few-Shot Text Classification Be.pdf; /Users/jacquesthibodeau/Zotero/storage/7AWPQDMG/2109.html; /Users/jacquesthibodeau/Zotero/storage/F27E5UKQ/Alex et al. - 2021 - RAFT A Real-World Few-Shot Text Classification Be.pdf; /Users/jacquesthibodeau/Zotero/storage/6QYMUHHM/Alex et al. - 2021 - RAFT A Real-World Few-Shot Text Classification Be.pdf; /Users/jacquesthibodeau/Zotero/storage/WLVHVW4Z/2109.html; /Users/jacquesthibodeau/Zotero/storage/DKJU8Z4U/2109.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
G82RS6Z9,journalArticle,2021,"Oesterheld, Caspar; Conitzer, Vincent",Safe Pareto Improvements for Delegated Game Playing,,,,,,"A set of players delegate playing a game to a set of representatives, one for each player. We imagine that each player trusts their respective representative’s strategic abilities. Thus, we might imagine that per default, the original players would simply instruct the representatives to play the original game as best as they can. In this paper, we ask: are there safe Pareto improvements on this default way of giving instructions? That is, we imagine that the original players can coordinate to tell their representatives to only consider some subset of the available strategies and to assign utilities to outcomes differently than the original players. Then can the original players do this in such a way that the payoff is guaranteed to be weakly higher than under the default instructions for all the original players? In particular, can they Pareto-improve without probabilistic assumptions about how the representatives play games? In this paper, we give some examples of safe Pareto improvements. We prove that the notion of safe Pareto improvements is closely related to a notion of outcome correspondence between games. We also show that under some specific assumptions about how the representatives play games, finding safe Pareto improvements is NP-complete.",2021,2022-03-10 20:52:27,2022-03-10 20:52:27,,17,,,,,,,,,,,,,en,,,,,Zotero,,,,/Users/jacquesthibodeau/Zotero/storage/3R3K7SK6/Oesterheld and Conitzer - 2021 - Safe Pareto Improvements for Delegated Game Playin.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ALFSA5Z4,journalArticle,2021,"Hendrycks, Dan; Burns, Collin; Basart, Steven; Critch, Andrew; Li, Jerry; Song, Dawn; Steinhardt, Jacob",Aligning AI With Shared Human Values,arXiv:2008.02275 [cs],,,,http://arxiv.org/abs/2008.02275,"We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the ETHICS dataset, we find that current language models have a promising but incomplete ability to predict basic human ethical judgements. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values.",2021-07-24,2022-03-10 20:55:48,2022-03-10 20:55:48,2022-03-10 20:55:48,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2008.02275,,/Users/jacquesthibodeau/Zotero/storage/MGAGEE3K/Hendrycks et al. - 2021 - Aligning AI With Shared Human Values.pdf; /Users/jacquesthibodeau/Zotero/storage/DKDNH6A4/2008.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Computers and Society; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GYISCUUW,journalArticle,2021,"Hernandez, Danny; Kaplan, Jared; Henighan, Tom; McCandlish, Sam",Scaling Laws for Transfer,arXiv:2102.01293 [cs],,,,http://arxiv.org/abs/2102.01293,"We study empirical scaling laws for transfer learning between distributions in an unsupervised, fine-tuning setting. When we train increasingly large neural networks from-scratch on a fixed-size dataset, they eventually become data-limited and stop improving in performance (cross-entropy loss). When we do the same for models pre-trained on a large language dataset, the slope in performance gains is merely reduced rather than going to zero. We calculate the effective data ""transferred"" from pre-training by determining how much data a transformer of the same size would have required to achieve the same loss when training from scratch. In other words, we focus on units of data while holding everything else fixed. We find that the effective data transferred is described well in the low data regime by a power-law of parameter count and fine-tuning dataset size. We believe the exponents in these power-laws correspond to measures of the generality of a model and proximity of distributions (in a directed rather than symmetric sense). We find that pre-training effectively multiplies the fine-tuning dataset size. Transfer, like overall performance, scales predictably in terms of parameters, data, and compute.",2021-02-01,2022-03-10 21:18:27,2022-03-10 21:18:27,2022-03-10 21:18:27,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2102.01293,,/Users/jacquesthibodeau/Zotero/storage/9VGIJ9L5/Hernandez et al. - 2021 - Scaling Laws for Transfer.pdf; /Users/jacquesthibodeau/Zotero/storage/788QIVAZ/2102.html,,,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EW6QZRN5,journalArticle,2021,"Mu, Jesse; Andreas, Jacob",Compositional Explanations of Neurons,"arXiv:2006.14032 [cs, stat]",,,,http://arxiv.org/abs/2006.14032,"We describe a procedure for explaining neurons in deep representations by identifying compositional logical concepts that closely approximate neuron behavior. Compared to prior work that uses atomic labels as explanations, analyzing neurons compositionally allows us to more precisely and expressively characterize their behavior. We use this procedure to answer several questions on interpretability in models for vision and natural language processing. First, we examine the kinds of abstractions learned by neurons. In image classification, we find that many neurons learn highly abstract but semantically coherent visual concepts, while other polysemantic neurons detect multiple unrelated features; in natural language inference (NLI), neurons learn shallow lexical heuristics from dataset biases. Second, we see whether compositional explanations give us insight into model performance: vision neurons that detect human-interpretable concepts are positively correlated with task performance, while NLI neurons that fire for shallow heuristics are negatively correlated with task performance. Finally, we show how compositional explanations provide an accessible way for end users to produce simple ""copy-paste"" adversarial examples that change model behavior in predictable ways.",2021-02-02,2022-03-10 21:26:59,2022-03-10 21:26:59,2022-03-10 21:26:59,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2006.14032,,/Users/jacquesthibodeau/Zotero/storage/IM56UFHZ/Mu and Andreas - 2021 - Compositional Explanations of Neurons.pdf; /Users/jacquesthibodeau/Zotero/storage/2ECK6GBR/2006.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8SJXHMS9,journalArticle,2019,"Das, Abhishek; Gkioxari, Georgia; Lee, Stefan; Parikh, Devi; Batra, Dhruv",Neural Modular Control for Embodied Question Answering,arXiv:1810.11181 [cs],,,,http://arxiv.org/abs/1810.11181,"We present a modular approach for learning policies for navigation over long planning horizons from language input. Our hierarchical policy operates at multiple timescales, where the higher-level master policy proposes subgoals to be executed by specialized sub-policies. Our choice of subgoals is compositional and semantic, i.e. they can be sequentially combined in arbitrary orderings, and assume human-interpretable descriptions (e.g. 'exit room', 'find kitchen', 'find refrigerator', etc.). We use imitation learning to warm-start policies at each level of the hierarchy, dramatically increasing sample efficiency, followed by reinforcement learning. Independent reinforcement learning at each level of hierarchy enables sub-policies to adapt to consequences of their actions and recover from errors. Subsequent joint hierarchical training enables the master policy to adapt to the sub-policies. On the challenging EQA (Das et al., 2018) benchmark in House3D (Wu et al., 2018), requiring navigating diverse realistic indoor environments, our approach outperforms prior work by a significant margin, both in terms of navigation and question answering.",2019-05-02,2022-03-10 21:27:19,2022-03-10 21:27:19,2022-03-10 21:27:19,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.11181,,/Users/jacquesthibodeau/Zotero/storage/97QNXI89/Das et al. - 2019 - Neural Modular Control for Embodied Question Answe.pdf; /Users/jacquesthibodeau/Zotero/storage/EK4LJR7V/1810.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JSM79E58,journalArticle,2020,"Jeon, Hong Jun; Milli, Smitha; Dragan, Anca D.",Reward-rational (implicit) choice: A unifying formalism for reward learning,arXiv:2002.04833 [cs],,,,http://arxiv.org/abs/2002.04833,"It is often difficult to hand-specify what the correct reward function is for a task, so researchers have instead aimed to learn reward functions from human behavior or feedback. The types of behavior interpreted as evidence of the reward function have expanded greatly in recent years. We've gone from demonstrations, to comparisons, to reading into the information leaked when the human is pushing the robot away or turning it off. And surely, there is more to come. How will a robot make sense of all these diverse types of behavior? Our key insight is that different types of behavior can be interpreted in a single unifying formalism - as a reward-rational choice that the human is making, often implicitly. The formalism offers both a unifying lens with which to view past work, as well as a recipe for interpreting new sources of information that are yet to be uncovered. We provide two examples to showcase this: interpreting a new feedback type, and reading into how the choice of feedback itself leaks information about the reward.",2020-12-11,2022-03-10 21:30:26,2022-03-11 01:38:27,2022-03-10 21:30:26,,,,,,,Reward-rational (implicit) choice,,,,,,,,,,,,arXiv.org,,arXiv: 2002.04833,,/Users/jacquesthibodeau/Zotero/storage/JBZJEXYL/Jeon et al. - 2020 - Reward-rational (implicit) choice A unifying form.pdf; /Users/jacquesthibodeau/Zotero/storage/KSBEA8B7/Jeon et al. - 2020 - Reward-rational (implicit) choice A unifying form.pdf; /Users/jacquesthibodeau/Zotero/storage/7ZBNWPTZ/Jeon et al. - 2020 - Reward-rational (implicit) choice A unifying form.pdf; /Users/jacquesthibodeau/Zotero/storage/9AUP7JBL/2002.html; /Users/jacquesthibodeau/Zotero/storage/YU23FGTP/2002.html; /Users/jacquesthibodeau/Zotero/storage/TA7VTUCB/2002.html,,,Computer Science - Artificial Intelligence; Computer Science - Human-Computer Interaction; Computer Science - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BJVFMNGC,journalArticle,2018,"Bau, David; Zhu, Jun-Yan; Strobelt, Hendrik; Zhou, Bolei; Tenenbaum, Joshua B.; Freeman, William T.; Torralba, Antonio",GAN Dissection: Visualizing and Understanding Generative Adversarial Networks,arXiv:1811.10597 [cs],,,,http://arxiv.org/abs/1811.10597,"Generative Adversarial Networks (GANs) have recently achieved impressive results for many real-world applications, and many GAN variants have emerged with improvements in sample quality and training stability. However, they have not been well visualized or understood. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models. In this work, we present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to object concepts using a segmentation-based network dissection method. Then, we quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. We examine the contextual relationship between these units and their surroundings by inserting the discovered object concepts into new images. We show several practical applications enabled by our framework, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing artifact-causing units, to interactively manipulating objects in a scene. We provide open source interpretation tools to help researchers and practitioners better understand their GAN models.",2018-12-08,2022-03-10 21:31:47,2022-03-11 01:38:40,2022-03-10 21:31:47,,,,,,,GAN Dissection,,,,,,,,,,,,arXiv.org,,arXiv: 1811.10597,,/Users/jacquesthibodeau/Zotero/storage/RDVZ2UIF/Bau et al. - 2018 - GAN Dissection Visualizing and Understanding Gene.pdf; /Users/jacquesthibodeau/Zotero/storage/2UA5JYZP/Bau et al. - 2018 - GAN Dissection Visualizing and Understanding Gene.pdf; /Users/jacquesthibodeau/Zotero/storage/UFQ64VTT/1811.html; /Users/jacquesthibodeau/Zotero/storage/IWI3G8UB/1811.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Graphics; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YWE98K5X,journalArticle,2018,"Xu, Tian; Zhan, Jiayu; Garrod, Oliver G. B.; Torr, Philip H. S.; Zhu, Song-Chun; Ince, Robin A. A.; Schyns, Philippe G.",Deeper Interpretability of Deep Networks,arXiv:1811.07807 [cs],,,,http://arxiv.org/abs/1811.07807,"Deep Convolutional Neural Networks (CNNs) have been one of the most influential recent developments in computer vision, particularly for categorization. There is an increasing demand for explainable AI as these systems are deployed in the real world. However, understanding the information represented and processed in CNNs remains in most cases challenging. Within this paper, we explore the use of new information theoretic techniques developed in the field of neuroscience to enable novel understanding of how a CNN represents information. We trained a 10-layer ResNet architecture to identify 2,000 face identities from 26M images generated using a rigorously controlled 3D face rendering model that produced variations of intrinsic (i.e. face morphology, gender, age, expression and ethnicity) and extrinsic factors (i.e. 3D pose, illumination, scale and 2D translation). With our methodology, we demonstrate that unlike human's network overgeneralizes face identities even with extreme changes of face shape, but it is more sensitive to changes of texture. To understand the processing of information underlying these counterintuitive properties, we visualize the features of shape and texture that the network processes to identify faces. Then, we shed a light into the inner workings of the black box and reveal how hidden layers represent these features and whether the representations are invariant to pose. We hope that our methodology will provide an additional valuable tool for interpretability of CNNs.",2018-11-20,2022-03-10 21:31:50,2022-03-11 01:38:38,2022-03-10 21:31:50,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1811.07807,,/Users/jacquesthibodeau/Zotero/storage/JVPLJLEJ/Xu et al. - 2018 - Deeper Interpretability of Deep Networks.pdf; /Users/jacquesthibodeau/Zotero/storage/H5PPH9ZH/Xu et al. - 2018 - Deeper Interpretability of Deep Networks.pdf; /Users/jacquesthibodeau/Zotero/storage/SLX9FB2N/1811.html; /Users/jacquesthibodeau/Zotero/storage/U5RDJ8GQ/1811.html,,,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P674JIV8,journalArticle,2019,"Bahdanau, Dzmitry; Hill, Felix; Leike, Jan; Hughes, Edward; Hosseini, Arian; Kohli, Pushmeet; Grefenstette, Edward",Learning to Understand Goal Specifications by Modelling Reward,arXiv:1806.01946 [cs],,,,http://arxiv.org/abs/1806.01946,"Recent work has shown that deep reinforcement-learning agents can learn to follow language-like instructions from infrequent environment rewards. However, this places on environment designers the onus of designing language-conditional reward functions which may not be easily or tractably implemented as the complexity of the environment and the language scales. To overcome this limitation, we present a framework within which instruction-conditional RL agents are trained using rewards obtained not from the environment, but from reward models which are jointly trained from expert examples. As reward models improve, they learn to accurately reward agents for completing tasks for environment configurations---and for instructions---not present amongst the expert data. This framework effectively separates the representation of what instructions require from how they can be executed. In a simple grid world, it enables an agent to learn a range of commands requiring interaction with blocks and understanding of spatial relations and underspecified abstract arrangements. We further show the method allows our agent to adapt to changes in the environment without requiring new expert examples.",2019-12-23,2022-03-10 21:32:52,2022-03-10 21:32:52,2022-03-10 21:32:52,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1806.01946,,/Users/jacquesthibodeau/Zotero/storage/SQVYQXKM/Bahdanau et al. - 2019 - Learning to Understand Goal Specifications by Mode.pdf; /Users/jacquesthibodeau/Zotero/storage/ZZZC8CB3/1806.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3QPWRQK4,journalArticle,2019,"Singh, Avi; Yang, Larry; Hartikainen, Kristian; Finn, Chelsea; Levine, Sergey",End-to-End Robotic Reinforcement Learning without Reward Engineering,"arXiv:1904.07854 [cs, stat]",,,,http://arxiv.org/abs/1904.07854,"The combination of deep neural network models and reinforcement learning algorithms can make it possible to learn policies for robotic behaviors that directly read in raw sensory inputs, such as camera images, effectively subsuming both estimation and control into one model. However, real-world applications of reinforcement learning must specify the goal of the task by means of a manually programmed reward function, which in practice requires either designing the very same perception pipeline that end-to-end reinforcement learning promises to avoid, or else instrumenting the environment with additional sensors to determine if the task has been performed successfully. In this paper, we propose an approach for removing the need for manual engineering of reward specifications by enabling a robot to learn from a modest number of examples of successful outcomes, followed by actively solicited queries, where the robot shows the user a state and asks for a label to determine whether that state represents successful completion of the task. While requesting labels for every single state would amount to asking the user to manually provide the reward signal, our method requires labels for only a tiny fraction of the states seen during training, making it an efficient and practical approach for learning skills without manually engineered rewards. We evaluate our method on real-world robotic manipulation tasks where the observations consist of images viewed by the robot's camera. In our experiments, our method effectively learns to arrange objects, place books, and drape cloth, directly from images and without any manually specified reward functions, and with only 1-4 hours of interaction with the real world.",2019-05-15,2022-03-10 21:51:33,2022-03-10 21:51:33,2022-03-10 21:51:33,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1904.07854,,/Users/jacquesthibodeau/Zotero/storage/5D3WPY5V/Singh et al. - 2019 - End-to-End Robotic Reinforcement Learning without .pdf; /Users/jacquesthibodeau/Zotero/storage/U2WNAYI7/1904.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6B2XCGSA,journalArticle,2021,"Cai, Feiyang; Ozdagli, Ali I.; Koutsoukos, Xenofon",Detection of Dataset Shifts in Learning-Enabled Cyber-Physical Systems using Variational Autoencoder for Regression,arXiv:2104.06613 [cs],,,,http://arxiv.org/abs/2104.06613,"Cyber-physical systems (CPSs) use learning-enabled components (LECs) extensively to cope with various complex tasks under high-uncertainty environments. However, the dataset shifts between the training and testing phase may lead the LECs to become ineffective to make large-error predictions, and further, compromise the safety of the overall system. In our paper, we first provide the formal definitions for different types of dataset shifts in learning-enabled CPS. Then, we propose an approach to detect the dataset shifts effectively for regression problems. Our approach is based on the inductive conformal anomaly detection and utilizes a variational autoencoder for regression model which enables the approach to take into consideration both LEC input and output for detecting dataset shifts. Additionally, in order to improve the robustness of detection, layer-wise relevance propagation (LRP) is incorporated into our approach. We demonstrate our approach by using an advanced emergency braking system implemented in an open-source simulator for self-driving cars. The evaluation results show that our approach can detect different types of dataset shifts with a small number of false alarms while the execution time is smaller than the sampling period of the system.",2021-04-13,2022-03-10 21:59:51,2022-03-10 21:59:51,2022-03-10 21:59:51,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2104.06613,,/Users/jacquesthibodeau/Zotero/storage/2BI9MNDE/Cai et al. - 2021 - Detection of Dataset Shifts in Learning-Enabled Cy.pdf; /Users/jacquesthibodeau/Zotero/storage/EJKILAZB/2104.html,,,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IHUAK8C2,journalArticle,2019,"Chollet, François",On the Measure of Intelligence,arXiv:1911.01547 [cs],,,,http://arxiv.org/abs/1911.01547,"To make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to ""buy"" arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.",2019-11-25,2022-03-10 22:02:10,2022-03-10 22:02:10,2022-03-10 22:02:10,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1911.01547,,/Users/jacquesthibodeau/Zotero/storage/S63K9L3I/Chollet - 2019 - On the Measure of Intelligence.pdf; /Users/jacquesthibodeau/Zotero/storage/SDFPCCLB/1911.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QRM6JQEV,journalArticle,1975,"Kerr, Steven","On the Folly of Rewarding A, While Hoping for B",Academy of Management Journal,,0001-4273,10.5465/255378,https://journals.aom.org/doi/full/10.5465/255378,"Illustrations are presented from society in general, and from organizations in particular, of reward systems that “pay off” for one behavior even though the rewarder hopes dearly for another. Portions of the reward systems of a manufacturing company and an insurance firm are examined and the consequences discussed.",1975-12,2022-03-10 22:05:16,2022-03-10 22:05:16,2022-03-10 22:05:16,769-783,,4,18,,AMJ,,,,,,,,,,,,,journals.aom.org (Atypon),,Publisher: Academy of Management,,,,,AWARDS; EMPLOYEES — Awards; INCENTIVES in industry — Research; INDUSTRIAL efficiency; INDUSTRIAL management — Research; MANAGEMENT styles; ORGANIZATION — Management; ORGANIZATIONAL effectiveness; ORGANIZATIONAL goals; PERFORMANCE awards,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4KULV732,journalArticle,2017,"Anthony, Thomas; Tian, Zheng; Barber, David",Thinking Fast and Slow with Deep Learning and Tree Search,arXiv:1705.08439 [cs],,,,http://arxiv.org/abs/1705.08439,"Sequential decision making problems, such as structured prediction, robotic control, and game playing, require a combination of planning policies and generalisation of those plans. In this paper, we present Expert Iteration (EXIT), a novel reinforcement learning algorithm which decomposes the problem into separate planning and generalisation tasks. Planning new policies is performed by tree search, while a deep neural network generalises those plans. Subsequently, tree search is improved by using the neural network policy to guide search, increasing the strength of new plans. In contrast, standard deep Reinforcement Learning algorithms rely on a neural network not only to generalise plans, but to discover them too. We show that EXIT outperforms REINFORCE for training a neural network to play the board game Hex, and our ﬁnal tree search agent, trained tabula rasa, defeats MOHEX 1.0, the most recent Olympiad Champion player to be publicly released.",2017-12-03,2022-03-10 22:11:57,2022-03-10 22:11:57,2022-03-10 22:11:57,,,,,,,,,,,,,,en,,,,,arXiv.org,,arXiv: 1705.08439,,/Users/jacquesthibodeau/Zotero/storage/ZA7ZFDBM/Anthony et al. - 2017 - Thinking Fast and Slow with Deep Learning and Tree.pdf,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5LPK4AFG,journalArticle,2017,"Doshi-Velez, Finale; Kim, Been",Towards A Rigorous Science of Interpretable Machine Learning,"arXiv:1702.08608 [cs, stat]",,,,http://arxiv.org/abs/1702.08608,"As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.",2017-03-02,2022-03-10 22:15:12,2022-03-10 22:15:12,2022-03-10 22:15:11,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1702.08608,,/Users/jacquesthibodeau/Zotero/storage/4ZB2VZYE/Doshi-Velez and Kim - 2017 - Towards A Rigorous Science of Interpretable Machin.pdf; /Users/jacquesthibodeau/Zotero/storage/AWGGGGH9/1702.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8LUTTTEJ,journalArticle,2012,"Neu, Gergely; Szepesvari, Csaba",Apprenticeship Learning using Inverse Reinforcement Learning and Gradient Methods,"arXiv:1206.5264 [cs, stat]",,,,http://arxiv.org/abs/1206.5264,"In this paper we propose a novel gradient algorithm to learn a policy from an expert's observed behavior assuming that the expert behaves optimally with respect to some unknown reward function of a Markovian Decision Problem. The algorithm's aim is to find a reward function such that the resulting optimal policy matches well the expert's observed behavior. The main difficulty is that the mapping from the parameters to policies is both nonsmooth and highly redundant. Resorting to subdifferentials solves the first difficulty, while the second one is over- come by computing natural gradients. We tested the proposed method in two artificial domains and found it to be more reliable and efficient than some previous methods.",2012-06-20,2022-03-10 22:16:38,2022-03-10 22:16:38,2022-03-10 22:16:38,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1206.5264,,/Users/jacquesthibodeau/Zotero/storage/99292G7B/Neu and Szepesvari - 2012 - Apprenticeship Learning using Inverse Reinforcemen.pdf; /Users/jacquesthibodeau/Zotero/storage/GBWMMHQ7/1206.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XVG5R2C7,journalArticle,,"Ramachandran, Deepak",Bayesian Inverse Reinforcement Learning,,,,,,Inverse Reinforcement Learning (IRL) is the problem of learning the reward function underlying a Markov Decision Process given the dynamics of the system and the behaviour of an expert. IRL is motivated by situations where knowledge of the rewards is a goal by itself (as in preference elicitation) and by the task of apprenticeship learning (learning policies from an expert). In this paper we show how to combine prior knowledge and evidence from the expert’s actions to derive a probability distribution over the space of reward functions. We present efﬁcient algorithms that ﬁnd solutions for the reward learning and apprenticeship learning tasks that generalize well over these distributions. Experimental results show strong improvement for our methods over previous heuristic-based approaches.,,2022-03-10 22:17:12,2022-03-10 22:17:12,,6,,,,,,,,,,,,,en,,,,,Zotero,,,,/Users/jacquesthibodeau/Zotero/storage/4LIPHWI9/Ramachandran - Bayesian Inverse Reinforcement Learning.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
I2AIWYLC,journalArticle,2020,"Garrabrant, Scott; Benson-Tilsen, Tsvi; Critch, Andrew; Soares, Nate; Taylor, Jessica",Logical Induction,"arXiv:1609.03543 [cs, math]",,,,http://arxiv.org/abs/1609.03543,"We present a computable algorithm that assigns probabilities to every logical statement in a given formal language, and refines those probabilities over time. For instance, if the language is Peano arithmetic, it assigns probabilities to all arithmetical statements, including claims about the twin prime conjecture, the outputs of long-running computations, and its own probabilities. We show that our algorithm, an instance of what we call a logical inductor, satisfies a number of intuitive desiderata, including: (1) it learns to predict patterns of truth and falsehood in logical statements, often long before having the resources to evaluate the statements, so long as the patterns can be written down in polynomial time; (2) it learns to use appropriate statistical summaries to predict sequences of statements whose truth values appear pseudorandom; and (3) it learns to have accurate beliefs about its own current beliefs, in a manner that avoids the standard paradoxes of self-reference. For example, if a given computer program only ever produces outputs in a certain range, a logical inductor learns this fact in a timely manner; and if late digits in the decimal expansion of $\pi$ are difficult to predict, then a logical inductor learns to assign $\approx 10\%$ probability to ""the $n$th digit of $\pi$ is a 7"" for large $n$. Logical inductors also learn to trust their future beliefs more than their current beliefs, and their beliefs are coherent in the limit (whenever $\phi \implies \psi$, $\mathbb{P}_\infty(\phi) \le \mathbb{P}_\infty(\psi)$, and so on); and logical inductors strictly dominate the universal semimeasure in the limit. These properties and many others all follow from a single logical induction criterion, which is motivated by a series of stock trading analogies. Roughly speaking, each logical sentence $\phi$ is associated with a stock that is worth \$1 per share if [...]",2020-12-07,2022-03-10 22:23:27,2022-03-10 22:23:27,2022-03-10 22:23:27,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1609.03543,,/Users/jacquesthibodeau/Zotero/storage/W3QAZJ9V/Garrabrant et al. - 2020 - Logical Induction.pdf; /Users/jacquesthibodeau/Zotero/storage/FAMS64EX/1609.html,,,Computer Science - Artificial Intelligence; Computer Science - Logic in Computer Science; Mathematics - Logic; Mathematics - Probability,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GLCZ83MV,journalArticle,2018,"Yudkowsky, Eliezer; Soares, Nate",Functional Decision Theory: A New Theory of Instrumental Rationality,arXiv:1710.05060 [cs],,,,http://arxiv.org/abs/1710.05060,"This paper describes and motivates a new decision theory known as functional decision theory (FDT), as distinct from causal decision theory and evidential decision theory. Functional decision theorists hold that the normative principle for action is to treat one's decision as the output of a fixed mathematical function that answers the question, ""Which output of this very function would yield the best outcome?"" Adhering to this principle delivers a number of benefits, including the ability to maximize wealth in an array of traditional decision-theoretic and game-theoretic problems where CDT and EDT perform poorly. Using one simple and coherent decision rule, functional decision theorists (for example) achieve more utility than CDT on Newcomb's problem, more utility than EDT on the smoking lesion problem, and more utility than both in Parfit's hitchhiker problem. In this paper, we define FDT, explore its prescriptions in a number of different decision problems, compare it to CDT and EDT, and give philosophical justifications for FDT as a normative theory of decision-making.",2018-05-22,2022-03-10 22:25:36,2022-03-10 22:25:36,2022-03-10 22:25:36,,,,,,,Functional Decision Theory,,,,,,,,,,,,arXiv.org,,arXiv: 1710.05060,,/Users/jacquesthibodeau/Zotero/storage/T8CUKSLW/Yudkowsky and Soares - 2018 - Functional Decision Theory A New Theory of Instru.pdf; /Users/jacquesthibodeau/Zotero/storage/SARGDYW9/1710.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TG4IEKSJ,journalArticle,2019,"Manheim, David; Garrabrant, Scott",Categorizing Variants of Goodhart's Law,"arXiv:1803.04585 [cs, q-fin, stat]",,,,http://arxiv.org/abs/1803.04585,"There are several distinct failure modes for overoptimization of systems on the basis of metrics. This occurs when a metric which can be used to improve a system is used to an extent that further optimization is ineffective or harmful, and is sometimes termed Goodhart's Law. This class of failure is often poorly understood, partly because terminology for discussing them is ambiguous, and partly because discussion using this ambiguous terminology ignores distinctions between different failure modes of this general type. This paper expands on an earlier discussion by Garrabrant, which notes there are ""(at least) four different mechanisms"" that relate to Goodhart's Law. This paper is intended to explore these mechanisms further, and specify more clearly how they occur. This discussion should be helpful in better understanding these types of failures in economic regulation, in public policy, in machine learning, and in Artificial Intelligence alignment. The importance of Goodhart effects depends on the amount of power directed towards optimizing the proxy, and so the increased optimization power offered by artificial intelligence makes it especially critical for that field.",2019-02-24,2022-03-10 22:26:31,2022-03-10 22:26:31,2022-03-10 22:26:31,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1803.04585,,/Users/jacquesthibodeau/Zotero/storage/JXTSSQGG/Manheim and Garrabrant - 2019 - Categorizing Variants of Goodhart's Law.pdf; /Users/jacquesthibodeau/Zotero/storage/NWZQRPYK/1803.html,,,91E45; Computer Science - Artificial Intelligence; Quantitative Finance - General Finance; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FRLRRRH3,journalArticle,2007,"Shoham, Yoav; Powers, Rob; Grenager, Trond","If multi-agent learning is the answer, what is the question?",Artificial Intelligence,,43702,10.1016/j.artint.2006.02.006,https://linkinghub.elsevier.com/retrieve/pii/S0004370207000495,,2007-05,2022-03-10 22:28:04,2022-03-10 22:28:04,2022-03-10 22:28:04,365-377,,7,171,,Artificial Intelligence,,,,,,,,en,,,,,DOI.org (Crossref),,,,"/Users/jacquesthibodeau/Zotero/storage/JG363XVU/Shoham et al. - 2007 - If multi-agent learning is the answer, what is the.pdf",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FK7BM423,journalArticle,,"Fallenstein, Benja; Soares, Nate",Vingean Reﬂection: Reliable Reasoning for Self-Improving Agents,,,,,,"Today, human-level machine intelligence is in the domain of futurism, but there is every reason to expect that it will be developed eventually. Once artiﬁcial agents become able to improve themselves further, they may far surpass human intelligence, making it vitally important to ensure that the result of an “intelligence explosion” is aligned with human interests. In this paper, we discuss one aspect of this challenge: ensuring that the initial agent’s reasoning about its future versions is reliable, even if these future versions are far more intelligent than the current reasoner. We refer to reasoning of this sort as Vingean reﬂection.",,2022-03-10 22:29:07,2022-03-10 22:29:07,,11,,,,,,,,,,,,,en,,,,,Zotero,,,,/Users/jacquesthibodeau/Zotero/storage/5H8P3D7Q/Fallenstein and Soares - Vingean Reﬂection Reliable Reasoning for Self-Imp.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R6IVH39E,journalArticle,2021,"Watson, Matthew; Hasan, Bashar Awwad Shiekh; Moubayed, Noura Al",Agree to Disagree: When Deep Learning Models With Identical Architectures Produce Distinct Explanations,arXiv:2105.06791 [cs],,,,http://arxiv.org/abs/2105.06791,"Deep Learning of neural networks has progressively become more prominent in healthcare with models reaching, or even surpassing, expert accuracy levels. However, these success stories are tainted by concerning reports on the lack of model transparency and bias against some medical conditions or patients' sub-groups. Explainable methods are considered the gateway to alleviate many of these concerns. In this study we demonstrate that the generated explanations are volatile to changes in model training that are perpendicular to the classification task and model structure. This raises further questions about trust in deep learning models for healthcare. Mainly, whether the models capture underlying causal links in the data or just rely on spurious correlations that are made visible via explanation methods. We demonstrate that the output of explainability methods on deep neural networks can vary significantly by changes of hyper-parameters, such as the random seed or how the training set is shuffled. We introduce a measure of explanation consistency which we use to highlight the identified problems on the MIMIC-CXR dataset. We find explanations of identical models but with different training setups have a low consistency: $\approx$ 33% on average. On the contrary, kernel methods are robust against any orthogonal changes, with explanation consistency at 94%. We conclude that current trends in model explanation are not sufficient to mitigate the risks of deploying models in real life healthcare applications.",2021-10-30,2022-03-10 22:32:04,2022-03-10 22:32:04,2022-03-10 22:32:04,,,,,,,Agree to Disagree,,,,,,,,,,,,arXiv.org,,arXiv: 2105.06791,,/Users/jacquesthibodeau/Zotero/storage/INWNTP2U/Watson et al. - 2021 - Agree to Disagree When Deep Learning Models With .pdf; /Users/jacquesthibodeau/Zotero/storage/55CC6S4E/2105.html,,,Computer Science - Machine Learning; I.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4S796YIG,journalArticle,1973,"Groves, Theodore",Incentives in Teams,Econometrica,,0012-9682,10.2307/1914085,https://www.jstor.org/stable/1914085,"This paper analyzes the problem of inducing the members of an organization to behave as if they formed a team. Considered is a conglomerate-type organization consisting of a set of semi-autonomous subunits that are coordinated by the organization's head. The head's incentive problem is to choose a set of employee compensation rules that will induce his subunit managers to communicate accurate information and take optimal decisions. The main result exhibits a particular set of compensation rules, an optimal incentive structure, that leads to team behavior. Particular attention is directed to the informational aspects of the problem. An extended example of a resource allocation model is discussed and the optimal incentive structure is interpreted in terms of prices charged by the head for resources allocated to the subunits.",1973,2022-03-10 22:34:57,2022-03-10 22:34:57,2022-03-10 22:34:57,617-631,,4,41,,,,,,,,,,,,,,,JSTOR,,"Publisher: [Wiley, Econometric Society]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8ND3DRSC,journalArticle,1979,"Myerson, Roger B.",Incentive Compatibility and the Bargaining Problem,Econometrica,,0012-9682,10.2307/1912346,https://www.jstor.org/stable/1912346,"Collective choice problems are studied from the Bayesian viewpoint. It is shown that the set of expected utility allocations which are feasible with incentive-compatible mechanisms is compact and convex, and includes the equilibrium allocations for all other mechanisms. The generalized Nash solution proposed by Harsanyi and Selten is then applied to this set to define a bargaining solution for Bayesian collective choice problems.",1979,2022-03-10 22:34:59,2022-03-10 22:34:59,2022-03-10 22:34:59,61-73,,1,47,,,,,,,,,,,,,,,JSTOR,,"Publisher: [Wiley, Econometric Society]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TV3BRGY3,journalArticle,2013,"Goodman, Noah D.; Stuhlmüller, Andreas",Knowledge and implicature: modeling language understanding as social cognition,Topics in Cognitive Science,,1756-8765,10.1111/tops.12007,,"Is language understanding a special case of social cognition? To help evaluate this view, we can formalize it as the rational speech-act theory: Listeners assume that speakers choose their utterances approximately optimally, and listeners interpret an utterance by using Bayesian inference to ""invert"" this model of the speaker. We apply this framework to model scalar implicature (""some"" implies ""not all,"" and ""N"" implies ""not more than N""). This model predicts an interaction between the speaker's knowledge state and the listener's interpretation. We test these predictions in two experiments and find good fit between model predictions and human judgments.",2013-01,2022-03-10 22:36:08,2022-03-10 22:36:08,,173-184,,1,5,,Top Cogn Sci,Knowledge and implicature,,,,,,,eng,,,,,PubMed,,PMID: 23335578,,/Users/jacquesthibodeau/Zotero/storage/N557FJFR/Goodman and Stuhlmüller - 2013 - Knowledge and implicature modeling language under.pdf;,http://www.ncbi.nlm.nih.gov/pubmed/23335578,,"Bayes Theorem; Comprehension; Data Interpretation, Statistical; Humans; Information Theory; Interpersonal Relations; Intuition; Judgment; Knowledge; Language; Models, Psychological; Psycholinguistics; Psychological Theory; Semantics; Social Behavior; Speech Perception",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XVGCV8EY,journalArticle,2002,"Wang, Xiao Fan; Chen, Guanrong",Synchronization in Small-World Dynamical Networks,Int. J. Bifurc. Chaos,,,10.1142/S0218127402004292,,"It is shown that, for any given coupling strength and a suciently large number of cells, the small-world dynamical network will synchronize, even if the original nearest-neighbor coupled network cannot achieve synchronization under the same condition. We investigate synchronization in a network of continuous-time dynamical systems with smallworld connections. The small-world network is obtained by randomly adding a small fraction of connection in an originally nearest-neighbor coupled network. We show that, for any given coupling strength and a suciently large number of cells, the small-world dynamical network will synchronize, even if the original nearest-neighbor coupled network cannot achieve synchronization under the same condition.",2002,2022-03-10 22:38:44,2022-03-10 22:38:44,,,,,,,,,,,,,,,,,,,,Semantic Scholar,,,,,https://www.semanticscholar.org/paper/Synchronization-in-Small-World-Dynamical-Networks-Wang-Chen/74d04fe177f02da2e94a895d1bb0b2a07918c20e?p2df,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
V77E9AAY,journalArticle,2002,"Bernstein, Daniel S.; Givan, Robert; Immerman, Neil; Zilberstein, Shlomo",The Complexity of Decentralized Control of Markov Decision Processes,Mathematics of Operations Research,,0364-765X,10.1287/moor.27.4.819.297,https://pubsonline.informs.org/doi/abs/10.1287/moor.27.4.819.297,"We consider decentralized control of Markov decision processes and give complexity bounds on the worst-case running time for algorithms that find optimal solutions. Generalizations of both the fully observable case and the partially observable case that allow for decentralized control are described. For even two agents, the finite-horizon problems corresponding to both of these models are hard for nondeterministic exponential time. These complexity results illustrate a fundamental difference between centralized and decentralized control of Markov decision processes. In contrast to the problems involving centralized control, the problems we consider provably do not admit polynomial-time algorithms. Furthermore, assuming EXP ≠ NEXP, the problems require superexponential time to solve in the worst case.",2002-11,2022-03-10 22:38:53,2022-03-10 22:38:53,2022-03-10 22:38:53,819-840,,4,27,,Mathematics of OR,,,,,,,,,,,,,pubsonline.informs.org (Atypon),,Publisher: INFORMS,,,,,Computational complexity; decentralized control; Markov decision process,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AJA9FL7H,journalArticle,2020,"Krueger, David; Maharaj, Tegan; Leike, Jan",Hidden Incentives for Auto-Induced Distributional Shift,"arXiv:2009.09153 [cs, stat]",,,,http://arxiv.org/abs/2009.09153,"Decisions made by machine learning systems have increasing influence on the world, yet it is common for machine learning algorithms to assume that no such influence exists. An example is the use of the i.i.d. assumption in content recommendation. In fact, the (choice of) content displayed can change users' perceptions and preferences, or even drive them away, causing a shift in the distribution of users. We introduce the term auto-induced distributional shift (ADS) to describe the phenomenon of an algorithm causing a change in the distribution of its own inputs. Our goal is to ensure that machine learning systems do not leverage ADS to increase performance when doing so could be undesirable. We demonstrate that changes to the learning algorithm, such as the introduction of meta-learning, can cause hidden incentives for auto-induced distributional shift (HI-ADS) to be revealed. To address this issue, we introduce `unit tests' and a mitigation strategy for HI-ADS, as well as a toy environment for modelling real-world issues with HI-ADS in content recommendation, where we demonstrate that strong meta-learners achieve gains in performance via ADS. We show meta-learning and Q-learning both sometimes fail unit tests, but pass when using our mitigation strategy.",2020-09-18,2022-03-10 22:45:45,2022-03-10 22:45:45,2022-03-10 22:45:45,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2009.09153,,/Users/jacquesthibodeau/Zotero/storage/BMHXUJKK/Krueger et al. - 2020 - Hidden Incentives for Auto-Induced Distributional .pdf; /Users/jacquesthibodeau/Zotero/storage/RG2UB9JY/2009.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VEDSTYU9,journalArticle,2019,"Shah, Rohin; Gundotra, Noah; Abbeel, Pieter; Dragan, Anca D.","On the Feasibility of Learning, Rather than Assuming, Human Biases for Reward Inference","arXiv:1906.09624 [cs, stat]",,,,http://arxiv.org/abs/1906.09624,"Our goal is for agents to optimize the right reward function, despite how difficult it is for us to specify what that is. Inverse Reinforcement Learning (IRL) enables us to infer reward functions from demonstrations, but it usually assumes that the expert is noisily optimal. Real people, on the other hand, often have systematic biases: risk-aversion, myopia, etc. One option is to try to characterize these biases and account for them explicitly during learning. But in the era of deep learning, a natural suggestion researchers make is to avoid mathematical models of human behavior that are fraught with specific assumptions, and instead use a purely data-driven approach. We decided to put this to the test -- rather than relying on assumptions about which specific bias the demonstrator has when planning, we instead learn the demonstrator's planning algorithm that they use to generate demonstrations, as a differentiable planner. Our exploration yielded mixed findings: on the one hand, learning the planner can lead to better reward inference than relying on the wrong assumption; on the other hand, this benefit is dwarfed by the loss we incur by going from an exact to a differentiable planner. This suggest that at least for the foreseeable future, agents need a middle ground between the flexibility of data-driven methods and the useful bias of known human biases. Code is available at https://tinyurl.com/learningbiases.",2019-06-23,2022-03-10 22:46:03,2022-03-10 22:46:03,2022-03-10 22:46:03,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1906.09624,,"/Users/jacquesthibodeau/Zotero/storage/WR3N7EQX/Shah et al. - 2019 - On the Feasibility of Learning, Rather than Assumi.pdf; /Users/jacquesthibodeau/Zotero/storage/BF5LA6UX/1906.html",,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9UMKK8YW,journalArticle,2020,"Hadfield-Menell, Dylan; Milli, Smitha; Abbeel, Pieter; Russell, Stuart; Dragan, Anca",Inverse Reward Design,arXiv:1711.02827 [cs],,,,http://arxiv.org/abs/1711.02827,"Autonomous agents optimize the reward function we give them. What they don't know is how hard it is for us to design a reward function that actually captures what we want. When designing the reward, we might think of some specific training scenarios, and make sure that the reward will lead to the right behavior in those scenarios. Inevitably, agents encounter new scenarios (e.g., new types of terrain) where optimizing that same reward may lead to undesired behavior. Our insight is that reward functions are merely observations about what the designer actually wants, and that they should be interpreted in the context in which they were designed. We introduce inverse reward design (IRD) as the problem of inferring the true objective based on the designed reward and the training MDP. We introduce approximate methods for solving IRD problems, and use their solution to plan risk-averse behavior in test MDPs. Empirical results suggest that this approach can help alleviate negative side effects of misspecified reward functions and mitigate reward hacking.",2020-10-07,2022-03-10 22:46:20,2022-03-10 22:46:20,2022-03-10 22:46:19,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1711.02827,,/Users/jacquesthibodeau/Zotero/storage/E3Y6SDXV/Hadfield-Menell et al. - 2020 - Inverse Reward Design.pdf; /Users/jacquesthibodeau/Zotero/storage/4NGCDF82/1711.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WXZEYNTU,journalArticle,2021,"Wainwright, Carroll L.; Eckersley, Peter",SafeLife 1.0: Exploring Side Effects in Complex Environments,arXiv:1912.01217 [cs],,,,http://arxiv.org/abs/1912.01217,"We present SafeLife, a publicly available reinforcement learning environment that tests the safety of reinforcement learning agents. It contains complex, dynamic, tunable, procedurally generated levels with many opportunities for unsafe behavior. Agents are graded both on their ability to maximize their explicit reward and on their ability to operate safely without unnecessary side effects. We train agents to maximize rewards using proximal policy optimization and score them on a suite of benchmark levels. The resulting agents are performant but not safe -- they tend to cause large side effects in their environments -- but they form a baseline against which future safety research can be measured.",2021-02-26,2022-03-10 22:48:05,2022-03-11 01:39:13,2022-03-10 22:48:05,,,,,,,SafeLife 1.0,,,,,,,,,,,,arXiv.org,,arXiv: 1912.01217,,/Users/jacquesthibodeau/Zotero/storage/FJVXPPKD/Wainwright and Eckersley - 2021 - SafeLife 1.0 Exploring Side Effects in Complex En.pdf; /Users/jacquesthibodeau/Zotero/storage/22SEDBIN/Wainwright and Eckersley - 2021 - SafeLife 1.0 Exploring Side Effects in Complex En.pdf; /Users/jacquesthibodeau/Zotero/storage/VSK7Y643/1912.html; /Users/jacquesthibodeau/Zotero/storage/VNRKWCXE/1912.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EKWNVHNB,journalArticle,2021,"Lindner, David; Matoba, Kyle; Meulemans, Alexander",Challenges for Using Impact Regularizers to Avoid Negative Side Effects,arXiv:2101.12509 [cs],,,,http://arxiv.org/abs/2101.12509,"Designing reward functions for reinforcement learning is difficult: besides specifying which behavior is rewarded for a task, the reward also has to discourage undesired outcomes. Misspecified reward functions can lead to unintended negative side effects, and overall unsafe behavior. To overcome this problem, recent work proposed to augment the specified reward function with an impact regularizer that discourages behavior that has a big impact on the environment. Although initial results with impact regularizers seem promising in mitigating some types of side effects, important challenges remain. In this paper, we examine the main current challenges of impact regularizers and relate them to fundamental design decisions. We discuss in detail which challenges recent approaches address and which remain unsolved. Finally, we explore promising directions to overcome the unsolved challenges in preventing negative side effects with impact regularizers.",2021-02-23,2022-03-10 22:53:21,2022-03-10 22:53:21,2022-03-10 22:53:21,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2101.12509,,/Users/jacquesthibodeau/Zotero/storage/YB68J72A/Lindner et al. - 2021 - Challenges for Using Impact Regularizers to Avoid .pdf; /Users/jacquesthibodeau/Zotero/storage/BVLWGZVU/2101.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DHQ4N3UT,journalArticle,2020,"Badia, Adrià Puigdomènech; Piot, Bilal; Kapturowski, Steven; Sprechmann, Pablo; Vitvitskyi, Alex; Guo, Daniel; Blundell, Charles",Agent57: Outperforming the Atari Human Benchmark,"arXiv:2003.13350 [cs, stat]",,,,http://arxiv.org/abs/2003.13350,"Atari games have been a long-standing benchmark in the reinforcement learning (RL) community for the past decade. This benchmark was proposed to test general competency of RL algorithms. Previous work has achieved good average performance by doing outstandingly well on many games of the set, but very poorly in several of the most challenging games. We propose Agent57, the first deep RL agent that outperforms the standard human benchmark on all 57 Atari games. To achieve this result, we train a neural network which parameterizes a family of policies ranging from very exploratory to purely exploitative. We propose an adaptive mechanism to choose which policy to prioritize throughout the training process. Additionally, we utilize a novel parameterization of the architecture that allows for more consistent and stable learning.",2020-03-30,2022-03-10 23:01:27,2022-03-10 23:01:27,2022-03-10 23:01:27,,,,,,,Agent57,,,,,,,,,,,,arXiv.org,,arXiv: 2003.13350,,/Users/jacquesthibodeau/Zotero/storage/Z9TJN4M2/Badia et al. - 2020 - Agent57 Outperforming the Atari Human Benchmark.pdf; /Users/jacquesthibodeau/Zotero/storage/AWF3YR3H/2003.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
L9QYGZFM,journalArticle,2018,"Serban, Iulian Vlad; Sankar, Chinnadhurai; Pieper, Michael; Pineau, Joelle; Bengio, Yoshua",The Bottleneck Simulator: A Model-based Deep Reinforcement Learning Approach,"arXiv:1807.04723 [cs, stat]",,,,http://arxiv.org/abs/1807.04723,"Deep reinforcement learning has recently shown many impressive successes. However, one major obstacle towards applying such methods to real-world problems is their lack of data-efficiency. To this end, we propose the Bottleneck Simulator: a model-based reinforcement learning method which combines a learned, factorized transition model of the environment with rollout simulations to learn an effective policy from few examples. The learned transition model employs an abstract, discrete (bottleneck) state, which increases sample efficiency by reducing the number of model parameters and by exploiting structural properties of the environment. We provide a mathematical analysis of the Bottleneck Simulator in terms of fixed points of the learned policy, which reveals how performance is affected by four distinct sources of error: an error related to the abstract space structure, an error related to the transition model estimation variance, an error related to the transition model estimation bias, and an error related to the transition model class bias. Finally, we evaluate the Bottleneck Simulator on two natural language processing tasks: a text adventure game and a real-world, complex dialogue response selection task. On both tasks, the Bottleneck Simulator yields excellent performance beating competing approaches.",2018-07-12,2022-03-10 23:15:47,2022-03-10 23:15:47,2022-03-10 23:15:47,,,,,,,The Bottleneck Simulator,,,,,,,,,,,,arXiv.org,,arXiv: 1807.04723,,/Users/jacquesthibodeau/Zotero/storage/HVW8RPTW/Serban et al. - 2018 - The Bottleneck Simulator A Model-based Deep Reinf.pdf; /Users/jacquesthibodeau/Zotero/storage/BLXYLMWH/1807.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; I.2.7; I.5.1; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VMV48MAY,journalArticle,2019,"Hafner, Danijar; Lillicrap, Timothy; Fischer, Ian; Villegas, Ruben; Ha, David; Lee, Honglak; Davidson, James",Learning Latent Dynamics for Planning from Pixels,"arXiv:1811.04551 [cs, stat]",,,,http://arxiv.org/abs/1811.04551,"Planning has been very successful for control tasks with known environment dynamics. To leverage planning in unknown environments, the agent needs to learn the dynamics from interactions with the world. However, learning dynamics models that are accurate enough for planning has been a long-standing challenge, especially in image-based domains. We propose the Deep Planning Network (PlaNet), a purely model-based agent that learns the environment dynamics from images and chooses actions through fast online planning in latent space. To achieve high performance, the dynamics model must accurately predict the rewards ahead for multiple time steps. We approach this using a latent dynamics model with both deterministic and stochastic transition components. Moreover, we propose a multi-step variational inference objective that we name latent overshooting. Using only pixel observations, our agent solves continuous control tasks with contact dynamics, partial observability, and sparse rewards, which exceed the difficulty of tasks that were previously solved by planning with learned models. PlaNet uses substantially fewer episodes and reaches final performance close to and sometimes higher than strong model-free algorithms.",2019-06-04,2022-03-10 23:16:11,2022-03-10 23:16:11,2022-03-10 23:16:11,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1811.04551,,/Users/jacquesthibodeau/Zotero/storage/QDDDSDGI/Hafner et al. - 2019 - Learning Latent Dynamics for Planning from Pixels.pdf; /Users/jacquesthibodeau/Zotero/storage/DJ293UAS/1811.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C54BP9NQ,journalArticle,2018,"Oh, Junhyuk; Guo, Yijie; Singh, Satinder; Lee, Honglak",Self-Imitation Learning,"arXiv:1806.05635 [cs, stat]",,,,http://arxiv.org/abs/1806.05635,"This paper proposes Self-Imitation Learning (SIL), a simple off-policy actor-critic algorithm that learns to reproduce the agent's past good decisions. This algorithm is designed to verify our hypothesis that exploiting past good experiences can indirectly drive deep exploration. Our empirical results show that SIL significantly improves advantage actor-critic (A2C) on several hard exploration Atari games and is competitive to the state-of-the-art count-based exploration methods. We also show that SIL improves proximal policy optimization (PPO) on MuJoCo tasks.",2018-06-14,2022-03-10 23:20:10,2022-03-10 23:20:10,2022-03-10 23:20:10,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1806.05635,,/Users/jacquesthibodeau/Zotero/storage/X2EK7DN7/Oh et al. - 2018 - Self-Imitation Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/PD5YS5PS/1806.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EXGWPGWF,journalArticle,2020,"Hong, Zhang-Wei; Fu, Tsu-Jui; Shann, Tzu-Yun; Chang, Yi-Hsiang; Lee, Chun-Yi",Adversarial Active Exploration for Inverse Dynamics Model Learning,"arXiv:1806.10019 [cs, stat]",,,,http://arxiv.org/abs/1806.10019,"We present an adversarial active exploration for inverse dynamics model learning, a simple yet effective learning scheme that incentivizes exploration in an environment without any human intervention. Our framework consists of a deep reinforcement learning (DRL) agent and an inverse dynamics model contesting with each other. The former collects training samples for the latter, with an objective to maximize the error of the latter. The latter is trained with samples collected by the former, and generates rewards for the former when it fails to predict the actual action taken by the former. In such a competitive setting, the DRL agent learns to generate samples that the inverse dynamics model fails to predict correctly, while the inverse dynamics model learns to adapt to the challenging samples. We further propose a reward structure that ensures the DRL agent to collect only moderately hard samples but not overly hard ones that prevent the inverse model from predicting effectively. We evaluate the effectiveness of our method on several robotic arm and hand manipulation tasks against multiple baseline models. Experimental results show that our method is comparable to those directly trained with expert demonstrations, and superior to the other baselines even without any human priors.",2020-03-16,2022-03-10 23:20:15,2022-03-10 23:20:15,2022-03-10 23:20:15,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1806.10019,,/Users/jacquesthibodeau/Zotero/storage/YKPHFUCB/Hong et al. - 2020 - Adversarial Active Exploration for Inverse Dynamic.pdf; /Users/jacquesthibodeau/Zotero/storage/PRE9BFQA/1806.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IK562NSL,journalArticle,2022,"Anthony, Thomas; Eccles, Tom; Tacchetti, Andrea; Kramár, János; Gemp, Ian; Hudson, Thomas C.; Porcel, Nicolas; Lanctot, Marc; Pérolat, Julien; Everett, Richard; Werpachowski, Roman; Singh, Satinder; Graepel, Thore; Bachrach, Yoram",Learning to Play No-Press Diplomacy with Best Response Policy Iteration,"arXiv:2006.04635 [cs, stat]",,,,http://arxiv.org/abs/2006.04635,"Recent advances in deep reinforcement learning (RL) have led to considerable progress in many 2-player zero-sum games, such as Go, Poker and Starcraft. The purely adversarial nature of such games allows for conceptually simple and principled application of RL methods. However real-world settings are many-agent, and agent interactions are complex mixtures of common-interest and competitive aspects. We consider Diplomacy, a 7-player board game designed to accentuate dilemmas resulting from many-agent interactions. It also features a large combinatorial action space and simultaneous moves, which are challenging for RL algorithms. We propose a simple yet effective approximate best response operator, designed to handle large combinatorial action spaces and simultaneous moves. We also introduce a family of policy iteration methods that approximate fictitious play. With these methods, we successfully apply RL to Diplomacy: we show that our agents convincingly outperform the previous state-of-the-art, and game theoretic equilibrium analysis shows that the new process yields consistent improvements.",2022-01-04,2022-03-10 23:21:15,2022-03-10 23:21:15,2022-03-10 23:21:15,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2006.04635,,/Users/jacquesthibodeau/Zotero/storage/I3DPELKX/Anthony et al. - 2022 - Learning to Play No-Press Diplomacy with Best Resp.pdf; /Users/jacquesthibodeau/Zotero/storage/NG5J3YBS/2006.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory; Computer Science - Machine Learning; Computer Science - Multiagent Systems; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
27D3XH96,journalArticle,2020,"Eysenbach, Benjamin; Geng, Xinyang; Levine, Sergey; Salakhutdinov, Ruslan",Rewriting History with Inverse RL: Hindsight Inference for Policy Improvement,"arXiv:2002.11089 [cs, stat]",,,,http://arxiv.org/abs/2002.11089,"Multi-task reinforcement learning (RL) aims to simultaneously learn policies for solving many tasks. Several prior works have found that relabeling past experience with different reward functions can improve sample efficiency. Relabeling methods typically ask: if, in hindsight, we assume that our experience was optimal for some task, for what task was it optimal? In this paper, we show that hindsight relabeling is inverse RL, an observation that suggests that we can use inverse RL in tandem for RL algorithms to efficiently solve many tasks. We use this idea to generalize goal-relabeling techniques from prior work to arbitrary classes of tasks. Our experiments confirm that relabeling data using inverse RL accelerates learning in general multi-task settings, including goal-reaching, domains with discrete sets of rewards, and those with linear reward functions.",2020-02-25,2022-03-10 23:21:21,2022-03-10 23:21:21,2022-03-10 23:21:21,,,,,,,Rewriting History with Inverse RL,,,,,,,,,,,,arXiv.org,,arXiv: 2002.11089,,/Users/jacquesthibodeau/Zotero/storage/F6ZD5MDF/Eysenbach et al. - 2020 - Rewriting History with Inverse RL Hindsight Infer.pdf; /Users/jacquesthibodeau/Zotero/storage/TFJYY2W4/2002.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EXDGG86N,journalArticle,2020,"Li, Alexander C.; Pinto, Lerrel; Abbeel, Pieter",Generalized Hindsight for Reinforcement Learning,"arXiv:2002.11708 [cs, stat]",,,,http://arxiv.org/abs/2002.11708,"One of the key reasons for the high sample complexity in reinforcement learning (RL) is the inability to transfer knowledge from one task to another. In standard multi-task RL settings, low-reward data collected while trying to solve one task provides little to no signal for solving that particular task and is hence effectively wasted. However, we argue that this data, which is uninformative for one task, is likely a rich source of information for other tasks. To leverage this insight and efficiently reuse data, we present Generalized Hindsight: an approximate inverse reinforcement learning technique for relabeling behaviors with the right tasks. Intuitively, given a behavior generated under one task, Generalized Hindsight returns a different task that the behavior is better suited for. Then, the behavior is relabeled with this new task before being used by an off-policy RL optimizer. Compared to standard relabeling techniques, Generalized Hindsight provides a substantially more efficient reuse of samples, which we empirically demonstrate on a suite of multi-task navigation and manipulation tasks. Videos and code can be accessed here: https://sites.google.com/view/generalized-hindsight.",2020-02-26,2022-03-10 23:23:05,2022-03-10 23:23:05,2022-03-10 23:23:05,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2002.11708,,/Users/jacquesthibodeau/Zotero/storage/NF8IWNNY/Li et al. - 2020 - Generalized Hindsight for Reinforcement Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/J6H7GUYL/2002.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FY86WZUC,journalArticle,2019,"Arjona-Medina, Jose A.; Gillhofer, Michael; Widrich, Michael; Unterthiner, Thomas; Brandstetter, Johannes; Hochreiter, Sepp",RUDDER: Return Decomposition for Delayed Rewards,"arXiv:1806.07857 [cs, math, stat]",,,,http://arxiv.org/abs/1806.07857,"We propose RUDDER, a novel reinforcement learning approach for delayed rewards in finite Markov decision processes (MDPs). In MDPs the Q-values are equal to the expected immediate reward plus the expected future rewards. The latter are related to bias problems in temporal difference (TD) learning and to high variance problems in Monte Carlo (MC) learning. Both problems are even more severe when rewards are delayed. RUDDER aims at making the expected future rewards zero, which simplifies Q-value estimation to computing the mean of the immediate reward. We propose the following two new concepts to push the expected future rewards toward zero. (i) Reward redistribution that leads to return-equivalent decision processes with the same optimal policies and, when optimal, zero expected future rewards. (ii) Return decomposition via contribution analysis which transforms the reinforcement learning task into a regression task at which deep learning excels. On artificial tasks with delayed rewards, RUDDER is significantly faster than MC and exponentially faster than Monte Carlo Tree Search (MCTS), TD({\lambda}), and reward shaping approaches. At Atari games, RUDDER on top of a Proximal Policy Optimization (PPO) baseline improves the scores, which is most prominent at games with delayed rewards. Source code is available at \url{https://github.com/ml-jku/rudder} and demonstration videos at \url{https://goo.gl/EQerZV}.",2019-09-10,2022-03-10 23:23:28,2022-03-10 23:23:28,2022-03-10 23:23:28,,,,,,,RUDDER,,,,,,,,,,,,arXiv.org,,arXiv: 1806.07857,,/Users/jacquesthibodeau/Zotero/storage/JTMQWGUL/Arjona-Medina et al. - 2019 - RUDDER Return Decomposition for Delayed Rewards.pdf; /Users/jacquesthibodeau/Zotero/storage/IGVXES3F/1806.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Mathematics - Optimization and Control; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
W47M8523,journalArticle,2020,"Andrychowicz, Marcin; Raichuk, Anton; Stańczyk, Piotr; Orsini, Manu; Girgin, Sertan; Marinier, Raphael; Hussenot, Léonard; Geist, Matthieu; Pietquin, Olivier; Michalski, Marcin; Gelly, Sylvain; Bachem, Olivier",What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study,"arXiv:2006.05990 [cs, stat]",,,,http://arxiv.org/abs/2006.05990,"In recent years, on-policy reinforcement learning (RL) has been successfully applied to many different continuous control tasks. While RL algorithms are often conceptually simple, their state-of-the-art implementations take numerous low- and high-level design decisions that strongly affect the performance of the resulting agents. Those choices are usually not extensively discussed in the literature, leading to discrepancy between published descriptions of algorithms and their implementations. This makes it hard to attribute progress in RL and slows down overall progress [Engstrom'20]. As a step towards filling that gap, we implement >50 such ``choices'' in a unified on-policy RL framework, allowing us to investigate their impact in a large-scale empirical study. We train over 250'000 agents in five continuous control environments of different complexity and provide insights and practical recommendations for on-policy training of RL agents.",2020-06-10,2022-03-10 23:23:29,2022-03-10 23:23:29,2022-03-10 23:23:28,,,,,,,What Matters In On-Policy Reinforcement Learning?,,,,,,,,,,,,arXiv.org,,arXiv: 2006.05990,,/Users/jacquesthibodeau/Zotero/storage/BCTI4FSA/Andrychowicz et al. - 2020 - What Matters In On-Policy Reinforcement Learning .pdf; /Users/jacquesthibodeau/Zotero/storage/HMHRKYF8/2006.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
52IVK8ZF,journalArticle,2020,"Kumar, Aviral; Gupta, Abhishek; Levine, Sergey",DisCor: Corrective Feedback in Reinforcement Learning via Distribution Correction,"arXiv:2003.07305 [cs, stat]",,,,http://arxiv.org/abs/2003.07305,"Deep reinforcement learning can learn effective policies for a wide range of tasks, but is notoriously difficult to use due to instability and sensitivity to hyperparameters. The reasons for this remain unclear. When using standard supervised methods (e.g., for bandits), on-policy data collection provides ""hard negatives"" that correct the model in precisely those states and actions that the policy is likely to visit. We call this phenomenon ""corrective feedback."" We show that bootstrapping-based Q-learning algorithms do not necessarily benefit from this corrective feedback, and training on the experience collected by the algorithm is not sufficient to correct errors in the Q-function. In fact, Q-learning and related methods can exhibit pathological interactions between the distribution of experience collected by the agent and the policy induced by training on that experience, leading to potential instability, sub-optimal convergence, and poor results when learning from noisy, sparse or delayed rewards. We demonstrate the existence of this problem, both theoretically and empirically. We then show that a specific correction to the data distribution can mitigate this issue. Based on these observations, we propose a new algorithm, DisCor, which computes an approximation to this optimal distribution and uses it to re-weight the transitions used for training, resulting in substantial improvements in a range of challenging RL settings, such as multi-task learning and learning from noisy reward signals. Blog post presenting a summary of this work is available at: https://bair.berkeley.edu/blog/2020/03/16/discor/.",2020-03-16,2022-03-10 23:23:45,2022-03-10 23:23:45,2022-03-10 23:23:45,,,,,,,DisCor,,,,,,,,,,,,arXiv.org,,arXiv: 2003.07305,,/Users/jacquesthibodeau/Zotero/storage/L8VU8MNM/Kumar et al. - 2020 - DisCor Corrective Feedback in Reinforcement Learn.pdf; /Users/jacquesthibodeau/Zotero/storage/SGQT53TW/2003.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QM2GFPBM,journalArticle,2021,"Janner, Michael; Mordatch, Igor; Levine, Sergey",Generative Temporal Difference Learning for Infinite-Horizon Prediction,arXiv:2010.14496 [cs],,,,http://arxiv.org/abs/2010.14496,"We introduce the $\gamma$-model, a predictive model of environment dynamics with an infinite probabilistic horizon. Replacing standard single-step models with $\gamma$-models leads to generalizations of the procedures central to model-based control, including the model rollout and model-based value estimation. The $\gamma$-model, trained with a generative reinterpretation of temporal difference learning, is a natural continuous analogue of the successor representation and a hybrid between model-free and model-based mechanisms. Like a value function, it contains information about the long-term future; like a standard predictive model, it is independent of task reward. We instantiate the $\gamma$-model as both a generative adversarial network and normalizing flow, discuss how its training reflects an inescapable tradeoff between training-time and testing-time compounding errors, and empirically investigate its utility for prediction and control.",2021-11-28,2022-03-10 23:23:53,2022-03-10 23:23:53,2022-03-10 23:23:53,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2010.14496,,/Users/jacquesthibodeau/Zotero/storage/MRI4YNY5/Janner et al. - 2021 - Generative Temporal Difference Learning for Infini.pdf; /Users/jacquesthibodeau/Zotero/storage/3S49W7MN/2010.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FJMI2TH6,journalArticle,2021,"Janner, Michael; Li, Qiyang; Levine, Sergey",Offline Reinforcement Learning as One Big Sequence Modeling Problem,arXiv:2106.02039 [cs],,,,http://arxiv.org/abs/2106.02039,"Reinforcement learning (RL) is typically concerned with estimating stationary policies or single-step models, leveraging the Markov property to factorize problems in time. However, we can also view RL as a generic sequence modeling problem, with the goal being to produce a sequence of actions that leads to a sequence of high rewards. Viewed in this way, it is tempting to consider whether high-capacity sequence prediction models that work well in other domains, such as natural-language processing, can also provide effective solutions to the RL problem. To this end, we explore how RL can be tackled with the tools of sequence modeling, using a Transformer architecture to model distributions over trajectories and repurposing beam search as a planning algorithm. Framing RL as sequence modeling problem simplifies a range of design decisions, allowing us to dispense with many of the components common in offline RL algorithms. We demonstrate the flexibility of this approach across long-horizon dynamics prediction, imitation learning, goal-conditioned RL, and offline RL. Further, we show that this approach can be combined with existing model-free algorithms to yield a state-of-the-art planner in sparse-reward, long-horizon tasks.",2021-11-28,2022-03-10 23:24:03,2022-03-10 23:24:03,2022-03-10 23:24:03,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2106.02039,,/Users/jacquesthibodeau/Zotero/storage/G5C7IV9S/Janner et al. - 2021 - Offline Reinforcement Learning as One Big Sequence.pdf; /Users/jacquesthibodeau/Zotero/storage/4TLTNBLK/2106.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8YUXMP6B,journalArticle,2018,"Wilson, Dennis G.; Cussat-Blanc, Sylvain; Luga, Hervé; Miller, Julian F.",Evolving simple programs for playing Atari games,arXiv:1806.05695 [cs],,,,http://arxiv.org/abs/1806.05695,"Cartesian Genetic Programming (CGP) has previously shown capabilities in image processing tasks by evolving programs with a function set specialized for computer vision. A similar approach can be applied to Atari playing. Programs are evolved using mixed type CGP with a function set suited for matrix operations, including image processing, but allowing for controller behavior to emerge. While the programs are relatively small, many controllers are competitive with state of the art methods for the Atari benchmark set and require less training time. By evaluating the programs of the best evolved individuals, simple but effective strategies can be found.",2018-06-14,2022-03-10 23:24:03,2022-03-10 23:24:03,2022-03-10 23:24:03,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1806.05695,,/Users/jacquesthibodeau/Zotero/storage/X7H78VC2/Wilson et al. - 2018 - Evolving simple programs for playing Atari games.pdf; /Users/jacquesthibodeau/Zotero/storage/UZJHHK5G/1806.html,,,Computer Science - Artificial Intelligence; Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
N6D9N3GB,journalArticle,2018,"Laterre, Alexandre; Fu, Yunguan; Jabri, Mohamed Khalil; Cohen, Alain-Sam; Kas, David; Hajjar, Karl; Dahl, Torbjorn S.; Kerkeni, Amine; Beguir, Karim",Ranked Reward: Enabling Self-Play Reinforcement Learning for Combinatorial Optimization,"arXiv:1807.01672 [cs, stat]",,,,http://arxiv.org/abs/1807.01672,"Adversarial self-play in two-player games has delivered impressive results when used with reinforcement learning algorithms that combine deep neural networks and tree search. Algorithms like AlphaZero and Expert Iteration learn tabula-rasa, producing highly informative training data on the fly. However, the self-play training strategy is not directly applicable to single-player games. Recently, several practically important combinatorial optimisation problems, such as the travelling salesman problem and the bin packing problem, have been reformulated as reinforcement learning problems, increasing the importance of enabling the benefits of self-play beyond two-player games. We present the Ranked Reward (R2) algorithm which accomplishes this by ranking the rewards obtained by a single agent over multiple games to create a relative performance metric. Results from applying the R2 algorithm to instances of a two-dimensional and three-dimensional bin packing problems show that it outperforms generic Monte Carlo tree search, heuristic algorithms and integer programming solvers. We also present an analysis of the ranked reward mechanism, in particular, the effects of problem instances with varying difficulty and different ranking thresholds.",2018-12-06,2022-03-10 23:24:03,2022-03-10 23:24:03,2022-03-10 23:24:03,,,,,,,Ranked Reward,,,,,,,,,,,,arXiv.org,,arXiv: 1807.01672,,/Users/jacquesthibodeau/Zotero/storage/6VW9BITR/Laterre et al. - 2018 - Ranked Reward Enabling Self-Play Reinforcement Le.pdf; /Users/jacquesthibodeau/Zotero/storage/I5ZY5SBP/1807.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NLAWIBAJ,journalArticle,2018,"Justesen, Niels; Torrado, Ruben Rodriguez; Bontrager, Philip; Khalifa, Ahmed; Togelius, Julian; Risi, Sebastian",Illuminating Generalization in Deep Reinforcement Learning through Procedural Level Generation,"arXiv:1806.10729 [cs, stat]",,,,http://arxiv.org/abs/1806.10729,"Deep reinforcement learning (RL) has shown impressive results in a variety of domains, learning directly from high-dimensional sensory streams. However, when neural networks are trained in a fixed environment, such as a single level in a video game, they will usually overfit and fail to generalize to new levels. When RL models overfit, even slight modifications to the environment can result in poor agent performance. This paper explores how procedurally generated levels during training can increase generality. We show that for some games procedural level generation enables generalization to new levels within the same distribution. Additionally, it is possible to achieve better performance with less data by manipulating the difficulty of the levels in response to the performance of the agent. The generality of the learned behaviors is also evaluated on a set of human-designed levels. The results suggest that the ability to generalize to human-designed levels highly depends on the design of the level generators. We apply dimensionality reduction and clustering techniques to visualize the generators' distributions of levels and analyze to what degree they can produce levels similar to those designed by a human.",2018-11-29,2022-03-10 23:24:04,2022-03-10 23:24:04,2022-03-10 23:24:03,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1806.10729,,/Users/jacquesthibodeau/Zotero/storage/PH7H6EP2/Justesen et al. - 2018 - Illuminating Generalization in Deep Reinforcement .pdf; /Users/jacquesthibodeau/Zotero/storage/Q39A5EXV/1806.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CU8THETM,journalArticle,2021,"Chen, Lili; Lu, Kevin; Rajeswaran, Aravind; Lee, Kimin; Grover, Aditya; Laskin, Michael; Abbeel, Pieter; Srinivas, Aravind; Mordatch, Igor",Decision Transformer: Reinforcement Learning via Sequence Modeling,arXiv:2106.01345 [cs],,,,http://arxiv.org/abs/2106.01345,"We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.",2021-06-24,2022-03-10 23:24:04,2022-03-10 23:24:04,2022-03-10 23:24:04,,,,,,,Decision Transformer,,,,,,,,,,,,arXiv.org,,arXiv: 2106.01345,,/Users/jacquesthibodeau/Zotero/storage/GNYB773T/Chen et al. - 2021 - Decision Transformer Reinforcement Learning via S.pdf; /Users/jacquesthibodeau/Zotero/storage/9QN2AB3A/2106.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MTEYH83D,journalArticle,2020,"Tachet, Remi; Bachman, Philip; van Seijen, Harm",Learning Invariances for Policy Generalization,"arXiv:1809.02591 [cs, stat]",,,,http://arxiv.org/abs/1809.02591,"While recent progress has spawned very powerful machine learning systems, those agents remain extremely specialized and fail to transfer the knowledge they gain to similar yet unseen tasks. In this paper, we study a simple reinforcement learning problem and focus on learning policies that encode the proper invariances for generalization to different settings. We evaluate three potential methods for policy generalization: data augmentation, meta-learning and adversarial training. We find our data augmentation method to be effective, and study the potential of meta-learning and adversarial learning as alternative task-agnostic approaches.",2020-12-12,2022-03-10 23:24:35,2022-03-10 23:24:35,2022-03-10 23:24:35,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1809.02591,,/Users/jacquesthibodeau/Zotero/storage/V775K3NR/Tachet et al. - 2020 - Learning Invariances for Policy Generalization.pdf; /Users/jacquesthibodeau/Zotero/storage/YUQ392Q4/1809.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
E4AA3RIR,journalArticle,2020,"Wu, Min; Wicker, Matthew; Ruan, Wenjie; Huang, Xiaowei; Kwiatkowska, Marta",A Game-Based Approximate Verification of Deep Neural Networks with Provable Guarantees,Theoretical Computer Science,,3043975,10.1016/j.tcs.2019.05.046,http://arxiv.org/abs/1807.03571,"Despite the improved accuracy of deep neural networks, the discovery of adversarial examples has raised serious safety concerns. In this paper, we study two variants of pointwise robustness, the maximum safe radius problem, which for a given input sample computes the minimum distance to an adversarial example, and the feature robustness problem, which aims to quantify the robustness of individual features to adversarial perturbations. We demonstrate that, under the assumption of Lipschitz continuity, both problems can be approximated using finite optimisation by discretising the input space, and the approximation has provable guarantees, i.e., the error is bounded. We then show that the resulting optimisation problems can be reduced to the solution of two-player turn-based games, where the first player selects features and the second perturbs the image within the feature. While the second player aims to minimise the distance to an adversarial example, depending on the optimisation objective the first player can be cooperative or competitive. We employ an anytime approach to solve the games, in the sense of approximating the value of a game by monotonically improving its upper and lower bounds. The Monte Carlo tree search algorithm is applied to compute upper bounds for both games, and the Admissible A* and the Alpha-Beta Pruning algorithms are, respectively, used to compute lower bounds for the maximum safety radius and feature robustness games. When working on the upper bound of the maximum safe radius problem, our tool demonstrates competitive performance against existing adversarial example crafting algorithms. Furthermore, we show how our framework can be deployed to evaluate pointwise robustness of neural networks in safety-critical applications such as traffic sign recognition in self-driving cars.",2020-02,2022-03-10 23:29:06,2022-03-10 23:29:06,2022-03-10 23:29:06,298-329,,,807,,Theoretical Computer Science,,,,,,,,,,,,,arXiv.org,,arXiv: 1807.03571,,/Users/jacquesthibodeau/Zotero/storage/DSYWMKMR/Wu et al. - 2020 - A Game-Based Approximate Verification of Deep Neur.pdf; /Users/jacquesthibodeau/Zotero/storage/9KA3D6NF/1807.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4KRFE544,journalArticle,2020,"Hunt, Nathan; Fulton, Nathan; Magliacane, Sara; Hoang, Nghia; Das, Subhro; Solar-Lezama, Armando",Verifiably Safe Exploration for End-to-End Reinforcement Learning,arXiv:2007.01223 [cs],,,,http://arxiv.org/abs/2007.01223,"Deploying deep reinforcement learning in safety-critical settings requires developing algorithms that obey hard constraints during exploration. This paper contributes a first approach toward enforcing formal safety constraints on end-to-end policies with visual inputs. Our approach draws on recent advances in object detection and automated reasoning for hybrid dynamical systems. The approach is evaluated on a novel benchmark that emphasizes the challenge of safely exploring in the presence of hard constraints. Our benchmark draws from several proposed problem sets for safe learning and includes problems that emphasize challenges such as reward signals that are not aligned with safety constraints. On each of these benchmark problems, our algorithm completely avoids unsafe behavior while remaining competitive at optimizing for as much reward as is safe. We also prove that our method of enforcing the safety constraints preserves all safe policies from the original environment.",2020-07-02,2022-03-10 23:29:23,2022-03-10 23:29:23,2022-03-10 23:29:23,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2007.01223,,/Users/jacquesthibodeau/Zotero/storage/83D3JM8W/Hunt et al. - 2020 - Verifiably Safe Exploration for End-to-End Reinfor.pdf; /Users/jacquesthibodeau/Zotero/storage/B97MVUSI/2007.html,,,Computer Science - Artificial Intelligence; Computer Science - Logic in Computer Science; Computer Science - Machine Learning; F.3.1; I.2.8,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5VDYLMZA,journalArticle,2021,"Sarma, Gopal; Koppel, James; Malecha, Gregory; Schultz, Patrick; Drexler, Eric; Kumar, Ramana; Roux, Cody; Zucker, Philip",Formal Methods for the Informal Engineer: Workshop Recommendations,"arXiv:2104.00739 [cs, q-bio]",,,10.31219/osf.io/t4qs8,http://arxiv.org/abs/2104.00739,"Formal Methods for the Informal Engineer (FMIE) was a workshop held at the Broad Institute of MIT and Harvard in 2021 to explore the potential role of verified software in the biomedical software ecosystem. The motivation for organizing FMIE was the recognition that the life sciences and medicine are undergoing a transition from being passive consumers of software and AI/ML technologies to fundamental drivers of new platforms, including those which will need to be mission and safety-critical. Drawing on conversations leading up to and during the workshop, we make five concrete recommendations to help software leaders organically incorporate tools, techniques, and perspectives from formal methods into their project planning and development trajectories.",2021-03-30,2022-03-10 23:29:25,2022-03-10 23:29:25,2022-03-10 23:29:25,,,,,,,Formal Methods for the Informal Engineer,,,,,,,,,,,,arXiv.org,,arXiv: 2104.00739,,/Users/jacquesthibodeau/Zotero/storage/L7QAPY7P/Sarma et al. - 2021 - Formal Methods for the Informal Engineer Workshop.pdf; /Users/jacquesthibodeau/Zotero/storage/BHT3UMSQ/2104.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Programming Languages; Computer Science - Software Engineering; Quantitative Biology - Other Quantitative Biology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MFLGZQLF,journalArticle,2018,"Wang, Shiqi; Chen, Yizheng; Abdou, Ahmed; Jana, Suman",MixTrain: Scalable Training of Verifiably Robust Neural Networks,"arXiv:1811.02625 [cs, stat]",,,,http://arxiv.org/abs/1811.02625,"Making neural networks robust against adversarial inputs has resulted in an arms race between new defenses and attacks. The most promising defenses, adversarially robust training and verifiably robust training, have limitations that restrict their practical applications. The adversarially robust training only makes the networks robust against a subclass of attackers and we reveal such weaknesses by developing a new attack based on interval gradients. By contrast, verifiably robust training provides protection against any L-p norm-bounded attacker but incurs orders of magnitude more computational and memory overhead than adversarially robust training. We propose two novel techniques, stochastic robust approximation and dynamic mixed training, to drastically improve the efficiency of verifiably robust training without sacrificing verified robustness. We leverage two critical insights: (1) instead of over the entire training set, sound over-approximations over randomly subsampled training data points are sufficient for efficiently guiding the robust training process; and (2) We observe that the test accuracy and verifiable robustness often conflict after certain training epochs. Therefore, we use a dynamic loss function to adaptively balance them for each epoch. We designed and implemented our techniques as part of MixTrain and evaluated it on six networks trained on three popular datasets including MNIST, CIFAR, and ImageNet-200. Our evaluations show that MixTrain can achieve up to $95.2\%$ verified robust accuracy against $L_\infty$ norm-bounded attackers while taking $15$ and $3$ times less training time than state-of-the-art verifiably robust training and adversarially robust training schemes, respectively. Furthermore, MixTrain easily scales to larger networks like the one trained on ImageNet-200, significantly outperforming the existing verifiably robust training methods.",2018-12-01,2022-03-10 23:29:27,2022-03-10 23:29:27,2022-03-10 23:29:27,,,,,,,MixTrain,,,,,,,,,,,,arXiv.org,,arXiv: 1811.02625,,/Users/jacquesthibodeau/Zotero/storage/2XJBPJ2Q/Wang et al. - 2018 - MixTrain Scalable Training of Verifiably Robust N.pdf; /Users/jacquesthibodeau/Zotero/storage/6H87KUBP/1811.html,,,Computer Science - Cryptography and Security; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KSPG75ES,journalArticle,2021,"Everett, Michael; Lutjens, Bjorn; How, Jonathan P.",Certifiable Robustness to Adversarial State Uncertainty in Deep Reinforcement Learning,IEEE Transactions on Neural Networks and Learning Systems,,"2162-237X, 2162-2388",10.1109/TNNLS.2021.3056046,http://arxiv.org/abs/2004.06496,"Deep Neural Network-based systems are now the state-of-the-art in many robotics tasks, but their application in safety-critical domains remains dangerous without formal guarantees on network robustness. Small perturbations to sensor inputs (from noise or adversarial examples) are often enough to change network-based decisions, which was recently shown to cause an autonomous vehicle to swerve into another lane. In light of these dangers, numerous algorithms have been developed as defensive mechanisms from these adversarial inputs, some of which provide formal robustness guarantees or certificates. This work leverages research on certified adversarial robustness to develop an online certifiably robust for deep reinforcement learning algorithms. The proposed defense computes guaranteed lower bounds on state-action values during execution to identify and choose a robust action under a worst-case deviation in input space due to possible adversaries or noise. Moreover, the resulting policy comes with a certificate of solution quality, even though the true state and optimal action are unknown to the certifier due to the perturbations. The approach is demonstrated on a Deep Q-Network policy and is shown to increase robustness to noise and adversaries in pedestrian collision avoidance scenarios and a classic control task. This work extends one of our prior works with new performance guarantees, extensions to other RL algorithms, expanded results aggregated across more scenarios, an extension into scenarios with adversarial behavior, comparisons with a more computationally expensive method, and visualizations that provide intuition about the robustness algorithm.",2021,2022-03-10 23:29:29,2022-03-10 23:29:29,2022-03-10 23:29:29,1-15,,,,,IEEE Trans. Neural Netw. Learning Syst.,,,,,,,,,,,,,arXiv.org,,arXiv: 2004.06496,,/Users/jacquesthibodeau/Zotero/storage/774PTZ8C/Everett et al. - 2021 - Certifiable Robustness to Adversarial State Uncert.pdf; /Users/jacquesthibodeau/Zotero/storage/B9C5PQ29/2004.html,,,Computer Science - Cryptography and Security; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JTH6VH9W,journalArticle,2018,"Bhupatiraju, Surya; Agrawal, Kumar Krishna; Singh, Rishabh",Towards Mixed Optimization for Reinforcement Learning with Program Synthesis,"arXiv:1807.00403 [cs, stat]",,,,http://arxiv.org/abs/1807.00403,"Deep reinforcement learning has led to several recent breakthroughs, though the learned policies are often based on black-box neural networks. This makes them difficult to interpret and to impose desired specification constraints during learning. We present an iterative framework, MORL, for improving the learned policies using program synthesis. Concretely, we propose to use synthesis techniques to obtain a symbolic representation of the learned policy, which can then be debugged manually or automatically using program repair. After the repair step, we use behavior cloning to obtain the policy corresponding to the repaired program, which is then further improved using gradient descent. This process continues until the learned policy satisfies desired constraints. We instantiate MORL for the simple CartPole problem and show that the programmatic representation allows for high-level modifications that in turn lead to improved learning of the policies.",2018-07-03,2022-03-10 23:29:31,2022-03-10 23:29:31,2022-03-10 23:29:31,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1807.00403,,/Users/jacquesthibodeau/Zotero/storage/ZMRTTGSN/Bhupatiraju et al. - 2018 - Towards Mixed Optimization for Reinforcement Learn.pdf; /Users/jacquesthibodeau/Zotero/storage/MJMV8B8R/1807.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
E963R5SM,journalArticle,2019,"Gowal, Sven; Dvijotham, Krishnamurthy; Stanforth, Robert; Bunel, Rudy; Qin, Chongli; Uesato, Jonathan; Arandjelovic, Relja; Mann, Timothy; Kohli, Pushmeet",On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models,"arXiv:1810.12715 [cs, stat]",,,,http://arxiv.org/abs/1810.12715,"Recent work has shown that it is possible to train deep neural networks that are provably robust to norm-bounded adversarial perturbations. Most of these methods are based on minimizing an upper bound on the worst-case loss over all possible adversarial perturbations. While these techniques show promise, they often result in difficult optimization procedures that remain hard to scale to larger networks. Through a comprehensive analysis, we show how a simple bounding technique, interval bound propagation (IBP), can be exploited to train large provably robust neural networks that beat the state-of-the-art in verified accuracy. While the upper bound computed by IBP can be quite weak for general networks, we demonstrate that an appropriate loss and clever hyper-parameter schedule allow the network to adapt such that the IBP bound is tight. This results in a fast and stable learning algorithm that outperforms more sophisticated methods and achieves state-of-the-art results on MNIST, CIFAR-10 and SVHN. It also allows us to train the largest model to be verified beyond vacuous bounds on a downscaled version of ImageNet.",2019-08-29,2022-03-10 23:29:33,2022-03-10 23:29:33,2022-03-10 23:29:33,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.12715,,/Users/jacquesthibodeau/Zotero/storage/T8XPMGNA/Gowal et al. - 2019 - On the Effectiveness of Interval Bound Propagation.pdf; /Users/jacquesthibodeau/Zotero/storage/72YVT3W8/1810.html,,,Computer Science - Cryptography and Security; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
S5879VA4,journalArticle,2020,"Liu, Changliu; Arnon, Tomer; Lazarus, Christopher; Strong, Christopher; Barrett, Clark; Kochenderfer, Mykel J.",Algorithms for Verifying Deep Neural Networks,"arXiv:1903.06758 [cs, stat]",,,,http://arxiv.org/abs/1903.06758,"Deep neural networks are widely used for nonlinear function approximation with applications ranging from computer vision to control. Although these networks involve the composition of simple arithmetic operations, it can be very challenging to verify whether a particular network satisfies certain input-output properties. This article surveys methods that have emerged recently for soundly verifying such properties. These methods borrow insights from reachability analysis, optimization, and search. We discuss fundamental differences and connections between existing algorithms. In addition, we provide pedagogical implementations of existing methods and compare them on a set of benchmark problems.",2020-10-15,2022-03-10 23:29:34,2022-03-10 23:29:34,2022-03-10 23:29:34,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1903.06758,,/Users/jacquesthibodeau/Zotero/storage/H2V6KQMT/Liu et al. - 2020 - Algorithms for Verifying Deep Neural Networks.pdf; /Users/jacquesthibodeau/Zotero/storage/3UX6DKD4/1903.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LK87P32P,journalArticle,2019,"Anderson, Greg; Pailoor, Shankara; Dillig, Isil; Chaudhuri, Swarat",Optimization and Abstraction: A Synergistic Approach for Analyzing Neural Network Robustness,Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation,,,10.1145/3314221.3314614,http://arxiv.org/abs/1904.09959,"In recent years, the notion of local robustness (or robustness for short) has emerged as a desirable property of deep neural networks. Intuitively, robustness means that small perturbations to an input do not cause the network to perform misclassifications. In this paper, we present a novel algorithm for verifying robustness properties of neural networks. Our method synergistically combines gradient-based optimization methods for counterexample search with abstraction-based proof search to obtain a sound and ({\delta}-)complete decision procedure. Our method also employs a data-driven approach to learn a verification policy that guides abstract interpretation during proof search. We have implemented the proposed approach in a tool called Charon and experimentally evaluated it on hundreds of benchmarks. Our experiments show that the proposed approach significantly outperforms three state-of-the-art tools, namely AI^2 , Reluplex, and Reluval.",2019-06-08,2022-03-10 23:29:36,2022-03-10 23:29:36,2022-03-10 23:29:36,731-744,,,,,,Optimization and Abstraction,,,,,,,,,,,,arXiv.org,,arXiv: 1904.09959,,/Users/jacquesthibodeau/Zotero/storage/M2KZN5CJ/Anderson et al. - 2019 - Optimization and Abstraction A Synergistic Approa.pdf; /Users/jacquesthibodeau/Zotero/storage/L2LEUNWR/1904.html,,,Computer Science - Machine Learning; Computer Science - Programming Languages,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WALGPFQY,journalArticle,2018,"Dvijotham, Krishnamurthy; Gowal, Sven; Stanforth, Robert; Arandjelovic, Relja; O'Donoghue, Brendan; Uesato, Jonathan; Kohli, Pushmeet",Training verified learners with learned verifiers,"arXiv:1805.10265 [cs, stat]",,,,http://arxiv.org/abs/1805.10265,"This paper proposes a new algorithmic framework, predictor-verifier training, to train neural networks that are verifiable, i.e., networks that provably satisfy some desired input-output properties. The key idea is to simultaneously train two networks: a predictor network that performs the task at hand,e.g., predicting labels given inputs, and a verifier network that computes a bound on how well the predictor satisfies the properties being verified. Both networks can be trained simultaneously to optimize a weighted combination of the standard data-fitting loss and a term that bounds the maximum violation of the property. Experiments show that not only is the predictor-verifier architecture able to train networks to achieve state of the art verified robustness to adversarial examples with much shorter training times (outperforming previous algorithms on small datasets like MNIST and SVHN), but it can also be scaled to produce the first known (to the best of our knowledge) verifiably robust networks for CIFAR-10.",2018-05-29,2022-03-10 23:29:37,2022-03-10 23:29:37,2022-03-10 23:29:37,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1805.10265,,/Users/jacquesthibodeau/Zotero/storage/KTWS6SJV/Dvijotham et al. - 2018 - Training verified learners with learned verifiers.pdf; /Users/jacquesthibodeau/Zotero/storage/94ZA5E22/1805.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Q33NH5ZE,journalArticle,2020,"Dathathri, Sumanth; Dvijotham, Krishnamurthy; Kurakin, Alexey; Raghunathan, Aditi; Uesato, Jonathan; Bunel, Rudy; Shankar, Shreya; Steinhardt, Jacob; Goodfellow, Ian; Liang, Percy; Kohli, Pushmeet",Enabling certification of verification-agnostic networks via memory-efficient semidefinite programming,arXiv:2010.11645 [cs],,,,http://arxiv.org/abs/2010.11645,"Convex relaxations have emerged as a promising approach for verifying desirable properties of neural networks like robustness to adversarial perturbations. Widely used Linear Programming (LP) relaxations only work well when networks are trained to facilitate verification. This precludes applications that involve verification-agnostic networks, i.e., networks not specially trained for verification. On the other hand, semidefinite programming (SDP) relaxations have successfully be applied to verification-agnostic networks, but do not currently scale beyond small networks due to poor time and space asymptotics. In this work, we propose a first-order dual SDP algorithm that (1) requires memory only linear in the total number of network activations, (2) only requires a fixed number of forward/backward passes through the network per iteration. By exploiting iterative eigenvector methods, we express all solver operations in terms of forward and backward passes through the network, enabling efficient use of hardware like GPUs/TPUs. For two verification-agnostic networks on MNIST and CIFAR-10, we significantly improve L-inf verified robust accuracy from 1% to 88% and 6% to 40% respectively. We also demonstrate tight verification of a quadratic stability specification for the decoder of a variational autoencoder.",2020-11-03,2022-03-10 23:29:39,2022-03-10 23:29:39,2022-03-10 23:29:39,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2010.11645,,/Users/jacquesthibodeau/Zotero/storage/CU84IIZX/Dathathri et al. - 2020 - Enabling certification of verification-agnostic ne.pdf; /Users/jacquesthibodeau/Zotero/storage/FW45H5BS/2010.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R8XHBNX8,journalArticle,2019,"Tjeng, Vincent; Xiao, Kai; Tedrake, Russ",Evaluating Robustness of Neural Networks with Mixed Integer Programming,arXiv:1711.07356 [cs],,,,http://arxiv.org/abs/1711.07356,"Neural networks have demonstrated considerable success on a wide variety of real-world problems. However, networks trained only to optimize for training accuracy can often be fooled by adversarial examples - slightly perturbed inputs that are misclassified with high confidence. Verification of networks enables us to gauge their vulnerability to such adversarial examples. We formulate verification of piecewise-linear neural networks as a mixed integer program. On a representative task of finding minimum adversarial distortions, our verifier is two to three orders of magnitude quicker than the state-of-the-art. We achieve this computational speedup via tight formulations for non-linearities, as well as a novel presolve algorithm that makes full use of all information available. The computational speedup allows us to verify properties on convolutional networks with an order of magnitude more ReLUs than networks previously verified by any complete verifier. In particular, we determine for the first time the exact adversarial accuracy of an MNIST classifier to perturbations with bounded $l_\infty$ norm $\epsilon=0.1$: for this classifier, we find an adversarial example for 4.38% of samples, and a certificate of robustness (to perturbations with bounded norm) for the remainder. Across all robust training procedures and network architectures considered, we are able to certify more samples than the state-of-the-art and find more adversarial examples than a strong first-order attack.",2019-02-17,2022-03-10 23:29:41,2022-03-10 23:29:41,2022-03-10 23:29:41,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1711.07356,,/Users/jacquesthibodeau/Zotero/storage/7PHSKGTD/Tjeng et al. - 2019 - Evaluating Robustness of Neural Networks with Mixe.pdf; /Users/jacquesthibodeau/Zotero/storage/DCVXYWTQ/1711.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Cryptography and Security; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HSDX24P8,journalArticle,2020,"Anderson, Greg; Verma, Abhinav; Dillig, Isil; Chaudhuri, Swarat",Neurosymbolic Reinforcement Learning with Formally Verified Exploration,"arXiv:2009.12612 [cs, stat]",,,,http://arxiv.org/abs/2009.12612,"We present Revel, a partially neural reinforcement learning (RL) framework for provably safe exploration in continuous state and action spaces. A key challenge for provably safe deep RL is that repeatedly verifying neural networks within a learning loop is computationally infeasible. We address this challenge using two policy classes: a general, neurosymbolic class with approximate gradients and a more restricted class of symbolic policies that allows efficient verification. Our learning algorithm is a mirror descent over policies: in each iteration, it safely lifts a symbolic policy into the neurosymbolic space, performs safe gradient updates to the resulting policy, and projects the updated policy into the safe symbolic subset, all without requiring explicit verification of neural networks. Our empirical results show that Revel enforces safe exploration in many scenarios in which Constrained Policy Optimization does not, and that it can discover policies that outperform those learned through prior approaches to verified exploration.",2020-10-26,2022-03-10 23:29:46,2022-03-10 23:29:46,2022-03-10 23:29:45,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2009.12612,,/Users/jacquesthibodeau/Zotero/storage/IAV5W3C5/Anderson et al. - 2020 - Neurosymbolic Reinforcement Learning with Formally.pdf; /Users/jacquesthibodeau/Zotero/storage/2LRGZHWW/2009.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KB8PHF2Y,journalArticle,2020,"Raghunathan, Aditi; Steinhardt, Jacob; Liang, Percy",Certified Defenses against Adversarial Examples,arXiv:1801.09344 [cs],,,,http://arxiv.org/abs/1801.09344,"While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses. Can we somehow end this arms race? In this work, we study this problem for neural networks with one hidden layer. We first propose a method based on a semidefinite relaxation that outputs a certificate that for a given network and test input, no attack can force the error to exceed a certain value. Second, as this certificate is differentiable, we jointly optimize it with the network parameters, providing an adaptive regularizer that encourages robustness against all attacks. On MNIST, our approach produces a network and a certificate that no attack that perturbs each pixel by at most \epsilon = 0.1 can cause more than 35% test error.",2020-10-31,2022-03-10 23:29:47,2022-03-10 23:29:47,2022-03-10 23:29:47,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1801.09344,,/Users/jacquesthibodeau/Zotero/storage/6RAHXUGT/Raghunathan et al. - 2020 - Certified Defenses against Adversarial Examples.pdf; /Users/jacquesthibodeau/Zotero/storage/RUEIXHE2/1801.html,,,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YXGV3JKA,journalArticle,2019,"Wu, Tailin; Tegmark, Max",Toward an AI Physicist for Unsupervised Learning,Physical Review E,,"2470-0045, 2470-0053",10.1103/PhysRevE.100.033311,http://arxiv.org/abs/1810.10525,"We investigate opportunities and challenges for improving unsupervised machine learning using four common strategies with a long history in physics: divide-and-conquer, Occam's razor, unification and lifelong learning. Instead of using one model to learn everything, we propose a novel paradigm centered around the learning and manipulation of *theories*, which parsimoniously predict both aspects of the future (from past observations) and the domain in which these predictions are accurate. Specifically, we propose a novel generalized-mean-loss to encourage each theory to specialize in its comparatively advantageous domain, and a differentiable description length objective to downweight bad data and ""snap"" learned theories into simple symbolic formulas. Theories are stored in a ""theory hub"", which continuously unifies learned theories and can propose theories when encountering new environments. We test our implementation, the toy ""AI Physicist"" learning agent, on a suite of increasingly complex physics environments. From unsupervised observation of trajectories through worlds involving random combinations of gravity, electromagnetism, harmonic motion and elastic bounces, our agent typically learns faster and produces mean-squared prediction errors about a billion times smaller than a standard feedforward neural net of comparable complexity, typically recovering integer and rational theory parameters exactly. Our agent successfully identifies domains with different laws of motion also for a nonlinear chaotic double pendulum in a piecewise constant force field.",2019-09-19,2022-03-10 23:29:51,2022-03-10 23:29:51,2022-03-10 23:29:51,33311,,3,100,,Phys. Rev. E,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.10525,,/Users/jacquesthibodeau/Zotero/storage/BK5ZQAGH/Wu and Tegmark - 2019 - Toward an AI Physicist for Unsupervised Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/K6K7LQJB/1810.html,,,Computer Science - Machine Learning; Condensed Matter - Disordered Systems and Neural Networks; Physics - Computational Physics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YLKUHEAL,journalArticle,2021,"Caron, Mathilde; Misra, Ishan; Mairal, Julien; Goyal, Priya; Bojanowski, Piotr; Joulin, Armand",Unsupervised Learning of Visual Features by Contrasting Cluster Assignments,arXiv:2006.09882 [cs],,,,http://arxiv.org/abs/2006.09882,"Unsupervised image representations have significantly reduced the gap with supervised pretraining, notably with the recent achievements of contrastive learning methods. These contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally challenging. In this paper, we propose an online algorithm, SwAV, that takes advantage of contrastive methods without requiring to compute pairwise comparisons. Specifically, our method simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or views) of the same image, instead of comparing features directly as in contrastive learning. Simply put, we use a swapped prediction mechanism where we predict the cluster assignment of a view from the representation of another view. Our method can be trained with large and small batches and can scale to unlimited amounts of data. Compared to previous contrastive methods, our method is more memory efficient since it does not require a large memory bank or a special momentum network. In addition, we also propose a new data augmentation strategy, multi-crop, that uses a mix of views with different resolutions in place of two full-resolution views, without increasing the memory or compute requirements much. We validate our findings by achieving 75.3% top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised pretraining on all the considered transfer tasks.",2021-01-08,2022-03-10 23:29:54,2022-03-10 23:29:54,2022-03-10 23:29:54,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2006.09882,,/Users/jacquesthibodeau/Zotero/storage/R6XYQHYG/Caron et al. - 2021 - Unsupervised Learning of Visual Features by Contra.pdf; /Users/jacquesthibodeau/Zotero/storage/QZGJEIKS/2006.html,,,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
K29D7ZK6,journalArticle,2018,"Achille, Alessandro; Eccles, Tom; Matthey, Loic; Burgess, Christopher P.; Watters, Nick; Lerchner, Alexander; Higgins, Irina",Life-Long Disentangled Representation Learning with Cross-Domain Latent Homologies,"arXiv:1808.06508 [cs, stat]",,,,http://arxiv.org/abs/1808.06508,"Intelligent behaviour in the real-world requires the ability to acquire new knowledge from an ongoing sequence of experiences while preserving and reusing past knowledge. We propose a novel algorithm for unsupervised representation learning from piece-wise stationary visual data: Variational Autoencoder with Shared Embeddings (VASE). Based on the Minimum Description Length principle, VASE automatically detects shifts in the data distribution and allocates spare representational capacity to new knowledge, while simultaneously protecting previously learnt representations from catastrophic forgetting. Our approach encourages the learnt representations to be disentangled, which imparts a number of desirable properties: VASE can deal sensibly with ambiguous inputs, it can enhance its own representations through imagination-based exploration, and most importantly, it exhibits semantically meaningful sharing of latents between different datasets. Compared to baselines with entangled representations, our approach is able to reason beyond surface-level statistics and perform semantically meaningful cross-domain inference.",2018-08-20,2022-03-10 23:29:56,2022-03-10 23:29:56,2022-03-10 23:29:56,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1808.06508,,/Users/jacquesthibodeau/Zotero/storage/UTSYSDTR/Achille et al. - 2018 - Life-Long Disentangled Representation Learning wit.pdf; /Users/jacquesthibodeau/Zotero/storage/A2NWGPLN/1808.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LJ59K6J8,journalArticle,2020,"Chen, Xinlei; Fan, Haoqi; Girshick, Ross; He, Kaiming",Improved Baselines with Momentum Contrastive Learning,arXiv:2003.04297 [cs],,,,http://arxiv.org/abs/2003.04297,"Contrastive unsupervised learning has recently shown encouraging progress, e.g., in Momentum Contrast (MoCo) and SimCLR. In this note, we verify the effectiveness of two of SimCLR's design improvements by implementing them in the MoCo framework. With simple modifications to MoCo---namely, using an MLP projection head and more data augmentation---we establish stronger baselines that outperform SimCLR and do not require large training batches. We hope this will make state-of-the-art unsupervised learning research more accessible. Code will be made public.",2020-03-09,2022-03-10 23:29:58,2022-03-10 23:29:58,2022-03-10 23:29:58,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2003.04297,,/Users/jacquesthibodeau/Zotero/storage/XMD2BEG6/Chen et al. - 2020 - Improved Baselines with Momentum Contrastive Learn.pdf; /Users/jacquesthibodeau/Zotero/storage/U28LPGU9/2003.html,,,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R2CRJMSZ,journalArticle,2020,"He, Kaiming; Fan, Haoqi; Wu, Yuxin; Xie, Saining; Girshick, Ross",Momentum Contrast for Unsupervised Visual Representation Learning,arXiv:1911.05722 [cs],,,,http://arxiv.org/abs/1911.05722,"We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.",2020-03-23,2022-03-10 23:30:01,2022-03-10 23:30:01,2022-03-10 23:30:00,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1911.05722,,/Users/jacquesthibodeau/Zotero/storage/ZTMXW29F/He et al. - 2020 - Momentum Contrast for Unsupervised Visual Represen.pdf; /Users/jacquesthibodeau/Zotero/storage/7ZVW4PJE/1911.html,,,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ISG7EU3W,journalArticle,2020,"Chen, Ting; Kornblith, Simon; Norouzi, Mohammad; Hinton, Geoffrey",A Simple Framework for Contrastive Learning of Visual Representations,"arXiv:2002.05709 [cs, stat]",,,,http://arxiv.org/abs/2002.05709,"This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.",2020-06-30,2022-03-10 23:30:04,2022-03-10 23:30:04,2022-03-10 23:30:03,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2002.05709,,/Users/jacquesthibodeau/Zotero/storage/VTEC7QZ3/Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf; /Users/jacquesthibodeau/Zotero/storage/HXE8H66X/2002.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3SE2ADJU,journalArticle,2020,"Chen, Ting; Kornblith, Simon; Swersky, Kevin; Norouzi, Mohammad; Hinton, Geoffrey",Big Self-Supervised Models are Strong Semi-Supervised Learners,"arXiv:2006.10029 [cs, stat]",,,,http://arxiv.org/abs/2006.10029,"One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to common approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of big (deep and wide) networks during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2, supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9% ImageNet top-1 accuracy with just 1% of the labels ($\le$13 labeled images per class) using ResNet-50, a $10\times$ improvement in label efficiency over the previous state-of-the-art. With 10% of labels, ResNet-50 trained with our method achieves 77.5% top-1 accuracy, outperforming standard supervised training with all of the labels.",2020-10-25,2022-03-10 23:30:05,2022-03-10 23:30:05,2022-03-10 23:30:05,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2006.10029,,/Users/jacquesthibodeau/Zotero/storage/LPFI5GQ9/Chen et al. - 2020 - Big Self-Supervised Models are Strong Semi-Supervi.pdf; /Users/jacquesthibodeau/Zotero/storage/NU5PK7GU/2006.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IZ6UQ6KF,journalArticle,2020,"Tian, Yonglong; Sun, Chen; Poole, Ben; Krishnan, Dilip; Schmid, Cordelia; Isola, Phillip",What Makes for Good Views for Contrastive Learning?,arXiv:2005.10243 [cs],,,,http://arxiv.org/abs/2005.10243,"Contrastive learning between multiple views of the data has recently achieved state of the art performance in the field of self-supervised representation learning. Despite its success, the influence of different view choices has been less studied. In this paper, we use theoretical and empirical analysis to better understand the importance of view selection, and argue that we should reduce the mutual information (MI) between views while keeping task-relevant information intact. To verify this hypothesis, we devise unsupervised and semi-supervised frameworks that learn effective views by aiming to reduce their MI. We also consider data augmentation as a way to reduce MI, and show that increasing data augmentation indeed leads to decreasing MI and improves downstream classification accuracy. As a by-product, we achieve a new state-of-the-art accuracy on unsupervised pre-training for ImageNet classification ($73\%$ top-1 linear readout with a ResNet-50). In addition, transferring our models to PASCAL VOC object detection and COCO instance segmentation consistently outperforms supervised pre-training. Code:http://github.com/HobbitLong/PyContrast",2020-12-18,2022-03-10 23:30:37,2022-03-10 23:30:37,2022-03-10 23:30:36,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2005.10243,,/Users/jacquesthibodeau/Zotero/storage/TEKWZMBS/Tian et al. - 2020 - What Makes for Good Views for Contrastive Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/ZBGSDZUW/2005.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XAJBTWKI,journalArticle,2019,"Hsu, Kyle; Levine, Sergey; Finn, Chelsea",Unsupervised Learning via Meta-Learning,"arXiv:1810.02334 [cs, stat]",,,,http://arxiv.org/abs/1810.02334,"A central goal of unsupervised learning is to acquire representations from unlabeled data or experience that can be used for more effective learning of downstream tasks from modest amounts of labeled data. Many prior unsupervised learning works aim to do so by developing proxy objectives based on reconstruction, disentanglement, prediction, and other metrics. Instead, we develop an unsupervised meta-learning method that explicitly optimizes for the ability to learn a variety of tasks from small amounts of data. To do so, we construct tasks from unlabeled data in an automatic way and run meta-learning over the constructed tasks. Surprisingly, we find that, when integrated with meta-learning, relatively simple task construction mechanisms, such as clustering embeddings, lead to good performance on a variety of downstream, human-specified tasks. Our experiments across four image datasets indicate that our unsupervised meta-learning approach acquires a learning algorithm without any labeled data that is applicable to a wide range of downstream classification tasks, improving upon the embedding learned by four prior unsupervised learning methods.",2019-03-21,2022-03-10 23:30:40,2022-03-10 23:30:40,2022-03-10 23:30:40,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.02334,,/Users/jacquesthibodeau/Zotero/storage/MGR7CLFY/Hsu et al. - 2019 - Unsupervised Learning via Meta-Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/V2AZ53GA/1810.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KG28ZHXU,journalArticle,2019,"Donahue, Jeff; Simonyan, Karen",Large Scale Adversarial Representation Learning,"arXiv:1907.02544 [cs, stat]",,,,http://arxiv.org/abs/1907.02544,"Adversarially trained generative models (GANs) have recently achieved compelling image synthesis results. But despite early successes in using GANs for unsupervised representation learning, they have since been superseded by approaches based on self-supervision. In this work we show that progress in image generation quality translates to substantially improved representation learning performance. Our approach, BigBiGAN, builds upon the state-of-the-art BigGAN model, extending it to representation learning by adding an encoder and modifying the discriminator. We extensively evaluate the representation learning and generation capabilities of these BigBiGAN models, demonstrating that these generation-based models achieve the state of the art in unsupervised representation learning on ImageNet, as well as in unconditional image generation. Pretrained BigBiGAN models -- including image generators and encoders -- are available on TensorFlow Hub (https://tfhub.dev/s?publisher=deepmind&q=bigbigan).",2019-11-05,2022-03-10 23:30:42,2022-03-10 23:30:42,2022-03-10 23:30:42,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1907.02544,,/Users/jacquesthibodeau/Zotero/storage/3TRCNRT2/Donahue and Simonyan - 2019 - Large Scale Adversarial Representation Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/NJ42RJ2H/1907.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5R85AEAQ,journalArticle,2019,"Such, Felipe Petroski; Rawal, Aditya; Lehman, Joel; Stanley, Kenneth O.; Clune, Jeff",Generative Teaching Networks: Accelerating Neural Architecture Search by Learning to Generate Synthetic Training Data,"arXiv:1912.07768 [cs, stat]",,,,http://arxiv.org/abs/1912.07768,"This paper investigates the intriguing question of whether we can create learning algorithms that automatically generate training data, learning environments, and curricula in order to help AI agents rapidly learn. We show that such algorithms are possible via Generative Teaching Networks (GTNs), a general approach that is, in theory, applicable to supervised, unsupervised, and reinforcement learning, although our experiments only focus on the supervised case. GTNs are deep neural networks that generate data and/or training environments that a learner (e.g. a freshly initialized neural network) trains on for a few SGD steps before being tested on a target task. We then differentiate through the entire learning process via meta-gradients to update the GTN parameters to improve performance on the target task. GTNs have the beneficial property that they can theoretically generate any type of data or training environment, making their potential impact large. This paper introduces GTNs, discusses their potential, and showcases that they can substantially accelerate learning. We also demonstrate a practical and exciting application of GTNs: accelerating the evaluation of candidate architectures for neural architecture search (NAS), which is rate-limited by such evaluations, enabling massive speed-ups in NAS. GTN-NAS improves the NAS state of the art, finding higher performing architectures when controlling for the search proposal mechanism. GTN-NAS also is competitive with the overall state of the art approaches, which achieve top performance while using orders of magnitude less computation than typical NAS methods. Speculating forward, GTNs may represent a first step toward the ambitious goal of algorithms that generate their own training data and, in doing so, open a variety of interesting new research questions and directions.",2019-12-16,2022-03-10 23:30:44,2022-03-10 23:30:44,2022-03-10 23:30:44,,,,,,,Generative Teaching Networks,,,,,,,,,,,,arXiv.org,,arXiv: 1912.07768,,/Users/jacquesthibodeau/Zotero/storage/4L34H87B/Such et al. - 2019 - Generative Teaching Networks Accelerating Neural .pdf; /Users/jacquesthibodeau/Zotero/storage/4F4KFVMX/1912.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IH2ZRA7F,journalArticle,2019,"Poole, Ben; Ozair, Sherjil; Oord, Aaron van den; Alemi, Alexander A.; Tucker, George",On Variational Bounds of Mutual Information,"arXiv:1905.06922 [cs, stat]",,,,http://arxiv.org/abs/1905.06922,"Estimating and optimizing Mutual Information (MI) is core to many problems in machine learning; however, bounding MI in high dimensions is challenging. To establish tractable and scalable objectives, recent work has turned to variational bounds parameterized by neural networks, but the relationships and tradeoffs between these bounds remains unclear. In this work, we unify these recent developments in a single framework. We find that the existing variational lower bounds degrade when the MI is large, exhibiting either high bias or high variance. To address this problem, we introduce a continuum of lower bounds that encompasses previous bounds and flexibly trades off bias and variance. On high-dimensional, controlled problems, we empirically characterize the bias and variance of the bounds and their gradients and demonstrate the effectiveness of our new bounds for estimation and representation learning.",2019-05-16,2022-03-10 23:30:47,2022-03-10 23:30:47,2022-03-10 23:30:47,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1905.06922,,/Users/jacquesthibodeau/Zotero/storage/PR4PFNKJ/Poole et al. - 2019 - On Variational Bounds of Mutual Information.pdf; /Users/jacquesthibodeau/Zotero/storage/5ENBWI8C/1905.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IGYJH9VF,journalArticle,2019,"Oord, Aaron van den; Li, Yazhe; Vinyals, Oriol",Representation Learning with Contrastive Predictive Coding,"arXiv:1807.03748 [cs, stat]",,,,http://arxiv.org/abs/1807.03748,"While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.",2019-01-22,2022-03-10 23:30:51,2022-03-10 23:30:51,2022-03-10 23:30:50,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1807.03748,,/Users/jacquesthibodeau/Zotero/storage/7DB8L8JV/Oord et al. - 2019 - Representation Learning with Contrastive Predictiv.pdf; /Users/jacquesthibodeau/Zotero/storage/58J4WGLT/1807.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EBALVEK3,journalArticle,2019,"Nalisnick, Eric; Matsukawa, Akihiro; Teh, Yee Whye; Gorur, Dilan; Lakshminarayanan, Balaji",Do Deep Generative Models Know What They Don't Know?,"arXiv:1810.09136 [cs, stat]",,,,http://arxiv.org/abs/1810.09136,"A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data. A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong. Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel, out-of-distribution inputs. In this paper we challenge this assumption. We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former. Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN. To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood. We find such behavior persists even when we restrict the flows to constant-volume transformations. These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.",2019-02-24,2022-03-10 23:30:54,2022-03-10 23:30:54,2022-03-10 23:30:53,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.09136,,/Users/jacquesthibodeau/Zotero/storage/WU6X5QAU/Nalisnick et al. - 2019 - Do Deep Generative Models Know What They Don't Kno.pdf; /Users/jacquesthibodeau/Zotero/storage/SC7UELE7/1810.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JWSF5CDY,journalArticle,2019,"Sarma, Gopal P.; Safron, Adam; Hay, Nick J.","Integrative Biological Simulation, Neuropsychology, and AI Safety","arXiv:1811.03493 [cs, q-bio]",,,,http://arxiv.org/abs/1811.03493,"We describe a biologically-inspired research agenda with parallel tracks aimed at AI and AI safety. The bottom-up component consists of building a sequence of biophysically realistic simulations of simple organisms such as the nematode $Caenorhabditis$ $elegans$, the fruit fly $Drosophila$ $melanogaster$, and the zebrafish $Danio$ $rerio$ to serve as platforms for research into AI algorithms and system architectures. The top-down component consists of an approach to value alignment that grounds AI goal structures in neuropsychology, broadly considered. Our belief is that parallel pursuit of these tracks will inform the development of value-aligned AI systems that have been inspired by embodied organisms with sensorimotor integration. An important set of side benefits is that the research trajectories we describe here are grounded in long-standing intellectual traditions within existing research communities and funding structures. In addition, these research programs overlap with significant contemporary themes in the biological and psychological sciences such as data/model integration and reproducibility.",2019-01-21,2022-03-10 23:30:57,2022-03-10 23:30:57,2022-03-10 23:30:56,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1811.03493,,"/Users/jacquesthibodeau/Zotero/storage/Q646HVPK/Sarma et al. - 2019 - Integrative Biological Simulation, Neuropsychology.pdf; /Users/jacquesthibodeau/Zotero/storage/NA46YJRB/1811.html",,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Quantitative Biology - Neurons and Cognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JCNAU9SA,journalArticle,2021,"Kenton, Zachary; Everitt, Tom; Weidinger, Laura; Gabriel, Iason; Mikulik, Vladimir; Irving, Geoffrey",Alignment of Language Agents,arXiv:2103.14659 [cs],,,,http://arxiv.org/abs/2103.14659,"For artificial intelligence to be beneficial to humans the behaviour of AI agents needs to be aligned with what humans want. In this paper we discuss some behavioural issues for language agents, arising from accidental misspecification by the system designer. We highlight some ways that misspecification can occur and discuss some behavioural issues that could arise from misspecification, including deceptive or manipulative language, and review some approaches for avoiding these issues.",2021-03-26,2022-03-10 23:31:14,2022-03-10 23:31:14,2022-03-10 23:31:14,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2103.14659,,/Users/jacquesthibodeau/Zotero/storage/I6V3PXR9/Kenton et al. - 2021 - Alignment of Language Agents.pdf; /Users/jacquesthibodeau/Zotero/storage/XNKXXRLT/2103.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7DHHIFUP,journalArticle,2018,"Hadfield-Menell, Dylan; Hadfield, Gillian",Incomplete Contracting and AI Alignment,arXiv:1804.04268 [cs],,,,http://arxiv.org/abs/1804.04268,"We suggest that the analysis of incomplete contracting developed by law and economics researchers can provide a useful framework for understanding the AI alignment problem and help to generate a systematic approach to finding solutions. We first provide an overview of the incomplete contracting literature and explore parallels between this work and the problem of AI alignment. As we emphasize, misalignment between principal and agent is a core focus of economic analysis. We highlight some technical results from the economics literature on incomplete contracts that may provide insights for AI alignment researchers. Our core contribution, however, is to bring to bear an insight that economists have been urged to absorb from legal scholars and other behavioral scientists: the fact that human contracting is supported by substantial amounts of external structure, such as generally available institutions (culture, law) that can supply implied terms to fill the gaps in incomplete contracts. We propose a research agenda for AI alignment work that focuses on the problem of how to build AI that can replicate the human cognitive processes that connect individual incomplete contracts with this supporting external structure.",2018-04-11,2022-03-10 23:31:19,2022-03-10 23:31:19,2022-03-10 23:31:19,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1804.04268,,/Users/jacquesthibodeau/Zotero/storage/4EI2HVCR/Hadfield-Menell and Hadfield - 2018 - Incomplete Contracting and AI Alignment.pdf; /Users/jacquesthibodeau/Zotero/storage/P92DGWTY/1804.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DIS5R3VI,journalArticle,2020,"Arora, Saurabh; Doshi, Prashant","A Survey of Inverse Reinforcement Learning: Challenges, Methods and Progress","arXiv:1806.06877 [cs, stat]",,,,http://arxiv.org/abs/1806.06877,"Inverse reinforcement learning (IRL) is the problem of inferring the reward function of an agent, given its policy or observed behavior. Analogous to RL, IRL is perceived both as a problem and as a class of methods. By categorically surveying the current literature in IRL, this article serves as a reference for researchers and practitioners of machine learning and beyond to understand the challenges of IRL and select the approaches best suited for the problem on hand. The survey formally introduces the IRL problem along with its central challenges such as the difficulty in performing accurate inference and its generalizability, its sensitivity to prior knowledge, and the disproportionate growth in solution complexity with problem size. The article elaborates how the current methods mitigate these challenges. We further discuss the extensions to traditional IRL methods for handling: inaccurate and incomplete perception, an incomplete model, multiple reward functions, and nonlinear reward functions. This survey concludes the discussion with some broad advances in the research area and currently open research questions.",2020-11-18,2022-03-10 23:31:23,2022-03-10 23:31:23,2022-03-10 23:31:23,,,,,,,A Survey of Inverse Reinforcement Learning,,,,,,,,,,,,arXiv.org,,arXiv: 1806.06877,,/Users/jacquesthibodeau/Zotero/storage/WEK8IJYX/Arora and Doshi - 2020 - A Survey of Inverse Reinforcement Learning Challe.pdf; /Users/jacquesthibodeau/Zotero/storage/XWANUQBY/1806.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A6HKXX9U,journalArticle,2018,"Fu, Justin; Luo, Katie; Levine, Sergey",Learning Robust Rewards with Adversarial Inverse Reinforcement Learning,arXiv:1710.11248 [cs],,,,http://arxiv.org/abs/1710.11248,"Reinforcement learning provides a powerful and general framework for decision making and control, but its application in practice is often hindered by the need for extensive feature and reward engineering. Deep reinforcement learning methods can remove the need for explicit engineering of policy or value features, but still require a manually specified reward function. Inverse reinforcement learning holds the promise of automatic reward acquisition, but has proven exceptionally difficult to apply to large, high-dimensional problems with unknown dynamics. In this work, we propose adverserial inverse reinforcement learning (AIRL), a practical and scalable inverse reinforcement learning algorithm based on an adversarial reward learning formulation. We demonstrate that AIRL is able to recover reward functions that are robust to changes in dynamics, enabling us to learn policies even under significant variation in the environment seen during training. Our experiments show that AIRL greatly outperforms prior methods in these transfer settings.",2018-08-13,2022-03-10 23:31:27,2022-03-10 23:31:27,2022-03-10 23:31:27,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1710.11248,,/Users/jacquesthibodeau/Zotero/storage/N6ZR9CXI/Fu et al. - 2018 - Learning Robust Rewards with Adversarial Inverse R.pdf; /Users/jacquesthibodeau/Zotero/storage/L7F788SS/1710.html,,,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
338YA4C6,journalArticle,2019,"Finlayson, Samuel G.; Chung, Hyung Won; Kohane, Isaac S.; Beam, Andrew L.",Adversarial Attacks Against Medical Deep Learning Systems,"arXiv:1804.05296 [cs, stat]",,,,http://arxiv.org/abs/1804.05296,"The discovery of adversarial examples has raised concerns about the practical deployment of deep learning systems. In this paper, we demonstrate that adversarial examples are capable of manipulating deep learning systems across three clinical domains. For each of our representative medical deep learning classifiers, both white and black box attacks were highly successful. Our models are representative of the current state of the art in medical computer vision and, in some cases, directly reflect architectures already seeing deployment in real world clinical settings. In addition to the technical contribution of our paper, we synthesize a large body of knowledge about the healthcare system to argue that medicine may be uniquely susceptible to adversarial attacks, both in terms of monetary incentives and technical vulnerability. To this end, we outline the healthcare economy and the incentives it creates for fraud and provide concrete examples of how and why such attacks could be realistically carried out. We urge practitioners to be aware of current vulnerabilities when deploying deep learning systems in clinical settings, and encourage the machine learning community to further investigate the domain-specific characteristics of medical learning systems.",2019-02-04,2022-03-10 23:31:30,2022-03-10 23:31:30,2022-03-10 23:31:30,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1804.05296,,/Users/jacquesthibodeau/Zotero/storage/GLJQX445/Finlayson et al. - 2019 - Adversarial Attacks Against Medical Deep Learning .pdf; /Users/jacquesthibodeau/Zotero/storage/7FMGFWFQ/1804.html,,,Computer Science - Computers and Society; Computer Science - Cryptography and Security; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WLKIYYC3,journalArticle,2019,"Wang, Haohan; He, Zexue; Lipton, Zachary C.; Xing, Eric P.",Learning Robust Representations by Projecting Superficial Statistics Out,arXiv:1903.06256 [cs],,,,http://arxiv.org/abs/1903.06256,"Despite impressive performance as evaluated on i.i.d. holdout data, deep neural networks depend heavily on superficial statistics of the training data and are liable to break under distribution shift. For example, subtle changes to the background or texture of an image can break a seemingly powerful classifier. Building on previous work on domain generalization, we hope to produce a classifier that will generalize to previously unseen domains, even when domain identifiers are not available during training. This setting is challenging because the model may extract many distribution-specific (superficial) signals together with distribution-agnostic (semantic) signals. To overcome this challenge, we incorporate the gray-level co-occurrence matrix (GLCM) to extract patterns that our prior knowledge suggests are superficial: they are sensitive to the texture but unable to capture the gestalt of an image. Then we introduce two techniques for improving our networks' out-of-sample performance. The first method is built on the reverse gradient method that pushes our model to learn representations from which the GLCM representation is not predictable. The second method is built on the independence introduced by projecting the model's representation onto the subspace orthogonal to GLCM representation's. We test our method on the battery of standard domain generalization data sets and, interestingly, achieve comparable or better performance as compared to other domain generalization methods that explicitly require samples from the target distribution for training.",2019-03-01,2022-03-10 23:31:33,2022-03-10 23:31:33,2022-03-10 23:31:33,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1903.06256,,/Users/jacquesthibodeau/Zotero/storage/VVT9JGP2/Wang et al. - 2019 - Learning Robust Representations by Projecting Supe.pdf; /Users/jacquesthibodeau/Zotero/storage/2BF7TC59/1903.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PR9VMLKF,journalArticle,2019,"Shu, Jun; Xie, Qi; Yi, Lixuan; Zhao, Qian; Zhou, Sanping; Xu, Zongben; Meng, Deyu",Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting,"arXiv:1902.07379 [cs, stat]",,,,http://arxiv.org/abs/1902.07379,"Current deep neural networks (DNNs) can easily overfit to biased training data with corrupted labels or class imbalance. Sample re-weighting strategy is commonly used to alleviate this issue by designing a weighting function mapping from training loss to sample weight, and then iterating between weight recalculating and classifier updating. Current approaches, however, need manually pre-specify the weighting function as well as its additional hyper-parameters. It makes them fairly hard to be generally applied in practice due to the significant variation of proper weighting schemes relying on the investigated problem and training data. To address this issue, we propose a method capable of adaptively learning an explicit weighting function directly from data. The weighting function is an MLP with one hidden layer, constituting a universal approximator to almost any continuous functions, making the method able to fit a wide range of weighting functions including those assumed in conventional research. Guided by a small amount of unbiased meta-data, the parameters of the weighting function can be finely updated simultaneously with the learning process of the classifiers. Synthetic and real experiments substantiate the capability of our method for achieving proper weighting functions in class imbalance and noisy label cases, fully complying with the common settings in traditional methods, and more complicated scenarios beyond conventional cases. This naturally leads to its better accuracy than other state-of-the-art methods.",2019-09-26,2022-03-10 23:31:37,2022-03-10 23:31:37,2022-03-10 23:31:36,,,,,,,Meta-Weight-Net,,,,,,,,,,,,arXiv.org,,arXiv: 1902.07379,,/Users/jacquesthibodeau/Zotero/storage/3G9Q94E8/Shu et al. - 2019 - Meta-Weight-Net Learning an Explicit Mapping For .pdf; /Users/jacquesthibodeau/Zotero/storage/AISTQGAD/1902.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5PRKZMIW,journalArticle,2020,"Wang, Jingkang; Liu, Yang; Li, Bo",Reinforcement Learning with Perturbed Rewards,"arXiv:1810.01032 [cs, stat]",,,,http://arxiv.org/abs/1810.01032,"Recent studies have shown that reinforcement learning (RL) models are vulnerable in various noisy scenarios. For instance, the observed reward channel is often subject to noise in practice (e.g., when rewards are collected through sensors), and is therefore not credible. In addition, for applications such as robotics, a deep reinforcement learning (DRL) algorithm can be manipulated to produce arbitrary errors by receiving corrupted rewards. In this paper, we consider noisy RL problems with perturbed rewards, which can be approximated with a confusion matrix. We develop a robust RL framework that enables agents to learn in noisy environments where only perturbed rewards are observed. Our solution framework builds on existing RL/DRL algorithms and firstly addresses the biased noisy reward setting without any assumptions on the true distribution (e.g., zero-mean Gaussian noise as made in previous works). The core ideas of our solution include estimating a reward confusion matrix and defining a set of unbiased surrogate rewards. We prove the convergence and sample complexity of our approach. Extensive experiments on different DRL platforms show that trained policies based on our estimated surrogate reward can achieve higher expected rewards, and converge faster than existing baselines. For instance, the state-of-the-art PPO algorithm is able to obtain 84.6% and 80.8% improvements on average score for five Atari games, with error rates as 10% and 30% respectively.",2020-02-01,2022-03-10 23:31:41,2022-03-10 23:31:41,2022-03-10 23:31:41,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.01032,,/Users/jacquesthibodeau/Zotero/storage/8KAIDS5X/Wang et al. - 2020 - Reinforcement Learning with Perturbed Rewards.pdf; /Users/jacquesthibodeau/Zotero/storage/WSD7A3JE/1810.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Cryptography and Security; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UGPIVX9K,journalArticle,2019,"Lee, Gilwoo; Hou, Brian; Mandalika, Aditya; Lee, Jeongseok; Choudhury, Sanjiban; Srinivasa, Siddhartha S.",Bayesian Policy Optimization for Model Uncertainty,arXiv:1810.01014 [cs],,,,http://arxiv.org/abs/1810.01014,"Addressing uncertainty is critical for autonomous systems to robustly adapt to the real world. We formulate the problem of model uncertainty as a continuous Bayes-Adaptive Markov Decision Process (BAMDP), where an agent maintains a posterior distribution over latent model parameters given a history of observations and maximizes its expected long-term reward with respect to this belief distribution. Our algorithm, Bayesian Policy Optimization, builds on recent policy optimization algorithms to learn a universal policy that navigates the exploration-exploitation trade-off to maximize the Bayesian value function. To address challenges from discretizing the continuous latent parameter space, we propose a new policy network architecture that encodes the belief distribution independently from the observable state. Our method significantly outperforms algorithms that address model uncertainty without explicitly reasoning about belief distributions and is competitive with state-of-the-art Partially Observable Markov Decision Process solvers.",2019-05-08,2022-03-10 23:31:45,2022-03-10 23:31:45,2022-03-10 23:31:44,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.01014,,/Users/jacquesthibodeau/Zotero/storage/RQHIJMH5/Lee et al. - 2019 - Bayesian Policy Optimization for Model Uncertainty.pdf; /Users/jacquesthibodeau/Zotero/storage/D95ZFZJB/1810.html,,,Computer Science - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GMK3D49K,journalArticle,2019,"Geirhos, Robert; Rubisch, Patricia; Michaelis, Claudio; Bethge, Matthias; Wichmann, Felix A.; Brendel, Wieland",ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness,"arXiv:1811.12231 [cs, q-bio, stat]",,,,http://arxiv.org/abs/1811.12231,"Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict. We show that ImageNet-trained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on ""Stylized-ImageNet"", a stylized version of ImageNet. This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation.",2019-01-14,2022-03-10 23:31:48,2022-03-10 23:31:48,2022-03-10 23:31:48,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1811.12231,,/Users/jacquesthibodeau/Zotero/storage/XG9RN9SM/Geirhos et al. - 2019 - ImageNet-trained CNNs are biased towards texture; .pdf; /Users/jacquesthibodeau/Zotero/storage/2P45AY2Z/1811.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Quantitative Biology - Neurons and Cognition; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MS2W42JX,journalArticle,2019,"Ford, Nic; Gilmer, Justin; Carlini, Nicolas; Cubuk, Dogus",Adversarial Examples Are a Natural Consequence of Test Error in Noise,"arXiv:1901.10513 [cs, stat]",,,,http://arxiv.org/abs/1901.10513,"Over the last few years, the phenomenon of adversarial examples --- maliciously constructed inputs that fool trained machine learning models --- has captured the attention of the research community, especially when the adversary is restricted to small modifications of a correctly handled input. Less surprisingly, image classifiers also lack human-level performance on randomly corrupted images, such as images with additive Gaussian noise. In this paper we provide both empirical and theoretical evidence that these are two manifestations of the same underlying phenomenon, establishing close connections between the adversarial robustness and corruption robustness research programs. This suggests that improving adversarial robustness should go hand in hand with improving performance in the presence of more general and realistic image corruptions. Based on our results we recommend that future adversarial defenses consider evaluating the robustness of their methods to distributional shift with benchmarks such as Imagenet-C.",2019-01-29,2022-03-10 23:31:53,2022-03-10 23:31:53,2022-03-10 23:31:53,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1901.10513,,/Users/jacquesthibodeau/Zotero/storage/Y4497GGQ/Ford et al. - 2019 - Adversarial Examples Are a Natural Consequence of .pdf; /Users/jacquesthibodeau/Zotero/storage/U9HB466V/1901.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8VI85HKE,journalArticle,2019,"Li, Mingchen; Soltanolkotabi, Mahdi; Oymak, Samet",Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks,"arXiv:1903.11680 [cs, stat]",,,,http://arxiv.org/abs/1903.11680,"Modern neural networks are typically trained in an over-parameterized regime where the parameters of the model far exceed the size of the training data. Such neural networks in principle have the capacity to (over)fit any set of labels including pure noise. Despite this, somewhat paradoxically, neural network models trained via first-order methods continue to predict well on yet unseen test data. This paper takes a step towards demystifying this phenomena. Under a rich dataset model, we show that gradient descent is provably robust to noise/corruption on a constant fraction of the labels despite overparameterization. In particular, we prove that: (i) In the first few iterations where the updates are still in the vicinity of the initialization gradient descent only fits to the correct labels essentially ignoring the noisy labels. (ii) to start to overfit to the noisy labels network must stray rather far from from the initialization which can only occur after many more iterations. Together, these results show that gradient descent with early stopping is provably robust to label noise and shed light on the empirical robustness of deep networks as well as commonly adopted heuristics to prevent overfitting.",2019-07-03,2022-03-10 23:32:08,2022-03-10 23:32:08,2022-03-10 23:32:08,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1903.11680,,/Users/jacquesthibodeau/Zotero/storage/L82S5IKG/Li et al. - 2019 - Gradient Descent with Early Stopping is Provably R.pdf; /Users/jacquesthibodeau/Zotero/storage/NU5BI2H5/1903.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2QAHLQLS,journalArticle,2020,"Jin, Di; Jin, Zhijing; Zhou, Joey Tianyi; Szolovits, Peter",Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment,arXiv:1907.11932 [cs],,,,http://arxiv.org/abs/1907.11932,"Machine learning algorithms are often vulnerable to adversarial examples that have imperceptible alterations from the original counterparts but can fool the state-of-the-art models. It is helpful to evaluate or even improve the robustness of these models by exposing the maliciously crafted adversarial examples. In this paper, we present TextFooler, a simple but strong baseline to generate natural adversarial text. By applying it to two fundamental natural language tasks, text classification and textual entailment, we successfully attacked three target models, including the powerful pre-trained BERT, and the widely used convolutional and recurrent neural networks. We demonstrate the advantages of this framework in three ways: (1) effective---it outperforms state-of-the-art attacks in terms of success rate and perturbation rate, (2) utility-preserving---it preserves semantic content and grammaticality, and remains correctly classified by humans, and (3) efficient---it generates adversarial text with computational complexity linear to the text length. *The code, pre-trained target models, and test examples are available at https://github.com/jind11/TextFooler.",2020-04-08,2022-03-10 23:32:12,2022-03-10 23:32:12,2022-03-10 23:32:12,,,,,,,Is BERT Really Robust?,,,,,,,,,,,,arXiv.org,,arXiv: 1907.11932,,/Users/jacquesthibodeau/Zotero/storage/A7EEW2Q3/Jin et al. - 2020 - Is BERT Really Robust A Strong Baseline for Natur.pdf; /Users/jacquesthibodeau/Zotero/storage/KZHEFNXF/1907.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VZ4VTA6B,journalArticle,2021,"Knott, Paul; Carroll, Micah; Devlin, Sam; Ciosek, Kamil; Hofmann, Katja; Dragan, A. D.; Shah, Rohin",Evaluating the Robustness of Collaborative Agents,arXiv:2101.05507 [cs],,,,http://arxiv.org/abs/2101.05507,"In order for agents trained by deep reinforcement learning to work alongside humans in realistic settings, we will need to ensure that the agents are \emph{robust}. Since the real world is very diverse, and human behavior often changes in response to agent deployment, the agent will likely encounter novel situations that have never been seen during training. This results in an evaluation challenge: if we cannot rely on the average training or validation reward as a metric, then how can we effectively evaluate robustness? We take inspiration from the practice of \emph{unit testing} in software engineering. Specifically, we suggest that when designing AI agents that collaborate with humans, designers should search for potential edge cases in \emph{possible partner behavior} and \emph{possible states encountered}, and write tests which check that the behavior of the agent in these edge cases is reasonable. We apply this methodology to build a suite of unit tests for the Overcooked-AI environment, and use this test suite to evaluate three proposals for improving robustness. We find that the test suite provides significant insight into the effects of these proposals that were generally not revealed by looking solely at the average validation reward.",2021-01-14,2022-03-10 23:32:16,2022-03-10 23:32:16,2022-03-10 23:32:16,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2101.05507,,/Users/jacquesthibodeau/Zotero/storage/YXLZ9BE4/Knott et al. - 2021 - Evaluating the Robustness of Collaborative Agents.pdf; /Users/jacquesthibodeau/Zotero/storage/6QGHNQAW/2101.html,,,Computer Science - Artificial Intelligence; Computer Science - Human-Computer Interaction; Computer Science - Machine Learning; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8XTDQMV5,journalArticle,2020,"Fischer, Ian; Alemi, Alexander A.",CEB Improves Model Robustness,Entropy,,1099-4300,10.3390/e22101081,http://arxiv.org/abs/2002.05380,"We demonstrate that the Conditional Entropy Bottleneck (CEB) can improve model robustness. CEB is an easy strategy to implement and works in tandem with data augmentation procedures. We report results of a large scale adversarial robustness study on CIFAR-10, as well as the ImageNet-C Common Corruptions Benchmark, ImageNet-A, and PGD attacks.",2020-09-25,2022-03-10 23:32:21,2022-03-10 23:32:21,2022-03-10 23:32:21,1081,,10,22,,Entropy,,,,,,,,,,,,,arXiv.org,,arXiv: 2002.05380,,/Users/jacquesthibodeau/Zotero/storage/5G7CCSFQ/Fischer and Alemi - 2020 - CEB Improves Model Robustness.pdf; /Users/jacquesthibodeau/Zotero/storage/H8B8ED27/2002.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KPQ9HIXG,journalArticle,2020,"Fischer, Ian",The Conditional Entropy Bottleneck,"arXiv:2002.05379 [cs, stat]",,,,http://arxiv.org/abs/2002.05379,"Much of the field of Machine Learning exhibits a prominent set of failure modes, including vulnerability to adversarial examples, poor out-of-distribution (OoD) detection, miscalibration, and willingness to memorize random labelings of datasets. We characterize these as failures of robust generalization, which extends the traditional measure of generalization as accuracy or related metrics on a held-out set. We hypothesize that these failures to robustly generalize are due to the learning systems retaining too much information about the training data. To test this hypothesis, we propose the Minimum Necessary Information (MNI) criterion for evaluating the quality of a model. In order to train models that perform well with respect to the MNI criterion, we present a new objective function, the Conditional Entropy Bottleneck (CEB), which is closely related to the Information Bottleneck (IB). We experimentally test our hypothesis by comparing the performance of CEB models with deterministic models and Variational Information Bottleneck (VIB) models on a variety of different datasets and robustness challenges. We find strong empirical evidence supporting our hypothesis that MNI models improve on these problems of robust generalization.",2020-02-13,2022-03-10 23:32:24,2022-03-10 23:32:24,2022-03-10 23:32:23,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2002.05379,,/Users/jacquesthibodeau/Zotero/storage/JHTX5VQV/Fischer - 2020 - The Conditional Entropy Bottleneck.pdf; /Users/jacquesthibodeau/Zotero/storage/PD5DB5JZ/2002.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NY834NMT,journalArticle,2020,"Lohn, Andrew J.",Estimating the Brittleness of AI: Safety Integrity Levels and the Need for Testing Out-Of-Distribution Performance,"arXiv:2009.00802 [cs, stat]",,,,http://arxiv.org/abs/2009.00802,"Test, Evaluation, Verification, and Validation (TEVV) for Artificial Intelligence (AI) is a challenge that threatens to limit the economic and societal rewards that AI researchers have devoted themselves to producing. A central task of TEVV for AI is estimating brittleness, where brittleness implies that the system functions well within some bounds and poorly outside of those bounds. This paper argues that neither of those criteria are certain of Deep Neural Networks. First, highly touted AI successes (eg. image classification and speech recognition) are orders of magnitude more failure-prone than are typically certified in critical systems even within design bounds (perfectly in-distribution sampling). Second, performance falls off only gradually as inputs become further Out-Of-Distribution (OOD). Enhanced emphasis is needed on designing systems that are resilient despite failure-prone AI components as well as on evaluating and improving OOD performance in order to get AI to where it can clear the challenging hurdles of TEVV and certification.",2020-09-01,2022-03-10 23:32:26,2022-03-10 23:32:26,2022-03-10 23:32:26,,,,,,,Estimating the Brittleness of AI,,,,,,,,,,,,arXiv.org,,arXiv: 2009.00802,,/Users/jacquesthibodeau/Zotero/storage/TE6LIEY2/Lohn - 2020 - Estimating the Brittleness of AI Safety Integrity.pdf; /Users/jacquesthibodeau/Zotero/storage/T4F57XBG/2009.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Computers and Society; Computer Science - Machine Learning; Computer Science - Software Engineering; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HMWHWPUK,journalArticle,2020,"D'Amour, Alexander; Heller, Katherine; Moldovan, Dan; Adlam, Ben; Alipanahi, Babak; Beutel, Alex; Chen, Christina; Deaton, Jonathan; Eisenstein, Jacob; Hoffman, Matthew D.; Hormozdiari, Farhad; Houlsby, Neil; Hou, Shaobo; Jerfel, Ghassen; Karthikesalingam, Alan; Lucic, Mario; Ma, Yian; McLean, Cory; Mincu, Diana; Mitani, Akinori; Montanari, Andrea; Nado, Zachary; Natarajan, Vivek; Nielson, Christopher; Osborne, Thomas F.; Raman, Rajiv; Ramasamy, Kim; Sayres, Rory; Schrouff, Jessica; Seneviratne, Martin; Sequeira, Shannon; Suresh, Harini; Veitch, Victor; Vladymyrov, Max; Wang, Xuezhi; Webster, Kellie; Yadlowsky, Steve; Yun, Taedong; Zhai, Xiaohua; Sculley, D.",Underspecification Presents Challenges for Credibility in Modern Machine Learning,"arXiv:2011.03395 [cs, stat]",,,,http://arxiv.org/abs/2011.03395,"ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.",2020-11-24,2022-03-10 23:32:31,2022-03-10 23:32:31,2022-03-10 23:32:31,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2011.03395,,/Users/jacquesthibodeau/Zotero/storage/QIEG7P5B/D'Amour et al. - 2020 - Underspecification Presents Challenges for Credibi.pdf; /Users/jacquesthibodeau/Zotero/storage/DHIPDEGK/2011.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
U9PCCVX2,journalArticle,2020,"Nakkiran, Preetum; Bansal, Yamini",Distributional Generalization: A New Kind of Generalization,"arXiv:2009.08092 [cs, math, stat]",,,,http://arxiv.org/abs/2009.08092,"We introduce a new notion of generalization -- Distributional Generalization -- which roughly states that outputs of a classifier at train and test time are close *as distributions*, as opposed to close in just their average error. For example, if we mislabel 30% of dogs as cats in the train set of CIFAR-10, then a ResNet trained to interpolation will in fact mislabel roughly 30% of dogs as cats on the *test set* as well, while leaving other classes unaffected. This behavior is not captured by classical generalization, which would only consider the average error and not the distribution of errors over the input domain. Our formal conjectures, which are much more general than this example, characterize the form of distributional generalization that can be expected in terms of problem parameters: model architecture, training procedure, number of samples, and data distribution. We give empirical evidence for these conjectures across a variety of domains in machine learning, including neural networks, kernel machines, and decision trees. Our results thus advance our empirical understanding of interpolating classifiers.",2020-10-14,2022-03-10 23:32:36,2022-03-10 23:32:36,2022-03-10 23:32:36,,,,,,,Distributional Generalization,,,,,,,,,,,,arXiv.org,,arXiv: 2009.08092,,/Users/jacquesthibodeau/Zotero/storage/978GXKWJ/Nakkiran and Bansal - 2020 - Distributional Generalization A New Kind of Gener.pdf; /Users/jacquesthibodeau/Zotero/storage/5YCRXHJB/2009.html,,,Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Mathematics - Statistics Theory; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MAAPD5JD,journalArticle,2020,"Xie, Qizhe; Luong, Minh-Thang; Hovy, Eduard; Le, Quoc V.",Self-training with Noisy Student improves ImageNet classification,"arXiv:1911.04252 [cs, stat]",,,,http://arxiv.org/abs/1911.04252,"We present Noisy Student Training, a semi-supervised learning approach that works well even when labeled data is abundant. Noisy Student Training achieves 88.4% top-1 accuracy on ImageNet, which is 2.0% better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 61.0% to 83.7%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean flip rate from 27.8 to 12.2. Noisy Student Training extends the idea of self-training and distillation with the use of equal-or-larger student models and noise added to the student during learning. On ImageNet, we first train an EfficientNet model on labeled images and use it as a teacher to generate pseudo labels for 300M unlabeled images. We then train a larger EfficientNet as a student model on the combination of labeled and pseudo labeled images. We iterate this process by putting back the student as the teacher. During the learning of the student, we inject noise such as dropout, stochastic depth, and data augmentation via RandAugment to the student so that the student generalizes better than the teacher. Models are available at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet. Code is available at https://github.com/google-research/noisystudent.",2020-06-19,2022-03-10 23:32:45,2022-03-10 23:32:45,2022-03-10 23:32:45,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1911.04252,,/Users/jacquesthibodeau/Zotero/storage/YGGXT272/Xie et al. - 2020 - Self-training with Noisy Student improves ImageNet.pdf; /Users/jacquesthibodeau/Zotero/storage/L5L4K4XS/1911.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XR37HB9N,journalArticle,2020,"Cerezo, Sergio Hernandez; Ballester, Guillem Duran",Fractal AI: A fragile theory of intelligence,arXiv:1803.05049 [cs],,,,http://arxiv.org/abs/1803.05049,"Fractal AI is a theory for general artificial intelligence. It allows deriving new mathematical tools that constitute the foundations for a new kind of stochastic calculus, by modelling information using cellular automaton-like structures instead of smooth functions. In the repository included we are presenting a new Agent, derived from the first principles of the theory, which is capable of solving Atari games several orders of magnitude more efficiently than other similar techniques, like Monte Carlo Tree Search. The code provided shows how it is now possible to beat some of the current State of The Art benchmarks on Atari games, without previous learning and using less than 1000 samples to calculate each one of the actions when standard MCTS uses 3 Million samples. Among other things, Fractal AI makes it possible to generate a huge database of top performing examples with a very little amount of computation required, transforming Reinforcement Learning into a supervised problem. The algorithm presented is capable of solving the exploration vs exploitation dilemma on both the discrete and continuous cases, while maintaining control over any aspect of the behaviour of the Agent. From a general approach, new techniques presented here have direct applications to other areas such as Non-equilibrium thermodynamics, chemistry, quantum physics, economics, information theory, and non-linear control theory.",2020-07-30,2022-03-10 23:36:16,2022-03-10 23:36:16,2022-03-10 23:36:16,,,,,,,Fractal AI,,,,,,,,,,,,arXiv.org,,arXiv: 1803.05049,,/Users/jacquesthibodeau/Zotero/storage/UNHUJV2B/Cerezo and Ballester - 2020 - Fractal AI A fragile theory of intelligence.pdf; /Users/jacquesthibodeau/Zotero/storage/899LAPUD/1803.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GFGVSXE5,journalArticle,2018,"Yaworsky, Paul",A Model for General Intelligence,arXiv:1811.02546 [cs],,,,http://arxiv.org/abs/1811.02546,"The overarching problem in artificial intelligence (AI) is that we do not understand the intelligence process well enough to enable the development of adequate computational models. Much work has been done in AI over the years at lower levels, but a big part of what has been missing involves the high level, abstract, general nature of intelligence. We address this gap by developing a model for general intelligence. To accomplish this, we focus on three basic aspects of intelligence. First, we must realize the general order and nature of intelligence at a high level. Second, we must come to know what these realizations mean with respect to the overall intelligence process. Third, we must describe these realizations as clearly as possible. We propose a hierarchical model to help capture and exploit the order within intelligence. The underlying order involves patterns of signals that become organized, stored and activated in space and time. These patterns can be described using a simple, general hierarchy, with physical signals at the lowest level, information in the middle, and abstract signal representations at the top. This high level perspective provides a big picture that literally helps us see the intelligence process, thereby enabling fundamental realizations, a better understanding and clear descriptions of the intelligence process. The resulting model can be used to support all kinds of information processing across multiple levels of abstraction. As computer technology improves, and as cooperation increases between humans and computers, people will become more efficient and more productive in performing their information processing tasks.",2018-11-14,2022-03-10 23:36:18,2022-03-10 23:36:18,2022-03-10 23:36:18,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1811.02546,,/Users/jacquesthibodeau/Zotero/storage/DVUHFKUS/Yaworsky - 2018 - A Model for General Intelligence.pdf; /Users/jacquesthibodeau/Zotero/storage/PE5NA37N/1811.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QXGWDD3E,journalArticle,2018,"Deng, Fei; Ren, Jinsheng; Chen, Feng",Abstraction Learning,arXiv:1809.03956 [cs],,,,http://arxiv.org/abs/1809.03956,"There has been a gap between artificial intelligence and human intelligence. In this paper, we identify three key elements forming human intelligence, and suggest that abstraction learning combines these elements and is thus a way to bridge the gap. Prior researches in artificial intelligence either specify abstraction by human experts, or take abstraction as a qualitative explanation for the model. This paper aims to learn abstraction directly. We tackle three main challenges: representation, objective function, and learning algorithm. Specifically, we propose a partition structure that contains pre-allocated abstraction neurons; we formulate abstraction learning as a constrained optimization problem, which integrates abstraction properties; we develop a network evolution algorithm to solve this problem. This complete framework is named ONE (Optimization via Network Evolution). In our experiments on MNIST, ONE shows elementary human-like intelligence, including low energy consumption, knowledge sharing, and lifelong learning.",2018-09-11,2022-03-10 23:36:20,2022-03-10 23:36:20,2022-03-10 23:36:20,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1809.03956,,/Users/jacquesthibodeau/Zotero/storage/G3G8QHL4/Deng et al. - 2018 - Abstraction Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/FQPL97DC/1809.html,,,Computer Science - Artificial Intelligence; Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Z4YAFQJQ,journalArticle,2018,"Orseau, Laurent; McGill, Simon McGregor; Legg, Shane",Agents and Devices: A Relative Definition of Agency,"arXiv:1805.12387 [cs, stat]",,,,http://arxiv.org/abs/1805.12387,"According to Dennett, the same system may be described using a `physical' (mechanical) explanatory stance, or using an `intentional' (belief- and goal-based) explanatory stance. Humans tend to find the physical stance more helpful for certain systems, such as planets orbiting a star, and the intentional stance for others, such as living animals. We define a formal counterpart of physical and intentional stances within computational theory: a description of a system as either a device, or an agent, with the key difference being that `devices' are directly described in terms of an input-output mapping, while `agents' are described in terms of the function they optimise. Bayes' rule can then be applied to calculate the subjective probability of a system being a device or an agent, based only on its behaviour. We illustrate this using the trajectories of an object in a toy grid-world domain.",2018-05-31,2022-03-10 23:36:22,2022-03-10 23:36:22,2022-03-10 23:36:22,,,,,,,Agents and Devices,,,,,,,,,,,,arXiv.org,,arXiv: 1805.12387,,/Users/jacquesthibodeau/Zotero/storage/SHCBW8Y6/Orseau et al. - 2018 - Agents and Devices A Relative Definition of Agenc.pdf; /Users/jacquesthibodeau/Zotero/storage/474DSKEW/1805.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZP6MSHPS,journalArticle,2018,"Gal, Yarin; Smith, Lewis",Sufficient Conditions for Idealised Models to Have No Adversarial Examples: a Theoretical and Empirical Study with Bayesian Neural Networks,"arXiv:1806.00667 [cs, stat]",,,,http://arxiv.org/abs/1806.00667,"We prove, under two sufficient conditions, that idealised models can have no adversarial examples. We discuss which idealised models satisfy our conditions, and show that idealised Bayesian neural networks (BNNs) satisfy these. We continue by studying near-idealised BNNs using HMC inference, demonstrating the theoretical ideas in practice. We experiment with HMC on synthetic data derived from MNIST for which we know the ground-truth image density, showing that near-perfect epistemic uncertainty correlates to density under image manifold, and that adversarial images lie off the manifold in our setting. This suggests why MC dropout, which can be seen as performing approximate inference, has been observed to be an effective defence against adversarial examples in practice; We highlight failure-cases of non-idealised BNNs relying on dropout, suggesting a new attack for dropout models and a new defence as well. Lastly, we demonstrate the defence on a cats-vs-dogs image classification task with a VGG13 variant.",2018-06-28,2022-03-10 23:36:24,2022-03-10 23:36:24,2022-03-10 23:36:24,,,,,,,Sufficient Conditions for Idealised Models to Have No Adversarial Examples,,,,,,,,,,,,arXiv.org,,arXiv: 1806.00667,,/Users/jacquesthibodeau/Zotero/storage/5EHLFZKI/Gal and Smith - 2018 - Sufficient Conditions for Idealised Models to Have.pdf; /Users/jacquesthibodeau/Zotero/storage/XXFSTRIK/1806.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CPZW5CXE,journalArticle,2018,"Ebrahimi, Javid; Lowd, Daniel; Dou, Dejing",On Adversarial Examples for Character-Level Neural Machine Translation,arXiv:1806.09030 [cs],,,,http://arxiv.org/abs/1806.09030,"Evaluating on adversarial examples has become a standard procedure to measure robustness of deep learning models. Due to the difficulty of creating white-box adversarial examples for discrete text input, most analyses of the robustness of NLP models have been done through black-box adversarial examples. We investigate adversarial examples for character-level neural machine translation (NMT), and contrast black-box adversaries with a novel white-box adversary, which employs differentiable string-edit operations to rank adversarial changes. We propose two novel types of attacks which aim to remove or change a word in a translation, rather than simply break the NMT. We demonstrate that white-box adversarial examples are significantly stronger than their black-box counterparts in different attack scenarios, which show more serious vulnerabilities than previously known. In addition, after performing adversarial training, which takes only 3 times longer than regular training, we can improve the model's robustness significantly.",2018-06-23,2022-03-10 23:36:27,2022-03-10 23:36:27,2022-03-10 23:36:27,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1806.09030,,/Users/jacquesthibodeau/Zotero/storage/ZJZ2XPGZ/Ebrahimi et al. - 2018 - On Adversarial Examples for Character-Level Neural.pdf; /Users/jacquesthibodeau/Zotero/storage/MHDTZNYR/1806.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TTAJPKGK,journalArticle,2019,"Bortolussi, Luca; Sanguinetti, Guido",Intrinsic Geometric Vulnerability of High-Dimensional Artificial Intelligence,"arXiv:1811.03571 [cs, stat]",,,,http://arxiv.org/abs/1811.03571,"The success of modern Artificial Intelligence (AI) technologies depends critically on the ability to learn non-linear functional dependencies from large, high dimensional data sets. Despite recent high-profile successes, empirical evidence indicates that the high predictive performance is often paired with low robustness, making AI systems potentially vulnerable to adversarial attacks. In this report, we provide a simple intuitive argument suggesting that high performance and vulnerability are intrinsically coupled, and largely dependent on the geometry of typical, high-dimensional data sets. Our work highlights a major potential pitfall of modern AI systems, and suggests practical research directions to ameliorate the problem.",2019-01-24,2022-03-10 23:36:28,2022-03-10 23:36:28,2022-03-10 23:36:27,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1811.03571,,/Users/jacquesthibodeau/Zotero/storage/FR2BWT9G/Bortolussi and Sanguinetti - 2019 - Intrinsic Geometric Vulnerability of High-Dimensio.pdf; /Users/jacquesthibodeau/Zotero/storage/U9Q6ZTWZ/1811.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
B7BXPZQ5,journalArticle,2018,"Kannan, Harini; Kurakin, Alexey; Goodfellow, Ian",Adversarial Logit Pairing,"arXiv:1803.06373 [cs, stat]",,,,http://arxiv.org/abs/1803.06373,"In this paper, we develop improved techniques for defending against adversarial examples at scale. First, we implement the state of the art version of adversarial training at unprecedented scale on ImageNet and investigate whether it remains effective in this setting - an important open scientific question (Athalye et al., 2018). Next, we introduce enhanced defenses using a technique we call logit pairing, a method that encourages logits for pairs of examples to be similar. When applied to clean examples and their adversarial counterparts, logit pairing improves accuracy on adversarial examples over vanilla adversarial training; we also find that logit pairing on clean examples only is competitive with adversarial training in terms of accuracy on two datasets. Finally, we show that adversarial logit pairing achieves the state of the art defense on ImageNet against PGD white box attacks, with an accuracy improvement from 1.5% to 27.9%. Adversarial logit pairing also successfully damages the current state of the art defense against black box attacks on ImageNet (Tramer et al., 2018), dropping its accuracy from 66.6% to 47.1%. With this new accuracy drop, adversarial logit pairing ties with Tramer et al.(2018) for the state of the art on black box attacks on ImageNet.",2018-03-16,2022-03-10 23:36:29,2022-03-10 23:36:29,2022-03-10 23:36:29,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1803.06373,,/Users/jacquesthibodeau/Zotero/storage/G7EYQKZQ/Kannan et al. - 2018 - Adversarial Logit Pairing.pdf; /Users/jacquesthibodeau/Zotero/storage/KCF3UUH9/1803.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ITT2S8WI,journalArticle,2018,"Goodfellow, Ian",Defense Against the Dark Arts: An overview of adversarial example security research and future research directions,"arXiv:1806.04169 [cs, stat]",,,,http://arxiv.org/abs/1806.04169,This article presents a summary of a keynote lecture at the Deep Learning Security workshop at IEEE Security and Privacy 2018. This lecture summarizes the state of the art in defenses against adversarial examples and provides recommendations for future research directions on this topic.,2018-06-11,2022-03-10 23:36:31,2022-03-10 23:36:31,2022-03-10 23:36:31,,,,,,,Defense Against the Dark Arts,,,,,,,,,,,,arXiv.org,,arXiv: 1806.04169,,/Users/jacquesthibodeau/Zotero/storage/HCT43IQT/Goodfellow - 2018 - Defense Against the Dark Arts An overview of adve.pdf; /Users/jacquesthibodeau/Zotero/storage/29E7B9CZ/1806.html,,,Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YP7EQA3G,journalArticle,2021,"Yuan, Li; Xiao, Will; Kreiman, Gabriel; Tay, Francis E. H.; Feng, Jiashi; Livingstone, Margaret S.",Adversarial images for the primate brain,"arXiv:2011.05623 [cs, eess, q-bio]",,,,http://arxiv.org/abs/2011.05623,"Convolutional neural networks (CNNs) are vulnerable to adversarial attack, the phenomenon that adding minuscule noise to an image can fool CNNs into misclassifying it. Because this noise is nearly imperceptible to human viewers, biological vision is assumed to be robust to adversarial attack. Despite this apparent difference in robustness, CNNs are currently the best models of biological vision, revealing a gap in explaining how the brain responds to adversarial images. Indeed, sensitivity to adversarial attack has not been measured for biological vision under normal conditions, nor have attack methods been specifically designed to affect biological vision. We studied the effects of adversarial attack on primate vision, measuring both monkey neuronal responses and human behavior. Adversarial images were created by modifying images from one category(such as human faces) to look like a target category(such as monkey faces), while limiting pixel value change. We tested three attack directions via several attack methods, including directly using CNN adversarial images and using a CNN-based predictive model to guide monkey visual neuron responses. We considered a wide range of image change magnitudes that covered attack success rates up to>90%. We found that adversarial images designed for CNNs were ineffective in attacking primate vision. Even when considering the best attack method, primate vision was more robust to adversarial attack than an ensemble of CNNs, requiring over 100-fold larger image change to attack successfully. The success of individual attack methods and images was correlated between monkey neurons and human behavior, but was less correlated between either and CNN categorization. Consistently, CNN-based models of neurons, when trained on natural images, did not generalize to explain neuronal responses to adversarial images.",2021-12-21,2022-03-10 23:36:34,2022-03-10 23:36:34,2022-03-10 23:36:33,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2011.05623,,/Users/jacquesthibodeau/Zotero/storage/YWZBQ374/Yuan et al. - 2021 - Adversarial images for the primate brain.pdf; /Users/jacquesthibodeau/Zotero/storage/G3GP9NLA/2011.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Neural and Evolutionary Computing; Electrical Engineering and Systems Science - Image and Video Processing; Quantitative Biology - Neurons and Cognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HEYUPS54,journalArticle,2018,"Elsayed, Gamaleldin F.; Goodfellow, Ian; Sohl-Dickstein, Jascha",Adversarial Reprogramming of Neural Networks,"arXiv:1806.11146 [cs, stat]",,,,http://arxiv.org/abs/1806.11146,"Deep neural networks are susceptible to \emph{adversarial} attacks. In computer vision, well-crafted perturbations to images can cause neural networks to make mistakes such as confusing a cat with a computer. Previous adversarial attacks have been designed to degrade performance of models or cause machine learning models to produce specific outputs chosen ahead of time by the attacker. We introduce attacks that instead {\em reprogram} the target model to perform a task chosen by the attacker---without the attacker needing to specify or compute the desired output for each test-time input. This attack finds a single adversarial perturbation, that can be added to all test-time inputs to a machine learning model in order to cause the model to perform a task chosen by the adversary---even if the model was not trained to do this task. These perturbations can thus be considered a program for the new task. We demonstrate adversarial reprogramming on six ImageNet classification models, repurposing these models to perform a counting task, as well as classification tasks: classification of MNIST and CIFAR-10 examples presented as inputs to the ImageNet model.",2018-11-29,2022-03-10 23:36:37,2022-03-10 23:36:37,2022-03-10 23:36:35,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1806.11146,,/Users/jacquesthibodeau/Zotero/storage/JSXYXNMB/Elsayed et al. - 2018 - Adversarial Reprogramming of Neural Networks.pdf; /Users/jacquesthibodeau/Zotero/storage/ELFN2RIC/1806.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Cryptography and Security; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
J3CBTR2R,journalArticle,2018,"Engstrom, Logan; Ilyas, Andrew; Athalye, Anish",Evaluating and Understanding the Robustness of Adversarial Logit Pairing,"arXiv:1807.10272 [cs, stat]",,,,http://arxiv.org/abs/1807.10272,"We evaluate the robustness of Adversarial Logit Pairing, a recently proposed defense against adversarial examples. We find that a network trained with Adversarial Logit Pairing achieves 0.6% accuracy in the threat model in which the defense is considered. We provide a brief overview of the defense and the threat models/claims considered, as well as a discussion of the methodology and results of our attack, which may offer insights into the reasons underlying the vulnerability of ALP to adversarial attack.",2018-11-23,2022-03-10 23:36:38,2022-03-10 23:36:38,2022-03-10 23:36:38,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1807.10272,,/Users/jacquesthibodeau/Zotero/storage/HFYZ4XE6/Engstrom et al. - 2018 - Evaluating and Understanding the Robustness of Adv.pdf; /Users/jacquesthibodeau/Zotero/storage/HAVM33IR/1807.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Cryptography and Security; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AFKVZASW,journalArticle,2018,"Lamb, Alex; Binas, Jonathan; Goyal, Anirudh; Serdyuk, Dmitriy; Subramanian, Sandeep; Mitliagkas, Ioannis; Bengio, Yoshua",Fortified Networks: Improving the Robustness of Deep Networks by Modeling the Manifold of Hidden Representations,"arXiv:1804.02485 [cs, stat]",,,,http://arxiv.org/abs/1804.02485,"Deep networks have achieved impressive results across a variety of important tasks. However a known weakness is a failure to perform well when evaluated on data which differ from the training distribution, even if these differences are very small, as is the case with adversarial examples. We propose Fortified Networks, a simple transformation of existing networks, which fortifies the hidden layers in a deep network by identifying when the hidden states are off of the data manifold, and maps these hidden states back to parts of the data manifold where the network performs well. Our principal contribution is to show that fortifying these hidden states improves the robustness of deep networks and our experiments (i) demonstrate improved robustness to standard adversarial attacks in both black-box and white-box threat models; (ii) suggest that our improvements are not primarily due to the gradient masking problem and (iii) show the advantage of doing this fortification in the hidden layers instead of the input space.",2018-04-06,2022-03-10 23:36:40,2022-03-10 23:36:40,2022-03-10 23:36:40,,,,,,,Fortified Networks,,,,,,,,,,,,arXiv.org,,arXiv: 1804.02485,,/Users/jacquesthibodeau/Zotero/storage/93KU2MHU/Lamb et al. - 2018 - Fortified Networks Improving the Robustness of De.pdf; /Users/jacquesthibodeau/Zotero/storage/ULASFUXQ/1804.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5KM7MWAP,journalArticle,2018,"Moosavi-Dezfooli, Seyed-Mohsen; Fawzi, Alhussein; Uesato, Jonathan; Frossard, Pascal","Robustness via curvature regularization, and vice versa","arXiv:1811.09716 [cs, stat]",,,,http://arxiv.org/abs/1811.09716,"State-of-the-art classifiers have been shown to be largely vulnerable to adversarial perturbations. One of the most effective strategies to improve robustness is adversarial training. In this paper, we investigate the effect of adversarial training on the geometry of the classification landscape and decision boundaries. We show in particular that adversarial training leads to a significant decrease in the curvature of the loss surface with respect to inputs, leading to a drastically more ""linear"" behaviour of the network. Using a locally quadratic approximation, we provide theoretical evidence on the existence of a strong relation between large robustness and small curvature. To further show the importance of reduced curvature for improving the robustness, we propose a new regularizer that directly minimizes curvature of the loss surface, and leads to adversarial robustness that is on par with adversarial training. Besides being a more efficient and principled alternative to adversarial training, the proposed regularizer confirms our claims on the importance of exhibiting quasi-linear behavior in the vicinity of data points in order to achieve robustness.",2018-11-23,2022-03-10 23:36:44,2022-03-10 23:36:44,2022-03-10 23:36:44,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1811.09716,,"/Users/jacquesthibodeau/Zotero/storage/PS4I6B3R/Moosavi-Dezfooli et al. - 2018 - Robustness via curvature regularization, and vice .pdf; /Users/jacquesthibodeau/Zotero/storage/93M52PK5/1811.html",,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Q6KXS2K9,journalArticle,2019,"Simon-Gabriel, Carl-Johann; Ollivier, Yann; Bottou, Léon; Schölkopf, Bernhard; Lopez-Paz, David",First-order Adversarial Vulnerability of Neural Networks and Input Dimension,"arXiv:1802.01421 [cs, stat]",,,,http://arxiv.org/abs/1802.01421,"Over the past few years, neural networks were proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. Surprisingly, vulnerability does not depend on network topology: for many standard network architectures, we prove that at initialization, the $\ell_1$-norm of these gradients grows as the square root of the input dimension, leaving the networks increasingly vulnerable with growing image size. We empirically show that this dimension dependence persists after either usual or robust training, but gets attenuated with higher regularization.",2019-06-16,2022-03-10 23:36:46,2022-03-10 23:36:46,2022-03-10 23:36:46,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1802.01421,,/Users/jacquesthibodeau/Zotero/storage/RK6SY8BM/Simon-Gabriel et al. - 2019 - First-order Adversarial Vulnerability of Neural Ne.pdf; /Users/jacquesthibodeau/Zotero/storage/TCV6Y6DE/1802.html,,,68T45; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; I.2.6; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QDAX6VWK,journalArticle,2019,"Zhang, Hongyang; Yu, Yaodong; Jiao, Jiantao; Xing, Eric P.; Ghaoui, Laurent El; Jordan, Michael I.",Theoretically Principled Trade-off between Robustness and Accuracy,"arXiv:1901.08573 [cs, stat]",,,,http://arxiv.org/abs/1901.08573,"We identify a trade-off between robustness and accuracy that serves as a guiding principle in the design of defenses against adversarial examples. Although this problem has been widely studied empirically, much remains unknown concerning the theory underlying this trade-off. In this work, we decompose the prediction error for adversarial examples (robust error) as the sum of the natural (classification) error and boundary error, and provide a differentiable upper bound using the theory of classification-calibrated loss, which is shown to be the tightest possible upper bound uniform over all probability distributions and measurable predictors. Inspired by our theoretical analysis, we also design a new defense method, TRADES, to trade adversarial robustness off against accuracy. Our proposed algorithm performs well experimentally in real-world datasets. The methodology is the foundation of our entry to the NeurIPS 2018 Adversarial Vision Challenge in which we won the 1st place out of ~2,000 submissions, surpassing the runner-up approach by $11.41\%$ in terms of mean $\ell_2$ perturbation distance.",2019-06-24,2022-03-10 23:36:48,2022-03-10 23:36:48,2022-03-10 23:36:48,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1901.08573,,/Users/jacquesthibodeau/Zotero/storage/6JKPK8NZ/Zhang et al. - 2019 - Theoretically Principled Trade-off between Robustn.pdf; /Users/jacquesthibodeau/Zotero/storage/YVV2VMZG/1901.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2D7FVE8A,journalArticle,2019,"Jordan, Matt; Manoj, Naren; Goel, Surbhi; Dimakis, Alexandros G.",Quantifying Perceptual Distortion of Adversarial Examples,"arXiv:1902.08265 [cs, stat]",,,,http://arxiv.org/abs/1902.08265,"Recent work has shown that additive threat models, which only permit the addition of bounded noise to the pixels of an image, are insufficient for fully capturing the space of imperceivable adversarial examples. For example, small rotations and spatial transformations can fool classifiers, remain imperceivable to humans, but have large additive distance from the original images. In this work, we leverage quantitative perceptual metrics like LPIPS and SSIM to define a novel threat model for adversarial attacks. To demonstrate the value of quantifying the perceptual distortion of adversarial examples, we present and employ a unifying framework fusing different attack styles. We first prove that our framework results in images that are unattainable by attack styles in isolation. We then perform adversarial training using attacks generated by our framework to demonstrate that networks are only robust to classes of adversarial perturbations they have been trained against, and combination attacks are stronger than any of their individual components. Finally, we experimentally demonstrate that our combined attacks retain the same perceptual distortion but induce far higher misclassification rates when compared against individual attacks.",2019-02-21,2022-03-10 23:36:50,2022-03-10 23:36:50,2022-03-10 23:36:50,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1902.08265,,/Users/jacquesthibodeau/Zotero/storage/WAGJS4SD/Jordan et al. - 2019 - Quantifying Perceptual Distortion of Adversarial E.pdf; /Users/jacquesthibodeau/Zotero/storage/Q2SFVJI7/1902.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RMWMGE28,journalArticle,2019,"Finlay, Chris; Pooladian, Aram-Alexandre; Oberman, Adam M.",The LogBarrier adversarial attack: making effective use of decision boundary information,"arXiv:1903.10396 [cs, stat]",,,,http://arxiv.org/abs/1903.10396,"Adversarial attacks for image classification are small perturbations to images that are designed to cause misclassification by a model. Adversarial attacks formally correspond to an optimization problem: find a minimum norm image perturbation, constrained to cause misclassification. A number of effective attacks have been developed. However, to date, no gradient-based attacks have used best practices from the optimization literature to solve this constrained minimization problem. We design a new untargeted attack, based on these best practices, using the established logarithmic barrier method. On average, our attack distance is similar or better than all state-of-the-art attacks on benchmark datasets (MNIST, CIFAR10, ImageNet-1K). In addition, our method performs significantly better on the most challenging images, those which normally require larger perturbations for misclassification. We employ the LogBarrier attack on several adversarially defended models, and show that it adversarially perturbs all images more efficiently than other attacks: the distance needed to perturb all images is significantly smaller with the LogBarrier attack than with other state-of-the-art attacks.",2019-03-25,2022-03-10 23:36:52,2022-03-10 23:36:52,2022-03-10 23:36:51,,,,,,,The LogBarrier adversarial attack,,,,,,,,,,,,arXiv.org,,arXiv: 1903.10396,,/Users/jacquesthibodeau/Zotero/storage/XC7DFM7F/Finlay et al. - 2019 - The LogBarrier adversarial attack making effectiv.pdf; /Users/jacquesthibodeau/Zotero/storage/R7QTYS5A/1903.html,,,Computer Science - Cryptography and Security; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IIHKAN3K,journalArticle,2019,"Kettunen, Markus; Härkönen, Erik; Lehtinen, Jaakko",E-LPIPS: Robust Perceptual Image Similarity via Random Transformation Ensembles,arXiv:1906.03973 [cs],,,,http://arxiv.org/abs/1906.03973,"It has been recently shown that the hidden variables of convolutional neural networks make for an efficient perceptual similarity metric that accurately predicts human judgment on relative image similarity assessment. First, we show that such learned perceptual similarity metrics (LPIPS) are susceptible to adversarial attacks that dramatically contradict human visual similarity judgment. While this is not surprising in light of neural networks' well-known weakness to adversarial perturbations, we proceed to show that self-ensembling with an infinite family of random transformations of the input --- a technique known not to render classification networks robust --- is enough to turn the metric robust against attack, while retaining predictive power on human judgments. Finally, we study the geometry imposed by our our novel self-ensembled metric (E-LPIPS) on the space of natural images. We find evidence of ""perceptual convexity"" by showing that convex combinations of similar-looking images retain appearance, and that discrete geodesics yield meaningful frame interpolation and texture morphing, all without explicit correspondences.",2019-06-11,2022-03-10 23:36:57,2022-03-10 23:36:57,2022-03-10 23:36:54,,,,,,,E-LPIPS,,,,,,,,,,,,arXiv.org,,arXiv: 1906.03973,,/Users/jacquesthibodeau/Zotero/storage/KP7GYK23/Kettunen et al. - 2019 - E-LPIPS Robust Perceptual Image Similarity via Ra.pdf; /Users/jacquesthibodeau/Zotero/storage/L3GBFZTY/1906.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KDNV35KT,journalArticle,2018,"Gilmer, Justin; Adams, Ryan P.; Goodfellow, Ian; Andersen, David; Dahl, George E.",Motivating the Rules of the Game for Adversarial Example Research,"arXiv:1807.06732 [cs, stat]",,,,http://arxiv.org/abs/1807.06732,"Advances in machine learning have led to broad deployment of systems with impressive performance on important problems. Nonetheless, these systems can be induced to make errors on data that are surprisingly similar to examples the learned system handles correctly. The existence of these errors raises a variety of questions about out-of-sample generalization and whether bad actors might use such examples to abuse deployed systems. As a result of these security concerns, there has been a flurry of recent papers proposing algorithms to defend against such malicious perturbations of correctly handled examples. It is unclear how such misclassifications represent a different kind of security problem than other errors, or even other attacker-produced examples that have no specific relationship to an uncorrupted input. In this paper, we argue that adversarial example defense papers have, to date, mostly considered abstract, toy games that do not relate to any specific security concern. Furthermore, defense papers have not yet precisely described all the abilities and limitations of attackers that would be relevant in practical security. Towards this end, we establish a taxonomy of motivations, constraints, and abilities for more plausible adversaries. Finally, we provide a series of recommendations outlining a path forward for future work to more clearly articulate the threat model and perform more meaningful evaluation.",2018-07-19,2022-03-10 23:36:59,2022-03-10 23:36:59,2022-03-10 23:36:58,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1807.06732,,/Users/jacquesthibodeau/Zotero/storage/2WV4PEAD/Gilmer et al. - 2018 - Motivating the Rules of the Game for Adversarial E.pdf; /Users/jacquesthibodeau/Zotero/storage/WN72FWE5/1807.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NKLEKPLA,journalArticle,2021,"Gleave, Adam; Dennis, Michael; Wild, Cody; Kant, Neel; Levine, Sergey; Russell, Stuart",Adversarial Policies: Attacking Deep Reinforcement Learning,"arXiv:1905.10615 [cs, stat]",,,,http://arxiv.org/abs/1905.10615,"Deep reinforcement learning (RL) policies are known to be vulnerable to adversarial perturbations to their observations, similar to adversarial examples for classifiers. However, an attacker is not usually able to directly modify another agent's observations. This might lead one to wonder: is it possible to attack an RL agent simply by choosing an adversarial policy acting in a multi-agent environment so as to create natural observations that are adversarial? We demonstrate the existence of adversarial policies in zero-sum games between simulated humanoid robots with proprioceptive observations, against state-of-the-art victims trained via self-play to be robust to opponents. The adversarial policies reliably win against the victims but generate seemingly random and uncoordinated behavior. We find that these policies are more successful in high-dimensional environments, and induce substantially different activations in the victim policy network than when the victim plays against a normal opponent. Videos are available at https://adversarialpolicies.github.io/.",2021-01-17,2022-03-10 23:37:03,2022-03-10 23:37:03,2022-03-10 23:36:56,,,,,,,Adversarial Policies,,,,,,,,,,,,arXiv.org,,arXiv: 1905.10615,,/Users/jacquesthibodeau/Zotero/storage/EPI9IHI6/Gleave et al. - 2021 - Adversarial Policies Attacking Deep Reinforcement.pdf; /Users/jacquesthibodeau/Zotero/storage/WF7YJTHQ/1905.html,,,Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security; Computer Science - Machine Learning; I.2.6; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FKUXY23K,journalArticle,2018,"Schott, Lukas; Rauber, Jonas; Bethge, Matthias; Brendel, Wieland",Towards the first adversarially robust neural network model on MNIST,arXiv:1805.09190 [cs],,,,http://arxiv.org/abs/1805.09190,"Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful defense by Madry et al. (1) overfits on the L-infinity metric (it's highly susceptible to L2 and L0 perturbations), (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-infinity perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.",2018-09-20,2022-03-10 23:37:05,2022-03-10 23:37:05,2022-03-10 23:37:00,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1805.09190,,/Users/jacquesthibodeau/Zotero/storage/TMDQ4BX8/Schott et al. - 2018 - Towards the first adversarially robust neural netw.pdf; /Users/jacquesthibodeau/Zotero/storage/PA5BMM7I/1805.html,,,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AZ2PRLZN,journalArticle,2018,"Charles, Zachary; Rosenberg, Harrison; Papailiopoulos, Dimitris",A Geometric Perspective on the Transferability of Adversarial Directions,"arXiv:1811.03531 [cs, stat]",,,,http://arxiv.org/abs/1811.03531,"State-of-the-art machine learning models frequently misclassify inputs that have been perturbed in an adversarial manner. Adversarial perturbations generated for a given input and a specific classifier often seem to be effective on other inputs and even different classifiers. In other words, adversarial perturbations seem to transfer between different inputs, models, and even different neural network architectures. In this work, we show that in the context of linear classifiers and two-layer ReLU networks, there provably exist directions that give rise to adversarial perturbations for many classifiers and data points simultaneously. We show that these ""transferable adversarial directions"" are guaranteed to exist for linear separators of a given set, and will exist with high probability for linear classifiers trained on independent sets drawn from the same distribution. We extend our results to large classes of two-layer ReLU networks. We further show that adversarial directions for ReLU networks transfer to linear classifiers while the reverse need not hold, suggesting that adversarial perturbations for more complex models are more likely to transfer to other classifiers. We validate our findings empirically, even for deeper ReLU networks.",2018-11-08,2022-03-10 23:37:05,2022-03-10 23:37:05,2022-03-10 23:37:03,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1811.03531,,/Users/jacquesthibodeau/Zotero/storage/QU7KMIED/Charles et al. - 2018 - A Geometric Perspective on the Transferability of .pdf; /Users/jacquesthibodeau/Zotero/storage/WFWVUZV3/1811.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DKPF69JY,journalArticle,2020,"Kang, Daniel; Sun, Yi; Hendrycks, Dan; Brown, Tom; Steinhardt, Jacob",Testing Robustness Against Unforeseen Adversaries,"arXiv:1908.08016 [cs, stat]",,,,http://arxiv.org/abs/1908.08016,"Most existing adversarial defenses only measure robustness to L_p adversarial attacks. Not only are adversaries unlikely to exclusively create small L_p perturbations, adversaries are unlikely to remain fixed. Adversaries adapt and evolve their attacks; hence adversarial defenses must be robust to a broad range of unforeseen attacks. We address this discrepancy between research and reality by proposing a new evaluation framework called ImageNet-UA. Our framework enables the research community to test ImageNet model robustness against attacks not encountered during training. To create ImageNet-UA's diverse attack suite, we introduce a total of four novel adversarial attacks. We also demonstrate that, in comparison to ImageNet-UA, prevailing L_inf robustness assessments give a narrow account of model robustness. By evaluating current defenses with ImageNet-UA, we find they provide little robustness to unforeseen attacks. We hope the greater variety and realism of ImageNet-UA enables development of more robust defenses which can generalize beyond attacks seen during training.",2020-06-09,2022-03-10 23:37:13,2022-03-10 23:37:13,2022-03-10 23:37:12,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1908.08016,,/Users/jacquesthibodeau/Zotero/storage/APXQY9W3/Kang et al. - 2020 - Testing Robustness Against Unforeseen Adversaries.pdf; /Users/jacquesthibodeau/Zotero/storage/LTQEEULC/1908.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Cryptography and Security; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SYMA2IBW,journalArticle,2021,"Hendrycks, Dan; Zhao, Kevin; Basart, Steven; Steinhardt, Jacob; Song, Dawn",Natural Adversarial Examples,"arXiv:1907.07174 [cs, stat]",,,,http://arxiv.org/abs/1907.07174,"We introduce two challenging datasets that reliably cause machine learning model performance to substantially degrade. The datasets are collected with a simple adversarial filtration technique to create datasets with limited spurious cues. Our datasets' real-world, unmodified examples transfer to various unseen models reliably, demonstrating that computer vision models have shared weaknesses. The first dataset is called ImageNet-A and is like the ImageNet test set, but it is far more challenging for existing models. We also curate an adversarial out-of-distribution detection dataset called ImageNet-O, which is the first out-of-distribution detection dataset created for ImageNet models. On ImageNet-A a DenseNet-121 obtains around 2% accuracy, an accuracy drop of approximately 90%, and its out-of-distribution detection performance on ImageNet-O is near random chance levels. We find that existing data augmentation techniques hardly boost performance, and using other public training datasets provides improvements that are limited. However, we find that improvements to computer vision architectures provide a promising path towards robust models.",2021-03-04,2022-03-10 23:37:17,2022-03-10 23:37:17,2022-03-10 23:37:16,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1907.07174,,/Users/jacquesthibodeau/Zotero/storage/QZ3CNMKJ/Hendrycks et al. - 2021 - Natural Adversarial Examples.pdf; /Users/jacquesthibodeau/Zotero/storage/3SR2KER9/1907.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DNI2UC9Q,journalArticle,2019,"Santurkar, Shibani; Tsipras, Dimitris; Tran, Brandon; Ilyas, Andrew; Engstrom, Logan; Madry, Aleksander",Image Synthesis with a Single (Robust) Classifier,"arXiv:1906.09453 [cs, stat]",,,,http://arxiv.org/abs/1906.09453,"We show that the basic classification framework alone can be used to tackle some of the most challenging tasks in image synthesis. In contrast to other state-of-the-art approaches, the toolkit we develop is rather minimal: it uses a single, off-the-shelf classifier for all these tasks. The crux of our approach is that we train this classifier to be adversarially robust. It turns out that adversarial robustness is precisely what we need to directly manipulate salient features of the input. Overall, our findings demonstrate the utility of robustness in the broader machine learning context. Code and models for our experiments can be found at https://git.io/robust-apps.",2019-08-08,2022-03-10 23:37:25,2022-03-10 23:37:25,2022-03-10 23:37:25,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1906.09453,,/Users/jacquesthibodeau/Zotero/storage/9US3I935/Santurkar et al. - 2019 - Image Synthesis with a Single (Robust) Classifier.pdf; /Users/jacquesthibodeau/Zotero/storage/9G7JF2MI/1906.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XVP3C2Y4,journalArticle,2019,"Engstrom, Logan; Ilyas, Andrew; Santurkar, Shibani; Tsipras, Dimitris; Tran, Brandon; Madry, Aleksander",Adversarial Robustness as a Prior for Learned Representations,"arXiv:1906.00945 [cs, stat]",,,,http://arxiv.org/abs/1906.00945,"An important goal in deep learning is to learn versatile, high-level feature representations of input data. However, standard networks' representations seem to possess shortcomings that, as we illustrate, prevent them from fully realizing this goal. In this work, we show that robust optimization can be re-cast as a tool for enforcing priors on the features learned by deep neural networks. It turns out that representations learned by robust models address the aforementioned shortcomings and make significant progress towards learning a high-level encoding of inputs. In particular, these representations are approximately invertible, while allowing for direct visualization and manipulation of salient input features. More broadly, our results indicate adversarial robustness as a promising avenue for improving learned representations. Our code and models for reproducing these results is available at https://git.io/robust-reps .",2019-09-27,2022-03-10 23:37:32,2022-03-10 23:37:32,2022-03-10 23:37:32,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1906.00945,,/Users/jacquesthibodeau/Zotero/storage/ASWRIKYU/Engstrom et al. - 2019 - Adversarial Robustness as a Prior for Learned Repr.pdf; /Users/jacquesthibodeau/Zotero/storage/6EYP38HZ/1906.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
I8LDKNL5,journalArticle,2019,"Ilyas, Andrew; Santurkar, Shibani; Tsipras, Dimitris; Engstrom, Logan; Tran, Brandon; Madry, Aleksander","Adversarial Examples Are Not Bugs, They Are Features","arXiv:1905.02175 [cs, stat]",,,,http://arxiv.org/abs/1905.02175,"Adversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. We demonstrate that adversarial examples can be directly attributed to the presence of non-robust features: features derived from patterns in the data distribution that are highly predictive, yet brittle and incomprehensible to humans. After capturing these features within a theoretical framework, we establish their widespread existence in standard datasets. Finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a misalignment between the (human-specified) notion of robustness and the inherent geometry of the data.",2019-08-12,2022-03-10 23:37:41,2022-03-10 23:37:41,2022-03-10 23:37:41,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1905.02175,,"/Users/jacquesthibodeau/Zotero/storage/7FU5N6AS/Ilyas et al. - 2019 - Adversarial Examples Are Not Bugs, They Are Featur.pdf; /Users/jacquesthibodeau/Zotero/storage/AUGF4UH4/1905.html",,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Cryptography and Security; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9XBGVE2J,journalArticle,2021,"Jones, Andy L.",Scaling Scaling Laws with Board Games,arXiv:2104.03113 [cs],,,,http://arxiv.org/abs/2104.03113,"The largest experiments in machine learning now require resources far beyond the budget of all but a few institutions. Fortunately, it has recently been shown that the results of these huge experiments can often be extrapolated from the results of a sequence of far smaller, cheaper experiments. In this work, we show that not only can the extrapolation be done based on the size of the model, but on the size of the problem as well. By conducting a sequence of experiments using AlphaZero and Hex, we show that the performance achievable with a fixed amount of compute degrades predictably as the game gets larger and harder. Along with our main result, we further show that the test-time and train-time compute available to an agent can be traded off while maintaining performance.",2021-04-15,2022-03-10 23:41:22,2022-03-10 23:41:22,2022-03-10 23:41:22,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2104.03113,,/Users/jacquesthibodeau/Zotero/storage/ED3ZQ86B/Jones - 2021 - Scaling Scaling Laws with Board Games.pdf; /Users/jacquesthibodeau/Zotero/storage/G33J7KPL/2104.html,,,Computer Science - Machine Learning; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XTL7FQYV,journalArticle,2021,"Aghajanyan, Armen; Gupta, Anchit; Shrivastava, Akshat; Chen, Xilun; Zettlemoyer, Luke; Gupta, Sonal",Muppet: Massive Multi-task Representations with Pre-Finetuning,arXiv:2101.11038 [cs],,,,http://arxiv.org/abs/2101.11038,"We propose pre-finetuning, an additional large-scale learning stage between language model pre-training and fine-tuning. Pre-finetuning is massively multi-task learning (around 50 datasets, over 4.8 million total labeled examples), and is designed to encourage learning of representations that generalize better to many different tasks. We show that pre-finetuning consistently improves performance for pretrained discriminators (e.g.~RoBERTa) and generation models (e.g.~BART) on a wide range of tasks (sentence prediction, commonsense reasoning, MRC, etc.), while also significantly improving sample efficiency during fine-tuning. We also show that large-scale multi-tasking is crucial; pre-finetuning can hurt performance when few tasks are used up until a critical point (usually above 15) after which performance improves linearly in the number of tasks.",2021-01-26,2022-03-10 23:41:29,2022-03-10 23:41:29,2022-03-10 23:41:28,,,,,,,Muppet,,,,,,,,,,,,arXiv.org,,arXiv: 2101.11038,,/Users/jacquesthibodeau/Zotero/storage/RSPFUDDT/Aghajanyan et al. - 2021 - Muppet Massive Multi-task Representations with Pr.pdf; /Users/jacquesthibodeau/Zotero/storage/6JXYC29H/2101.html,,,Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MHTZMIFW,journalArticle,2021,"Wu, Zonghan; Pan, Shirui; Chen, Fengwen; Long, Guodong; Zhang, Chengqi; Yu, Philip S.",A Comprehensive Survey on Graph Neural Networks,IEEE Transactions on Neural Networks and Learning Systems,,"2162-237X, 2162-2388",10.1109/TNNLS.2020.2978386,http://arxiv.org/abs/1901.00596,"Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this survey, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art graph neural networks into four categories, namely recurrent graph neural networks, convolutional graph neural networks, graph autoencoders, and spatial-temporal graph neural networks. We further discuss the applications of graph neural networks across various domains and summarize the open source codes, benchmark data sets, and model evaluation of graph neural networks. Finally, we propose potential research directions in this rapidly growing field.",2021-01,2022-03-10 23:41:31,2022-03-10 23:41:31,2022-03-10 23:41:31,4-24,,1,32,,IEEE Trans. Neural Netw. Learning Syst.,,,,,,,,,,,,,arXiv.org,,arXiv: 1901.00596,,/Users/jacquesthibodeau/Zotero/storage/QPJLPMR2/Wu et al. - 2021 - A Comprehensive Survey on Graph Neural Networks.pdf; /Users/jacquesthibodeau/Zotero/storage/R2448YMR/1901.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A2PGPHFH,journalArticle,2020,"Zhou, Hattie; Lan, Janice; Liu, Rosanne; Yosinski, Jason","Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask","arXiv:1905.01067 [cs, stat]",,,,http://arxiv.org/abs/1905.01067,"The recent ""Lottery Ticket Hypothesis"" paper by Frankle & Carbin showed that a simple approach to creating sparse networks (keeping the large weights) results in models that are trainable from scratch, but only when starting from the same initial weights. The performance of these networks often exceeds the performance of the non-sparse base model, but for reasons that were not well understood. In this paper we study the three critical components of the Lottery Ticket (LT) algorithm, showing that each may be varied significantly without impacting the overall results. Ablating these factors leads to new insights for why LT networks perform as well as they do. We show why setting weights to zero is important, how signs are all you need to make the reinitialized network train, and why masking behaves like training. Finally, we discover the existence of Supermasks, masks that can be applied to an untrained, randomly initialized network to produce a model with performance far better than chance (86% on MNIST, 41% on CIFAR-10).",2020-03-03,2022-03-10 23:42:04,2022-03-10 23:42:04,2022-03-10 23:42:04,,,,,,,Deconstructing Lottery Tickets,,,,,,,,,,,,arXiv.org,,arXiv: 1905.01067,,"/Users/jacquesthibodeau/Zotero/storage/KNZQ86PM/Zhou et al. - 2020 - Deconstructing Lottery Tickets Zeros, Signs, and .pdf; /Users/jacquesthibodeau/Zotero/storage/6XAB8N4C/1905.html",,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQPSYRCT,journalArticle,2019,"Wang, Po-Wei; Donti, Priya L.; Wilder, Bryan; Kolter, Zico",SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver,"arXiv:1905.12149 [cs, stat]",,,,http://arxiv.org/abs/1905.12149,"Integrating logical reasoning within deep learning architectures has been a major goal of modern AI systems. In this paper, we propose a new direction toward this goal by introducing a differentiable (smoothed) maximum satisfiability (MAXSAT) solver that can be integrated into the loop of larger deep learning systems. Our (approximate) solver is based upon a fast coordinate descent approach to solving the semidefinite program (SDP) associated with the MAXSAT problem. We show how to analytically differentiate through the solution to this SDP and efficiently solve the associated backward pass. We demonstrate that by integrating this solver into end-to-end learning systems, we can learn the logical structure of challenging problems in a minimally supervised fashion. In particular, we show that we can learn the parity function using single-bit supervision (a traditionally hard task for deep networks) and learn how to play 9x9 Sudoku solely from examples. We also solve a ""visual Sudok"" problem that maps images of Sudoku puzzles to their associated logical solutions by combining our MAXSAT solver with a traditional convolutional architecture. Our approach thus shows promise in integrating logical structures within deep learning.",2019-05-28,2022-03-10 23:42:10,2022-03-10 23:42:10,2022-03-10 23:42:10,,,,,,,SATNet,,,,,,,,,,,,arXiv.org,,arXiv: 1905.12149,,/Users/jacquesthibodeau/Zotero/storage/3K55GX52/Wang et al. - 2019 - SATNet Bridging deep learning and logical reasoni.pdf; /Users/jacquesthibodeau/Zotero/storage/UWD3W7BW/1905.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3W62CSPY,journalArticle,2019,"Nakkiran, Preetum",More Data Can Hurt for Linear Regression: Sample-wise Double Descent,"arXiv:1912.07242 [cs, math, stat]",,,,http://arxiv.org/abs/1912.07242,"In this expository note we describe a surprising phenomenon in overparameterized linear regression, where the dimension exceeds the number of samples: there is a regime where the test risk of the estimator found by gradient descent increases with additional samples. In other words, more data actually hurts the estimator. This behavior is implicit in a recent line of theoretical works analyzing ""double-descent"" phenomenon in linear models. In this note, we isolate and understand this behavior in an extremely simple setting: linear regression with isotropic Gaussian covariates. In particular, this occurs due to an unconventional type of bias-variance tradeoff in the overparameterized regime: the bias decreases with more samples, but variance increases.",2019-12-16,2022-03-10 23:42:14,2022-03-10 23:42:14,2022-03-10 23:42:14,,,,,,,More Data Can Hurt for Linear Regression,,,,,,,,,,,,arXiv.org,,arXiv: 1912.07242,,/Users/jacquesthibodeau/Zotero/storage/6LRJ62CD/Nakkiran - 2019 - More Data Can Hurt for Linear Regression Sample-w.pdf; /Users/jacquesthibodeau/Zotero/storage/3UGH6U85/1912.html,,,Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Mathematics - Statistics Theory; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8NXGZC5P,journalArticle,2020,"Yang, Zitong; Yu, Yaodong; You, Chong; Steinhardt, Jacob; Ma, Yi",Rethinking Bias-Variance Trade-off for Generalization of Neural Networks,"arXiv:2002.11328 [cs, stat]",,,,http://arxiv.org/abs/2002.11328,"The classical bias-variance trade-off predicts that bias decreases and variance increase with model complexity, leading to a U-shaped risk curve. Recent work calls this into question for neural networks and other over-parameterized models, for which it is often observed that larger models generalize better. We provide a simple explanation for this by measuring the bias and variance of neural networks: while the bias is monotonically decreasing as in the classical theory, the variance is unimodal or bell-shaped: it increases then decreases with the width of the network. We vary the network architecture, loss function, and choice of dataset and confirm that variance unimodality occurs robustly for all models we considered. The risk curve is the sum of the bias and variance curves and displays different qualitative shapes depending on the relative scale of bias and variance, with the double descent curve observed in recent literature as a special case. We corroborate these empirical results with a theoretical analysis of two-layer linear networks with random first layer. Finally, evaluation on out-of-distribution data shows that most of the drop in accuracy comes from increased bias while variance increases by a relatively small amount. Moreover, we find that deeper models decrease bias and increase variance for both in-distribution and out-of-distribution data.",2020-12-07,2022-03-10 23:42:22,2022-03-10 23:42:22,2022-03-10 23:42:22,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2002.11328,,/Users/jacquesthibodeau/Zotero/storage/L6J8CFUW/Yang et al. - 2020 - Rethinking Bias-Variance Trade-off for Generalizat.pdf; /Users/jacquesthibodeau/Zotero/storage/VQB52LKM/2002.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SN2C3E8H,journalArticle,2021,"Lu, Kevin; Grover, Aditya; Abbeel, Pieter; Mordatch, Igor",Pretrained Transformers as Universal Computation Engines,arXiv:2103.05247 [cs],,,,http://arxiv.org/abs/2103.05247,"We investigate the capability of a transformer pretrained on natural language to generalize to other modalities with minimal finetuning -- in particular, without finetuning of the self-attention and feedforward layers of the residual blocks. We consider such a model, which we call a Frozen Pretrained Transformer (FPT), and study finetuning it on a variety of sequence classification tasks spanning numerical computation, vision, and protein fold prediction. In contrast to prior works which investigate finetuning on the same modality as the pretraining dataset, we show that pretraining on natural language can improve performance and compute efficiency on non-language downstream tasks. Additionally, we perform an analysis of the architecture, comparing the performance of a random initialized transformer to a random LSTM. Combining the two insights, we find language-pretrained transformers can obtain strong performance on a variety of non-language tasks.",2021-06-30,2022-03-10 23:42:42,2022-03-10 23:42:42,2022-03-10 23:42:42,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2103.05247,,/Users/jacquesthibodeau/Zotero/storage/RRW63G5Z/Lu et al. - 2021 - Pretrained Transformers as Universal Computation E.pdf; /Users/jacquesthibodeau/Zotero/storage/K4M9R59B/2103.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZDPBRRCK,journalArticle,2021,"Perez, Ethan; Kiela, Douwe; Cho, Kyunghyun",True Few-Shot Learning with Language Models,"arXiv:2105.11447 [cs, stat]",,,,http://arxiv.org/abs/2105.11447,"Pretrained language models (LMs) perform well on many tasks even when learning from a few examples, but prior work uses many held-out examples to tune various aspects of learning, such as hyperparameters, training objectives, and natural language templates (""prompts""). Here, we evaluate the few-shot ability of LMs when such held-out examples are unavailable, a setting we call true few-shot learning. We test two model selection criteria, cross-validation and minimum description length, for choosing LM prompts and hyperparameters in the true few-shot setting. On average, both marginally outperform random selection and greatly underperform selection based on held-out examples. Moreover, selection criteria often prefer models that perform significantly worse than randomly-selected ones. We find similar results even when taking into account our uncertainty in a model's true performance during selection, as well as when varying the amount of computation and number of examples used for selection. Overall, our findings suggest that prior work significantly overestimated the true few-shot ability of LMs given the difficulty of few-shot model selection.",2021-05-24,2022-03-10 23:42:48,2022-03-10 23:42:48,2022-03-10 23:42:48,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2105.11447,,/Users/jacquesthibodeau/Zotero/storage/2SNVCTPG/Perez et al. - 2021 - True Few-Shot Learning with Language Models.pdf; /Users/jacquesthibodeau/Zotero/storage/GILNT25Q/2105.html,,,Computer Science - Computation and Language; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5QMD5MDU,journalArticle,2021,"Austin, Jacob; Odena, Augustus; Nye, Maxwell; Bosma, Maarten; Michalewski, Henryk; Dohan, David; Jiang, Ellen; Cai, Carrie; Terry, Michael; Le, Quoc; Sutton, Charles",Program Synthesis with Large Language Models,arXiv:2108.07732 [cs],,,,http://arxiv.org/abs/2108.07732,"This paper explores the limits of the current generation of large language models for program synthesis in general purpose programming languages. We evaluate a collection of such models (with between 244M and 137B parameters) on two new benchmarks, MBPP and MathQA-Python, in both the few-shot and fine-tuning regimes. Our benchmarks are designed to measure the ability of these models to synthesize short Python programs from natural language descriptions. The Mostly Basic Programming Problems (MBPP) dataset contains 974 programming tasks, designed to be solvable by entry-level programmers. The MathQA-Python dataset, a Python version of the MathQA benchmark, contains 23914 problems that evaluate the ability of the models to synthesize code from more complex text. On both datasets, we find that synthesis performance scales log-linearly with model size. Our largest models, even without finetuning on a code dataset, can synthesize solutions to 59.6 percent of the problems from MBPP using few-shot learning with a well-designed prompt. Fine-tuning on a held-out portion of the dataset improves performance by about 10 percentage points across most model sizes. On the MathQA-Python dataset, the largest fine-tuned model achieves 83.8 percent accuracy. Going further, we study the model's ability to engage in dialog about code, incorporating human feedback to improve its solutions. We find that natural language feedback from a human halves the error rate compared to the model's initial prediction. Additionally, we conduct an error analysis to shed light on where these models fall short and what types of programs are most difficult to generate. Finally, we explore the semantic grounding of these models by fine-tuning them to predict the results of program execution. We find that even our best models are generally unable to predict the output of a program given a specific input.",2021-08-15,2022-03-10 23:42:57,2022-03-10 23:42:57,2022-03-10 23:42:57,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2108.07732,,/Users/jacquesthibodeau/Zotero/storage/HT3SYX4A/Austin et al. - 2021 - Program Synthesis with Large Language Models.pdf; /Users/jacquesthibodeau/Zotero/storage/EXGXSEVP/2108.html,,,Computer Science - Machine Learning; Computer Science - Programming Languages,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3MG4RYUH,journalArticle,2020,"Adiwardana, Daniel; Luong, Minh-Thang; So, David R.; Hall, Jamie; Fiedel, Noah; Thoppilan, Romal; Yang, Zi; Kulshreshtha, Apoorv; Nemade, Gaurav; Lu, Yifeng; Le, Quoc V.",Towards a Human-like Open-Domain Chatbot,"arXiv:2001.09977 [cs, stat]",,,,http://arxiv.org/abs/2001.09977,"We present Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token. We also propose a human evaluation metric called Sensibleness and Specificity Average (SSA), which captures key elements of a human-like multi-turn conversation. Our experiments show strong correlation between perplexity and SSA. The fact that the best perplexity end-to-end trained Meena scores high on SSA (72% on multi-turn evaluation) suggests that a human-level SSA of 86% is potentially within reach if we can better optimize perplexity. Additionally, the full version of Meena (with a filtering mechanism and tuned decoding) scores 79% SSA, 23% higher in absolute SSA than the existing chatbots we evaluated.",2020-02-27,2022-03-10 23:43:03,2022-03-10 23:43:03,2022-03-10 23:43:03,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2001.09977,,/Users/jacquesthibodeau/Zotero/storage/9N7RF3HY/Adiwardana et al. - 2020 - Towards a Human-like Open-Domain Chatbot.pdf; /Users/jacquesthibodeau/Zotero/storage/IUABBQL7/2001.html,,,Computer Science - Computation and Language; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PRNNR53J,journalArticle,2019,"Rosenfeld, Jonathan S.; Rosenfeld, Amir; Belinkov, Yonatan; Shavit, Nir",A Constructive Prediction of the Generalization Error Across Scales,"arXiv:1909.12673 [cs, stat]",,,,http://arxiv.org/abs/1909.12673,"The dependency of the generalization error of neural networks on model and dataset size is of critical importance both in practice and for understanding the theory of neural networks. Nevertheless, the functional form of this dependency remains elusive. In this work, we present a functional form which approximates well the generalization error in practice. Capitalizing on the successful concept of model scaling (e.g., width, depth), we are able to simultaneously construct such a form and specify the exact models which can attain it across model/data scales. Our construction follows insights obtained from observations conducted over a range of model/data scales, in various model types and datasets, in vision and language tasks. We show that the form both fits the observations well across scales, and provides accurate predictions from small- to large-scale models and data.",2019-12-20,2022-03-10 23:43:10,2022-03-10 23:43:10,2022-03-10 23:43:09,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1909.12673,,/Users/jacquesthibodeau/Zotero/storage/FU3S8BXC/Rosenfeld et al. - 2019 - A Constructive Prediction of the Generalization Er.pdf; /Users/jacquesthibodeau/Zotero/storage/G2D29J3X/1909.html,,,Computer Science - Computation and Language; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SIN4RSU7,journalArticle,2020,"Kaplan, Jared; McCandlish, Sam; Henighan, Tom; Brown, Tom B.; Chess, Benjamin; Child, Rewon; Gray, Scott; Radford, Alec; Wu, Jeffrey; Amodei, Dario",Scaling Laws for Neural Language Models,"arXiv:2001.08361 [cs, stat]",,,,http://arxiv.org/abs/2001.08361,"We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.",2020-01-22,2022-03-10 23:43:13,2022-03-11 01:38:09,2022-03-10 23:43:13,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2001.08361,,/Users/jacquesthibodeau/Zotero/storage/ATE2YTHH/Kaplan et al. - 2020 - Scaling Laws for Neural Language Models.pdf; /Users/jacquesthibodeau/Zotero/storage/DPVDJBAM/Kaplan et al. - 2020 - Scaling Laws for Neural Language Models.pdf; /Users/jacquesthibodeau/Zotero/storage/P64K2X38/2001.html; /Users/jacquesthibodeau/Zotero/storage/WPPV6WBH/2001.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
77X22GNW,journalArticle,2018,"Chauvet, Jean-Marie",The 30-Year Cycle In The AI Debate,arXiv:1810.04053 [cs],,,,http://arxiv.org/abs/1810.04053,"In the last couple of years, the rise of Artificial Intelligence and the successes of academic breakthroughs in the field have been inescapable. Vast sums of money have been thrown at AI start-ups. Many existing tech companies -- including the giants like Google, Amazon, Facebook, and Microsoft -- have opened new research labs. The rapid changes in these everyday work and entertainment tools have fueled a rising interest in the underlying technology itself; journalists write about AI tirelessly, and companies -- of tech nature or not -- brand themselves with AI, Machine Learning or Deep Learning whenever they get a chance. Confronting squarely this media coverage, several analysts are starting to voice concerns about over-interpretation of AI's blazing successes and the sometimes poor public reporting on the topic. This paper reviews briefly the track-record in AI and Machine Learning and finds this pattern of early dramatic successes, followed by philosophical critique and unexpected difficulties, if not downright stagnation, returning almost to the clock in 30-year cycles since 1958.",2018-10-08,2022-03-10 23:43:17,2022-03-10 23:43:17,2022-03-10 23:43:17,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.04053,,/Users/jacquesthibodeau/Zotero/storage/HTASGEW6/Chauvet - 2018 - The 30-Year Cycle In The AI Debate.pdf; /Users/jacquesthibodeau/Zotero/storage/9W3UJPQK/1810.html,,,Computer Science - Artificial Intelligence; I.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EM6S3KBF,journalArticle,2022,"Klinger, Joel; Mateos-Garcia, Juan; Stathoulopoulos, Konstantinos",A narrowing of AI research?,arXiv:2009.10385 [cs],,,,http://arxiv.org/abs/2009.10385,"The arrival of deep learning techniques able to infer patterns from large datasets has dramatically improved the performance of Artificial Intelligence (AI) systems. Deep learning's rapid development and adoption, in great part led by large technology companies, has however created concerns about a premature narrowing in the technological trajectory of AI research despite its weaknesses, which include lack of robustness, high environmental costs, and potentially unfair outcomes. We seek to improve the evidence base with a semantic analysis of AI research in arXiv, a popular pre-prints database. We study the evolution of the thematic diversity of AI research, compare the thematic diversity of AI research in academia and the private sector and measure the influence of private companies in AI research through the citations they receive and their collaborations with other institutions. Our results suggest that diversity in AI research has stagnated in recent years, and that AI research involving the private sector tends to be less diverse and more influential than research in academia. We also find that private sector AI researchers tend to specialise in data-hungry and computationally intensive deep learning methods at the expense of research involving other AI methods, research that considers the societal and ethical implications of AI, and applications in sectors like health. Our results provide a rationale for policy action to prevent a premature narrowing of AI research that could constrain its societal benefits, but we note the informational, incentive and scale hurdles standing in the way of such interventions.",2022-01-11,2022-03-10 23:43:21,2022-03-10 23:43:21,2022-03-10 23:43:21,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2009.10385,,/Users/jacquesthibodeau/Zotero/storage/S64T9AAN/Klinger et al. - 2022 - A narrowing of AI research.pdf; /Users/jacquesthibodeau/Zotero/storage/EDQUJXVG/2009.html,,,Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UHSHAGTW,journalArticle,2019,"Carbune, Victor; Coppey, Thierry; Daryin, Alexander; Deselaers, Thomas; Sarda, Nikhil; Yagnik, Jay",SmartChoices: Hybridizing Programming and Machine Learning,"arXiv:1810.00619 [cs, stat]",,,,http://arxiv.org/abs/1810.00619,"We present SmartChoices, an approach to making machine learning (ML) a first class citizen in programming languages which we see as one way to lower the entrance cost to applying ML to problems in new domains. There is a growing divide in approaches to building systems: on the one hand, programming leverages human experts to define a system while on the other hand behavior is learned from data in machine learning. We propose to hybridize these two by providing a 3-call API which we expose through an object called SmartChoice. We describe the SmartChoices-interface, how it can be used in programming with minimal code changes, and demonstrate that it is an easy to use but still powerful tool by demonstrating improvements over not using ML at all on three algorithmic problems: binary search, QuickSort, and caches. In these three examples, we replace the commonly used heuristics with an ML model entirely encapsulated within a SmartChoice and thus requiring minimal code changes. As opposed to previous work applying ML to algorithmic problems, our proposed approach does not require to drop existing implementations but seamlessly integrates into the standard software development workflow and gives full control to the software developer over how ML methods are applied. Our implementation relies on standard Reinforcement Learning (RL) methods. To learn faster, we use the heuristic function, which they are replacing, as an initial function. We show how this initial function can be used to speed up and stabilize learning while providing a safety net that prevents performance to become substantially worse -- allowing for a safe deployment in critical applications in real life.",2019-06-13,2022-03-10 23:43:26,2022-03-10 23:43:26,2022-03-10 23:43:25,,,,,,,SmartChoices,,,,,,,,,,,,arXiv.org,,arXiv: 1810.00619,,/Users/jacquesthibodeau/Zotero/storage/GGXH2H3R/Carbune et al. - 2019 - SmartChoices Hybridizing Programming and Machine .pdf; /Users/jacquesthibodeau/Zotero/storage/B67S53QR/1810.html,,,Computer Science - Machine Learning; Computer Science - Programming Languages; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3NMIUFE6,journalArticle,2019,"Haldar, Malay; Abdool, Mustafa; Ramanathan, Prashant; Xu, Tao; Yang, Shulin; Duan, Huizhong; Zhang, Qing; Barrow-Williams, Nick; Turnbull, Bradley C.; Collins, Brendan M.; Legrand, Thomas",Applying Deep Learning To Airbnb Search,Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,,,10.1145/3292500.3330658,http://arxiv.org/abs/1810.09591,"The application to search ranking is one of the biggest machine learning success stories at Airbnb. Much of the initial gains were driven by a gradient boosted decision tree model. The gains, however, plateaued over time. This paper discusses the work done in applying neural networks in an attempt to break out of that plateau. We present our perspective not with the intention of pushing the frontier of new modeling techniques. Instead, ours is a story of the elements we found useful in applying neural networks to a real life product. Deep learning was steep learning for us. To other teams embarking on similar journeys, we hope an account of our struggles and triumphs will provide some useful pointers. Bon voyage!",2019-07-25,2022-03-10 23:43:31,2022-03-10 23:43:31,2022-03-10 23:43:31,1927-1935,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.09591,,/Users/jacquesthibodeau/Zotero/storage/CLQZCJRI/Haldar et al. - 2019 - Applying Deep Learning To Airbnb Search.pdf; /Users/jacquesthibodeau/Zotero/storage/392UNZHS/1810.html,,,Computer Science - Artificial Intelligence; Computer Science - Information Retrieval; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2829PFQA,journalArticle,2019,"Dulac-Arnold, Gabriel; Mankowitz, Daniel; Hester, Todd",Challenges of Real-World Reinforcement Learning,"arXiv:1904.12901 [cs, stat]",,,,http://arxiv.org/abs/1904.12901,"Reinforcement learning (RL) has proven its worth in a series of artificial domains, and is beginning to show some successes in real-world scenarios. However, much of the research advances in RL are often hard to leverage in real-world systems due to a series of assumptions that are rarely satisfied in practice. We present a set of nine unique challenges that must be addressed to productionize RL to real world problems. For each of these challenges, we specify the exact meaning of the challenge, present some approaches from the literature, and specify some metrics for evaluating that challenge. An approach that addresses all nine challenges would be applicable to a large number of real world problems. We also present an example domain that has been modified to present these challenges as a testbed for practical RL research.",2019-04-29,2022-03-10 23:43:48,2022-03-10 23:43:48,2022-03-10 23:43:48,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1904.12901,,/Users/jacquesthibodeau/Zotero/storage/2HBDXPXL/Dulac-Arnold et al. - 2019 - Challenges of Real-World Reinforcement Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/CVKGCPKQ/1904.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UIJW3MYL,journalArticle,2019,"Rolnick, David; Donti, Priya L.; Kaack, Lynn H.; Kochanski, Kelly; Lacoste, Alexandre; Sankaran, Kris; Ross, Andrew Slavin; Milojevic-Dupont, Nikola; Jaques, Natasha; Waldman-Brown, Anna; Luccioni, Alexandra; Maharaj, Tegan; Sherwin, Evan D.; Mukkavilli, S. Karthik; Kording, Konrad P.; Gomes, Carla; Ng, Andrew Y.; Hassabis, Demis; Platt, John C.; Creutzig, Felix; Chayes, Jennifer; Bengio, Yoshua",Tackling Climate Change with Machine Learning,"arXiv:1906.05433 [cs, stat]",,,,http://arxiv.org/abs/1906.05433,"Climate change is one of the greatest challenges facing humanity, and we, as machine learning experts, may wonder how we can help. Here we describe how machine learning can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. From smart grids to disaster management, we identify high impact problems where existing gaps can be filled by machine learning, in collaboration with other fields. Our recommendations encompass exciting research questions as well as promising business opportunities. We call on the machine learning community to join the global effort against climate change.",2019-11-05,2022-03-10 23:43:50,2022-03-10 23:43:50,2022-03-10 23:43:50,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1906.05433,,/Users/jacquesthibodeau/Zotero/storage/HVKSMKWY/Rolnick et al. - 2019 - Tackling Climate Change with Machine Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/464T74WB/1906.html,,,Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PA59LU7D,journalArticle,2019,"Kott, Alexander; Théron, Paul; Drašar, Martin; Dushku, Edlira; LeBlanc, Benoît; Losiewicz, Paul; Guarino, Alessandro; Mancini, Luigi; Panico, Agostino; Pihelgas, Mauno; Rzadca, Krzysztof",Autonomous Intelligent Cyber-defense Agent (AICA) Reference Architecture. Release 2.0,arXiv:1803.10664 [cs],,,,http://arxiv.org/abs/1803.10664,"This report - a major revision of its previous release - describes a reference architecture for intelligent software agents performing active, largely autonomous cyber-defense actions on military networks of computing and communicating devices. The report is produced by the North Atlantic Treaty Organization (NATO) Research Task Group (RTG) IST-152 ""Intelligent Autonomous Agents for Cyber Defense and Resilience"". In a conflict with a technically sophisticated adversary, NATO military tactical networks will operate in a heavily contested battlefield. Enemy software cyber agents - malware - will infiltrate friendly networks and attack friendly command, control, communications, computers, intelligence, surveillance, and reconnaissance and computerized weapon systems. To fight them, NATO needs artificial cyber hunters - intelligent, autonomous, mobile agents specialized in active cyber defense. With this in mind, in 2016, NATO initiated RTG IST-152. Its objective has been to help accelerate the development and transition to practice of such software agents by producing a reference architecture and technical roadmap. This report presents the concept and architecture of an Autonomous Intelligent Cyber-defense Agent (AICA). We describe the rationale of the AICA concept, explain the methodology and purpose that drive the definition of the AICA Reference Architecture, and review some of the main features and challenges of AICAs.",2019-09-18,2022-03-10 23:43:53,2022-03-10 23:43:53,2022-03-10 23:43:53,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1803.10664,,/Users/jacquesthibodeau/Zotero/storage/CSTKWQ8A/Kott et al. - 2019 - Autonomous Intelligent Cyber-defense Agent (AICA) .pdf; /Users/jacquesthibodeau/Zotero/storage/VDUGIA2E/1803.html,,,Computer Science - Cryptography and Security,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5SQCEQGA,journalArticle,2019,"Ovadya, Aviv; Whittlestone, Jess",Reducing malicious use of synthetic media research: Considerations and potential release practices for machine learning,arXiv:1907.11274 [cs],,,,http://arxiv.org/abs/1907.11274,"The aim of this paper is to facilitate nuanced discussion around research norms and practices to mitigate the harmful impacts of advances in machine learning (ML). We focus particularly on the use of ML to create ""synthetic media"" (e.g. to generate or manipulate audio, video, images, and text), and the question of what publication and release processes around such research might look like, though many of the considerations discussed will apply to ML research more broadly. We are not arguing for any specific approach on when or how research should be distributed, but instead try to lay out some useful tools, analogies, and options for thinking about these issues. We begin with some background on the idea that ML research might be misused in harmful ways, and why advances in synthetic media, in particular, are raising concerns. We then outline in more detail some of the different paths to harm from ML research, before reviewing research risk mitigation strategies in other fields and identifying components that seem most worth emulating in the ML and synthetic media research communities. Next, we outline some important dimensions of disagreement on these issues which risk polarizing conversations. Finally, we conclude with recommendations, suggesting that the machine learning community might benefit from: working with subject matter experts to increase understanding of the risk landscape and possible mitigation strategies; building a community and norms around understanding the impacts of ML research, e.g. through regular workshops at major conferences; and establishing institutions and systems to support release practices that would otherwise be onerous and error-prone.",2019-07-28,2022-03-10 23:44:01,2022-03-10 23:44:01,2022-03-10 23:44:01,,,,,,,Reducing malicious use of synthetic media research,,,,,,,,,,,,arXiv.org,,arXiv: 1907.11274,,/Users/jacquesthibodeau/Zotero/storage/PJEWY7UN/Ovadya and Whittlestone - 2019 - Reducing malicious use of synthetic media research.pdf; /Users/jacquesthibodeau/Zotero/storage/BPNB88MY/1907.html,,,Computer Science - Computers and Society; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5YUDGWXF,journalArticle,2018,"Hwang, Tim",Computational Power and the Social Impact of Artificial Intelligence,arXiv:1803.08971 [cs],,,,http://arxiv.org/abs/1803.08971,"Machine learning is a computational process. To that end, it is inextricably tied to computational power - the tangible material of chips and semiconductors that the algorithms of machine intelligence operate on. Most obviously, computational power and computing architectures shape the speed of training and inference in machine learning, and therefore influence the rate of progress in the technology. But, these relationships are more nuanced than that: hardware shapes the methods used by researchers and engineers in the design and development of machine learning models. Characteristics such as the power consumption of chips also define where and how machine learning can be used in the real world. Despite this, many analyses of the social impact of the current wave of progress in AI have not substantively brought the dimension of hardware into their accounts. While a common trope in both the popular press and scholarly literature is to highlight the massive increase in computational power that has enabled the recent breakthroughs in machine learning, the analysis frequently goes no further than this observation around magnitude. This paper aims to dig more deeply into the relationship between computational power and the development of machine learning. Specifically, it examines how changes in computing architectures, machine learning methodologies, and supply chains might influence the future of AI. In doing so, it seeks to trace a set of specific relationships between this underlying hardware layer and the broader social impacts and risks around AI.",2018-03-23,2022-03-10 23:44:05,2022-03-10 23:44:05,2022-03-10 23:44:05,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1803.08971,,/Users/jacquesthibodeau/Zotero/storage/A9RPSM6T/Hwang - 2018 - Computational Power and the Social Impact of Artif.pdf; /Users/jacquesthibodeau/Zotero/storage/8W66HSA9/1803.html,,,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
82GP3J4H,journalArticle,2020,"Zellers, Rowan; Holtzman, Ari; Rashkin, Hannah; Bisk, Yonatan; Farhadi, Ali; Roesner, Franziska; Choi, Yejin",Defending Against Neural Fake News,arXiv:1905.12616 [cs],,,,http://arxiv.org/abs/1905.12616,"Recent progress in natural language generation has raised dual-use concerns. While applications like summarization and translation are positive, the underlying technology also might enable adversaries to generate neural fake news: targeted propaganda that closely mimics the style of real news. Modern computer security relies on careful threat modeling: identifying potential threats and vulnerabilities from an adversary's point of view, and exploring potential mitigations to these threats. Likewise, developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models. We thus present a model for controllable text generation called Grover. Given a headline like `Link Found Between Vaccines and Autism,' Grover can generate the rest of the article; humans find these generations to be more trustworthy than human-written disinformation. Developing robust verification techniques against generators like Grover is critical. We find that best current discriminators can classify neural fake news from real, human-written, news with 73% accuracy, assuming access to a moderate level of training data. Counterintuitively, the best defense against Grover turns out to be Grover itself, with 92% accuracy, demonstrating the importance of public release of strong generators. We investigate these results further, showing that exposure bias -- and sampling strategies that alleviate its effects -- both leave artifacts that similar discriminators can pick up on. We conclude by discussing ethical issues regarding the technology, and plan to release Grover publicly, helping pave the way for better detection of neural fake news.",2020-12-11,2022-03-10 23:44:16,2022-03-10 23:44:16,2022-03-10 23:44:16,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1905.12616,,/Users/jacquesthibodeau/Zotero/storage/B6J63LEU/Zellers et al. - 2020 - Defending Against Neural Fake News.pdf; /Users/jacquesthibodeau/Zotero/storage/U7HZRI7Y/1905.html,,,Computer Science - Computation and Language; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YTTIVTBF,journalArticle,2020,"Tucker, Aaron D.; Anderljung, Markus; Dafoe, Allan",Social and Governance Implications of Improved Data Efficiency,"Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3375627.3375863,http://arxiv.org/abs/2001.05068,"Many researchers work on improving the data efficiency of machine learning. What would happen if they succeed? This paper explores the social-economic impact of increased data efficiency. Specifically, we examine the intuition that data efficiency will erode the barriers to entry protecting incumbent data-rich AI firms, exposing them to more competition from data-poor firms. We find that this intuition is only partially correct: data efficiency makes it easier to create ML applications, but large AI firms may have more to gain from higher performing AI systems. Further, we find that the effect on privacy, data markets, robustness, and misuse are complex. For example, while it seems intuitive that misuse risk would increase along with data efficiency -- as more actors gain access to any level of capability -- the net effect crucially depends on how much defensive measures are improved. More investigation into data efficiency, as well as research into the ""AI production function"", will be key to understanding the development of the AI industry and its societal impacts.",2020-02-07,2022-03-10 23:44:19,2022-03-10 23:44:19,2022-03-10 23:44:19,378-384,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2001.05068,,/Users/jacquesthibodeau/Zotero/storage/D82TK4WV/Tucker et al. - 2020 - Social and Governance Implications of Improved Dat.pdf; /Users/jacquesthibodeau/Zotero/storage/XAXBBTL2/2001.html,,,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DDG2FD5K,journalArticle,2021,"Abdalla, Mohamed; Abdalla, Moustafa","The Grey Hoodie Project: Big Tobacco, Big Tech, and the threat on academic integrity","Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3461702.3462563,http://arxiv.org/abs/2009.13676,"As governmental bodies rely on academics' expert advice to shape policy regarding Artificial Intelligence, it is important that these academics not have conflicts of interests that may cloud or bias their judgement. Our work explores how Big Tech can actively distort the academic landscape to suit its needs. By comparing the well-studied actions of another industry (Big Tobacco) to the current actions of Big Tech we see similar strategies employed by both industries. These strategies enable either industry to sway and influence academic and public discourse. We examine the funding of academic research as a tool used by Big Tech to put forward a socially responsible public image, influence events hosted by and decisions made by funded universities, influence the research questions and plans of individual scientists, and discover receptive academics who can be leveraged. We demonstrate how Big Tech can affect academia from the institutional level down to individual researchers. Thus, we believe that it is vital, particularly for universities and other institutions of higher learning, to discuss the appropriateness and the tradeoffs of accepting funding from Big Tech, and what limitations or conditions should be put in place.",2021-07-21,2022-03-10 23:44:21,2022-03-10 23:44:21,2022-03-10 23:44:20,287-297,,,,,,The Grey Hoodie Project,,,,,,,,,,,,arXiv.org,,arXiv: 2009.13676,,"/Users/jacquesthibodeau/Zotero/storage/FD6UC3C3/Abdalla and Abdalla - 2021 - The Grey Hoodie Project Big Tobacco, Big Tech, an.pdf; /Users/jacquesthibodeau/Zotero/storage/YBF7XPL5/2009.html",,,Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NCR6MLKN,journalArticle,2021,"Evans, Owain; Cotton-Barratt, Owen; Finnveden, Lukas; Bales, Adam; Balwit, Avital; Wills, Peter; Righetti, Luca; Saunders, William",Truthful AI: Developing and governing AI that does not lie,arXiv:2110.06674 [cs],,,,http://arxiv.org/abs/2110.06674,"In many contexts, lying -- the use of verbal falsehoods to deceive -- is harmful. While lying has traditionally been a human affair, AI systems that make sophisticated verbal statements are becoming increasingly prevalent. This raises the question of how we should limit the harm caused by AI ""lies"" (i.e. falsehoods that are actively selected for). Human truthfulness is governed by social norms and by laws (against defamation, perjury, and fraud). Differences between AI and humans present an opportunity to have more precise standards of truthfulness for AI, and to have these standards rise over time. This could provide significant benefits to public epistemics and the economy, and mitigate risks of worst-case AI futures. Establishing norms or laws of AI truthfulness will require significant work to: (1) identify clear truthfulness standards; (2) create institutions that can judge adherence to those standards; and (3) develop AI systems that are robustly truthful. Our initial proposals for these areas include: (1) a standard of avoiding ""negligent falsehoods"" (a generalisation of lies that is easier to assess); (2) institutions to evaluate AI systems before and after real-world deployment; and (3) explicitly training AI systems to be truthful via curated datasets and human interaction. A concerning possibility is that evaluation mechanisms for eventual truthfulness standards could be captured by political interests, leading to harmful censorship and propaganda. Avoiding this might take careful attention. And since the scale of AI speech acts might grow dramatically over the coming decades, early truthfulness standards might be particularly important because of the precedents they set.",2021-10-13,2022-03-10 23:44:22,2022-03-10 23:44:22,2022-03-10 23:44:21,,,,,,,Truthful AI,,,,,,,,,,,,arXiv.org,,arXiv: 2110.06674,,/Users/jacquesthibodeau/Zotero/storage/BIY2B2AL/Evans et al. - 2021 - Truthful AI Developing and governing AI that does.pdf; /Users/jacquesthibodeau/Zotero/storage/RSAQUYYN/2110.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Computers and Society; I.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QPPDDWEL,journalArticle,2020,"Shevlane, Toby; Dafoe, Allan",The Offense-Defense Balance of Scientific Knowledge: Does Publishing AI Research Reduce Misuse?,arXiv:2001.00463 [cs],,,,http://arxiv.org/abs/2001.00463,"There is growing concern over the potential misuse of artificial intelligence (AI) research. Publishing scientific research can facilitate misuse of the technology, but the research can also contribute to protections against misuse. This paper addresses the balance between these two effects. Our theoretical framework elucidates the factors governing whether the published research will be more useful for attackers or defenders, such as the possibility for adequate defensive measures, or the independent discovery of the knowledge outside of the scientific community. The balance will vary across scientific fields. However, we show that the existing conversation within AI has imported concepts and conclusions from prior debates within computer security over the disclosure of software vulnerabilities. While disclosure of software vulnerabilities often favours defence, this cannot be assumed for AI research. The AI research community should consider concepts and policies from a broad set of adjacent fields, and ultimately needs to craft policy well-suited to its particular challenges.",2020-01-09,2022-03-10 23:44:22,2022-03-10 23:44:22,2022-03-10 23:44:22,,,,,,,The Offense-Defense Balance of Scientific Knowledge,,,,,,,,,,,,arXiv.org,,arXiv: 2001.00463,,/Users/jacquesthibodeau/Zotero/storage/N97PVNYU/Shevlane and Dafoe - 2020 - The Offense-Defense Balance of Scientific Knowledg.pdf; /Users/jacquesthibodeau/Zotero/storage/ZIUK4FSW/2001.html,,,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PCKNIL9X,journalArticle,2021,"Bahri, Yasaman; Dyer, Ethan; Kaplan, Jared; Lee, Jaehoon; Sharma, Utkarsh",Explaining Neural Scaling Laws,"arXiv:2102.06701 [cond-mat, stat]",,,,http://arxiv.org/abs/2102.06701,"The test loss of well-trained neural networks often follows precise power-law scaling relations with either the size of the training dataset or the number of parameters in the network. We propose a theory that explains and connects these scaling laws. We identify variance-limited and resolution-limited scaling behavior for both dataset and model size, for a total of four scaling regimes. The variance-limited scaling follows simply from the existence of a well-behaved infinite data or infinite width limit, while the resolution-limited regime can be explained by positing that models are effectively resolving a smooth data manifold. In the large width limit, this can be equivalently obtained from the spectrum of certain kernels, and we present evidence that large width and large dataset resolution-limited scaling exponents are related by a duality. We exhibit all four scaling regimes in the controlled setting of large random feature and pretrained models and test the predictions empirically on a range of standard architectures and datasets. We also observe several empirical relationships between datasets and scaling exponents: super-classing image tasks does not change exponents, while changing input distribution (via changing datasets or adding noise) has a strong effect. We further explore the effect of architecture aspect ratio on scaling exponents.",2021-02-12,2022-03-10 23:44:33,2022-03-10 23:44:33,2022-03-10 23:44:33,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2102.06701,,/Users/jacquesthibodeau/Zotero/storage/8S68RB3V/Bahri et al. - 2021 - Explaining Neural Scaling Laws.pdf; /Users/jacquesthibodeau/Zotero/storage/JPAKSCXB/2102.html,,,Computer Science - Machine Learning; Condensed Matter - Disordered Systems and Neural Networks; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PCQZU39W,journalArticle,2020,"Sharma, Utkarsh; Kaplan, Jared",A Neural Scaling Law from the Dimension of the Data Manifold,"arXiv:2004.10802 [cs, stat]",,,,http://arxiv.org/abs/2004.10802,"When data is plentiful, the loss achieved by well-trained neural networks scales as a power-law $L \propto N^{-\alpha}$ in the number of network parameters $N$. This empirical scaling law holds for a wide variety of data modalities, and may persist over many orders of magnitude. The scaling law can be explained if neural models are effectively just performing regression on a data manifold of intrinsic dimension $d$. This simple theory predicts that the scaling exponents $\alpha \approx 4/d$ for cross-entropy and mean-squared error losses. We confirm the theory by independently measuring the intrinsic dimension and the scaling exponents in a teacher/student framework, where we can study a variety of $d$ and $\alpha$ by dialing the properties of random teacher networks. We also test the theory with CNN image classifiers on several datasets and with GPT-type language models.",2020-04-22,2022-03-10 23:44:35,2022-03-10 23:44:35,2022-03-10 23:44:35,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2004.10802,,/Users/jacquesthibodeau/Zotero/storage/MZSQ5X5L/Sharma and Kaplan - 2020 - A Neural Scaling Law from the Dimension of the Dat.pdf; /Users/jacquesthibodeau/Zotero/storage/GMV8XV59/2004.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ACYS2UU4,journalArticle,2018,"Trask, Andrew; Hill, Felix; Reed, Scott; Rae, Jack; Dyer, Chris; Blunsom, Phil",Neural Arithmetic Logic Units,arXiv:1808.00508 [cs],,,,http://arxiv.org/abs/1808.00508,"Neural networks can learn to represent and manipulate numerical information, but they seldom generalize well outside of the range of numerical values encountered during training. To encourage more systematic numerical extrapolation, we propose an architecture that represents numerical quantities as linear activations which are manipulated using primitive arithmetic operators, controlled by learned gates. We call this module a neural arithmetic logic unit (NALU), by analogy to the arithmetic logic unit in traditional processors. Experiments show that NALU-enhanced neural networks can learn to track time, perform arithmetic over images of numbers, translate numerical language into real-valued scalars, execute computer code, and count objects in images. In contrast to conventional architectures, we obtain substantially better generalization both inside and outside of the range of numerical values encountered during training, often extrapolating orders of magnitude beyond trained numerical ranges.",2018-08-01,2022-03-10 23:46:58,2022-03-10 23:46:58,2022-03-10 23:46:58,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1808.00508,,/Users/jacquesthibodeau/Zotero/storage/7F3XLF6L/Trask et al. - 2018 - Neural Arithmetic Logic Units.pdf; /Users/jacquesthibodeau/Zotero/storage/5W4UZ5TJ/1808.html,,,Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RCS66UJY,journalArticle,2019,"Ardizzone, Lynton; Kruse, Jakob; Wirkert, Sebastian; Rahner, Daniel; Pellegrini, Eric W.; Klessen, Ralf S.; Maier-Hein, Lena; Rother, Carsten; Köthe, Ullrich",Analyzing Inverse Problems with Invertible Neural Networks,"arXiv:1808.04730 [cs, stat]",,,,http://arxiv.org/abs/1808.04730,"In many tasks, in particular in natural science, the goal is to determine hidden system parameters from a set of measurements. Often, the forward process from parameter- to measurement-space is a well-defined function, whereas the inverse problem is ambiguous: one measurement may map to multiple different sets of parameters. In this setting, the posterior parameter distribution, conditioned on an input measurement, has to be determined. We argue that a particular class of neural networks is well suited for this task -- so-called Invertible Neural Networks (INNs). Although INNs are not new, they have, so far, received little attention in literature. While classical neural networks attempt to solve the ambiguous inverse problem directly, INNs are able to learn it jointly with the well-defined forward process, using additional latent output variables to capture the information otherwise lost. Given a specific measurement and sampled latent variables, the inverse pass of the INN provides a full distribution over parameter space. We verify experimentally, on artificial data and real-world problems from astrophysics and medicine, that INNs are a powerful analysis tool to find multi-modalities in parameter space, to uncover parameter correlations, and to identify unrecoverable parameters.",2019-02-06,2022-03-10 23:47:05,2022-03-10 23:47:05,2022-03-10 23:47:05,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1808.04730,,/Users/jacquesthibodeau/Zotero/storage/2Q8DNWCN/Ardizzone et al. - 2019 - Analyzing Inverse Problems with Invertible Neural .pdf; /Users/jacquesthibodeau/Zotero/storage/N3XJWWNW/1808.html,,,68T01; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
L4QAIU2V,journalArticle,2018,"Tang, Gongbo; Müller, Mathias; Rios, Annette; Sennrich, Rico",Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures,arXiv:1808.08946 [cs],,,,http://arxiv.org/abs/1808.08946,"Recently, non-recurrent architectures (convolutional, self-attentional) have outperformed RNNs in neural machine translation. CNNs and self-attentional networks can connect distant words via shorter network paths than RNNs, and it has been speculated that this improves their ability to model long-range dependencies. However, this theoretical argument has not been tested empirically, nor have alternative explanations for their strong performance been explored in-depth. We hypothesize that the strong performance of CNNs and self-attentional networks could also be due to their ability to extract semantic features from the source text, and we evaluate RNNs, CNNs and self-attention networks on two tasks: subject-verb agreement (where capturing long-range dependencies is required) and word sense disambiguation (where semantic feature extraction is required). Our experimental results show that: 1) self-attentional networks and CNNs do not outperform RNNs in modeling subject-verb agreement over long distances; 2) self-attentional networks perform distinctly better than RNNs and CNNs on word sense disambiguation.",2018-11-11,2022-03-10 23:47:08,2022-03-10 23:47:08,2022-03-10 23:47:08,,,,,,,Why Self-Attention?,,,,,,,,,,,,arXiv.org,,arXiv: 1808.08946,,/Users/jacquesthibodeau/Zotero/storage/8MZ5XB8H/Tang et al. - 2018 - Why Self-Attention A Targeted Evaluation of Neura.pdf; /Users/jacquesthibodeau/Zotero/storage/HR28YSX9/1808.html,,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MDWBVRU3,journalArticle,2020,"Anil, Rohan; Pereyra, Gabriel; Passos, Alexandre; Ormandi, Robert; Dahl, George E.; Hinton, Geoffrey E.",Large scale distributed neural network training through online distillation,"arXiv:1804.03235 [cs, stat]",,,,http://arxiv.org/abs/1804.03235,"Techniques such as ensembling and distillation promise model quality improvements when paired with almost any base model. However, due to increased test-time cost (for ensembles) and increased complexity of the training pipeline (for distillation), these techniques are challenging to use in industrial settings. In this paper we explore a variant of distillation which is relatively straightforward to use as it does not require a complicated multi-stage setup or many new hyperparameters. Our first claim is that online distillation enables us to use extra parallelism to fit very large datasets about twice as fast. Crucially, we can still speed up training even after we have already reached the point at which additional parallelism provides no benefit for synchronous or asynchronous stochastic gradient descent. Two neural networks trained on disjoint subsets of the data can share knowledge by encouraging each model to agree with the predictions the other model would have made. These predictions can come from a stale version of the other model so they can be safely computed using weights that only rarely get transmitted. Our second claim is that online distillation is a cost-effective way to make the exact predictions of a model dramatically more reproducible. We support our claims using experiments on the Criteo Display Ad Challenge dataset, ImageNet, and the largest to-date dataset used for neural language modeling, containing $6\times 10^{11}$ tokens and based on the Common Crawl repository of web data.",2020-08-20,2022-03-10 23:47:10,2022-03-10 23:47:10,2022-03-10 23:47:10,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1804.03235,,/Users/jacquesthibodeau/Zotero/storage/JY45HHWP/Anil et al. - 2020 - Large scale distributed neural network training th.pdf; /Users/jacquesthibodeau/Zotero/storage/UEP8DWDL/1804.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RVNWRTNL,journalArticle,2018,"Hamrick, Jessica B.; Allen, Kelsey R.; Bapst, Victor; Zhu, Tina; McKee, Kevin R.; Tenenbaum, Joshua B.; Battaglia, Peter W.",Relational inductive bias for physical construction in humans and machines,"arXiv:1806.01203 [cs, stat]",,,,http://arxiv.org/abs/1806.01203,"While current deep learning systems excel at tasks such as object classification, language processing, and gameplay, few can construct or modify a complex system such as a tower of blocks. We hypothesize that what these systems lack is a ""relational inductive bias"": a capacity for reasoning about inter-object relations and making choices over a structured description of a scene. To test this hypothesis, we focus on a task that involves gluing pairs of blocks together to stabilize a tower, and quantify how well humans perform. We then introduce a deep reinforcement learning agent which uses object- and relation-centric scene and policy representations and apply it to the task. Our results show that these structured representations allow the agent to outperform both humans and more naive approaches, suggesting that relational inductive bias is an important component in solving structured reasoning problems and for building more intelligent, flexible machines.",2018-06-04,2022-03-10 23:47:17,2022-03-10 23:47:17,2022-03-10 23:47:17,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1806.01203,,/Users/jacquesthibodeau/Zotero/storage/WJA4VXIQ/Hamrick et al. - 2018 - Relational inductive bias for physical constructio.pdf; /Users/jacquesthibodeau/Zotero/storage/VHIYX6TE/1806.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7TQ9R75L,journalArticle,2018,"Steenbrugge, Xander; Leroux, Sam; Verbelen, Tim; Dhoedt, Bart",Improving Generalization for Abstract Reasoning Tasks Using Disentangled Feature Representations,"arXiv:1811.04784 [cs, stat]",,,,http://arxiv.org/abs/1811.04784,"In this work we explore the generalization characteristics of unsupervised representation learning by leveraging disentangled VAE's to learn a useful latent space on a set of relational reasoning problems derived from Raven Progressive Matrices. We show that the latent representations, learned by unsupervised training using the right objective function, significantly outperform the same architectures trained with purely supervised learning, especially when it comes to generalization.",2018-11-12,2022-03-10 23:47:20,2022-03-10 23:47:20,2022-03-10 23:47:20,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1811.04784,,/Users/jacquesthibodeau/Zotero/storage/H7QT7665/Steenbrugge et al. - 2018 - Improving Generalization for Abstract Reasoning Ta.pdf; /Users/jacquesthibodeau/Zotero/storage/X9YDGXDG/1811.html,,,Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HUCET9H5,journalArticle,2019,"Oliver, Avital; Odena, Augustus; Raffel, Colin; Cubuk, Ekin D.; Goodfellow, Ian J.",Realistic Evaluation of Deep Semi-Supervised Learning Algorithms,"arXiv:1804.09170 [cs, stat]",,,,http://arxiv.org/abs/1804.09170,"Semi-supervised learning (SSL) provides a powerful framework for leveraging unlabeled data when labels are limited or expensive to obtain. SSL algorithms based on deep neural networks have recently proven successful on standard benchmark tasks. However, we argue that these benchmarks fail to address many issues that these algorithms would face in real-world applications. After creating a unified reimplementation of various widely-used SSL techniques, we test them in a suite of experiments designed to address these issues. We find that the performance of simple baselines which do not use unlabeled data is often underreported, that SSL methods differ in sensitivity to the amount of labeled and unlabeled data, and that performance can degrade substantially when the unlabeled dataset contains out-of-class examples. To help guide SSL research towards real-world applicability, we make our unified reimplemention and evaluation platform publicly available.",2019-06-17,2022-03-10 23:47:21,2022-03-10 23:47:21,2022-03-10 23:47:21,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1804.09170,,/Users/jacquesthibodeau/Zotero/storage/AYAJ6TJU/Oliver et al. - 2019 - Realistic Evaluation of Deep Semi-Supervised Learn.pdf; /Users/jacquesthibodeau/Zotero/storage/H9NQXYF6/1804.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JNEVAD9D,journalArticle,2019,"Kim, Hyunjik; Mnih, Andriy; Schwarz, Jonathan; Garnelo, Marta; Eslami, Ali; Rosenbaum, Dan; Vinyals, Oriol; Teh, Yee Whye",Attentive Neural Processes,"arXiv:1901.05761 [cs, stat]",,,,http://arxiv.org/abs/1901.05761,"Neural Processes (NPs) (Garnelo et al 2018a;b) approach regression by learning to map a context set of observed input-output pairs to a distribution over regression functions. Each function models the distribution of the output given an input, conditioned on the context. NPs have the benefit of fitting observed data efficiently with linear complexity in the number of context input-output pairs, and can learn a wide family of conditional distributions; they learn predictive distributions conditioned on context sets of arbitrary size. Nonetheless, we show that NPs suffer a fundamental drawback of underfitting, giving inaccurate predictions at the inputs of the observed data they condition on. We address this issue by incorporating attention into NPs, allowing each input location to attend to the relevant context points for the prediction. We show that this greatly improves the accuracy of predictions, results in noticeably faster training, and expands the range of functions that can be modelled.",2019-07-09,2022-03-10 23:47:23,2022-03-10 23:47:23,2022-03-10 23:47:23,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1901.05761,,/Users/jacquesthibodeau/Zotero/storage/AU9HZ9EI/Kim et al. - 2019 - Attentive Neural Processes.pdf; /Users/jacquesthibodeau/Zotero/storage/WDY7XIK7/1901.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WW2UAZ2P,journalArticle,2019,"MacKay, Matthew; Vicol, Paul; Lorraine, Jon; Duvenaud, David; Grosse, Roger",Self-Tuning Networks: Bilevel Optimization of Hyperparameters using Structured Best-Response Functions,"arXiv:1903.03088 [cs, stat]",,,,http://arxiv.org/abs/1903.03088,"Hyperparameter optimization can be formulated as a bilevel optimization problem, where the optimal parameters on the training set depend on the hyperparameters. We aim to adapt regularization hyperparameters for neural networks by fitting compact approximations to the best-response function, which maps hyperparameters to optimal weights and biases. We show how to construct scalable best-response approximations for neural networks by modeling the best-response as a single network whose hidden units are gated conditionally on the regularizer. We justify this approximation by showing the exact best-response for a shallow linear network with L2-regularized Jacobian can be represented by a similar gating mechanism. We fit this model using a gradient-based hyperparameter optimization algorithm which alternates between approximating the best-response around the current hyperparameters and optimizing the hyperparameters using the approximate best-response function. Unlike other gradient-based approaches, we do not require differentiating the training loss with respect to the hyperparameters, allowing us to tune discrete hyperparameters, data augmentation hyperparameters, and dropout probabilities. Because the hyperparameters are adapted online, our approach discovers hyperparameter schedules that can outperform fixed hyperparameter values. Empirically, our approach outperforms competing hyperparameter optimization methods on large-scale deep learning problems. We call our networks, which update their own hyperparameters online during training, Self-Tuning Networks (STNs).",2019-03-07,2022-03-10 23:47:25,2022-03-10 23:47:25,2022-03-10 23:47:24,,,,,,,Self-Tuning Networks,,,,,,,,,,,,arXiv.org,,arXiv: 1903.03088,,/Users/jacquesthibodeau/Zotero/storage/95BA94VN/MacKay et al. - 2019 - Self-Tuning Networks Bilevel Optimization of Hype.pdf; /Users/jacquesthibodeau/Zotero/storage/M5QYWIGR/1903.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6EVUL2MF,journalArticle,2018,"Chen, Mia Xu; Firat, Orhan; Bapna, Ankur; Johnson, Melvin; Macherey, Wolfgang; Foster, George; Jones, Llion; Parmar, Niki; Schuster, Mike; Chen, Zhifeng; Wu, Yonghui; Hughes, Macduff",The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation,arXiv:1804.09849 [cs],,,,http://arxiv.org/abs/1804.09849,"The past year has witnessed rapid advances in sequence-to-sequence (seq2seq) modeling for Machine Translation (MT). The classic RNN-based approaches to MT were first out-performed by the convolutional seq2seq model, which was then out-performed by the more recent Transformer model. Each of these new approaches consists of a fundamental architecture accompanied by a set of modeling and training techniques that are in principle applicable to other seq2seq architectures. In this paper, we tease apart the new architectures and their accompanying techniques in two ways. First, we identify several key modeling and training techniques, and apply them to the RNN architecture, yielding a new RNMT+ model that outperforms all of the three fundamental architectures on the benchmark WMT'14 English to French and English to German tasks. Second, we analyze the properties of each fundamental seq2seq architecture and devise new hybrid architectures intended to combine their strengths. Our hybrid models obtain further improvements, outperforming the RNMT+ model on both benchmark datasets.",2018-04-26,2022-03-10 23:47:27,2022-03-10 23:47:27,2022-03-10 23:47:27,,,,,,,The Best of Both Worlds,,,,,,,,,,,,arXiv.org,,arXiv: 1804.09849,,/Users/jacquesthibodeau/Zotero/storage/UKZWXNL4/Chen et al. - 2018 - The Best of Both Worlds Combining Recent Advances.pdf; /Users/jacquesthibodeau/Zotero/storage/9V5GXEL4/1804.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DZRKTHAP,journalArticle,2019,"Schaul, Tom; Borsa, Diana; Modayil, Joseph; Pascanu, Razvan",Ray Interference: a Source of Plateaus in Deep Reinforcement Learning,"arXiv:1904.11455 [cs, stat]",,,,http://arxiv.org/abs/1904.11455,"Rather than proposing a new method, this paper investigates an issue present in existing learning algorithms. We study the learning dynamics of reinforcement learning (RL), specifically a characteristic coupling between learning and data generation that arises because RL agents control their future data distribution. In the presence of function approximation, this coupling can lead to a problematic type of 'ray interference', characterized by learning dynamics that sequentially traverse a number of performance plateaus, effectively constraining the agent to learn one thing at a time even when learning in parallel is better. We establish the conditions under which ray interference occurs, show its relation to saddle points and obtain the exact learning dynamics in a restricted setting. We characterize a number of its properties and discuss possible remedies.",2019-04-25,2022-03-10 23:47:29,2022-03-10 23:47:29,2022-03-10 23:47:29,,,,,,,Ray Interference,,,,,,,,,,,,arXiv.org,,arXiv: 1904.11455,,/Users/jacquesthibodeau/Zotero/storage/ZLSQNWIP/Schaul et al. - 2019 - Ray Interference a Source of Plateaus in Deep Rei.pdf; /Users/jacquesthibodeau/Zotero/storage/UGMWMRU3/1904.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WIYDJD87,journalArticle,2020,"Yang, Zhilin; Dai, Zihang; Yang, Yiming; Carbonell, Jaime; Salakhutdinov, Ruslan; Le, Quoc V.",XLNet: Generalized Autoregressive Pretraining for Language Understanding,arXiv:1906.08237 [cs],,,,http://arxiv.org/abs/1906.08237,"With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.",2020-01-02,2022-03-10 23:47:33,2022-03-10 23:47:33,2022-03-10 23:47:33,,,,,,,XLNet,,,,,,,,,,,,arXiv.org,,arXiv: 1906.08237,,/Users/jacquesthibodeau/Zotero/storage/7QEBR9FP/Yang et al. - 2020 - XLNet Generalized Autoregressive Pretraining for .pdf; /Users/jacquesthibodeau/Zotero/storage/QYND8SGV/1906.html,,,Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Z65LPG6N,journalArticle,2018,"Yu, Yuan; Abadi, Martín; Barham, Paul; Brevdo, Eugene; Burrows, Mike; Davis, Andy; Dean, Jeff; Ghemawat, Sanjay; Harley, Tim; Hawkins, Peter; Isard, Michael; Kudlur, Manjunath; Monga, Rajat; Murray, Derek; Zheng, Xiaoqiang",Dynamic Control Flow in Large-Scale Machine Learning,Proceedings of the Thirteenth EuroSys Conference,,,10.1145/3190508.3190551,http://arxiv.org/abs/1805.01772,"Many recent machine learning models rely on fine-grained dynamic control flow for training and inference. In particular, models based on recurrent neural networks and on reinforcement learning depend on recurrence relations, data-dependent conditional execution, and other features that call for dynamic control flow. These applications benefit from the ability to make rapid control-flow decisions across a set of computing devices in a distributed system. For performance, scalability, and expressiveness, a machine learning system must support dynamic control flow in distributed and heterogeneous environments. This paper presents a programming model for distributed machine learning that supports dynamic control flow. We describe the design of the programming model, and its implementation in TensorFlow, a distributed machine learning system. Our approach extends the use of dataflow graphs to represent machine learning models, offering several distinctive features. First, the branches of conditionals and bodies of loops can be partitioned across many machines to run on a set of heterogeneous devices, including CPUs, GPUs, and custom ASICs. Second, programs written in our model support automatic differentiation and distributed gradient computations, which are necessary for training machine learning models that use control flow. Third, our choice of non-strict semantics enables multiple loop iterations to execute in parallel across machines, and to overlap compute and I/O operations. We have done our work in the context of TensorFlow, and it has been used extensively in research and production. We evaluate it using several real-world applications, and demonstrate its performance and scalability.",2018-04-23,2022-03-10 23:47:37,2022-03-10 23:47:37,2022-03-10 23:47:37,1-15,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1805.01772,,/Users/jacquesthibodeau/Zotero/storage/V66Q5PTK/Yu et al. - 2018 - Dynamic Control Flow in Large-Scale Machine Learni.pdf; /Users/jacquesthibodeau/Zotero/storage/SY4UKQAR/1805.html,,,"Computer Science - Distributed, Parallel, and Cluster Computing; Computer Science - Machine Learning",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TZYB42SZ,journalArticle,2019,"Gaier, Adam; Ha, David",Weight Agnostic Neural Networks,"arXiv:1906.04358 [cs, stat]",,,,http://arxiv.org/abs/1906.04358,"Not all neural network architectures are created equal, some perform much better than others for certain tasks. But how important are the weight parameters of a neural network compared to its architecture? In this work, we question to what extent neural network architectures alone, without learning any weight parameters, can encode solutions for a given task. We propose a search method for neural network architectures that can already perform a task without any explicit weight training. To evaluate these networks, we populate the connections with a single shared weight parameter sampled from a uniform random distribution, and measure the expected performance. We demonstrate that our method can find minimal neural network architectures that can perform several reinforcement learning tasks without weight training. On a supervised learning domain, we find network architectures that achieve much higher than chance accuracy on MNIST using random weights. Interactive version of this paper at https://weightagnostic.github.io/",2019-09-05,2022-03-10 23:47:49,2022-03-10 23:47:49,2022-03-10 23:47:49,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1906.04358,,/Users/jacquesthibodeau/Zotero/storage/2PCFWPKS/Gaier and Ha - 2019 - Weight Agnostic Neural Networks.pdf; /Users/jacquesthibodeau/Zotero/storage/JB8YT5GI/1906.html,,,Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NZIZU342,journalArticle,2019,"Lample, Guillaume; Charton, François",Deep Learning for Symbolic Mathematics,arXiv:1912.01412 [cs],,,,http://arxiv.org/abs/1912.01412,"Neural networks have a reputation for being better at solving statistical or approximate problems than at performing calculations or working with symbolic data. In this paper, we show that they can be surprisingly good at more elaborated tasks in mathematics, such as symbolic integration and solving differential equations. We propose a syntax for representing mathematical problems, and methods for generating large datasets that can be used to train sequence-to-sequence models. We achieve results that outperform commercial Computer Algebra Systems such as Matlab or Mathematica.",2019-12-02,2022-03-10 23:47:52,2022-03-10 23:47:52,2022-03-10 23:47:52,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1912.01412,,/Users/jacquesthibodeau/Zotero/storage/QEPEBQYK/Lample and Charton - 2019 - Deep Learning for Symbolic Mathematics.pdf; /Users/jacquesthibodeau/Zotero/storage/FTWCHZBI/1912.html,,,Computer Science - Machine Learning; Computer Science - Symbolic Computation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SXT53EYL,journalArticle,2019,"Kornblith, Simon; Shlens, Jonathon; Le, Quoc V.",Do Better ImageNet Models Transfer Better?,"arXiv:1805.08974 [cs, stat]",,,,http://arxiv.org/abs/1805.08974,"Transfer learning is a cornerstone of computer vision, yet little work has been done to evaluate the relationship between architecture and transfer. An implicit hypothesis in modern computer vision research is that models that perform better on ImageNet necessarily perform better on other vision tasks. However, this hypothesis has never been systematically tested. Here, we compare the performance of 16 classification networks on 12 image classification datasets. We find that, when networks are used as fixed feature extractors or fine-tuned, there is a strong correlation between ImageNet accuracy and transfer accuracy ($r = 0.99$ and $0.96$, respectively). In the former setting, we find that this relationship is very sensitive to the way in which networks are trained on ImageNet; many common forms of regularization slightly improve ImageNet accuracy but yield penultimate layer features that are much worse for transfer learning. Additionally, we find that, on two small fine-grained image classification datasets, pretraining on ImageNet provides minimal benefits, indicating the learned features from ImageNet do not transfer well to fine-grained tasks. Together, our results show that ImageNet architectures generalize well across datasets, but ImageNet features are less general than previously suggested.",2019-06-17,2022-03-10 23:47:55,2022-03-10 23:47:55,2022-03-10 23:47:55,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1805.08974,,/Users/jacquesthibodeau/Zotero/storage/869AYEJ8/Kornblith et al. - 2019 - Do Better ImageNet Models Transfer Better.pdf; /Users/jacquesthibodeau/Zotero/storage/F28AXZEB/1805.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6WKUQCA4,journalArticle,2020,"Frankle, Jonathan; Dziugaite, Gintare Karolina; Roy, Daniel M.; Carbin, Michael",Linear Mode Connectivity and the Lottery Ticket Hypothesis,"arXiv:1912.05671 [cs, stat]",,,,http://arxiv.org/abs/1912.05671,"We study whether a neural network optimizes to the same, linearly connected minimum under different samples of SGD noise (e.g., random data order and augmentation). We find that standard vision models become stable to SGD noise in this way early in training. From then on, the outcome of optimization is determined to a linearly connected region. We use this technique to study iterative magnitude pruning (IMP), the procedure used by work on the lottery ticket hypothesis to identify subnetworks that could have trained in isolation to full accuracy. We find that these subnetworks only reach full accuracy when they are stable to SGD noise, which either occurs at initialization for small-scale settings (MNIST) or early in training for large-scale settings (ResNet-50 and Inception-v3 on ImageNet).",2020-07-18,2022-03-10 23:47:57,2022-03-10 23:47:57,2022-03-10 23:47:57,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1912.05671,,/Users/jacquesthibodeau/Zotero/storage/6G762FLV/Frankle et al. - 2020 - Linear Mode Connectivity and the Lottery Ticket Hy.pdf; /Users/jacquesthibodeau/Zotero/storage/477CKHZF/1912.html,,,Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9QTXTB7E,journalArticle,2020,"Real, Esteban; Liang, Chen; So, David R.; Le, Quoc V.",AutoML-Zero: Evolving Machine Learning Algorithms From Scratch,"arXiv:2003.03384 [cs, stat]",,,,http://arxiv.org/abs/2003.03384,"Machine learning research has advanced in multiple aspects, including model structures and learning methods. The effort to automate such research, known as AutoML, has also made significant progress. However, this progress has largely focused on the architecture of neural networks, where it has relied on sophisticated expert-designed layers as building blocks---or similarly restrictive search spaces. Our goal is to show that AutoML can go further: it is possible today to automatically discover complete machine learning algorithms just using basic mathematical operations as building blocks. We demonstrate this by introducing a novel framework that significantly reduces human bias through a generic search space. Despite the vastness of this space, evolutionary search can still discover two-layer neural networks trained by backpropagation. These simple neural networks can then be surpassed by evolving directly on tasks of interest, e.g. CIFAR-10 variants, where modern techniques emerge in the top algorithms, such as bilinear interactions, normalized gradients, and weight averaging. Moreover, evolution adapts algorithms to different task types: e.g., dropout-like techniques appear when little data is available. We believe these preliminary successes in discovering machine learning algorithms from scratch indicate a promising new direction for the field.",2020-06-30,2022-03-10 23:48:01,2022-03-10 23:48:01,2022-03-10 23:48:01,,,,,,,AutoML-Zero,,,,,,,,,,,,arXiv.org,,arXiv: 2003.03384,,/Users/jacquesthibodeau/Zotero/storage/6BZVBDCJ/Real et al. - 2020 - AutoML-Zero Evolving Machine Learning Algorithms .pdf; /Users/jacquesthibodeau/Zotero/storage/5SRYQUMU/2003.html,,,Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; I.2.2; I.2.6; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KK2AMHZK,journalArticle,2018,"Srinivas, Aravind; Jabri, Allan; Abbeel, Pieter; Levine, Sergey; Finn, Chelsea",Universal Planning Networks,"arXiv:1804.00645 [cs, stat]",,,,http://arxiv.org/abs/1804.00645,"A key challenge in complex visuomotor control is learning abstract representations that are effective for specifying goals, planning, and generalization. To this end, we introduce universal planning networks (UPN). UPNs embed differentiable planning within a goal-directed policy. This planning computation unrolls a forward model in a latent space and infers an optimal action plan through gradient descent trajectory optimization. The plan-by-gradient-descent process and its underlying representations are learned end-to-end to directly optimize a supervised imitation learning objective. We find that the representations learned are not only effective for goal-directed visual imitation via gradient-based trajectory optimization, but can also provide a metric for specifying goals using images. The learned representations can be leveraged to specify distance-based rewards to reach new target states for model-free reinforcement learning, resulting in substantially more effective learning when solving new tasks described via image-based goals. We were able to achieve successful transfer of visuomotor planning strategies across robots with significantly different morphologies and actuation capabilities.",2018-04-04,2022-03-10 23:48:04,2022-03-10 23:48:04,2022-03-10 23:48:04,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1804.00645,,/Users/jacquesthibodeau/Zotero/storage/MAMYQKQW/Srinivas et al. - 2018 - Universal Planning Networks.pdf; /Users/jacquesthibodeau/Zotero/storage/QJ39MKK6/1804.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EIB68Z5I,journalArticle,2020,"Lepikhin, Dmitry; Lee, HyoukJoong; Xu, Yuanzhong; Chen, Dehao; Firat, Orhan; Huang, Yanping; Krikun, Maxim; Shazeer, Noam; Chen, Zhifeng",GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding,"arXiv:2006.16668 [cs, stat]",,,,http://arxiv.org/abs/2006.16668,"Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.",2020-06-30,2022-03-10 23:48:17,2022-03-10 23:48:17,2022-03-10 23:48:17,,,,,,,GShard,,,,,,,,,,,,arXiv.org,,arXiv: 2006.16668,,/Users/jacquesthibodeau/Zotero/storage/K6M84XP7/Lepikhin et al. - 2020 - GShard Scaling Giant Models with Conditional Comp.pdf; /Users/jacquesthibodeau/Zotero/storage/N77EETHT/2006.html,,,Computer Science - Computation and Language; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XMJFPXQ9,journalArticle,2019,"Chandra, Kartik; Meijer, Erik; Andow, Samantha; Arroyo-Fang, Emilio; Dea, Irene; George, Johann; Grueter, Melissa; Hosmer, Basil; Stumpos, Steffi; Tempest, Alanna; Yang, Shannon",Gradient Descent: The Ultimate Optimizer,"arXiv:1909.13371 [cs, stat]",,,,http://arxiv.org/abs/1909.13371,"Working with any gradient-based machine learning algorithm involves the tedious task of tuning the optimizer's hyperparameters, such as the learning rate. There exist many techniques for automated hyperparameter optimization, but they typically introduce even more hyperparameters to control the hyperparameter optimization process. We propose to instead learn the hyperparameters themselves by gradient descent, and furthermore to learn the hyper-hyperparameters by gradient descent as well, and so on ad infinitum. As these towers of gradient-based optimizers grow, they become significantly less sensitive to the choice of top-level hyperparameters, hence decreasing the burden on the user to search for optimal values.",2019-09-29,2022-03-10 23:48:20,2022-03-10 23:48:20,2022-03-10 23:48:20,,,,,,,Gradient Descent,,,,,,,,,,,,arXiv.org,,arXiv: 1909.13371,,/Users/jacquesthibodeau/Zotero/storage/NQC9CVXB/Chandra et al. - 2019 - Gradient Descent The Ultimate Optimizer.pdf; /Users/jacquesthibodeau/Zotero/storage/GGEKXAY4/1909.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
F56HULHH,journalArticle,2021,"Hutter, Marcus",Learning Curve Theory,"arXiv:2102.04074 [cs, stat]",,,,http://arxiv.org/abs/2102.04074,"Recently a number of empirical ""universal"" scaling law papers have been published, most notably by OpenAI. `Scaling laws' refers to power-law decreases of training or test error w.r.t. more data, larger neural networks, and/or more compute. In this work we focus on scaling w.r.t. data size $n$. Theoretical understanding of this phenomenon is largely lacking, except in finite-dimensional models for which error typically decreases with $n^{-1/2}$ or $n^{-1}$, where $n$ is the sample size. We develop and theoretically analyse the simplest possible (toy) model that can exhibit $n^{-\beta}$ learning curves for arbitrary power $\beta>0$, and determine whether power laws are universal or depend on the data distribution.",2021-02-08,2022-03-10 23:48:27,2022-03-10 23:48:27,2022-03-10 23:48:27,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2102.04074,,/Users/jacquesthibodeau/Zotero/storage/CJHPJ7UB/Hutter - 2021 - Learning Curve Theory.pdf; /Users/jacquesthibodeau/Zotero/storage/Z23SDKW9/2102.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
M6DTSSI7,journalArticle,2019,"Scholten, Jan; Wout, Daan; Celemin, Carlos; Kober, Jens",Deep Reinforcement Learning with Feedback-based Exploration,2019 IEEE 58th Conference on Decision and Control (CDC),,,10.1109/CDC40024.2019.9029503,http://arxiv.org/abs/1903.06151,"Deep Reinforcement Learning has enabled the control of increasingly complex and high-dimensional problems. However, the need of vast amounts of data before reasonable performance is attained prevents its widespread application. We employ binary corrective feedback as a general and intuitive manner to incorporate human intuition and domain knowledge in model-free machine learning. The uncertainty in the policy and the corrective feedback is combined directly in the action space as probabilistic conditional exploration. As a result, the greatest part of the otherwise ignorant learning process can be avoided. We demonstrate the proposed method, Predictive Probabilistic Merging of Policies (PPMP), in combination with DDPG. In experiments on continuous control problems of the OpenAI Gym, we achieve drastic improvements in sample efficiency, final performance, and robustness to erroneous feedback, both for human and synthetic feedback. Additionally, we show solutions beyond the demonstrated knowledge.",2019-12,2022-03-10 23:51:33,2022-03-10 23:51:33,2022-03-10 23:51:33,803-808,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1903.06151,,/Users/jacquesthibodeau/Zotero/storage/HTQJ9X8G/Scholten et al. - 2019 - Deep Reinforcement Learning with Feedback-based Ex.pdf; /Users/jacquesthibodeau/Zotero/storage/KC59WTZV/1903.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RDRVYL6S,journalArticle,2019,"Jiang, Jiechuan; Lu, Zongqing",Generative Exploration and Exploitation,"arXiv:1904.09605 [cs, stat]",,,,http://arxiv.org/abs/1904.09605,"Sparse reward is one of the biggest challenges in reinforcement learning (RL). In this paper, we propose a novel method called Generative Exploration and Exploitation (GENE) to overcome sparse reward. GENE automatically generates start states to encourage the agent to explore the environment and to exploit received reward signals. GENE can adaptively tradeoff between exploration and exploitation according to the varying distributions of states experienced by the agent as the learning progresses. GENE relies on no prior knowledge about the environment and can be combined with any RL algorithm, no matter on-policy or off-policy, single-agent or multi-agent. Empirically, we demonstrate that GENE significantly outperforms existing methods in three tasks with only binary rewards, including Maze, Maze Ant, and Cooperative Navigation. Ablation studies verify the emergence of progressive exploration and automatic reversing.",2019-11-20,2022-03-10 23:51:35,2022-03-10 23:51:35,2022-03-10 23:51:35,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1904.09605,,/Users/jacquesthibodeau/Zotero/storage/KYH6PZF2/Jiang and Lu - 2019 - Generative Exploration and Exploitation.pdf; /Users/jacquesthibodeau/Zotero/storage/VG7S2VDU/1904.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AQMJ7REU,journalArticle,2019,"Janz, David; Hron, Jiri; Mazur, Przemysław; Hofmann, Katja; Hernández-Lobato, José Miguel; Tschiatschek, Sebastian",Successor Uncertainties: Exploration and Uncertainty in Temporal Difference Learning,"arXiv:1810.06530 [cs, stat]",,,,http://arxiv.org/abs/1810.06530,"Posterior sampling for reinforcement learning (PSRL) is an effective method for balancing exploration and exploitation in reinforcement learning. Randomised value functions (RVF) can be viewed as a promising approach to scaling PSRL. However, we show that most contemporary algorithms combining RVF with neural network function approximation do not possess the properties which make PSRL effective, and provably fail in sparse reward problems. Moreover, we find that propagation of uncertainty, a property of PSRL previously thought important for exploration, does not preclude this failure. We use these insights to design Successor Uncertainties (SU), a cheap and easy to implement RVF algorithm that retains key properties of PSRL. SU is highly effective on hard tabular exploration benchmarks. Furthermore, on the Atari 2600 domain, it surpasses human performance on 38 of 49 games tested (achieving a median human normalised score of 2.09), and outperforms its closest RVF competitor, Bootstrapped DQN, on 36 of those.",2019-12-03,2022-03-10 23:51:37,2022-03-10 23:51:37,2022-03-10 23:51:37,,,,,,,Successor Uncertainties,,,,,,,,,,,,arXiv.org,,arXiv: 1810.06530,,/Users/jacquesthibodeau/Zotero/storage/UGE2B9CS/Janz et al. - 2019 - Successor Uncertainties Exploration and Uncertain.pdf; /Users/jacquesthibodeau/Zotero/storage/I7F47DX3/1810.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QJJEGWTA,journalArticle,2020,"Tavakoli, Arash; Levdik, Vitaly; Islam, Riashat; Smith, Christopher M.; Kormushev, Petar",Exploring Restart Distributions,"arXiv:1811.11298 [cs, stat]",,,,http://arxiv.org/abs/1811.11298,"We consider the generic approach of using an experience memory to help exploration by adapting a restart distribution. That is, given the capacity to reset the state with those corresponding to the agent's past observations, we help exploration by promoting faster state-space coverage via restarting the agent from a more diverse set of initial states, as well as allowing it to restart in states associated with significant past experiences. This approach is compatible with both on-policy and off-policy methods. However, a caveat is that altering the distribution of initial states could change the optimal policies when searching within a restricted class of policies. To reduce this unsought learning bias, we evaluate our approach in deep reinforcement learning which benefits from the high representational capacity of deep neural networks. We instantiate three variants of our approach, each inspired by an idea in the context of experience replay. Using these variants, we show that performance gains can be achieved, especially in hard exploration problems.",2020-08-17,2022-03-10 23:51:38,2022-03-10 23:51:38,2022-03-10 23:51:38,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1811.11298,,/Users/jacquesthibodeau/Zotero/storage/XEB9QTDB/Tavakoli et al. - 2020 - Exploring Restart Distributions.pdf; /Users/jacquesthibodeau/Zotero/storage/56E8FMI9/1811.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KK2NPY8B,journalArticle,2019,"Azar, Mohammad Gheshlaghi; Piot, Bilal; Pires, Bernardo Avila; Grill, Jean-Bastien; Altché, Florent; Munos, Rémi",World Discovery Models,"arXiv:1902.07685 [cs, stat]",,,,http://arxiv.org/abs/1902.07685,"As humans we are driven by a strong desire for seeking novelty in our world. Also upon observing a novel pattern we are capable of refining our understanding of the world based on the new information---humans can discover their world. The outstanding ability of the human mind for discovery has led to many breakthroughs in science, art and technology. Here we investigate the possibility of building an agent capable of discovering its world using the modern AI technology. In particular we introduce NDIGO, Neural Differential Information Gain Optimisation, a self-supervised discovery model that aims at seeking new information to construct a global view of its world from partial and noisy observations. Our experiments on some controlled 2-D navigation tasks show that NDIGO outperforms state-of-the-art information-seeking methods in terms of the quality of the learned representation. The improvement in performance is particularly significant in the presence of white or structured noise where other information-seeking methods follow the noise instead of discovering their world.",2019-03-01,2022-03-10 23:51:40,2022-03-10 23:51:40,2022-03-10 23:51:40,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1902.07685,,/Users/jacquesthibodeau/Zotero/storage/7ZFXMJ5D/Azar et al. - 2019 - World Discovery Models.pdf; /Users/jacquesthibodeau/Zotero/storage/UYURCN33/1902.html,,,Computer Science - Artificial Intelligence; Statistics - Applications; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZRDNG74J,journalArticle,2019,"Chen, Tao; Gupta, Saurabh; Gupta, Abhinav",Learning Exploration Policies for Navigation,arXiv:1903.01959 [cs],,,,http://arxiv.org/abs/1903.01959,"Numerous past works have tackled the problem of task-driven navigation. But, how to effectively explore a new environment to enable a variety of down-stream tasks has received much less attention. In this work, we study how agents can autonomously explore realistic and complex 3D environments without the context of task-rewards. We propose a learning-based approach and investigate different policy architectures, reward functions, and training paradigms. We find that the use of policies with spatial memory that are bootstrapped with imitation learning and finally finetuned with coverage rewards derived purely from on-board sensors can be effective at exploring novel environments. We show that our learned exploration policies can explore better than classical approaches based on geometry alone and generic learning-based exploration techniques. Finally, we also show how such task-agnostic exploration can be used for down-stream tasks. Code and Videos are available at: https://sites.google.com/view/exploration-for-nav.",2019-03-05,2022-03-10 23:51:43,2022-03-10 23:51:43,2022-03-10 23:51:42,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1903.01959,,/Users/jacquesthibodeau/Zotero/storage/AJIMVJB9/Chen et al. - 2019 - Learning Exploration Policies for Navigation.pdf; /Users/jacquesthibodeau/Zotero/storage/M9XNH4QG/1903.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZUWLZG56,journalArticle,2020,"Sekar, Ramanan; Rybkin, Oleh; Daniilidis, Kostas; Abbeel, Pieter; Hafner, Danijar; Pathak, Deepak",Planning to Explore via Self-Supervised World Models,"arXiv:2005.05960 [cs, stat]",,,,http://arxiv.org/abs/2005.05960,"Reinforcement learning allows solving complex tasks, however, the learning tends to be task-specific and the sample efficiency remains a challenge. We present Plan2Explore, a self-supervised reinforcement learning agent that tackles both these challenges through a new approach to self-supervised exploration and fast adaptation to new tasks, which need not be known during exploration. During exploration, unlike prior methods which retrospectively compute the novelty of observations after the agent has already reached them, our agent acts efficiently by leveraging planning to seek out expected future novelty. After exploration, the agent quickly adapts to multiple downstream tasks in a zero or a few-shot manner. We evaluate on challenging control tasks from high-dimensional image inputs. Without any training supervision or task-specific interaction, Plan2Explore outperforms prior self-supervised exploration methods, and in fact, almost matches the performances oracle which has access to rewards. Videos and code at https://ramanans1.github.io/plan2explore/",2020-06-30,2022-03-10 23:51:45,2022-03-10 23:51:45,2022-03-10 23:51:44,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2005.05960,,/Users/jacquesthibodeau/Zotero/storage/4RSYKVMM/Sekar et al. - 2020 - Planning to Explore via Self-Supervised World Mode.pdf; /Users/jacquesthibodeau/Zotero/storage/DD2UQI5H/2005.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
E556NSLA,journalArticle,2021,"Matusch, Brendon; Ba, Jimmy; Hafner, Danijar",Evaluating Agents without Rewards,arXiv:2012.11538 [cs],,,,http://arxiv.org/abs/2012.11538,"Reinforcement learning has enabled agents to solve challenging tasks in unknown environments. However, manually crafting reward functions can be time consuming, expensive, and error prone to human error. Competing objectives have been proposed for agents to learn without external supervision, but it has been unclear how well they reflect task rewards or human behavior. To accelerate the development of intrinsic objectives, we retrospectively compute potential objectives on pre-collected datasets of agent behavior, rather than optimizing them online, and compare them by analyzing their correlations. We study input entropy, information gain, and empowerment across seven agents, three Atari games, and the 3D game Minecraft. We find that all three intrinsic objectives correlate more strongly with a human behavior similarity metric than with task reward. Moreover, input entropy and information gain correlate more strongly with human similarity than task reward does, suggesting the use of intrinsic objectives for designing agents that behave similarly to human players.",2021-02-09,2022-03-10 23:51:46,2022-03-10 23:51:46,2022-03-10 23:51:46,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2012.11538,,/Users/jacquesthibodeau/Zotero/storage/TBB5F2HB/Matusch et al. - 2021 - Evaluating Agents without Rewards.pdf; /Users/jacquesthibodeau/Zotero/storage/N243R9R3/2012.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3YDU6KVX,journalArticle,2019,"Lee, Gyeong Taek; Kim, Chang Ouk",Amplifying the Imitation Effect for Reinforcement Learning of UCAV's Mission Execution,arXiv:1901.05856 [cs],,,,http://arxiv.org/abs/1901.05856,"This paper proposes a new reinforcement learning (RL) algorithm that enhances exploration by amplifying the imitation effect (AIE). This algorithm consists of self-imitation learning and random network distillation algorithms. We argue that these two algorithms complement each other and that combining these two algorithms can amplify the imitation effect for exploration. In addition, by adding an intrinsic penalty reward to the state that the RL agent frequently visits and using replay memory for learning the feature state when using an exploration bonus, the proposed approach leads to deep exploration and deviates from the current converged policy. We verified the exploration performance of the algorithm through experiments in a two-dimensional grid environment. In addition, we applied the algorithm to a simulated environment of unmanned combat aerial vehicle (UCAV) mission execution, and the empirical results show that AIE is very effective for finding the UCAV's shortest flight path to avoid an enemy's missiles.",2019-01-17,2022-03-10 23:51:52,2022-03-10 23:51:52,2022-03-10 23:51:51,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1901.05856,,/Users/jacquesthibodeau/Zotero/storage/6MB2HMTR/Lee and Kim - 2019 - Amplifying the Imitation Effect for Reinforcement .pdf; /Users/jacquesthibodeau/Zotero/storage/E5H6PFTC/1901.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5X6VEY8M,journalArticle,2019,"Paine, Tom Le; Gulcehre, Caglar; Shahriari, Bobak; Denil, Misha; Hoffman, Matt; Soyer, Hubert; Tanburn, Richard; Kapturowski, Steven; Rabinowitz, Neil; Williams, Duncan; Barth-Maron, Gabriel; Wang, Ziyu; de Freitas, Nando; Team, Worlds",Making Efficient Use of Demonstrations to Solve Hard Exploration Problems,arXiv:1909.01387 [cs],,,,http://arxiv.org/abs/1909.01387,"This paper introduces R2D3, an agent that makes efficient use of demonstrations to solve hard exploration problems in partially observable environments with highly variable initial conditions. We also introduce a suite of eight tasks that combine these three properties, and show that R2D3 can solve several of the tasks where other state of the art methods (both with and without demonstrations) fail to see even a single successful trajectory after tens of billions of steps of exploration.",2019-09-03,2022-03-10 23:52:08,2022-03-10 23:52:08,2022-03-10 23:52:08,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1909.01387,,/Users/jacquesthibodeau/Zotero/storage/HBX4J5VH/Paine et al. - 2019 - Making Efficient Use of Demonstrations to Solve Ha.pdf; /Users/jacquesthibodeau/Zotero/storage/RM9BJJL9/1909.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7M4CQKS4,journalArticle,2019,"Jakubovitz, Daniel; Giryes, Raja; Rodrigues, Miguel R. D.",Generalization Error in Deep Learning,"arXiv:1808.01174 [cs, stat]",,,,http://arxiv.org/abs/1808.01174,"Deep learning models have lately shown great performance in various fields such as computer vision, speech recognition, speech translation, and natural language processing. However, alongside their state-of-the-art performance, it is still generally unclear what is the source of their generalization ability. Thus, an important question is what makes deep neural networks able to generalize well from the training set to new data. In this article, we provide an overview of the existing theory and bounds for the characterization of the generalization error of deep neural networks, combining both classical and more recent theoretical and empirical results.",2019-04-06,2022-03-10 23:52:10,2022-03-10 23:52:10,2022-03-10 23:52:10,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1808.01174,,/Users/jacquesthibodeau/Zotero/storage/X3GG28IB/Jakubovitz et al. - 2019 - Generalization Error in Deep Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/MI6R4EY9/1808.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JSS55QMN,journalArticle,2019,"Metz, Luke; Maheswaranathan, Niru; Cheung, Brian; Sohl-Dickstein, Jascha",Meta-Learning Update Rules for Unsupervised Representation Learning,"arXiv:1804.00222 [cs, stat]",,,,http://arxiv.org/abs/1804.00222,"A major goal of unsupervised learning is to discover data representations that are useful for subsequent tasks, without access to supervised labels during training. Typically, this involves minimizing a surrogate objective, such as the negative log likelihood of a generative model, with the hope that representations useful for subsequent tasks will arise as a side effect. In this work, we propose instead to directly target later desired tasks by meta-learning an unsupervised learning rule which leads to representations useful for those tasks. Specifically, we target semi-supervised classification performance, and we meta-learn an algorithm -- an unsupervised weight update rule -- that produces representations useful for this task. Additionally, we constrain our unsupervised update rule to a be a biologically-motivated, neuron-local function, which enables it to generalize to different neural network architectures, datasets, and data modalities. We show that the meta-learned update rule produces useful features and sometimes outperforms existing unsupervised learning techniques. We further show that the meta-learned unsupervised update rule generalizes to train networks with different widths, depths, and nonlinearities. It also generalizes to train on data with randomly permuted input dimensions and even generalizes from image datasets to a text task.",2019-02-26,2022-03-10 23:52:12,2022-03-10 23:52:12,2022-03-10 23:52:12,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1804.00222,,/Users/jacquesthibodeau/Zotero/storage/AK87KNNA/Metz et al. - 2019 - Meta-Learning Update Rules for Unsupervised Repres.pdf; /Users/jacquesthibodeau/Zotero/storage/4ADBD27W/1804.html,,,Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
L9PLTSVS,journalArticle,2018,"LaLonde, Rodney; Bagci, Ulas",Capsules for Object Segmentation,"arXiv:1804.04241 [cs, stat]",,,,http://arxiv.org/abs/1804.04241,"Convolutional neural networks (CNNs) have shown remarkable results over the last several years for a wide range of computer vision tasks. A new architecture recently introduced by Sabour et al., referred to as a capsule networks with dynamic routing, has shown great initial results for digit recognition and small image classification. The success of capsule networks lies in their ability to preserve more information about the input by replacing max-pooling layers with convolutional strides and dynamic routing, allowing for preservation of part-whole relationships in the data. This preservation of the input is demonstrated by reconstructing the input from the output capsule vectors. Our work expands the use of capsule networks to the task of object segmentation for the first time in the literature. We extend the idea of convolutional capsules with locally-connected routing and propose the concept of deconvolutional capsules. Further, we extend the masked reconstruction to reconstruct the positive input class. The proposed convolutional-deconvolutional capsule network, called SegCaps, shows strong results for the task of object segmentation with substantial decrease in parameter space. As an example application, we applied the proposed SegCaps to segment pathological lungs from low dose CT scans and compared its accuracy and efficiency with other U-Net-based architectures. SegCaps is able to handle large image sizes (512 x 512) as opposed to baseline capsules (typically less than 32 x 32). The proposed SegCaps reduced the number of parameters of U-Net architecture by 95.4% while still providing a better segmentation accuracy.",2018-04-11,2022-03-10 23:52:17,2022-03-10 23:52:17,2022-03-10 23:52:17,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1804.04241,,/Users/jacquesthibodeau/Zotero/storage/95RBHWXE/LaLonde and Bagci - 2018 - Capsules for Object Segmentation.pdf; /Users/jacquesthibodeau/Zotero/storage/34VPVJPY/1804.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EBHIZI8K,journalArticle,2020,"Frankle, Jonathan; Dziugaite, Gintare Karolina; Roy, Daniel M.; Carbin, Michael",Stabilizing the Lottery Ticket Hypothesis,"arXiv:1903.01611 [cs, stat]",,,,http://arxiv.org/abs/1903.01611,"Pruning is a well-established technique for removing unnecessary structure from neural networks after training to improve the performance of inference. Several recent results have explored the possibility of pruning at initialization time to provide similar benefits during training. In particular, the ""lottery ticket hypothesis"" conjectures that typical neural networks contain small subnetworks that can train to similar accuracy in a commensurate number of steps. The evidence for this claim is that a procedure based on iterative magnitude pruning (IMP) reliably finds such subnetworks retroactively on small vision tasks. However, IMP fails on deeper networks, and proposed methods to prune before training or train pruned networks encounter similar scaling limitations. In this paper, we argue that these efforts have struggled on deeper networks because they have focused on pruning precisely at initialization. We modify IMP to search for subnetworks that could have been obtained by pruning early in training (0.1% to 7% through) rather than at iteration 0. With this change, it finds small subnetworks of deeper networks (e.g., 80% sparsity on Resnet-50) that can complete the training process to match the accuracy of the original network on more challenging tasks (e.g., ImageNet). In situations where IMP fails at iteration 0, the accuracy benefits of delaying pruning accrue rapidly over the earliest iterations of training. To explain these behaviors, we study subnetwork ""stability,"" finding that - as accuracy improves in this fashion - IMP subnetworks train to parameters closer to those of the full network and do so with improved consistency in the face of gradient noise. These results offer new insights into the opportunity to prune large-scale networks early in training and the behaviors underlying the lottery ticket hypothesis",2020-07-20,2022-03-10 23:52:21,2022-03-10 23:52:21,2022-03-10 23:52:21,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1903.01611,,/Users/jacquesthibodeau/Zotero/storage/4JD2525V/Frankle et al. - 2020 - Stabilizing the Lottery Ticket Hypothesis.pdf; /Users/jacquesthibodeau/Zotero/storage/4PMYLKYB/1903.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
J2Z66B3U,journalArticle,2018,"Chang, Oscar; Lipson, Hod",Neural Network Quine,arXiv:1803.05859 [cs],,,,http://arxiv.org/abs/1803.05859,"Self-replication is a key aspect of biological life that has been largely overlooked in Artificial Intelligence systems. Here we describe how to build and train self-replicating neural networks. The network replicates itself by learning to output its own weights. The network is designed using a loss function that can be optimized with either gradient-based or non-gradient-based methods. We also describe a method we call regeneration to train the network without explicit optimization, by injecting the network with predictions of its own parameters. The best solution for a self-replicating network was found by alternating between regeneration and optimization steps. Finally, we describe a design for a self-replicating neural network that can solve an auxiliary task such as MNIST image classification. We observe that there is a trade-off between the network's ability to classify images and its ability to replicate, but training is biased towards increasing its specialization at image classification at the expense of replication. This is analogous to the trade-off between reproduction and other tasks observed in nature. We suggest that a self-replication mechanism for artificial intelligence is useful because it introduces the possibility of continual improvement through natural selection.",2018-05-24,2022-03-10 23:52:28,2022-03-10 23:52:28,2022-03-10 23:52:28,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1803.05859,,/Users/jacquesthibodeau/Zotero/storage/67DZNFIM/Chang and Lipson - 2018 - Neural Network Quine.pdf; /Users/jacquesthibodeau/Zotero/storage/U3TBS7IJ/1803.html,,,Computer Science - Artificial Intelligence; Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XVBHJYKY,journalArticle,2019,"Jing, Longlong; Tian, Yingli",Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey,arXiv:1902.06162 [cs],,,,http://arxiv.org/abs/1902.06162,"Large-scale labeled data are generally required to train deep neural networks in order to obtain better performance in visual feature learning from images or videos for computer vision applications. To avoid extensive cost of collecting and annotating large-scale datasets, as a subset of unsupervised learning methods, self-supervised learning methods are proposed to learn general image and video features from large-scale unlabeled data without using any human-annotated labels. This paper provides an extensive review of deep learning-based self-supervised general visual feature learning methods from images or videos. First, the motivation, general pipeline, and terminologies of this field are described. Then the common deep neural network architectures that used for self-supervised learning are summarized. Next, the main components and evaluation metrics of self-supervised learning methods are reviewed followed by the commonly used image and video datasets and the existing self-supervised visual feature learning methods. Finally, quantitative performance comparisons of the reviewed methods on benchmark datasets are summarized and discussed for both image and video feature learning. At last, this paper is concluded and lists a set of promising future directions for self-supervised visual feature learning.",2019-02-16,2022-03-10 23:52:31,2022-03-10 23:52:31,2022-03-10 23:52:31,,,,,,,Self-supervised Visual Feature Learning with Deep Neural Networks,,,,,,,,,,,,arXiv.org,,arXiv: 1902.06162,,/Users/jacquesthibodeau/Zotero/storage/GHWTQLCP/Jing and Tian - 2019 - Self-supervised Visual Feature Learning with Deep .pdf; /Users/jacquesthibodeau/Zotero/storage/KF96U6ZJ/1902.html,,,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XEWFWNQQ,journalArticle,2019,"Belkin, Mikhail; Hsu, Daniel; Ma, Siyuan; Mandal, Soumik",Reconciling modern machine learning practice and the bias-variance trade-off,"arXiv:1812.11118 [cs, stat]",,,,http://arxiv.org/abs/1812.11118,"Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias-variance trade-off, appears to be at odds with the observed behavior of methods used in the modern machine learning practice. The bias-variance trade-off implies that a model should balance under-fitting and over-fitting: rich enough to express underlying structure in data, simple enough to avoid fitting spurious patterns. However, in the modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered over-fit, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This ""double descent"" curve subsumes the textbook U-shaped bias-variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine learning models delineates the limits of classical analyses, and has implications for both the theory and practice of machine learning.",2019-09-10,2022-03-10 23:52:34,2022-03-10 23:52:34,2022-03-10 23:52:34,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1812.11118,,/Users/jacquesthibodeau/Zotero/storage/DHGHKYL6/Belkin et al. - 2019 - Reconciling modern machine learning practice and t.pdf; /Users/jacquesthibodeau/Zotero/storage/S6XT726D/1812.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
M489WRD8,journalArticle,2020,"Marcus, Gary",The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence,arXiv:2002.06177 [cs],,,,http://arxiv.org/abs/2002.06177,"Recent research in artificial intelligence and machine learning has largely emphasized general-purpose learning and ever-larger training sets and more and more compute. In contrast, I propose a hybrid, knowledge-driven, reasoning-based approach, centered around cognitive models, that could provide the substrate for a richer, more robust AI than is currently possible.",2020-02-19,2022-03-10 23:52:38,2022-03-10 23:52:38,2022-03-10 23:52:38,,,,,,,The Next Decade in AI,,,,,,,,,,,,arXiv.org,,arXiv: 2002.06177,,/Users/jacquesthibodeau/Zotero/storage/GPY8PUHP/Marcus - 2020 - The Next Decade in AI Four Steps Towards Robust A.pdf; /Users/jacquesthibodeau/Zotero/storage/YLZBI5SL/2002.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; I.2; I.2.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3GFTLZ3V,journalArticle,2019,"Yadav, Chhavi; Bottou, Léon",Cold Case: The Lost MNIST Digits,"arXiv:1905.10498 [cs, stat]",,,,http://arxiv.org/abs/1905.10498,"Although the popular MNIST dataset [LeCun et al., 1994] is derived from the NIST database [Grother and Hanaoka, 1995], the precise processing steps for this derivation have been lost to time. We propose a reconstruction that is accurate enough to serve as a replacement for the MNIST dataset, with insignificant changes in accuracy. We trace each MNIST digit to its NIST source and its rich metadata such as writer identifier, partition identifier, etc. We also reconstruct the complete MNIST test set with 60,000 samples instead of the usual 10,000. Since the balance 50,000 were never distributed, they enable us to investigate the impact of twenty-five years of MNIST experiments on the reported testing performances. Our results unambiguously confirm the trends observed by Recht et al. [2018, 2019]: although the misclassification rates are slightly off, classifier ordering and model selection remain broadly reliable. We attribute this phenomenon to the pairing benefits of comparing classifiers on the same digits.",2019-11-04,2022-03-10 23:52:41,2022-03-10 23:52:41,2022-03-10 23:52:41,,,,,,,Cold Case,,,,,,,,,,,,arXiv.org,,arXiv: 1905.10498,,/Users/jacquesthibodeau/Zotero/storage/SZUKZJAU/Yadav and Bottou - 2019 - Cold Case The Lost MNIST Digits.pdf; /Users/jacquesthibodeau/Zotero/storage/KPHPDRB2/1905.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NG5KNHI5,journalArticle,2020,"Shuster, Kurt; Urbanek, Jack; Dinan, Emily; Szlam, Arthur; Weston, Jason",Deploying Lifelong Open-Domain Dialogue Learning,arXiv:2008.08076 [cs],,,,http://arxiv.org/abs/2008.08076,"Much of NLP research has focused on crowdsourced static datasets and the supervised learning paradigm of training once and then evaluating test performance. As argued in de Vries et al. (2020), crowdsourced data has the issues of lack of naturalness and relevance to real-world use cases, while the static dataset paradigm does not allow for a model to learn from its experiences of using language (Silver et al., 2013). In contrast, one might hope for machine learning systems that become more useful as they interact with people. In this work, we build and deploy a role-playing game, whereby human players converse with learning agents situated in an open-domain fantasy world. We show that by training models on the conversations they have with humans in the game the models progressively improve, as measured by automatic metrics and online engagement scores. This learning is shown to be more efficient than crowdsourced data when applied to conversations with real users, as well as being far cheaper to collect.",2020-08-19,2022-03-10 23:52:45,2022-03-10 23:52:45,2022-03-10 23:52:45,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2008.08076,,/Users/jacquesthibodeau/Zotero/storage/AJBDTBVQ/Shuster et al. - 2020 - Deploying Lifelong Open-Domain Dialogue Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/5X7FQZQ7/2008.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2FB6ZUEK,journalArticle,2021,"Dosovitskiy, Alexey; Beyer, Lucas; Kolesnikov, Alexander; Weissenborn, Dirk; Zhai, Xiaohua; Unterthiner, Thomas; Dehghani, Mostafa; Minderer, Matthias; Heigold, Georg; Gelly, Sylvain; Uszkoreit, Jakob; Houlsby, Neil",An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,arXiv:2010.11929 [cs],,,,http://arxiv.org/abs/2010.11929,"While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",2021-06-03,2022-03-10 23:53:02,2022-03-10 23:53:02,2022-03-10 23:53:02,,,,,,,An Image is Worth 16x16 Words,,,,,,,,,,,,arXiv.org,,arXiv: 2010.11929,,/Users/jacquesthibodeau/Zotero/storage/ZZZU74PJ/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf; /Users/jacquesthibodeau/Zotero/storage/5LTEPGQJ/2010.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YGHJ37U4,journalArticle,2018,"Zhou, Yanqi; Ebrahimi, Siavash; Arık, Sercan Ö; Yu, Haonan; Liu, Hairong; Diamos, Greg",Resource-Efficient Neural Architect,arXiv:1806.07912 [cs],,,,http://arxiv.org/abs/1806.07912,"Neural Architecture Search (NAS) is a laborious process. Prior work on automated NAS targets mainly on improving accuracy, but lacks consideration of computational resource use. We propose the Resource-Efficient Neural Architect (RENA), an efficient resource-constrained NAS using reinforcement learning with network embedding. RENA uses a policy network to process the network embeddings to generate new configurations. We demonstrate RENA on image recognition and keyword spotting (KWS) problems. RENA can find novel architectures that achieve high performance even with tight resource constraints. For CIFAR10, it achieves 2.95% test error when compute intensity is greater than 100 FLOPs/byte, and 3.87% test error when model size is less than 3M parameters. For Google Speech Commands Dataset, RENA achieves the state-of-the-art accuracy without resource constraints, and it outperforms the optimized architectures with tight resource constraints.",2018-06-12,2022-03-10 23:53:06,2022-03-10 23:53:06,2022-03-10 23:53:06,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1806.07912,,/Users/jacquesthibodeau/Zotero/storage/EF6F9NH4/Zhou et al. - 2018 - Resource-Efficient Neural Architect.pdf; /Users/jacquesthibodeau/Zotero/storage/W76VHVT6/1806.html,,,Computer Science - Artificial Intelligence; Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SIZ8EK8A,journalArticle,2019,"Liu, Hanxiao; Simonyan, Karen; Yang, Yiming",DARTS: Differentiable Architecture Search,"arXiv:1806.09055 [cs, stat]",,,,http://arxiv.org/abs/1806.09055,"This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.",2019-04-23,2022-03-10 23:53:08,2022-03-10 23:53:08,2022-03-10 23:53:07,,,,,,,DARTS,,,,,,,,,,,,arXiv.org,,arXiv: 1806.09055,,/Users/jacquesthibodeau/Zotero/storage/QDG9WX2J/Liu et al. - 2019 - DARTS Differentiable Architecture Search.pdf; /Users/jacquesthibodeau/Zotero/storage/SMUJLUJJ/1806.html,,,Computer Science - Computation and Language; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZJ9FITUL,journalArticle,2021,"Lester, Brian; Al-Rfou, Rami; Constant, Noah",The Power of Scale for Parameter-Efficient Prompt Tuning,arXiv:2104.08691 [cs],,,,http://arxiv.org/abs/2104.08691,"In this work, we explore ""prompt tuning"", a simple yet effective mechanism for learning ""soft prompts"" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signal from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's ""few-shot"" learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method ""closes the gap"" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant in that large models are costly to share and serve, and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed ""prefix tuning"" of Li and Liang (2021), and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer, as compared to full model tuning.",2021-09-02,2022-03-10 23:53:14,2022-03-10 23:53:14,2022-03-10 23:53:14,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2104.08691,,/Users/jacquesthibodeau/Zotero/storage/AL6RX66Q/Lester et al. - 2021 - The Power of Scale for Parameter-Efficient Prompt .pdf; /Users/jacquesthibodeau/Zotero/storage/DUG8DW4S/2104.html,,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XAFA4A9U,journalArticle,2019,"Dehghani, Mostafa; Gouws, Stephan; Vinyals, Oriol; Uszkoreit, Jakob; Kaiser, Łukasz",Universal Transformers,"arXiv:1807.03819 [cs, stat]",,,,http://arxiv.org/abs/1807.03819,"Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks. However, their inherently sequential computation makes them slow to train. Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times. Despite these successes, however, popular feed-forward sequence models like the Transformer fail to generalize in many simple tasks that recurrent models handle with ease, e.g. copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time. We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues. UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs. We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks. In contrast to the standard Transformer, under certain assumptions, UTs can be shown to be Turing-complete. Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset.",2019-03-05,2022-03-10 23:53:17,2022-03-10 23:53:17,2022-03-10 23:53:17,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1807.03819,,/Users/jacquesthibodeau/Zotero/storage/E8IN3VJC/Dehghani et al. - 2019 - Universal Transformers.pdf; /Users/jacquesthibodeau/Zotero/storage/UV9M5L4W/1807.html,,,Computer Science - Computation and Language; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4EE938X2,journalArticle,2021,"Ortega, Pedro A.; Kunesch, Markus; Delétang, Grégoire; Genewein, Tim; Grau-Moya, Jordi; Veness, Joel; Buchli, Jonas; Degrave, Jonas; Piot, Bilal; Perolat, Julien; Everitt, Tom; Tallec, Corentin; Parisotto, Emilio; Erez, Tom; Chen, Yutian; Reed, Scott; Hutter, Marcus; de Freitas, Nando; Legg, Shane",Shaking the foundations: delusions in sequence models for interaction and control,arXiv:2110.10819 [cs],,,,http://arxiv.org/abs/2110.10819,"The recent phenomenal success of language models has reinvigorated machine learning research, and large sequence models such as transformers are being applied to a variety of domains. One important problem class that has remained relatively elusive however is purposeful adaptive behavior. Currently there is a common perception that sequence models ""lack the understanding of the cause and effect of their actions"" leading them to draw incorrect inferences due to auto-suggestive delusions. In this report we explain where this mismatch originates, and show that it can be resolved by treating actions as causal interventions. Finally, we show that in supervised learning, one can teach a system to condition or intervene on data by training with factual and counterfactual error signals respectively.",2021-10-20,2022-03-10 23:53:20,2022-03-10 23:53:20,2022-03-10 23:53:20,,,,,,,Shaking the foundations,,,,,,,,,,,,arXiv.org,,arXiv: 2110.10819,,/Users/jacquesthibodeau/Zotero/storage/KXGQE6LU/Ortega et al. - 2021 - Shaking the foundations delusions in sequence mod.pdf; /Users/jacquesthibodeau/Zotero/storage/83LBVJIS/2110.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CGLBC3YV,journalArticle,2018,"Odena, Augustus; Goodfellow, Ian",TensorFuzz: Debugging Neural Networks with Coverage-Guided Fuzzing,"arXiv:1807.10875 [cs, stat]",,,,http://arxiv.org/abs/1807.10875,"Machine learning models are notoriously difficult to interpret and debug. This is particularly true of neural networks. In this work, we introduce automated software testing techniques for neural networks that are well-suited to discovering errors which occur only for rare inputs. Specifically, we develop coverage-guided fuzzing (CGF) methods for neural networks. In CGF, random mutations of inputs to a neural network are guided by a coverage metric toward the goal of satisfying user-specified constraints. We describe how fast approximate nearest neighbor algorithms can provide this coverage metric. We then discuss the application of CGF to the following goals: finding numerical errors in trained neural networks, generating disagreements between neural networks and quantized versions of those networks, and surfacing undesirable behavior in character level language models. Finally, we release an open source library called TensorFuzz that implements the described techniques.",2018-07-27,2022-03-10 23:53:22,2022-03-10 23:53:22,2022-03-10 23:53:22,,,,,,,TensorFuzz,,,,,,,,,,,,arXiv.org,,arXiv: 1807.10875,,/Users/jacquesthibodeau/Zotero/storage/DHEIV74L/Odena and Goodfellow - 2018 - TensorFuzz Debugging Neural Networks with Coverag.pdf; /Users/jacquesthibodeau/Zotero/storage/KX77ZN7D/1807.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XVREE8AG,journalArticle,2020,"Yang, Jiachen; Nakhaei, Alireza; Isele, David; Fujimura, Kikuo; Zha, Hongyuan",CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning,"arXiv:1809.05188 [cs, stat]",,,,http://arxiv.org/abs/1809.05188,"A variety of cooperative multi-agent control problems require agents to achieve individual goals while contributing to collective success. This multi-goal multi-agent setting poses difficulties for recent algorithms, which primarily target settings with a single global reward, due to two new challenges: efficient exploration for learning both individual goal attainment and cooperation for others' success, and credit-assignment for interactions between actions and goals of different agents. To address both challenges, we restructure the problem into a novel two-stage curriculum, in which single-agent goal attainment is learned prior to learning multi-agent cooperation, and we derive a new multi-goal multi-agent policy gradient with a credit function for localized credit assignment. We use a function augmentation scheme to bridge value and policy functions across the curriculum. The complete architecture, called CM3, learns significantly faster than direct adaptations of existing algorithms on three challenging multi-goal multi-agent problems: cooperative navigation in difficult formations, negotiating multi-vehicle lane changes in the SUMO traffic simulator, and strategic cooperation in a Checkers environment.",2020-01-24,2022-03-10 23:55:31,2022-03-10 23:55:31,2022-03-10 23:55:31,,,,,,,CM3,,,,,,,,,,,,arXiv.org,,arXiv: 1809.05188,,/Users/jacquesthibodeau/Zotero/storage/PY3SBANR/Yang et al. - 2020 - CM3 Cooperative Multi-goal Multi-stage Multi-agen.pdf; /Users/jacquesthibodeau/Zotero/storage/YD4S7YNM/1809.html,,,Computer Science - Machine Learning; Computer Science - Multiagent Systems; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TG5WPPZB,journalArticle,2021,"Ndousse, Kamal; Eck, Douglas; Levine, Sergey; Jaques, Natasha",Emergent Social Learning via Multi-agent Reinforcement Learning,"arXiv:2010.00581 [cs, stat]",,,,http://arxiv.org/abs/2010.00581,"Social learning is a key component of human and animal intelligence. By taking cues from the behavior of experts in their environment, social learners can acquire sophisticated behavior and rapidly adapt to new circumstances. This paper investigates whether independent reinforcement learning (RL) agents in a multi-agent environment can learn to use social learning to improve their performance. We find that in most circumstances, vanilla model-free RL agents do not use social learning. We analyze the reasons for this deficiency, and show that by imposing constraints on the training environment and introducing a model-based auxiliary loss we are able to obtain generalized social learning policies which enable agents to: i) discover complex skills that are not learned from single-agent training, and ii) adapt online to novel environments by taking cues from experts present in the new environment. In contrast, agents trained with model-free RL or imitation learning generalize poorly and do not succeed in the transfer tasks. By mixing multi-agent and solo training, we can obtain agents that use social learning to gain skills that they can deploy when alone, even out-performing agents trained alone from the start.",2021-06-22,2022-03-10 23:55:32,2022-03-10 23:55:32,2022-03-10 23:55:31,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2010.00581,,/Users/jacquesthibodeau/Zotero/storage/HPDA867X/Ndousse et al. - 2021 - Emergent Social Learning via Multi-agent Reinforce.pdf; /Users/jacquesthibodeau/Zotero/storage/P7GQ444W/2010.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Multiagent Systems; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2LGSSL9R,journalArticle,2019,"Lerer, Adam; Peysakhovich, Alexander",Learning Existing Social Conventions via Observationally Augmented Self-Play,arXiv:1806.10071 [cs],,,,http://arxiv.org/abs/1806.10071,"In order for artificial agents to coordinate effectively with people, they must act consistently with existing conventions (e.g. how to navigate in traffic, which language to speak, or how to coordinate with teammates). A group's conventions can be viewed as a choice of equilibrium in a coordination game. We consider the problem of an agent learning a policy for a coordination game in a simulated environment and then using this policy when it enters an existing group. When there are multiple possible conventions we show that learning a policy via multi-agent reinforcement learning (MARL) is likely to find policies which achieve high payoffs at training time but fail to coordinate with the real group into which the agent enters. We assume access to a small number of samples of behavior from the true convention and show that we can augment the MARL objective to help it find policies consistent with the real group's convention. In three environments from the literature - traffic, communication, and team coordination - we observe that augmenting MARL with a small amount of imitation learning greatly increases the probability that the strategy found by MARL fits well with the existing social convention. We show that this works even in an environment where standard training methods very rarely find the true convention of the agent's partners.",2019-03-13,2022-03-10 23:55:32,2022-03-10 23:55:32,2022-03-10 23:55:32,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1806.10071,,/Users/jacquesthibodeau/Zotero/storage/QZ8JDCN4/Lerer and Peysakhovich - 2019 - Learning Existing Social Conventions via Observati.pdf; /Users/jacquesthibodeau/Zotero/storage/RDKG466A/1806.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MBCX7HTK,journalArticle,2018,"Ortega, Pedro A.; Legg, Shane",Modeling Friends and Foes,arXiv:1807.00196 [cs],,,,http://arxiv.org/abs/1807.00196,"How can one detect friendly and adversarial behavior from raw data? Detecting whether an environment is a friend, a foe, or anything in between, remains a poorly understood yet desirable ability for safe and robust agents. This paper proposes a definition of these environmental ""attitudes"" based on an characterization of the environment's ability to react to the agent's private strategy. We define an objective function for a one-shot game that allows deriving the environment's probability distribution under friendly and adversarial assumptions alongside the agent's optimal strategy. Furthermore, we present an algorithm to compute these equilibrium strategies, and show experimentally that both friendly and adversarial environments possess non-trivial optimal strategies.",2018-06-30,2022-03-10 23:55:36,2022-03-10 23:55:36,2022-03-10 23:55:36,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1807.00196,,/Users/jacquesthibodeau/Zotero/storage/KWDE53TZ/Ortega and Legg - 2018 - Modeling Friends and Foes.pdf; /Users/jacquesthibodeau/Zotero/storage/GBJNZ64T/1807.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KHX99S7S,journalArticle,2018,"Song, Jiaming; Ren, Hongyu; Sadigh, Dorsa; Ermon, Stefano",Multi-Agent Generative Adversarial Imitation Learning,"arXiv:1807.09936 [cs, stat]",,,,http://arxiv.org/abs/1807.09936,"Imitation learning algorithms can be used to learn a policy from expert demonstrations without access to a reward signal. However, most existing approaches are not applicable in multi-agent settings due to the existence of multiple (Nash) equilibria and non-stationary environments. We propose a new framework for multi-agent imitation learning for general Markov games, where we build upon a generalized notion of inverse reinforcement learning. We further introduce a practical multi-agent actor-critic algorithm with good empirical performance. Our method can be used to imitate complex behaviors in high-dimensional environments with multiple cooperative or competing agents.",2018-07-25,2022-03-10 23:55:38,2022-03-10 23:55:38,2022-03-10 23:55:38,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1807.09936,,/Users/jacquesthibodeau/Zotero/storage/LU7MTBYM/Song et al. - 2018 - Multi-Agent Generative Adversarial Imitation Learn.pdf; /Users/jacquesthibodeau/Zotero/storage/MP8XWNQ5/1807.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Multiagent Systems; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
78YIGW9Z,journalArticle,2019,"Gallego, Victor; Naveiro, Roi; Insua, David Rios",Reinforcement Learning under Threats,Proceedings of the AAAI Conference on Artificial Intelligence,,"2374-3468, 2159-5399",10.1609/aaai.v33i01.33019939,http://arxiv.org/abs/1809.01560,"In several reinforcement learning (RL) scenarios, mainly in security settings, there may be adversaries trying to interfere with the reward generating process. In this paper, we introduce Threatened Markov Decision Processes (TMDPs), which provide a framework to support a decision maker against a potential adversary in RL. Furthermore, we propose a level-$k$ thinking scheme resulting in a new learning framework to deal with TMDPs. After introducing our framework and deriving theoretical results, relevant empirical evidence is given via extensive experiments, showing the benefits of accounting for adversaries while the agent learns.",2019-07-17,2022-03-10 23:55:40,2022-03-10 23:55:40,2022-03-10 23:55:40,9939-9940,,,33,,AAAI,,,,,,,,,,,,,arXiv.org,,arXiv: 1809.01560,,/Users/jacquesthibodeau/Zotero/storage/U23QEDYQ/Gallego et al. - 2019 - Reinforcement Learning under Threats.pdf; /Users/jacquesthibodeau/Zotero/storage/63QD4DVP/1809.html,,,Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9RMMDNFZ,journalArticle,2020,"Mazumdar, Eric; Ratliff, Lillian J.; Sastry, S. Shankar",On Gradient-Based Learning in Continuous Games,SIAM Journal on Mathematics of Data Science,,2577-0187,10.1137/18M1231298,http://arxiv.org/abs/1804.05464,"We formulate a general framework for competitive gradient-based learning that encompasses a wide breadth of multi-agent learning algorithms, and analyze the limiting behavior of competitive gradient-based learning algorithms using dynamical systems theory. For both general-sum and potential games, we characterize a non-negligible subset of the local Nash equilibria that will be avoided if each agent employs a gradient-based learning algorithm. We also shed light on the issue of convergence to non-Nash strategies in general- and zero-sum games, which may have no relevance to the underlying game, and arise solely due to the choice of algorithm. The existence and frequency of such strategies may explain some of the difficulties encountered when using gradient descent in zero-sum games as, e.g., in the training of generative adversarial networks. To reinforce the theoretical contributions, we provide empirical results that highlight the frequency of linear quadratic dynamic games (a benchmark for multi-agent reinforcement learning) that admit global Nash equilibria that are almost surely avoided by policy gradient.",2020-01,2022-03-10 23:55:42,2022-03-10 23:55:42,2022-03-10 23:55:42,103-131,,1,2,,SIAM Journal on Mathematics of Data Science,,,,,,,,,,,,,arXiv.org,,arXiv: 1804.05464,,/Users/jacquesthibodeau/Zotero/storage/F4V3WVHB/Mazumdar et al. - 2020 - On Gradient-Based Learning in Continuous Games.pdf; /Users/jacquesthibodeau/Zotero/storage/23LBJDM5/1804.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3RRNBU9D,journalArticle,2019,"Rhinehart, Nicholas; McAllister, Rowan; Kitani, Kris; Levine, Sergey",PRECOG: PREdiction Conditioned On Goals in Visual Multi-Agent Settings,"arXiv:1905.01296 [cs, stat]",,,,http://arxiv.org/abs/1905.01296,"For autonomous vehicles (AVs) to behave appropriately on roads populated by human-driven vehicles, they must be able to reason about the uncertain intentions and decisions of other drivers from rich perceptual information. Towards these capabilities, we present a probabilistic forecasting model of future interactions between a variable number of agents. We perform both standard forecasting and the novel task of conditional forecasting, which reasons about how all agents will likely respond to the goal of a controlled agent (here, the AV). We train models on real and simulated data to forecast vehicle trajectories given past positions and LIDAR. Our evaluation shows that our model is substantially more accurate in multi-agent driving scenarios compared to existing state-of-the-art. Beyond its general ability to perform conditional forecasting queries, we show that our model's predictions of all agents improve when conditioned on knowledge of the AV's goal, further illustrating its capability to model agent interactions.",2019-09-30,2022-03-10 23:55:43,2022-03-10 23:55:43,2022-03-10 23:55:43,,,,,,,PRECOG,,,,,,,,,,,,arXiv.org,,arXiv: 1905.01296,,/Users/jacquesthibodeau/Zotero/storage/MJTWYKWZ/Rhinehart et al. - 2019 - PRECOG PREdiction Conditioned On Goals in Visual .pdf; /Users/jacquesthibodeau/Zotero/storage/N3EUA5B8/1905.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3K67UH64,journalArticle,2020,"Rivera, Corban G.; Lyons, Olivia; Summitt, Arielle; Fatima, Ayman; Pak, Ji; Shao, William; Chalmers, Robert; Englander, Aryeh; Staley, Edward W.; Wang, I.-Jeng; Llorens, Ashley J.",TanksWorld: A Multi-Agent Environment for AI Safety Research,arXiv:2002.11174 [cs],,,,http://arxiv.org/abs/2002.11174,"The ability to create artificial intelligence (AI) capable of performing complex tasks is rapidly outpacing our ability to ensure the safe and assured operation of AI-enabled systems. Fortunately, a landscape of AI safety research is emerging in response to this asymmetry and yet there is a long way to go. In particular, recent simulation environments created to illustrate AI safety risks are relatively simple or narrowly-focused on a particular issue. Hence, we see a critical need for AI safety research environments that abstract essential aspects of complex real-world applications. In this work, we introduce the AI safety TanksWorld as an environment for AI safety research with three essential aspects: competing performance objectives, human-machine teaming, and multi-agent competition. The AI safety TanksWorld aims to accelerate the advancement of safe multi-agent decision-making algorithms by providing a software framework to support competitions with both system performance and safety objectives. As a work in progress, this paper introduces our research objectives and learning environment with reference code and baseline performance metrics to follow in a future work.",2020-02-25,2022-03-10 23:55:45,2022-03-10 23:55:45,2022-03-10 23:55:45,,,,,,,TanksWorld,,,,,,,,,,,,arXiv.org,,arXiv: 2002.11174,,/Users/jacquesthibodeau/Zotero/storage/FCFX3F9Q/Rivera et al. - 2020 - TanksWorld A Multi-Agent Environment for AI Safet.pdf; /Users/jacquesthibodeau/Zotero/storage/ZNXK7FFW/2002.html,,,Computer Science - Artificial Intelligence; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UZBSAV9C,journalArticle,2022,"Strouse, D. J.; McKee, Kevin R.; Botvinick, Matt; Hughes, Edward; Everett, Richard",Collaborating with Humans without Human Data,arXiv:2110.08176 [cs],,,,http://arxiv.org/abs/2110.08176,"Collaborating with humans requires rapidly adapting to their individual strengths, weaknesses, and preferences. Unfortunately, most standard multi-agent reinforcement learning techniques, such as self-play (SP) or population play (PP), produce agents that overfit to their training partners and do not generalize well to humans. Alternatively, researchers can collect human data, train a human model using behavioral cloning, and then use that model to train ""human-aware"" agents (""behavioral cloning play"", or BCP). While such an approach can improve the generalization of agents to new human co-players, it involves the onerous and expensive step of collecting large amounts of human data first. Here, we study the problem of how to train agents that collaborate well with human partners without using human data. We argue that the crux of the problem is to produce a diverse set of training partners. Drawing inspiration from successful multi-agent approaches in competitive domains, we find that a surprisingly simple approach is highly effective. We train our agent partner as the best response to a population of self-play agents and their past checkpoints taken throughout training, a method we call Fictitious Co-Play (FCP). Our experiments focus on a two-player collaborative cooking simulator that has recently been proposed as a challenge problem for coordination with humans. We find that FCP agents score significantly higher than SP, PP, and BCP when paired with novel agent and human partners. Furthermore, humans also report a strong subjective preference to partnering with FCP agents over all baselines.",2022-01-07,2022-03-10 23:55:50,2022-03-11 01:38:41,2022-03-10 23:55:50,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2110.08176,,/Users/jacquesthibodeau/Zotero/storage/ZZX3SPXN/Strouse et al. - 2022 - Collaborating with Humans without Human Data.pdf; /Users/jacquesthibodeau/Zotero/storage/6BLZ2JJL/Strouse et al. - 2022 - Collaborating with Humans without Human Data.pdf; /Users/jacquesthibodeau/Zotero/storage/HMP8NIXA/2110.html; /Users/jacquesthibodeau/Zotero/storage/AT6RPJK8/2110.html,,,Computer Science - Human-Computer Interaction; Computer Science - Machine Learning; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8FQ6JESE,journalArticle,2018,"Hadfield-Menell, Dylan; Andrus, McKane; Hadfield, Gillian K.",Legible Normativity for AI Alignment: The Value of Silly Rules,arXiv:1811.01267 [cs],,,,http://arxiv.org/abs/1811.01267,"It has become commonplace to assert that autonomous agents will have to be built to follow human rules of behavior--social norms and laws. But human laws and norms are complex and culturally varied systems, in many cases agents will have to learn the rules. This requires autonomous agents to have models of how human rule systems work so that they can make reliable predictions about rules. In this paper we contribute to the building of such models by analyzing an overlooked distinction between important rules and what we call silly rules--rules with no discernible direct impact on welfare. We show that silly rules render a normative system both more robust and more adaptable in response to shocks to perceived stability. They make normativity more legible for humans, and can increase legibility for AI systems as well. For AI systems to integrate into human normative systems, we suggest, it may be important for them to have models that include representations of silly rules.",2018-11-03,2022-03-10 23:55:51,2022-03-10 23:55:51,2022-03-10 23:55:51,,,,,,,Legible Normativity for AI Alignment,,,,,,,,,,,,arXiv.org,,arXiv: 1811.01267,,/Users/jacquesthibodeau/Zotero/storage/STP5MYTC/Hadfield-Menell et al. - 2018 - Legible Normativity for AI Alignment The Value of.pdf; /Users/jacquesthibodeau/Zotero/storage/TJNA8KPW/1811.html,,,Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Computer Science - Human-Computer Interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9SZQL6YJ,journalArticle,2019,"Gruetzemacher, Ross; Paradice, David; Lee, Kang Bok",Forecasting Transformative AI: An Expert Survey,arXiv:1901.08579 [cs],,,,http://arxiv.org/abs/1901.08579,"Transformative AI technologies have the potential to reshape critical aspects of society in the near future. However, in order to properly prepare policy initiatives for the arrival of such technologies accurate forecasts and timelines are necessary. A survey was administered to attendees of three AI conferences during the summer of 2018 (ICML, IJCAI and the HLAI conference). The survey included questions for estimating AI capabilities over the next decade, questions for forecasting five scenarios of transformative AI and questions concerning the impact of computational resources in AI research. Respondents indicated a median of 21.5% of human tasks (i.e., all tasks that humans are currently paid to do) can be feasibly automated now, and that this figure would rise to 40% in 5 years and 60% in 10 years. Median forecasts indicated a 50% probability of AI systems being capable of automating 90% of current human tasks in 25 years and 99% of current human tasks in 50 years. The conference of attendance was found to have a statistically significant impact on all forecasts, with attendees of HLAI providing more optimistic timelines with less uncertainty. These findings suggest that AI experts expect major advances in AI technology to continue over the next decade to a degree that will likely have profound transformative impacts on society.",2019-07-16,2022-03-10 23:55:54,2022-03-10 23:55:54,2022-03-10 23:55:53,,,,,,,Forecasting Transformative AI,,,,,,,,,,,,arXiv.org,,arXiv: 1901.08579,,/Users/jacquesthibodeau/Zotero/storage/PQDL8ZYC/Gruetzemacher et al. - 2019 - Forecasting Transformative AI An Expert Survey.pdf; /Users/jacquesthibodeau/Zotero/storage/BLZZY2LH/1901.html,,,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8DQ3CDFV,journalArticle,2020,"Gruetzemacher, Ross; Dorner, Florian; Bernaola-Alvarez, Niko; Giattino, Charlie; Manheim, David",Forecasting AI Progress: A Research Agenda,arXiv:2008.01848 [cs],,,,http://arxiv.org/abs/2008.01848,"Forecasting AI progress is essential to reducing uncertainty in order to appropriately plan for research efforts on AI safety and AI governance. While this is generally considered to be an important topic, little work has been conducted on it and there is no published document that gives and objective overview of the field. Moreover, the field is very diverse and there is no published consensus regarding its direction. This paper describes the development of a research agenda for forecasting AI progress which utilized the Delphi technique to elicit and aggregate experts' opinions on what questions and methods to prioritize. The results of the Delphi are presented; the remainder of the paper follow the structure of these results, briefly reviewing relevant literature and suggesting future work for each topic. Experts indicated that a wide variety of methods should be considered for forecasting AI progress. Moreover, experts identified salient questions that were both general and completely unique to the problem of forecasting AI progress. Some of the highest priority topics include the validation of (partially unresolved) forecasts, how to make forecasting action-guiding and the quality of different performance metrics. While statistical methods seem more promising, there is also recognition that supplementing judgmental techniques can be quite beneficial.",2020-08-04,2022-03-10 23:55:56,2022-03-10 23:55:56,2022-03-10 23:55:56,,,,,,,Forecasting AI Progress,,,,,,,,,,,,arXiv.org,,arXiv: 2008.01848,,/Users/jacquesthibodeau/Zotero/storage/LRHMLW4G/Gruetzemacher et al. - 2020 - Forecasting AI Progress A Research Agenda.pdf; /Users/jacquesthibodeau/Zotero/storage/4VCPDBQW/2008.html,,,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3G6WZSBV,journalArticle,2021,"Hendrycks, Dan; Burns, Collin; Basart, Steven; Zou, Andy; Mazeika, Mantas; Song, Dawn; Steinhardt, Jacob",Measuring Massive Multitask Language Understanding,arXiv:2009.03300 [cs],,,,http://arxiv.org/abs/2009.03300,"We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",2021-01-12,2022-03-10 23:55:59,2022-03-10 23:55:59,2022-03-10 23:55:58,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2009.03300,,/Users/jacquesthibodeau/Zotero/storage/9PPPMYDK/Hendrycks et al. - 2021 - Measuring Massive Multitask Language Understanding.pdf; /Users/jacquesthibodeau/Zotero/storage/6ZG62BJY/2009.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Computers and Society; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
82EI8G54,journalArticle,2021,"Hendrycks, Dan; Burns, Collin; Kadavath, Saurav; Arora, Akul; Basart, Steven; Tang, Eric; Song, Dawn; Steinhardt, Jacob",Measuring Mathematical Problem Solving With the MATH Dataset,arXiv:2103.03874 [cs],,,,http://arxiv.org/abs/2103.03874,"Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.",2021-11-08,2022-03-10 23:56:00,2022-03-10 23:56:00,2022-03-10 23:56:00,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2103.03874,,/Users/jacquesthibodeau/Zotero/storage/Z4C7U8IP/Hendrycks et al. - 2021 - Measuring Mathematical Problem Solving With the MA.pdf; /Users/jacquesthibodeau/Zotero/storage/6TBZ9V2X/2103.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C4BLM25M,journalArticle,2020,"Henighan, Tom; Kaplan, Jared; Katz, Mor; Chen, Mark; Hesse, Christopher; Jackson, Jacob; Jun, Heewoo; Brown, Tom B.; Dhariwal, Prafulla; Gray, Scott; Hallacy, Chris; Mann, Benjamin; Radford, Alec; Ramesh, Aditya; Ryder, Nick; Ziegler, Daniel M.; Schulman, John; Amodei, Dario; McCandlish, Sam",Scaling Laws for Autoregressive Generative Modeling,arXiv:2010.14701 [cs],,,,http://arxiv.org/abs/2010.14701,"We identify empirical scaling laws for the cross-entropy loss in four domains: generative image modeling, video modeling, multimodal image$\leftrightarrow$text models, and mathematical problem solving. In all cases autoregressive Transformers smoothly improve in performance as model size and compute budgets increase, following a power-law plus constant scaling law. The optimal model size also depends on the compute budget through a power-law, with exponents that are nearly universal across all data domains. The cross-entropy loss has an information theoretic interpretation as $S($True$) + D_{\mathrm{KL}}($True$||$Model$)$, and the empirical scaling laws suggest a prediction for both the true data distribution's entropy and the KL divergence between the true and model distributions. With this interpretation, billion-parameter Transformers are nearly perfect models of the YFCC100M image distribution downsampled to an $8\times 8$ resolution, and we can forecast the model size needed to achieve any given reducible loss (ie $D_{\mathrm{KL}}$) in nats/image for other resolutions. We find a number of additional scaling laws in specific domains: (a) we identify a scaling relation for the mutual information between captions and images in multimodal models, and show how to answer the question ""Is a picture worth a thousand words?""; (b) in the case of mathematical problem solving, we identify scaling laws for model performance when extrapolating beyond the training distribution; (c) we finetune generative image models for ImageNet classification and find smooth scaling of the classification loss and error rate, even as the generative loss levels off. Taken together, these results strengthen the case that scaling laws have important implications for neural network performance, including on downstream tasks.",2020-11-05,2022-03-10 23:56:02,2022-03-11 01:37:00,2022-03-10 23:56:02,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2010.14701,,/Users/jacquesthibodeau/Zotero/storage/HVL9XGE5/Henighan et al. - 2020 - Scaling Laws for Autoregressive Generative Modelin.pdf; /Users/jacquesthibodeau/Zotero/storage/MCEEWHXM/2010.html; /Users/jacquesthibodeau/Zotero/storage/7DQHXUL8/Henighan et al. - 2020 - Scaling Laws for Autoregressive Generative Modelin.pdf; /Users/jacquesthibodeau/Zotero/storage/AQCFSXDE/2010.html,,,Computer Science - Computation and Language; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7UVT7BWI,journalArticle,2021,"Hendrycks, Dan; Basart, Steven; Kadavath, Saurav; Mazeika, Mantas; Arora, Akul; Guo, Ethan; Burns, Collin; Puranik, Samir; He, Horace; Song, Dawn; Steinhardt, Jacob",Measuring Coding Challenge Competence With APPS,arXiv:2105.09938 [cs],,,,http://arxiv.org/abs/2105.09938,"While programming is one of the most broadly applicable skills in modern society, modern machine learning models still cannot code solutions to basic problems. Despite its importance, there has been surprisingly little work on evaluating code generation, and it can be difficult to accurately assess code generation performance rigorously. To meet this challenge, we introduce APPS, a benchmark for code generation. Unlike prior work in more restricted settings, our benchmark measures the ability of models to take an arbitrary natural language specification and generate satisfactory Python code. Similar to how companies assess candidate software developers, we then evaluate models by checking their generated code on test cases. Our benchmark includes 10,000 problems, which range from having simple one-line solutions to being substantial algorithmic challenges. We fine-tune large language models on both GitHub and our training set, and we find that the prevalence of syntax errors is decreasing exponentially as models improve. Recent models such as GPT-Neo can pass approximately 20% of the test cases of introductory problems, so we find that machine learning models are now beginning to learn how to code. As the social significance of automatic code generation increases over the coming years, our benchmark can provide an important measure for tracking advancements.",2021-11-08,2022-03-10 23:56:04,2022-03-10 23:56:04,2022-03-10 23:56:03,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2105.09938,,/Users/jacquesthibodeau/Zotero/storage/GENWHUH7/Hendrycks et al. - 2021 - Measuring Coding Challenge Competence With APPS.pdf; /Users/jacquesthibodeau/Zotero/storage/JDUAMF25/2105.html,,,Computer Science - Computation and Language; Computer Science - Machine Learning; Computer Science - Software Engineering,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WDNU877J,journalArticle,2019,"Kim, Byungju; Kim, Hyunwoo; Kim, Kyungsu; Kim, Sungjin; Kim, Junmo",Learning Not to Learn: Training Deep Neural Networks with Biased Data,arXiv:1812.10352 [cs],,,,http://arxiv.org/abs/1812.10352,"We propose a novel regularization algorithm to train deep neural networks, in which data at training time is severely biased. Since a neural network efficiently learns data distribution, a network is likely to learn the bias information to categorize input data. It leads to poor performance at test time, if the bias is, in fact, irrelevant to the categorization. In this paper, we formulate a regularization loss based on mutual information between feature embedding and bias. Based on the idea of minimizing this mutual information, we propose an iterative algorithm to unlearn the bias information. We employ an additional network to predict the bias distribution and train the network adversarially against the feature embedding network. At the end of learning, the bias prediction network is not able to predict the bias not because it is poorly trained, but because the feature embedding network successfully unlearns the bias information. We also demonstrate quantitative and qualitative experimental results which show that our algorithm effectively removes the bias information from feature embedding.",2019-04-15,2022-03-10 23:56:07,2022-03-10 23:56:07,2022-03-10 23:56:06,,,,,,,Learning Not to Learn,,,,,,,,,,,,arXiv.org,,arXiv: 1812.10352,,/Users/jacquesthibodeau/Zotero/storage/QH8ESMDV/Kim et al. - 2019 - Learning Not to Learn Training Deep Neural Networ.pdf; /Users/jacquesthibodeau/Zotero/storage/32KDX9Q5/1812.html,,,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9H6U5FLE,journalArticle,2019,"Jiang, Heinrich; Nachum, Ofir",Identifying and Correcting Label Bias in Machine Learning,"arXiv:1901.04966 [cs, stat]",,,,http://arxiv.org/abs/1901.04966,"Datasets often contain biases which unfairly disadvantage certain groups, and classifiers trained on such datasets can inherit these biases. In this paper, we provide a mathematical formulation of how this bias can arise. We do so by assuming the existence of underlying, unknown, and unbiased labels which are overwritten by an agent who intends to provide accurate labels but may have biases against certain groups. Despite the fact that we only observe the biased labels, we are able to show that the bias may nevertheless be corrected by re-weighting the data points without changing the labels. We show, with theoretical guarantees, that training on the re-weighted dataset corresponds to training on the unobserved but unbiased labels, thus leading to an unbiased machine learning classifier. Our procedure is fast and robust and can be used with virtually any learning algorithm. We evaluate on a number of standard machine learning fairness datasets and a variety of fairness notions, finding that our method outperforms standard approaches in achieving fair classification.",2019-01-15,2022-03-10 23:56:08,2022-03-10 23:56:08,2022-03-10 23:56:08,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1901.04966,,/Users/jacquesthibodeau/Zotero/storage/NJ7VFGBP/Jiang and Nachum - 2019 - Identifying and Correcting Label Bias in Machine L.pdf; /Users/jacquesthibodeau/Zotero/storage/LUQIBFRB/1901.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KCGAUFB8,journalArticle,2020,"Fazelpour, Sina; Lipton, Zachary C.",Algorithmic Fairness from a Non-ideal Perspective,"arXiv:2001.09773 [cs, stat]",,,,http://arxiv.org/abs/2001.09773,"Inspired by recent breakthroughs in predictive modeling, practitioners in both industry and government have turned to machine learning with hopes of operationalizing predictions to drive automated decisions. Unfortunately, many social desiderata concerning consequential decisions, such as justice or fairness, have no natural formulation within a purely predictive framework. In efforts to mitigate these problems, researchers have proposed a variety of metrics for quantifying deviations from various statistical parities that we might expect to observe in a fair world and offered a variety of algorithms in attempts to satisfy subsets of these parities or to trade off the degree to which they are satisfied against utility. In this paper, we connect this approach to \emph{fair machine learning} to the literature on ideal and non-ideal methodological approaches in political philosophy. The ideal approach requires positing the principles according to which a just world would operate. In the most straightforward application of ideal theory, one supports a proposed policy by arguing that it closes a discrepancy between the real and the perfectly just world. However, by failing to account for the mechanisms by which our non-ideal world arose, the responsibilities of various decision-makers, and the impacts of proposed policies, naive applications of ideal thinking can lead to misguided interventions. In this paper, we demonstrate a connection between the fair machine learning literature and the ideal approach in political philosophy, and argue that the increasingly apparent shortcomings of proposed fair machine learning algorithms reflect broader troubles faced by the ideal approach. We conclude with a critical discussion of the harms of misguided solutions, a reinterpretation of impossibility results, and directions for future research.",2020-01-08,2022-03-10 23:56:11,2022-03-10 23:56:11,2022-03-10 23:56:10,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2001.09773,,/Users/jacquesthibodeau/Zotero/storage/LUTZYLVM/Fazelpour and Lipton - 2020 - Algorithmic Fairness from a Non-ideal Perspective.pdf; /Users/jacquesthibodeau/Zotero/storage/599X2E2M/2001.html,,,Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YVBURMER,journalArticle,2018,"Annasamy, Raghuram Mandyam; Sycara, Katia",Towards Better Interpretability in Deep Q-Networks,"arXiv:1809.05630 [cs, stat]",,,,http://arxiv.org/abs/1809.05630,"Deep reinforcement learning techniques have demonstrated superior performance in a wide variety of environments. As improvements in training algorithms continue at a brisk pace, theoretical or empirical studies on understanding what these networks seem to learn, are far behind. In this paper we propose an interpretable neural network architecture for Q-learning which provides a global explanation of the model's behavior using key-value memories, attention and reconstructible embeddings. With a directed exploration strategy, our model can reach training rewards comparable to the state-of-the-art deep Q-learning models. However, results suggest that the features extracted by the neural network are extremely shallow and subsequent testing using out-of-sample examples shows that the agent can easily overfit to trajectories seen during training.",2018-11-14,2022-03-10 23:59:32,2022-03-10 23:59:32,2022-03-10 23:59:32,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1809.05630,,/Users/jacquesthibodeau/Zotero/storage/QSXAIQI2/Annasamy and Sycara - 2018 - Towards Better Interpretability in Deep Q-Networks.pdf; /Users/jacquesthibodeau/Zotero/storage/JEINU682/1809.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8VFZBET9,journalArticle,2018,"Yeh, Chih-Kuan; Kim, Joon Sik; Yen, Ian E. H.; Ravikumar, Pradeep",Representer Point Selection for Explaining Deep Neural Networks,"arXiv:1811.09720 [cs, stat]",,,,http://arxiv.org/abs/1811.09720,"We propose to explain the predictions of a deep neural network, by pointing to the set of what we call representer points in the training set, for a given test point prediction. Specifically, we show that we can decompose the pre-activation prediction of a neural network into a linear combination of activations of training points, with the weights corresponding to what we call representer values, which thus capture the importance of that training point on the learned parameters of the network. But it provides a deeper understanding of the network than simply training point influence: with positive representer values corresponding to excitatory training points, and negative values corresponding to inhibitory points, which as we show provides considerably more insight. Our method is also much more scalable, allowing for real-time feedback in a manner not feasible with influence functions.",2018-11-23,2022-03-10 23:59:35,2022-03-10 23:59:35,2022-03-10 23:59:35,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1811.09720,,/Users/jacquesthibodeau/Zotero/storage/UT98EUMF/Yeh et al. - 2018 - Representer Point Selection for Explaining Deep Ne.pdf; /Users/jacquesthibodeau/Zotero/storage/MLLPF6LY/1811.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6IIVPKJE,journalArticle,2018,"Ross, Andrew Slavin",Training Machine Learning Models by Regularizing their Explanations,"arXiv:1810.00869 [cs, stat]",,,,http://arxiv.org/abs/1810.00869,"Neural networks are among the most accurate supervised learning methods in use today. However, their opacity makes them difficult to trust in critical applications, especially when conditions in training may differ from those in practice. Recent efforts to develop explanations for neural networks and machine learning models more generally have produced tools to shed light on the implicit rules behind predictions. These tools can help us identify when models are right for the wrong reasons. However, they do not always scale to explaining predictions for entire datasets, are not always at the right level of abstraction, and most importantly cannot correct the problems they reveal. In this thesis, we explore the possibility of training machine learning models (with a particular focus on neural networks) using explanations themselves. We consider approaches where models are penalized not only for making incorrect predictions but also for providing explanations that are either inconsistent with domain knowledge or overly complex. These methods let us train models which can not only provide more interpretable rationales for their predictions but also generalize better when training data is confounded or meaningfully different from test data (even adversarially so).",2018-09-29,2022-03-10 23:59:37,2022-03-10 23:59:37,2022-03-10 23:59:32,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.00869,,/Users/jacquesthibodeau/Zotero/storage/4467TYAZ/Ross - 2018 - Training Machine Learning Models by Regularizing t.pdf; /Users/jacquesthibodeau/Zotero/storage/P3MJPJZ2/1810.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KW9N2RD8,journalArticle,2018,"Yang, John; Lee, Gyujeong; Hyun, Minsung; Chang, Simyung; Kwak, Nojun",Towards Governing Agent's Efficacy: Action-Conditional $\beta$-VAE for Deep Transparent Reinforcement Learning,"arXiv:1811.04350 [cs, stat]",,,,http://arxiv.org/abs/1811.04350,"We tackle the blackbox issue of deep neural networks in the settings of reinforcement learning (RL) where neural agents learn towards maximizing reward gains in an uncontrollable way. Such learning approach is risky when the interacting environment includes an expanse of state space because it is then almost impossible to foresee all unwanted outcomes and penalize them with negative rewards beforehand. Unlike reverse analysis of learned neural features from previous works, our proposed method \nj{tackles the blackbox issue by encouraging} an RL policy network to learn interpretable latent features through an implementation of a disentangled representation learning method. Toward this end, our method allows an RL agent to understand self-efficacy by distinguishing its influences from uncontrollable environmental factors, which closely resembles the way humans understand their scenes. Our experimental results show that the learned latent factors not only are interpretable, but also enable modeling the distribution of entire visited state space with a specific action condition. We have experimented that this characteristic of the proposed structure can lead to ex post facto governance for desired behaviors of RL agents.",2018-11-10,2022-03-10 23:59:38,2022-03-10 23:59:38,2022-03-10 23:59:33,,,,,,,Towards Governing Agent's Efficacy,,,,,,,,,,,,arXiv.org,,arXiv: 1811.04350,,/Users/jacquesthibodeau/Zotero/storage/KW8UMGWH/Yang et al. - 2018 - Towards Governing Agent's Efficacy Action-Conditi.pdf; /Users/jacquesthibodeau/Zotero/storage/Y2FGVKGI/1811.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IPM7DBTI,journalArticle,2019,"Mittelstadt, Brent; Russell, Chris; Wachter, Sandra",Explaining Explanations in AI,"Proceedings of the Conference on Fairness, Accountability, and Transparency",,,10.1145/3287560.3287574,http://arxiv.org/abs/1811.01439,"Recent work on interpretability in machine learning and AI has focused on the building of simplified models that approximate the true criteria used to make decisions. These models are a useful pedagogical device for teaching trained professionals how to predict what decisions will be made by the complex system, and most importantly how the system might break. However, when considering any such model it's important to remember Box's maxim that ""All models are wrong but some are useful."" We focus on the distinction between these models and explanations in philosophy and sociology. These models can be understood as a ""do it yourself kit"" for explanations, allowing a practitioner to directly answer ""what if questions"" or generate contrastive explanations without external assistance. Although a valuable ability, giving these models as explanations appears more difficult than necessary, and other forms of explanation may not have the same trade-offs. We contrast the different schools of thought on what makes an explanation, and suggest that machine learning might benefit from viewing the problem more broadly.",2019-01-29,2022-03-10 23:59:38,2022-03-10 23:59:38,2022-03-10 23:59:34,279-288,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1811.01439,,/Users/jacquesthibodeau/Zotero/storage/XA2RNLCU/Mittelstadt et al. - 2019 - Explaining Explanations in AI.pdf; /Users/jacquesthibodeau/Zotero/storage/GH95LPHE/1811.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5TEGNSIS,journalArticle,2020,"Plumb, Gregory; Al-Shedivat, Maruan; Cabrera, Angel Alexander; Perer, Adam; Xing, Eric; Talwalkar, Ameet",Regularizing Black-box Models for Improved Interpretability,"arXiv:1902.06787 [cs, stat]",,,,http://arxiv.org/abs/1902.06787,"Most of the work on interpretable machine learning has focused on designing either inherently interpretable models, which typically trade-off accuracy for interpretability, or post-hoc explanation systems, whose explanation quality can be unpredictable. Our method, ExpO, is a hybridization of these approaches that regularizes a model for explanation quality at training time. Importantly, these regularizers are differentiable, model agnostic, and require no domain knowledge to define. We demonstrate that post-hoc explanations for ExpO-regularized models have better explanation quality, as measured by the common fidelity and stability metrics. We verify that improving these metrics leads to significantly more useful explanations with a user study on a realistic task.",2020-11-08,2022-03-10 23:59:38,2022-03-10 23:59:38,2022-03-10 23:59:37,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1902.06787,,/Users/jacquesthibodeau/Zotero/storage/ASJ2KIDM/Plumb et al. - 2020 - Regularizing Black-box Models for Improved Interpr.pdf; /Users/jacquesthibodeau/Zotero/storage/PRXLSQKN/1902.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
U362KKGR,journalArticle,2017,"Ross, Andrew Slavin; Hughes, Michael C.; Doshi-Velez, Finale",Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations,"arXiv:1703.03717 [cs, stat]",,,,http://arxiv.org/abs/1703.03717,"Neural networks are among the most accurate supervised learning methods in use today, but their opacity makes them difficult to trust in critical applications, especially when conditions in training differ from those in test. Recent work on explanations for black-box models has produced tools (e.g. LIME) to show the implicit rules behind predictions, which can help us identify when models are right for the wrong reasons. However, these methods do not scale to explaining entire datasets and cannot correct the problems they reveal. We introduce a method for efficiently explaining and regularizing differentiable models by examining and selectively penalizing their input gradients, which provide a normal to the decision boundary. We apply these penalties both based on expert annotation and in an unsupervised fashion that encourages diverse models with qualitatively different decision boundaries for the same classification problem. On multiple datasets, we show our approach generates faithful explanations and models that generalize much better when conditions differ between training and test.",2017-05-25,2022-03-10 23:59:39,2022-03-10 23:59:39,2022-03-10 23:59:39,,,,,,,Right for the Right Reasons,,,,,,,,,,,,arXiv.org,,arXiv: 1703.03717,,/Users/jacquesthibodeau/Zotero/storage/LUFSDJPR/Ross et al. - 2017 - Right for the Right Reasons Training Differentiab.pdf; /Users/jacquesthibodeau/Zotero/storage/RUZF3ZM6/1703.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3VT773I5,journalArticle,2018,"Zhang, Quanshi; Yang, Yu; Liu, Yuchen; Wu, Ying Nian; Zhu, Song-Chun",Unsupervised Learning of Neural Networks to Explain Neural Networks,arXiv:1805.07468 [cs],,,,http://arxiv.org/abs/1805.07468,"This paper presents an unsupervised method to learn a neural network, namely an explainer, to interpret a pre-trained convolutional neural network (CNN), i.e., explaining knowledge representations hidden in middle conv-layers of the CNN. Given feature maps of a certain conv-layer of the CNN, the explainer performs like an auto-encoder, which first disentangles the feature maps into object-part features and then inverts object-part features back to features of higher conv-layers of the CNN. More specifically, the explainer contains interpretable conv-layers, where each filter disentangles the representation of a specific object part from chaotic input feature maps. As a paraphrase of CNN features, the disentangled representations of object parts help people understand the logic inside the CNN. We also learn the explainer to use object-part features to reconstruct features of higher CNN layers, in order to minimize loss of information during the feature disentanglement. More crucially, we learn the explainer via network distillation without using any annotations of sample labels, object parts, or textures for supervision. We have applied our method to different types of CNNs for evaluation, and explainers have significantly boosted the interpretability of CNN features.",2018-05-18,2022-03-10 23:59:42,2022-03-10 23:59:42,2022-03-10 23:59:42,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1805.07468,,/Users/jacquesthibodeau/Zotero/storage/5IVJIRHP/Zhang et al. - 2018 - Unsupervised Learning of Neural Networks to Explai.pdf; /Users/jacquesthibodeau/Zotero/storage/VH3CYUM5/1805.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8KNGYUD6,journalArticle,2020,"Pruthi, Garima; Liu, Frederick; Sundararajan, Mukund; Kale, Satyen",Estimating Training Data Influence by Tracing Gradient Descent,"arXiv:2002.08484 [cs, stat]",,,,http://arxiv.org/abs/2002.08484,"We introduce a method called TracIn that computes the influence of a training example on a prediction made by the model. The idea is to trace how the loss on the test point changes during the training process whenever the training example of interest was utilized. We provide a scalable implementation of TracIn via: (a) a first-order gradient approximation to the exact computation, (b) saved checkpoints of standard training procedures, and (c) cherry-picking layers of a deep neural network. In contrast with previously proposed methods, TracIn is simple to implement; all it needs is the ability to work with gradients, checkpoints, and loss functions. The method is general. It applies to any machine learning model trained using stochastic gradient descent or a variant of it, agnostic of architecture, domain and task. We expect the method to be widely useful within processes that study and improve training data.",2020-11-14,2022-03-10 23:59:48,2022-03-10 23:59:48,2022-03-10 23:59:47,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2002.08484,,/Users/jacquesthibodeau/Zotero/storage/KY6XNKTY/Pruthi et al. - 2020 - Estimating Training Data Influence by Tracing Grad.pdf; /Users/jacquesthibodeau/Zotero/storage/CTE9JBE9/2002.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WMYZYDC6,journalArticle,2021,"Poursabzi-Sangdeh, Forough; Goldstein, Daniel G.; Hofman, Jake M.; Vaughan, Jennifer Wortman; Wallach, Hanna",Manipulating and Measuring Model Interpretability,arXiv:1802.07810 [cs],,,,http://arxiv.org/abs/1802.07810,"With machine learning models being increasingly used to aid decision making even in high-stakes domains, there has been a growing interest in developing interpretable models. Although many supposedly interpretable models have been proposed, there have been relatively few experimental studies investigating whether these models achieve their intended effects, such as making people more closely follow a model's predictions when it is beneficial for them to do so or enabling them to detect when a model has made a mistake. We present a sequence of pre-registered experiments (N=3,800) in which we showed participants functionally identical models that varied only in two factors commonly thought to make machine learning models more or less interpretable: the number of features and the transparency of the model (i.e., whether the model internals are clear or black box). Predictably, participants who saw a clear model with few features could better simulate the model's predictions. However, we did not find that participants more closely followed its predictions. Furthermore, showing participants a clear model meant that they were less able to detect and correct for the model's sizable mistakes, seemingly due to information overload. These counterintuitive findings emphasize the importance of testing over intuition when developing interpretable models.",2021-08-15,2022-03-10 23:59:52,2022-03-10 23:59:52,2022-03-10 23:59:51,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1802.07810,,/Users/jacquesthibodeau/Zotero/storage/3JZSNQ53/Poursabzi-Sangdeh et al. - 2021 - Manipulating and Measuring Model Interpretability.pdf; /Users/jacquesthibodeau/Zotero/storage/IIQ9P9V4/1802.html,,,Computer Science - Artificial Intelligence; Computer Science - Computers and Society; I.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
42E8CDCP,journalArticle,2021,"Hilgard, Sophie; Rosenfeld, Nir; Banaji, Mahzarin R.; Cao, Jack; Parkes, David C.","Learning Representations by Humans, for Humans","arXiv:1905.12686 [cs, stat]",,,,http://arxiv.org/abs/1905.12686,"When machine predictors can achieve higher performance than the human decision-makers they support, improving the performance of human decision-makers is often conflated with improving machine accuracy. Here we propose a framework to directly support human decision-making, in which the role of machines is to reframe problems rather than to prescribe actions through prediction. Inspired by the success of representation learning in improving performance of machine predictors, our framework learns human-facing representations optimized for human performance. This ""Mind Composed with Machine"" framework incorporates a human decision-making model directly into the representation learning paradigm and is trained with a novel human-in-the-loop training procedure. We empirically demonstrate the successful application of the framework to various tasks and representational forms.",2021-09-15,2022-03-11 00:00:00,2022-03-11 00:00:00,2022-03-10 23:59:59,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1905.12686,,"/Users/jacquesthibodeau/Zotero/storage/RKWEUSMD/Hilgard et al. - 2021 - Learning Representations by Humans, for Humans.pdf; /Users/jacquesthibodeau/Zotero/storage/24HVBKZQ/1905.html",,,Computer Science - Human-Computer Interaction; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GAN9YD2L,journalArticle,2020,"Ghorbani, Amirata; Zou, James",Neuron Shapley: Discovering the Responsible Neurons,"arXiv:2002.09815 [cs, stat]",,,,http://arxiv.org/abs/2002.09815,"We develop Neuron Shapley as a new framework to quantify the contribution of individual neurons to the prediction and performance of a deep network. By accounting for interactions across neurons, Neuron Shapley is more effective in identifying important filters compared to common approaches based on activation patterns. Interestingly, removing just 30 filters with the highest Shapley scores effectively destroys the prediction accuracy of Inception-v3 on ImageNet. Visualization of these few critical filters provides insights into how the network functions. Neuron Shapley is a flexible framework and can be applied to identify responsible neurons in many tasks. We illustrate additional applications of identifying filters that are responsible for biased prediction in facial recognition and filters that are vulnerable to adversarial attacks. Removing these filters is a quick way to repair models. Enabling all these applications is a new multi-arm bandit algorithm that we developed to efficiently estimate Neuron Shapley values.",2020-11-13,2022-03-11 00:00:01,2022-03-11 00:00:01,2022-03-11 00:00:01,,,,,,,Neuron Shapley,,,,,,,,,,,,arXiv.org,,arXiv: 2002.09815,,/Users/jacquesthibodeau/Zotero/storage/UTLNPKTB/Ghorbani and Zou - 2020 - Neuron Shapley Discovering the Responsible Neuron.pdf; /Users/jacquesthibodeau/Zotero/storage/R7KD3Y9H/2002.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3R6AHTBG,journalArticle,2021,"Wong, Eric; Santurkar, Shibani; Mądry, Aleksander",Leveraging Sparse Linear Layers for Debuggable Deep Networks,"arXiv:2105.04857 [cs, stat]",,,,http://arxiv.org/abs/2105.04857,"We show how fitting sparse linear models over learned deep feature representations can lead to more debuggable neural networks. These networks remain highly accurate while also being more amenable to human interpretation, as we demonstrate quantiatively via numerical and human experiments. We further illustrate how the resulting sparse explanations can help to identify spurious correlations, explain misclassifications, and diagnose model biases in vision and language tasks. The code for our toolkit can be found at https://github.com/madrylab/debuggabledeepnetworks.",2021-05-11,2022-03-11 00:00:11,2022-03-11 00:00:11,2022-03-11 00:00:11,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2105.04857,,/Users/jacquesthibodeau/Zotero/storage/ZBVZ5K79/Wong et al. - 2021 - Leveraging Sparse Linear Layers for Debuggable Dee.pdf; /Users/jacquesthibodeau/Zotero/storage/AUQFCHGA/2105.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PETL843C,journalArticle,2020,"Modhe, Nirbhay; Chattopadhyay, Prithvijit; Sharma, Mohit; Das, Abhishek; Parikh, Devi; Batra, Dhruv; Vedantam, Ramakrishna",IR-VIC: Unsupervised Discovery of Sub-goals for Transfer in RL,Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence,,,10.24963/ijcai.2020/280,http://arxiv.org/abs/1907.10580,"We propose a novel framework to identify sub-goals useful for exploration in sequential decision making tasks under partial observability. We utilize the variational intrinsic control framework (Gregor et.al., 2016) which maximizes empowerment -- the ability to reliably reach a diverse set of states and show how to identify sub-goals as states with high necessary option information through an information theoretic regularizer. Despite being discovered without explicit goal supervision, our sub-goals provide better exploration and sample complexity on challenging grid-world navigation tasks compared to supervised counterparts in prior work.",2020-07,2022-03-11 00:00:16,2022-03-11 00:00:16,2022-03-11 00:00:16,2022-2028,,,,,,IR-VIC,,,,,,,,,,,,arXiv.org,,arXiv: 1907.10580,,/Users/jacquesthibodeau/Zotero/storage/2SM7IM2X/Modhe et al. - 2020 - IR-VIC Unsupervised Discovery of Sub-goals for Tr.pdf; /Users/jacquesthibodeau/Zotero/storage/ZV8LRYIY/1907.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QJI948CT,journalArticle,2020,"Sharma, Archit; Gu, Shixiang; Levine, Sergey; Kumar, Vikash; Hausman, Karol",Dynamics-Aware Unsupervised Discovery of Skills,"arXiv:1907.01657 [cs, stat]",,,,http://arxiv.org/abs/1907.01657,"Conventionally, model-based reinforcement learning (MBRL) aims to learn a global model for the dynamics of the environment. A good model can potentially enable planning algorithms to generate a large variety of behaviors and solve diverse tasks. However, learning an accurate model for complex dynamical systems is difficult, and even then, the model might not generalize well outside the distribution of states on which it was trained. In this work, we combine model-based learning with model-free learning of primitives that make model-based planning easy. To that end, we aim to answer the question: how can we discover skills whose outcomes are easy to predict? We propose an unsupervised learning algorithm, Dynamics-Aware Discovery of Skills (DADS), which simultaneously discovers predictable behaviors and learns their dynamics. Our method can leverage continuous skill spaces, theoretically, allowing us to learn infinitely many behaviors even for high-dimensional state-spaces. We demonstrate that zero-shot planning in the learned latent space significantly outperforms standard MBRL and model-free goal-conditioned RL, can handle sparse-reward tasks, and substantially improves over prior hierarchical RL methods for unsupervised skill discovery.",2020-02-14,2022-03-11 00:00:29,2022-03-11 00:00:29,2022-03-11 00:00:28,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1907.01657,,/Users/jacquesthibodeau/Zotero/storage/8J8HLLY8/Sharma et al. - 2020 - Dynamics-Aware Unsupervised Discovery of Skills.pdf; /Users/jacquesthibodeau/Zotero/storage/LF7FRGVS/1907.html,,,Computer Science - Machine Learning; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MS999SGE,journalArticle,2019,"Merel, Josh; Ahuja, Arun; Pham, Vu; Tunyasuvunakool, Saran; Liu, Siqi; Tirumala, Dhruva; Heess, Nicolas; Wayne, Greg",Hierarchical visuomotor control of humanoids,arXiv:1811.09656 [cs],,,,http://arxiv.org/abs/1811.09656,"We aim to build complex humanoid agents that integrate perception, motor control, and memory. In this work, we partly factor this problem into low-level motor control from proprioception and high-level coordination of the low-level skills informed by vision. We develop an architecture capable of surprisingly flexible, task-directed motor control of a relatively high-DoF humanoid body by combining pre-training of low-level motor controllers with a high-level, task-focused controller that switches among low-level sub-policies. The resulting system is able to control a physically-simulated humanoid body to solve tasks that require coupling visual perception from an unstabilized egocentric RGB camera during locomotion in the environment. For a supplementary video link, see https://youtu.be/7GISvfbykLE .",2019-01-15,2022-03-11 00:00:31,2022-03-11 00:00:31,2022-03-11 00:00:31,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1811.09656,,/Users/jacquesthibodeau/Zotero/storage/8J8S7YEY/Merel et al. - 2019 - Hierarchical visuomotor control of humanoids.pdf; /Users/jacquesthibodeau/Zotero/storage/84H72A5P/1811.html,,,Computer Science - Artificial Intelligence; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C54IQMQI,journalArticle,2019,"Osa, Takayuki; Tangkaratt, Voot; Sugiyama, Masashi",Hierarchical Reinforcement Learning via Advantage-Weighted Information Maximization,"arXiv:1901.01365 [cs, stat]",,,,http://arxiv.org/abs/1901.01365,"Real-world tasks are often highly structured. Hierarchical reinforcement learning (HRL) has attracted research interest as an approach for leveraging the hierarchical structure of a given task in reinforcement learning (RL). However, identifying the hierarchical policy structure that enhances the performance of RL is not a trivial task. In this paper, we propose an HRL method that learns a latent variable of a hierarchical policy using mutual information maximization. Our approach can be interpreted as a way to learn a discrete and latent representation of the state-action space. To learn option policies that correspond to modes of the advantage function, we introduce advantage-weighted importance sampling. In our HRL method, the gating policy learns to select option policies based on an option-value function, and these option policies are optimized based on the deterministic policy gradient method. This framework is derived by leveraging the analogy between a monolithic policy in standard RL and a hierarchical policy in HRL by using a deterministic option policy. Experimental results indicate that our HRL approach can learn a diversity of options and that it can enhance the performance of RL in continuous control tasks.",2019-03-07,2022-03-11 00:00:33,2022-03-11 00:00:33,2022-03-11 00:00:33,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1901.01365,,/Users/jacquesthibodeau/Zotero/storage/CN58PRL7/Osa et al. - 2019 - Hierarchical Reinforcement Learning via Advantage-.pdf; /Users/jacquesthibodeau/Zotero/storage/UEGT374T/1901.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9I3AUFA2,journalArticle,2019,"Wu, Bohan; Gupta, Jayesh K.; Kochenderfer, Mykel J.",Model Primitive Hierarchical Lifelong Reinforcement Learning,arXiv:1903.01567 [cs],,,,http://arxiv.org/abs/1903.01567,"Learning interpretable and transferable subpolicies and performing task decomposition from a single, complex task is difficult. Some traditional hierarchical reinforcement learning techniques enforce this decomposition in a top-down manner, while meta-learning techniques require a task distribution at hand to learn such decompositions. This paper presents a framework for using diverse suboptimal world models to decompose complex task solutions into simpler modular subpolicies. This framework performs automatic decomposition of a single source task in a bottom up manner, concurrently learning the required modular subpolicies as well as a controller to coordinate them. We perform a series of experiments on high dimensional continuous action control tasks to demonstrate the effectiveness of this approach at both complex single task learning and lifelong learning. Finally, we perform ablation studies to understand the importance and robustness of different elements in the framework and limitations to this approach.",2019-03-04,2022-03-11 00:00:36,2022-03-11 00:00:36,2022-03-11 00:00:36,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1903.01567,,/Users/jacquesthibodeau/Zotero/storage/WNXT3R2H/Wu et al. - 2019 - Model Primitive Hierarchical Lifelong Reinforcemen.pdf; /Users/jacquesthibodeau/Zotero/storage/2YP26PQX/1903.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
695GMJ6V,journalArticle,2020,"Igl, Maximilian; Gambardella, Andrew; He, Jinke; Nardelli, Nantas; Siddharth, N.; Böhmer, Wendelin; Whiteson, Shimon",Multitask Soft Option Learning,"arXiv:1904.01033 [cs, stat]",,,,http://arxiv.org/abs/1904.01033,"We present Multitask Soft Option Learning(MSOL), a hierarchical multitask framework based on Planning as Inference. MSOL extends the concept of options, using separate variational posteriors for each task, regularized by a shared prior. This ''soft'' version of options avoids several instabilities during training in a multitask setting, and provides a natural way to learn both intra-option policies and their terminations. Furthermore, it allows fine-tuning of options for new tasks without forgetting their learned policies, leading to faster training without reducing the expressiveness of the hierarchical policy. We demonstrate empirically that MSOL significantly outperforms both hierarchical and flat transfer-learning baselines.",2020-06-21,2022-03-11 00:00:39,2022-03-11 00:00:39,2022-03-11 00:00:38,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1904.01033,,/Users/jacquesthibodeau/Zotero/storage/ZWQ6NWCV/Igl et al. - 2020 - Multitask Soft Option Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/JJL84VCP/1904.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WGBEN2DE,journalArticle,2019,"Levy, Andrew; Platt, Robert; Saenko, Kate",Hierarchical Reinforcement Learning with Hindsight,"arXiv:1805.08180 [cs, stat]",,,,http://arxiv.org/abs/1805.08180,"Reinforcement Learning (RL) algorithms can suffer from poor sample efficiency when rewards are delayed and sparse. We introduce a solution that enables agents to learn temporally extended actions at multiple levels of abstraction in a sample efficient and automated fashion. Our approach combines universal value functions and hindsight learning, allowing agents to learn policies belonging to different time scales in parallel. We show that our method significantly accelerates learning in a variety of discrete and continuous tasks.",2019-03-08,2022-03-11 00:00:41,2022-03-11 00:00:41,2022-03-11 00:00:41,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1805.08180,,/Users/jacquesthibodeau/Zotero/storage/KSWE988D/1805.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IKA52ESY,journalArticle,2019,"Goyal, Anirudh; Sodhani, Shagun; Binas, Jonathan; Peng, Xue Bin; Levine, Sergey; Bengio, Yoshua",Reinforcement Learning with Competitive Ensembles of Information-Constrained Primitives,"arXiv:1906.10667 [cs, stat]",,,,http://arxiv.org/abs/1906.10667,"Reinforcement learning agents that operate in diverse and complex environments can benefit from the structured decomposition of their behavior. Often, this is addressed in the context of hierarchical reinforcement learning, where the aim is to decompose a policy into lower-level primitives or options, and a higher-level meta-policy that triggers the appropriate behaviors for a given situation. However, the meta-policy must still produce appropriate decisions in all states. In this work, we propose a policy design that decomposes into primitives, similarly to hierarchical reinforcement learning, but without a high-level meta-policy. Instead, each primitive can decide for themselves whether they wish to act in the current state. We use an information-theoretic mechanism for enabling this decentralized decision: each primitive chooses how much information it needs about the current state to make a decision and the primitive that requests the most information about the current state acts in the world. The primitives are regularized to use as little information as possible, which leads to natural competition and specialization. We experimentally demonstrate that this policy architecture improves over both flat and hierarchical policies in terms of generalization.",2019-06-25,2022-03-11 00:00:44,2022-03-11 00:00:44,2022-03-11 00:00:44,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1906.10667,,/Users/jacquesthibodeau/Zotero/storage/CACZBKN2/Goyal et al. - 2019 - Reinforcement Learning with Competitive Ensembles .pdf; /Users/jacquesthibodeau/Zotero/storage/YNDP5B2S/1906.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XL394W8S,journalArticle,2019,"Baumann, Tobias; Graepel, Thore; Shawe-Taylor, John",Adaptive Mechanism Design: Learning to Promote Cooperation,arXiv:1806.04067 [cs],,,,http://arxiv.org/abs/1806.04067,"In the future, artificial learning agents are likely to become increasingly widespread in our society. They will interact with both other learning agents and humans in a variety of complex settings including social dilemmas. We consider the problem of how an external agent can promote cooperation between artificial learners by distributing additional rewards and punishments based on observing the learners' actions. We propose a rule for automatically learning how to create right incentives by considering the players' anticipated parameter updates. Using this learning rule leads to cooperation with high social welfare in matrix games in which the agents would otherwise learn to defect with high probability. We show that the resulting cooperative outcome is stable in certain games even if the planning agent is turned off after a given number of episodes, while other games require ongoing intervention to maintain mutual cooperation. However, even in the latter case, the amount of necessary additional incentives decreases over time.",2019-11-20,2022-03-11 00:00:48,2022-03-11 00:00:48,2022-03-11 00:00:47,,,,,,,Adaptive Mechanism Design,,,,,,,,,,,,arXiv.org,,arXiv: 1806.04067,,/Users/jacquesthibodeau/Zotero/storage/5KL5REW3/Baumann et al. - 2019 - Adaptive Mechanism Design Learning to Promote Coo.pdf; /Users/jacquesthibodeau/Zotero/storage/Q47ZCUHU/1806.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GM7X35JM,journalArticle,2018,"Cao, Kris; Lazaridou, Angeliki; Lanctot, Marc; Leibo, Joel Z.; Tuyls, Karl; Clark, Stephen",Emergent Communication through Negotiation,arXiv:1804.03980 [cs],,,,http://arxiv.org/abs/1804.03980,"Multi-agent reinforcement learning offers a way to study how communication could emerge in communities of agents needing to solve specific problems. In this paper, we study the emergence of communication in the negotiation environment, a semi-cooperative model of agent interaction. We introduce two communication protocols -- one grounded in the semantics of the game, and one which is \textit{a priori} ungrounded and is a form of cheap talk. We show that self-interested agents can use the pre-grounded communication channel to negotiate fairly, but are unable to effectively use the ungrounded channel. However, prosocial agents do learn to use cheap talk to find an optimal negotiating strategy, suggesting that cooperation is necessary for language to emerge. We also study communication behaviour in a setting where one agent interacts with agents in a community with different levels of prosociality and show how agent identifiability can aid negotiation.",2018-04-11,2022-03-11 00:00:50,2022-03-11 00:00:50,2022-03-11 00:00:50,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1804.03980,,/Users/jacquesthibodeau/Zotero/storage/E2MY25ZK/Cao et al. - 2018 - Emergent Communication through Negotiation.pdf; /Users/jacquesthibodeau/Zotero/storage/BNE4HTTC/1804.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Machine Learning; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZI5T2IHL,journalArticle,2019,"Zhan, Eric; Zheng, Stephan; Yue, Yisong; Sha, Long; Lucey, Patrick",Generating Multi-Agent Trajectories using Programmatic Weak Supervision,"arXiv:1803.07612 [cs, stat]",,,,http://arxiv.org/abs/1803.07612,"We study the problem of training sequential generative models for capturing coordinated multi-agent trajectory behavior, such as offensive basketball gameplay. When modeling such settings, it is often beneficial to design hierarchical models that can capture long-term coordination using intermediate variables. Furthermore, these intermediate variables should capture interesting high-level behavioral semantics in an interpretable and manipulatable way. We present a hierarchical framework that can effectively learn such sequential generative models. Our approach is inspired by recent work on leveraging programmatically produced weak labels, which we extend to the spatiotemporal regime. In addition to synthetic settings, we show how to instantiate our framework to effectively model complex interactions between basketball players and generate realistic multi-agent trajectories of basketball gameplay over long time periods. We validate our approach using both quantitative and qualitative evaluations, including a user study comparison conducted with professional sports analysts.",2019-02-22,2022-03-11 00:00:53,2022-03-11 00:00:53,2022-03-11 00:00:53,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1803.07612,,/Users/jacquesthibodeau/Zotero/storage/W4JRS7FL/Zhan et al. - 2019 - Generating Multi-Agent Trajectories using Programm.pdf; /Users/jacquesthibodeau/Zotero/storage/IH3AC9ES/1803.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
E2HBSFRC,journalArticle,2018,"Bobu, Andreea; Bajcsy, Andrea; Fisac, Jaime F.; Dragan, Anca D.",Learning under Misspecified Objective Spaces,"arXiv:1810.05157 [cs, stat]",,,,http://arxiv.org/abs/1810.05157,"Learning robot objective functions from human input has become increasingly important, but state-of-the-art techniques assume that the human's desired objective lies within the robot's hypothesis space. When this is not true, even methods that keep track of uncertainty over the objective fail because they reason about which hypothesis might be correct, and not whether any of the hypotheses are correct. We focus specifically on learning from physical human corrections during the robot's task execution, where not having a rich enough hypothesis space leads to the robot updating its objective in ways that the person did not actually intend. We observe that such corrections appear irrelevant to the robot, because they are not the best way of achieving any of the candidate objectives. Instead of naively trusting and learning from every human interaction, we propose robots learn conservatively by reasoning in real time about how relevant the human's correction is for the robot's hypothesis space. We test our inference method in an experiment with human interaction data, and demonstrate that this alleviates unintended learning in an in-person user study with a 7DoF robot manipulator.",2018-10-26,2022-03-11 00:04:10,2022-03-11 00:04:10,2022-03-11 00:04:10,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.05157,,/Users/jacquesthibodeau/Zotero/storage/QBEPYDZB/Bobu et al. - 2018 - Learning under Misspecified Objective Spaces.pdf; /Users/jacquesthibodeau/Zotero/storage/RVGYEV9B/1810.html,,,Computer Science - Artificial Intelligence; Computer Science - Human-Computer Interaction; Computer Science - Machine Learning; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
J6AK7JCQ,journalArticle,2019,"Chevalier-Boisvert, Maxime; Bahdanau, Dzmitry; Lahlou, Salem; Willems, Lucas; Saharia, Chitwan; Nguyen, Thien Huu; Bengio, Yoshua",BabyAI: A Platform to Study the Sample Efficiency of Grounded Language Learning,arXiv:1810.08272 [cs],,,,http://arxiv.org/abs/1810.08272,"Allowing humans to interactively train artificial agents to understand language instructions is desirable for both practical and scientific reasons, but given the poor data efficiency of the current learning methods, this goal may require substantial research efforts. Here, we introduce the BabyAI research platform to support investigations towards including humans in the loop for grounded language learning. The BabyAI platform comprises an extensible suite of 19 levels of increasing difficulty. The levels gradually lead the agent towards acquiring a combinatorially rich synthetic language which is a proper subset of English. The platform also provides a heuristic expert agent for the purpose of simulating a human teacher. We report baseline results and estimate the amount of human involvement that would be required to train a neural network-based agent on some of the BabyAI levels. We put forward strong evidence that current deep learning methods are not yet sufficiently sample efficient when it comes to learning a language with compositional properties.",2019-12-19,2022-03-11 00:04:13,2022-03-11 00:04:13,2022-03-11 00:04:13,,,,,,,BabyAI,,,,,,,,,,,,arXiv.org,,arXiv: 1810.08272,,/Users/jacquesthibodeau/Zotero/storage/DY53NJLL/Chevalier-Boisvert et al. - 2019 - BabyAI A Platform to Study the Sample Efficiency .pdf; /Users/jacquesthibodeau/Zotero/storage/DGP6ISHI/1810.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZSJHPY6U,journalArticle,2019,"Co-Reyes, John D.; Gupta, Abhishek; Sanjeev, Suvansh; Altieri, Nick; Andreas, Jacob; DeNero, John; Abbeel, Pieter; Levine, Sergey",Guiding Policies with Language via Meta-Learning,arXiv:1811.07882 [cs],,,,http://arxiv.org/abs/1811.07882,"Behavioral skills or policies for autonomous agents are conventionally learned from reward functions, via reinforcement learning, or from demonstrations, via imitation learning. However, both modes of task specification have their disadvantages: reward functions require manual engineering, while demonstrations require a human expert to be able to actually perform the task in order to generate the demonstration. Instruction following from natural language instructions provides an appealing alternative: in the same way that we can specify goals to other humans simply by speaking or writing, we would like to be able to specify tasks for our machines. However, a single instruction may be insufficient to fully communicate our intent or, even if it is, may be insufficient for an autonomous agent to actually understand how to perform the desired task. In this work, we propose an interactive formulation of the task specification problem, where iterative language corrections are provided to an autonomous agent, guiding it in acquiring the desired skill. Our proposed language-guided policy learning algorithm can integrate an instruction and a sequence of corrections to acquire new skills very quickly. In our experiments, we show that this method can enable a policy to follow instructions and corrections for simulated navigation and manipulation tasks, substantially outperforming direct, non-interactive instruction following.",2019-01-29,2022-03-11 00:04:15,2022-03-11 00:04:15,2022-03-11 00:04:15,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1811.07882,,/Users/jacquesthibodeau/Zotero/storage/ZGLA7ZB7/Co-Reyes et al. - 2019 - Guiding Policies with Language via Meta-Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/CG3EWGJN/1811.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Human-Computer Interaction; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZYJ5LVA9,journalArticle,2020,"Peng, Xue Bin; Kanazawa, Angjoo; Toyer, Sam; Abbeel, Pieter; Levine, Sergey","Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow","arXiv:1810.00821 [cs, stat]",,,,http://arxiv.org/abs/1810.00821,"Adversarial learning methods have been proposed for a wide range of applications, but the training of adversarial models can be notoriously unstable. Effectively balancing the performance of the generator and discriminator is critical, since a discriminator that achieves very high accuracy will produce relatively uninformative gradients. In this work, we propose a simple and general technique to constrain information flow in the discriminator by means of an information bottleneck. By enforcing a constraint on the mutual information between the observations and the discriminator's internal representation, we can effectively modulate the discriminator's accuracy and maintain useful and informative gradients. We demonstrate that our proposed variational discriminator bottleneck (VDB) leads to significant improvements across three distinct application areas for adversarial learning algorithms. Our primary evaluation studies the applicability of the VDB to imitation learning of dynamic continuous control skills, such as running. We show that our method can learn such skills directly from \emph{raw} video demonstrations, substantially outperforming prior adversarial imitation learning methods. The VDB can also be combined with adversarial inverse reinforcement learning to learn parsimonious reward functions that can be transferred and re-optimized in new settings. Finally, we demonstrate that VDB can train GANs more effectively for image generation, improving upon a number of prior stabilization methods.",2020-08-24,2022-03-11 00:04:17,2022-03-11 00:04:17,2022-03-11 00:04:17,,,,,,,Variational Discriminator Bottleneck,,,,,,,,,,,,arXiv.org,,arXiv: 1810.00821,,/Users/jacquesthibodeau/Zotero/storage/A2QICRFQ/Peng et al. - 2020 - Variational Discriminator Bottleneck Improving Im.pdf; /Users/jacquesthibodeau/Zotero/storage/Z5KPFJX4/1810.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6XJX26EQ,journalArticle,2019,"Yu, Tianhe; Shevchuk, Gleb; Sadigh, Dorsa; Finn, Chelsea",Unsupervised Visuomotor Control through Distributional Planning Networks,"arXiv:1902.05542 [cs, stat]",,,,http://arxiv.org/abs/1902.05542,"While reinforcement learning (RL) has the potential to enable robots to autonomously acquire a wide range of skills, in practice, RL usually requires manual, per-task engineering of reward functions, especially in real world settings where aspects of the environment needed to compute progress are not directly accessible. To enable robots to autonomously learn skills, we instead consider the problem of reinforcement learning without access to rewards. We aim to learn an unsupervised embedding space under which the robot can measure progress towards a goal for itself. Our approach explicitly optimizes for a metric space under which action sequences that reach a particular state are optimal when the goal is the final state reached. This enables learning effective and control-centric representations that lead to more autonomous reinforcement learning algorithms. Our experiments on three simulated environments and two real-world manipulation problems show that our method can learn effective goal metrics from unlabeled interaction, and use the learned goal metrics for autonomous reinforcement learning.",2019-02-14,2022-03-11 00:04:19,2022-03-11 00:04:19,2022-03-11 00:04:19,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1902.05542,,/Users/jacquesthibodeau/Zotero/storage/CPVRKPC4/Yu et al. - 2019 - Unsupervised Visuomotor Control through Distributi.pdf; /Users/jacquesthibodeau/Zotero/storage/TGGF8UWW/1902.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4S3LPQXT,journalArticle,2019,"Milli, Smitha; Dragan, Anca D.",Literal or Pedagogic Human? Analyzing Human Model Misspecification in Objective Learning,arXiv:1903.03877 [cs],,,,http://arxiv.org/abs/1903.03877,"It is incredibly easy for a system designer to misspecify the objective for an autonomous system (""robot''), thus motivating the desire to have the robot learn the objective from human behavior instead. Recent work has suggested that people have an interest in the robot performing well, and will thus behave pedagogically, choosing actions that are informative to the robot. In turn, robots benefit from interpreting the behavior by accounting for this pedagogy. In this work, we focus on misspecification: we argue that robots might not know whether people are being pedagogic or literal and that it is important to ask which assumption is safer to make. We cast objective learning into the more general form of a common-payoff game between the robot and human, and prove that in any such game literal interpretation is more robust to misspecification. Experiments with human data support our theoretical results and point to the sensitivity of the pedagogic assumption.",2019-06-28,2022-03-11 00:04:21,2022-03-11 00:04:21,2022-03-11 00:04:21,,,,,,,Literal or Pedagogic Human?,,,,,,,,,,,,arXiv.org,,arXiv: 1903.03877,,/Users/jacquesthibodeau/Zotero/storage/IJYXDX3K/Milli and Dragan - 2019 - Literal or Pedagogic Human Analyzing Human Model .pdf; /Users/jacquesthibodeau/Zotero/storage/KUMDIKJ8/1903.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EVSXTIBW,journalArticle,2019,"Arumugam, Dilip; Lee, Jun Ki; Saskin, Sophie; Littman, Michael L.",Deep Reinforcement Learning from Policy-Dependent Human Feedback,"arXiv:1902.04257 [cs, stat]",,,,http://arxiv.org/abs/1902.04257,"To widen their accessibility and increase their utility, intelligent agents must be able to learn complex behaviors as specified by (non-expert) human users. Moreover, they will need to learn these behaviors within a reasonable amount of time while efficiently leveraging the sparse feedback a human trainer is capable of providing. Recent work has shown that human feedback can be characterized as a critique of an agent's current behavior rather than as an alternative reward signal to be maximized, culminating in the COnvergent Actor-Critic by Humans (COACH) algorithm for making direct policy updates based on human feedback. Our work builds on COACH, moving to a setting where the agent's policy is represented by a deep neural network. We employ a series of modifications on top of the original COACH algorithm that are critical for successfully learning behaviors from high-dimensional observations, while also satisfying the constraint of obtaining reduced sample complexity. We demonstrate the effectiveness of our Deep COACH algorithm in the rich 3D world of Minecraft with an agent that learns to complete tasks by mapping from raw pixels to actions using only real-time human feedback in 10-15 minutes of interaction.",2019-02-12,2022-03-11 00:04:24,2022-03-11 00:04:24,2022-03-11 00:04:23,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1902.04257,,/Users/jacquesthibodeau/Zotero/storage/G2VKKUDZ/Arumugam et al. - 2019 - Deep Reinforcement Learning from Policy-Dependent .pdf; /Users/jacquesthibodeau/Zotero/storage/F5UWFQAB/1902.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RCNPV2LK,journalArticle,2019,"Brown, Daniel S.; Goo, Wonjoon; Nagarajan, Prabhat; Niekum, Scott",Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations,"arXiv:1904.06387 [cs, stat]",,,,http://arxiv.org/abs/1904.06387,"A critical flaw of existing inverse reinforcement learning (IRL) methods is their inability to significantly outperform the demonstrator. This is because IRL typically seeks a reward function that makes the demonstrator appear near-optimal, rather than inferring the underlying intentions of the demonstrator that may have been poorly executed in practice. In this paper, we introduce a novel reward-learning-from-observation algorithm, Trajectory-ranked Reward EXtrapolation (T-REX), that extrapolates beyond a set of (approximately) ranked demonstrations in order to infer high-quality reward functions from a set of potentially poor demonstrations. When combined with deep reinforcement learning, T-REX outperforms state-of-the-art imitation learning and IRL methods on multiple Atari and MuJoCo benchmark tasks and achieves performance that is often more than twice the performance of the best demonstration. We also demonstrate that T-REX is robust to ranking noise and can accurately extrapolate intention by simply watching a learner noisily improve at a task over time.",2019-07-08,2022-03-11 00:04:35,2022-03-11 00:04:35,2022-03-11 00:04:35,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1904.06387,,/Users/jacquesthibodeau/Zotero/storage/M2TSN68L/Brown et al. - 2019 - Extrapolating Beyond Suboptimal Demonstrations via.pdf; /Users/jacquesthibodeau/Zotero/storage/4X8966I4/1904.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5B4VDQ2W,journalArticle,2020,"Ke, Liyiming; Choudhury, Sanjiban; Barnes, Matt; Sun, Wen; Lee, Gilwoo; Srinivasa, Siddhartha",Imitation Learning as $f$-Divergence Minimization,"arXiv:1905.12888 [cs, math, stat]",,,,http://arxiv.org/abs/1905.12888,"We address the problem of imitation learning with multi-modal demonstrations. Instead of attempting to learn all modes, we argue that in many tasks it is sufficient to imitate any one of them. We show that the state-of-the-art methods such as GAIL and behavior cloning, due to their choice of loss function, often incorrectly interpolate between such modes. Our key insight is to minimize the right divergence between the learner and the expert state-action distributions, namely the reverse KL divergence or I-projection. We propose a general imitation learning framework for estimating and minimizing any f-Divergence. By plugging in different divergences, we are able to recover existing algorithms such as Behavior Cloning (Kullback-Leibler), GAIL (Jensen Shannon) and Dagger (Total Variation). Empirical results show that our approximate I-projection technique is able to imitate multi-modal behaviors more reliably than GAIL and behavior cloning.",2020-05-31,2022-03-11 00:04:35,2022-03-11 00:04:35,2022-03-11 00:04:35,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1905.12888,,/Users/jacquesthibodeau/Zotero/storage/K3LHVVMH/Ke et al. - 2020 - Imitation Learning as $f$-Divergence Minimization.pdf; /Users/jacquesthibodeau/Zotero/storage/JWQGEIQS/1905.html,,,Computer Science - Information Theory; Computer Science - Machine Learning; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EKDNWQAL,journalArticle,2019,"Xu, Danfei; Denil, Misha",Positive-Unlabeled Reward Learning,"arXiv:1911.00459 [cs, stat]",,,,http://arxiv.org/abs/1911.00459,"Learning reward functions from data is a promising path towards achieving scalable Reinforcement Learning (RL) for robotics. However, a major challenge in training agents from learned reward models is that the agent can learn to exploit errors in the reward model to achieve high reward behaviors that do not correspond to the intended task. These reward delusions can lead to unintended and even dangerous behaviors. On the other hand, adversarial imitation learning frameworks tend to suffer the opposite problem, where the discriminator learns to trivially distinguish agent and expert behavior, resulting in reward models that produce low reward signal regardless of the input state. In this paper, we connect these two classes of reward learning methods to positive-unlabeled (PU) learning, and we show that by applying a large-scale PU learning algorithm to the reward learning problem, we can address both the reward under- and over-estimation problems simultaneously. Our approach drastically improves both GAIL and supervised reward learning, without any additional assumptions.",2019-11-01,2022-03-11 00:04:36,2022-03-11 00:04:36,2022-03-11 00:04:35,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1911.00459,,/Users/jacquesthibodeau/Zotero/storage/FCF2YMHI/Xu and Denil - 2019 - Positive-Unlabeled Reward Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/97WWEK2F/1911.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SJTECJE2,journalArticle,2019,"Brown, Daniel S.; Goo, Wonjoon; Niekum, Scott",Better-than-Demonstrator Imitation Learning via Automatically-Ranked Demonstrations,"arXiv:1907.03976 [cs, stat]",,,,http://arxiv.org/abs/1907.03976,"The performance of imitation learning is typically upper-bounded by the performance of the demonstrator. While recent empirical results demonstrate that ranked demonstrations allow for better-than-demonstrator performance, preferences over demonstrations may be difficult to obtain, and little is known theoretically about when such methods can be expected to successfully extrapolate beyond the performance of the demonstrator. To address these issues, we first contribute a sufficient condition for better-than-demonstrator imitation learning and provide theoretical results showing why preferences over demonstrations can better reduce reward function ambiguity when performing inverse reinforcement learning. Building on this theory, we introduce Disturbance-based Reward Extrapolation (D-REX), a ranking-based imitation learning method that injects noise into a policy learned through behavioral cloning to automatically generate ranked demonstrations. These ranked demonstrations are used to efficiently learn a reward function that can then be optimized using reinforcement learning. We empirically validate our approach on simulated robot and Atari imitation learning benchmarks and show that D-REX outperforms standard imitation learning approaches and can significantly surpass the performance of the demonstrator. D-REX is the first imitation learning approach to achieve significant extrapolation beyond the demonstrator's performance without additional side-information or supervision, such as rewards or human preferences. By generating rankings automatically, we show that preference-based inverse reinforcement learning can be applied in traditional imitation learning settings where only unlabeled demonstrations are available.",2019-10-14,2022-03-11 00:04:36,2022-03-11 00:04:36,2022-03-11 00:04:35,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1907.03976,,/Users/jacquesthibodeau/Zotero/storage/JJG2UYZC/Brown et al. - 2019 - Better-than-Demonstrator Imitation Learning via Au.pdf; /Users/jacquesthibodeau/Zotero/storage/WIKH9Y5L/1907.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9XK3US9W,journalArticle,2018,"Warnell, Garrett; Waytowich, Nicholas; Lawhern, Vernon; Stone, Peter",Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces,arXiv:1709.10163 [cs],,,,http://arxiv.org/abs/1709.10163,"While recent advances in deep reinforcement learning have allowed autonomous learning agents to succeed at a variety of complex tasks, existing algorithms generally require a lot of training data. One way to increase the speed at which agents are able to learn to perform tasks is by leveraging the input of human trainers. Although such input can take many forms, real-time, scalar-valued feedback is especially useful in situations where it proves difficult or impossible for humans to provide expert demonstrations. Previous approaches have shown the usefulness of human input provided in this fashion (e.g., the TAMER framework), but they have thus far not considered high-dimensional state spaces or employed the use of deep learning. In this paper, we do both: we propose Deep TAMER, an extension of the TAMER framework that leverages the representational power of deep neural networks in order to learn complex tasks in just a short amount of time with a human trainer. We demonstrate Deep TAMER's success by using it and just 15 minutes of human-provided feedback to train an agent that performs better than humans on the Atari game of Bowling - a task that has proven difficult for even state-of-the-art reinforcement learning methods.",2018-01-19,2022-03-11 00:04:55,2022-03-11 00:04:55,2022-03-11 00:04:55,,,,,,,Deep TAMER,,,,,,,,,,,,arXiv.org,,arXiv: 1709.10163,,/Users/jacquesthibodeau/Zotero/storage/KFS9J423/Warnell et al. - 2018 - Deep TAMER Interactive Agent Shaping in High-Dime.pdf; /Users/jacquesthibodeau/Zotero/storage/EZZ2QAIV/1709.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UB98BTXM,journalArticle,2017,"MacGlashan, James; Ho, Mark K.; Loftin, Robert; Peng, Bei; Roberts, David; Taylor, Matthew E.; Littman, Michael L.",Interactive Learning from Policy-Dependent Human Feedback,arXiv:1701.06049 [cs],,,,http://arxiv.org/abs/1701.06049,"For agents and robots to become more useful, they must be able to quickly learn from non-technical users. This paper investigates the problem of interactively learning behaviors communicated by a human teacher using positive and negative feedback. Much previous work on this problem has made the assumption that people provide feedback for decisions that is dependent on the behavior they are teaching and is independent from the learner's current policy. We present empirical results that show this assumption to be false---whether human trainers give a positive or negative feedback for a decision is influenced by the learner's current policy. We argue that policy-dependent feedback, in addition to being commonplace, enables useful training strategies from which agents should benefit. Based on this insight, we introduce Convergent Actor-Critic by Humans (COACH), an algorithm for learning from policy-dependent feedback that converges to a local optimum. Finally, we demonstrate that COACH can successfully learn multiple behaviors on a physical robot, even with noisy image features.",2017-01-21,2022-03-11 00:04:57,2022-03-11 00:04:57,2022-03-11 00:04:57,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1701.06049,,/Users/jacquesthibodeau/Zotero/storage/UGK5A8CQ/MacGlashan et al. - 2017 - Interactive Learning from Policy-Dependent Human F.pdf; /Users/jacquesthibodeau/Zotero/storage/JNFKWSYF/1701.html,,,Computer Science - Artificial Intelligence; I.2.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Z7GHCDU3,journalArticle,2018,"Ibarz, Borja; Leike, Jan; Pohlen, Tobias; Irving, Geoffrey; Legg, Shane; Amodei, Dario",Reward learning from human preferences and demonstrations in Atari,"arXiv:1811.06521 [cs, stat]",,,,http://arxiv.org/abs/1811.06521,"To solve complex real-world problems with reinforcement learning, we cannot rely on manually specified reward functions. Instead, we can have humans communicate an objective to the agent directly. In this work, we combine two approaches to learning from human feedback: expert demonstrations and trajectory preferences. We train a deep neural network to model the reward function and use its predicted reward to train an DQN-based deep reinforcement learning agent on 9 Atari games. Our approach beats the imitation learning baseline in 7 games and achieves strictly superhuman performance on 2 games without using game rewards. Additionally, we investigate the goodness of fit of the reward model, present some reward hacking problems, and study the effects of noise in the human labels.",2018-11-15,2022-03-11 00:04:59,2022-03-11 00:04:59,2022-03-11 00:04:59,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1811.06521,,/Users/jacquesthibodeau/Zotero/storage/9QDHZKM9/Ibarz et al. - 2018 - Reward learning from human preferences and demonst.pdf; /Users/jacquesthibodeau/Zotero/storage/9RUG6HJD/1811.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FZZXBWL2,journalArticle,2021,"Reddy, Siddharth; Dragan, Anca D.; Levine, Sergey; Legg, Shane; Leike, Jan",Learning Human Objectives by Evaluating Hypothetical Behavior,"arXiv:1912.05652 [cs, stat]",,,,http://arxiv.org/abs/1912.05652,"We seek to align agent behavior with a user's objectives in a reinforcement learning setting with unknown dynamics, an unknown reward function, and unknown unsafe states. The user knows the rewards and unsafe states, but querying the user is expensive. To address this challenge, we propose an algorithm that safely and interactively learns a model of the user's reward function. We start with a generative model of initial states and a forward dynamics model trained on off-policy data. Our method uses these models to synthesize hypothetical behaviors, asks the user to label the behaviors with rewards, and trains a neural network to predict the rewards. The key idea is to actively synthesize the hypothetical behaviors from scratch by maximizing tractable proxies for the value of information, without interacting with the environment. We call this method reward query synthesis via trajectory optimization (ReQueST). We evaluate ReQueST with simulated users on a state-based 2D navigation task and the image-based Car Racing video game. The results show that ReQueST significantly outperforms prior methods in learning reward models that transfer to new environments with different initial state distributions. Moreover, ReQueST safely trains the reward model to detect unsafe states, and corrects reward hacking before deploying the agent.",2021-03-24,2022-03-11 00:05:01,2022-03-11 01:38:25,2022-03-11 00:05:01,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1912.05652,,/Users/jacquesthibodeau/Zotero/storage/W5NX3B7V/Reddy et al. - 2021 - Learning Human Objectives by Evaluating Hypothetic.pdf; /Users/jacquesthibodeau/Zotero/storage/RVWLACVB/Reddy et al. - 2021 - Learning Human Objectives by Evaluating Hypothetic.pdf; /Users/jacquesthibodeau/Zotero/storage/3I2JDXY9/1912.html; /Users/jacquesthibodeau/Zotero/storage/4B542U2D/1912.html,,,Computer Science - Computers and Society; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
U79SKTKT,journalArticle,2015,"Javdani, Shervin; Srinivasa, Siddhartha S.; Bagnell, J. Andrew",Shared Autonomy via Hindsight Optimization,arXiv:1503.07619 [cs],,,,http://arxiv.org/abs/1503.07619,"In shared autonomy, user input and robot autonomy are combined to control a robot to achieve a goal. Often, the robot does not know a priori which goal the user wants to achieve, and must both predict the user's intended goal, and assist in achieving that goal. We formulate the problem of shared autonomy as a Partially Observable Markov Decision Process with uncertainty over the user's goal. We utilize maximum entropy inverse optimal control to estimate a distribution over the user's goal based on the history of inputs. Ideally, the robot assists the user by solving for an action which minimizes the expected cost-to-go for the (unknown) goal. As solving the POMDP to select the optimal action is intractable, we use hindsight optimization to approximate the solution. In a user study, we compare our method to a standard predict-then-blend approach. We find that our method enables users to accomplish tasks more quickly while utilizing less input. However, when asked to rate each system, users were mixed in their assessment, citing a tradeoff between maintaining control authority and accomplishing tasks quickly.",2015-04-17,2022-03-11 00:05:04,2022-03-11 00:05:04,2022-03-11 00:05:04,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1503.07619,,/Users/jacquesthibodeau/Zotero/storage/SU3VLID5/Javdani et al. - 2015 - Shared Autonomy via Hindsight Optimization.pdf; /Users/jacquesthibodeau/Zotero/storage/EXWK6RUB/1503.html,,,Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9QDYV7XI,journalArticle,2020,"Armstrong, Stuart; Leike, Jan; Orseau, Laurent; Legg, Shane",Pitfalls of learning a reward function online,arXiv:2004.13654 [cs],,,,http://arxiv.org/abs/2004.13654,"In some agent designs like inverse reinforcement learning an agent needs to learn its own reward function. Learning the reward function and optimising for it are typically two different processes, usually performed at different stages. We consider a continual (``one life'') learning approach where the agent both learns the reward function and optimises for it at the same time. We show that this comes with a number of pitfalls, such as deliberately manipulating the learning process in one direction, refusing to learn, ``learning'' facts already known to the agent, and making decisions that are strictly dominated (for all relevant reward functions). We formally introduce two desirable properties: the first is `unriggability', which prevents the agent from steering the learning process in the direction of a reward function that is easier to optimise. The second is `uninfluenceability', whereby the reward-function learning process operates by learning facts about the environment. We show that an uninfluenceable process is automatically unriggable, and if the set of possible environments is sufficiently rich, the converse is true too.",2020-04-28,2022-03-11 00:05:06,2022-03-11 00:05:06,2022-03-11 00:05:06,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2004.13654,,/Users/jacquesthibodeau/Zotero/storage/VUG3B7N5/Armstrong et al. - 2020 - Pitfalls of learning a reward function online.pdf; /Users/jacquesthibodeau/Zotero/storage/UTVE8ETZ/2004.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7HJTT8R4,journalArticle,2021,"Lynch, Corey; Sermanet, Pierre",Language Conditioned Imitation Learning over Unstructured Data,arXiv:2005.07648 [cs],,,,http://arxiv.org/abs/2005.07648,"Natural language is perhaps the most flexible and intuitive way for humans to communicate tasks to a robot. Prior work in imitation learning typically requires each task be specified with a task id or goal image -- something that is often impractical in open-world environments. On the other hand, previous approaches in instruction following allow agent behavior to be guided by language, but typically assume structure in the observations, actuators, or language that limit their applicability to complex settings like robotics. In this work, we present a method for incorporating free-form natural language conditioning into imitation learning. Our approach learns perception from pixels, natural language understanding, and multitask continuous control end-to-end as a single neural network. Unlike prior work in imitation learning, our method is able to incorporate unlabeled and unstructured demonstration data (i.e. no task or language labels). We show this dramatically improves language conditioned performance, while reducing the cost of language annotation to less than 1% of total data. At test time, a single language conditioned visuomotor policy trained with our method can perform a wide variety of robotic manipulation skills in a 3D environment, specified only with natural language descriptions of each task (e.g. ""open the drawer...now pick up the block...now press the green button...""). To scale up the number of instructions an agent can follow, we propose combining text conditioned policies with large pretrained neural language models. We find this allows a policy to be robust to many out-of-distribution synonym instructions, without requiring new demonstrations. See videos of a human typing live text commands to our agent at language-play.github.io",2021-07-07,2022-03-11 00:05:24,2022-03-11 00:05:24,2022-03-11 00:05:24,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2005.07648,,/Users/jacquesthibodeau/Zotero/storage/Y8DZNKMW/Lynch and Sermanet - 2021 - Language Conditioned Imitation Learning over Unstr.pdf; /Users/jacquesthibodeau/Zotero/storage/2Y8VWVWI/2005.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7LXTN6NY,journalArticle,2020,"Hill, Felix; Mokra, Sona; Wong, Nathaniel; Harley, Tim",Human Instruction-Following with Deep Reinforcement Learning via Transfer-Learning from Text,arXiv:2005.09382 [cs],,,,http://arxiv.org/abs/2005.09382,"Recent work has described neural-network-based agents that are trained with reinforcement learning (RL) to execute language-like commands in simulated worlds, as a step towards an intelligent agent or robot that can be instructed by human users. However, the optimisation of multi-goal motor policies via deep RL from scratch requires many episodes of experience. Consequently, instruction-following with deep RL typically involves language generated from templates (by an environment simulator), which does not reflect the varied or ambiguous expressions of real users. Here, we propose a conceptually simple method for training instruction-following agents with deep RL that are robust to natural human instructions. By applying our method with a state-of-the-art pre-trained text-based language model (BERT), on tasks requiring agents to identify and position everyday objects relative to other objects in a naturalistic 3D simulated room, we demonstrate substantially-above-chance zero-shot transfer from synthetic template commands to natural instructions given by humans. Our approach is a general recipe for training any deep RL-based system to interface with human users, and bridges the gap between two research directions of notable recent success: agent-centric motor behavior and text-based representation learning.",2020-05-19,2022-03-11 00:05:26,2022-03-11 00:05:26,2022-03-11 00:05:26,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2005.09382,,/Users/jacquesthibodeau/Zotero/storage/F4XVCPYC/Hill et al. - 2020 - Human Instruction-Following with Deep Reinforcemen.pdf; /Users/jacquesthibodeau/Zotero/storage/EZ576CQ3/2005.html,,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3BSR77V4,journalArticle,2021,"Gleave, Adam; Dennis, Michael; Legg, Shane; Russell, Stuart; Leike, Jan",Quantifying Differences in Reward Functions,"arXiv:2006.13900 [cs, stat]",,,,http://arxiv.org/abs/2006.13900,"For many tasks, the reward function is inaccessible to introspection or too complex to be specified procedurally, and must instead be learned from user data. Prior work has evaluated learned reward functions by evaluating policies optimized for the learned reward. However, this method cannot distinguish between the learned reward function failing to reflect user preferences and the policy optimization process failing to optimize the learned reward. Moreover, this method can only tell us about behavior in the evaluation environment, but the reward may incentivize very different behavior in even a slightly different deployment environment. To address these problems, we introduce the Equivalent-Policy Invariant Comparison (EPIC) distance to quantify the difference between two reward functions directly, without a policy optimization step. We prove EPIC is invariant on an equivalence class of reward functions that always induce the same optimal policy. Furthermore, we find EPIC can be efficiently approximated and is more robust than baselines to the choice of coverage distribution. Finally, we show that EPIC distance bounds the regret of optimal policies even under different transition dynamics, and we confirm empirically that it predicts policy training success. Our source code is available at https://github.com/HumanCompatibleAI/evaluating-rewards.",2021-03-17,2022-03-11 00:05:28,2022-03-11 00:05:28,2022-03-11 00:05:28,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2006.13900,,/Users/jacquesthibodeau/Zotero/storage/5D274AIX/Gleave et al. - 2021 - Quantifying Differences in Reward Functions.pdf; /Users/jacquesthibodeau/Zotero/storage/7QT547WD/2006.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; I.2.6; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FFS8VH5C,journalArticle,2021,"Bobu, Andreea; Wiggert, Marius; Tomlin, Claire; Dragan, Anca D.",Feature Expansive Reward Learning: Rethinking Human Input,Proceedings of the 2021 ACM/IEEE International Conference on Human-Robot Interaction,,,10.1145/3434073.3444667,http://arxiv.org/abs/2006.13208,"When a person is not satisfied with how a robot performs a task, they can intervene to correct it. Reward learning methods enable the robot to adapt its reward function online based on such human input, but they rely on handcrafted features. When the correction cannot be explained by these features, recent work in deep Inverse Reinforcement Learning (IRL) suggests that the robot could ask for task demonstrations and recover a reward defined over the raw state space. Our insight is that rather than implicitly learning about the missing feature(s) from demonstrations, the robot should instead ask for data that explicitly teaches it about what it is missing. We introduce a new type of human input in which the person guides the robot from states where the feature being taught is highly expressed to states where it is not. We propose an algorithm for learning the feature from the raw state space and integrating it into the reward function. By focusing the human input on the missing feature, our method decreases sample complexity and improves generalization of the learned reward over the above deep IRL baseline. We show this in experiments with a physical 7DOF robot manipulator, as well as in a user study conducted in a simulated environment.",2021-03-08,2022-03-11 00:05:33,2022-03-11 00:05:33,2022-03-11 00:05:33,216-224,,,,,,Feature Expansive Reward Learning,,,,,,,,,,,,arXiv.org,,arXiv: 2006.13208,,/Users/jacquesthibodeau/Zotero/storage/IUBKLWZG/Bobu et al. - 2021 - Feature Expansive Reward Learning Rethinking Huma.pdf; /Users/jacquesthibodeau/Zotero/storage/DN6AGS7C/2006.html,,,Computer Science - Artificial Intelligence; Computer Science - Human-Computer Interaction; Computer Science - Machine Learning; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9ZM6YULI,journalArticle,2019,"Rhinehart, Nicholas; McAllister, Rowan; Levine, Sergey","Deep Imitative Models for Flexible Inference, Planning, and Control","arXiv:1810.06544 [cs, stat]",,,,http://arxiv.org/abs/1810.06544,"Imitation Learning (IL) is an appealing approach to learn desirable autonomous behavior. However, directing IL to achieve arbitrary goals is difficult. In contrast, planning-based algorithms use dynamics models and reward functions to achieve goals. Yet, reward functions that evoke desirable behavior are often difficult to specify. In this paper, we propose Imitative Models to combine the benefits of IL and goal-directed planning. Imitative Models are probabilistic predictive models of desirable behavior able to plan interpretable expert-like trajectories to achieve specified goals. We derive families of flexible goal objectives, including constrained goal regions, unconstrained goal sets, and energy-based goals. We show that our method can use these objectives to successfully direct behavior. Our method substantially outperforms six IL approaches and a planning-based approach in a dynamic simulated autonomous driving task, and is efficiently learned from expert demonstrations without online data collection. We also show our approach is robust to poorly specified goals, such as goals on the wrong side of the road.",2019-09-30,2022-03-11 00:05:35,2022-03-11 00:05:35,2022-03-11 00:05:35,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.06544,,"/Users/jacquesthibodeau/Zotero/storage/QEVSDEDH/Rhinehart et al. - 2019 - Deep Imitative Models for Flexible Inference, Plan.pdf; /Users/jacquesthibodeau/Zotero/storage/GIHT83S4/1810.html",,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UQ2J89WY,journalArticle,2020,"Swamy, Gokul; Schulz, Jens; Choudhury, Rohan; Hadfield-Menell, Dylan; Dragan, Anca",On the Utility of Model Learning in HRI,"arXiv:1901.01291 [cs, stat]",,,,http://arxiv.org/abs/1901.01291,"Fundamental to robotics is the debate between model-based and model-free learning: should the robot build an explicit model of the world, or learn a policy directly? In the context of HRI, part of the world to be modeled is the human. One option is for the robot to treat the human as a black box and learn a policy for how they act directly. But it can also model the human as an agent, and rely on a ""theory of mind"" to guide or bias the learning (grey box). We contribute a characterization of the performance of these methods for an autonomous driving task under the optimistic case of having an ideal theory of mind, as well as under different scenarios in which the assumptions behind the robot's theory of mind for the human are wrong, as they inevitably will be in practice.",2020-05-21,2022-03-11 00:05:37,2022-03-11 00:05:37,2022-03-11 00:05:37,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1901.01291,,/Users/jacquesthibodeau/Zotero/storage/G44U74VG/Swamy et al. - 2020 - On the Utility of Model Learning in HRI.pdf; /Users/jacquesthibodeau/Zotero/storage/YGSF3IJ7/1901.html,,,Computer Science - Machine Learning; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PD9H6GNQ,journalArticle,2018,"Wang, Xin; Chen, Wenhu; Wang, Yuan-Fang; Wang, William Yang",No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling,arXiv:1804.09160 [cs],,,,http://arxiv.org/abs/1804.09160,"Though impressive results have been achieved in visual captioning, the task of generating abstract stories from photo streams is still a little-tapped problem. Different from captions, stories have more expressive language styles and contain many imaginary concepts that do not appear in the images. Thus it poses challenges to behavioral cloning algorithms. Furthermore, due to the limitations of automatic metrics on evaluating story quality, reinforcement learning methods with hand-crafted rewards also face difficulties in gaining an overall performance boost. Therefore, we propose an Adversarial REward Learning (AREL) framework to learn an implicit reward function from human demonstrations, and then optimize policy search with the learned reward function. Though automatic eval- uation indicates slight performance boost over state-of-the-art (SOTA) methods in cloning expert behaviors, human evaluation shows that our approach achieves significant improvement in generating more human-like stories than SOTA systems.",2018-07-08,2022-03-11 00:05:39,2022-03-11 00:05:39,2022-03-11 00:05:39,,,,,,,No Metrics Are Perfect,,,,,,,,,,,,arXiv.org,,arXiv: 1804.09160,,/Users/jacquesthibodeau/Zotero/storage/RRI4D35P/Wang et al. - 2018 - No Metrics Are Perfect Adversarial Reward Learnin.pdf; /Users/jacquesthibodeau/Zotero/storage/LHRVXR7E/1804.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
28Z838EQ,journalArticle,2019,"Woodward, Mark; Finn, Chelsea; Hausman, Karol",Learning to Interactively Learn and Assist,arXiv:1906.10187 [cs],,,,http://arxiv.org/abs/1906.10187,"When deploying autonomous agents in the real world, we need effective ways of communicating objectives to them. Traditional skill learning has revolved around reinforcement and imitation learning, each with rigid constraints on the format of information exchanged between the human and the agent. While scalar rewards carry little information, demonstrations require significant effort to provide and may carry more information than is necessary. Furthermore, rewards and demonstrations are often defined and collected before training begins, when the human is most uncertain about what information would help the agent. In contrast, when humans communicate objectives with each other, they make use of a large vocabulary of informative behaviors, including non-verbal communication, and often communicate throughout learning, responding to observed behavior. In this way, humans communicate intent with minimal effort. In this paper, we propose such interactive learning as an alternative to reward or demonstration-driven learning. To accomplish this, we introduce a multi-agent training framework that enables an agent to learn from another agent who knows the current task. Through a series of experiments, we demonstrate the emergence of a variety of interactive learning behaviors, including information-sharing, information-seeking, and question-answering. Most importantly, we find that our approach produces an agent that is capable of learning interactively from a human user, without a set of explicit demonstrations or a reward function, and achieving significantly better performance cooperatively with a human than a human performing the task alone.",2019-11-19,2022-03-11 00:05:56,2022-03-11 00:05:56,2022-03-11 00:05:56,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1906.10187,,/Users/jacquesthibodeau/Zotero/storage/RUACYQZ7/Woodward et al. - 2019 - Learning to Interactively Learn and Assist.pdf; /Users/jacquesthibodeau/Zotero/storage/PCUYVP4Z/1906.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A222AIBT,journalArticle,2019,"Reddy, Siddharth; Dragan, Anca D.; Levine, Sergey",Where Do You Think You're Going?: Inferring Beliefs about Dynamics from Behavior,"arXiv:1805.08010 [cs, stat]",,,,http://arxiv.org/abs/1805.08010,"Inferring intent from observed behavior has been studied extensively within the frameworks of Bayesian inverse planning and inverse reinforcement learning. These methods infer a goal or reward function that best explains the actions of the observed agent, typically a human demonstrator. Another agent can use this inferred intent to predict, imitate, or assist the human user. However, a central assumption in inverse reinforcement learning is that the demonstrator is close to optimal. While models of suboptimal behavior exist, they typically assume that suboptimal actions are the result of some type of random noise or a known cognitive bias, like temporal inconsistency. In this paper, we take an alternative approach, and model suboptimal behavior as the result of internal model misspecification: the reason that user actions might deviate from near-optimal actions is that the user has an incorrect set of beliefs about the rules -- the dynamics -- governing how actions affect the environment. Our insight is that while demonstrated actions may be suboptimal in the real world, they may actually be near-optimal with respect to the user's internal model of the dynamics. By estimating these internal beliefs from observed behavior, we arrive at a new method for inferring intent. We demonstrate in simulation and in a user study with 12 participants that this approach enables us to more accurately model human intent, and can be used in a variety of applications, including offering assistance in a shared autonomy framework and inferring human preferences.",2019-01-05,2022-03-11 00:05:58,2022-03-11 00:05:58,2022-03-11 00:05:57,,,,,,,Where Do You Think You're Going?,,,,,,,,,,,,arXiv.org,,arXiv: 1805.08010,,/Users/jacquesthibodeau/Zotero/storage/P554XAWK/Reddy et al. - 2019 - Where Do You Think You're Going Inferring Belief.pdf; /Users/jacquesthibodeau/Zotero/storage/CBATLDJS/1805.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
D6TU5FZV,journalArticle,2019,"Perez, Ethan; Karamcheti, Siddharth; Fergus, Rob; Weston, Jason; Kiela, Douwe; Cho, Kyunghyun",Finding Generalizable Evidence by Learning to Convince Q&A Models,arXiv:1909.05863 [cs],,,,http://arxiv.org/abs/1909.05863,"We propose a system that finds the strongest supporting evidence for a given answer to a question, using passage-based question-answering (QA) as a testbed. We train evidence agents to select the passage sentences that most convince a pretrained QA model of a given answer, if the QA model received those sentences instead of the full passage. Rather than finding evidence that convinces one model alone, we find that agents select evidence that generalizes; agent-chosen evidence increases the plausibility of the supported answer, as judged by other QA models and humans. Given its general nature, this approach improves QA in a robust manner: using agent-selected evidence (i) humans can correctly answer questions with only ~20% of the full passage and (ii) QA models can generalize to longer passages and harder questions.",2019-09-12,2022-03-11 00:06:03,2022-03-11 00:06:03,2022-03-11 00:06:03,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1909.05863,,/Users/jacquesthibodeau/Zotero/storage/TDRGQLPA/Perez et al. - 2019 - Finding Generalizable Evidence by Learning to Conv.pdf; /Users/jacquesthibodeau/Zotero/storage/Z7REYBD6/1909.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Information Retrieval; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C2QJBG3W,journalArticle,2018,"Irving, Geoffrey; Christiano, Paul; Amodei, Dario",AI safety via debate,"arXiv:1805.00899 [cs, stat]",,,,http://arxiv.org/abs/1805.00899,"To make AI systems broadly useful for challenging real-world tasks, we need them to learn complex human goals and preferences. One approach to specifying complex goals asks humans to judge during training which agent behaviors are safe and useful, but this approach can fail if the task is too complicated for a human to directly judge. To help address this concern, we propose training agents via self play on a zero sum debate game. Given a question or proposed action, two agents take turns making short statements up to a limit, then a human judges which of the agents gave the most true, useful information. In an analogy to complexity theory, debate with optimal play can answer any question in PSPACE given polynomial time judges (direct judging answers only NP questions). In practice, whether debate works involves empirical questions about humans and the tasks we want AIs to perform, plus theoretical questions about the meaning of AI alignment. We report results on an initial MNIST experiment where agents compete to convince a sparse classifier, boosting the classifier's accuracy from 59.4% to 88.9% given 6 pixels and from 48.2% to 85.2% given 4 pixels. Finally, we discuss theoretical and practical aspects of the debate model, focusing on potential weaknesses as the model scales up, and we propose future human and computer experiments to test these properties.",2018-10-22,2022-03-11 00:06:23,2022-03-11 00:06:23,2022-03-11 00:06:23,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1805.00899,,/Users/jacquesthibodeau/Zotero/storage/JQH8VEZ6/Irving et al. - 2018 - AI safety via debate.pdf; /Users/jacquesthibodeau/Zotero/storage/RVH3AFRF/1805.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EKYNN4QF,journalArticle,2021,"Kovařík, Vojtěch; Carey, Ryan",(When) Is Truth-telling Favored in AI Debate?,arXiv:1911.04266 [cs],,,,http://arxiv.org/abs/1911.04266,"For some problems, humans may not be able to accurately judge the goodness of AI-proposed solutions. Irving et al. (2018) propose that in such cases, we may use a debate between two AI systems to amplify the problem-solving capabilities of a human judge. We introduce a mathematical framework that can model debates of this type and propose that the quality of debate designs should be measured by the accuracy of the most persuasive answer. We describe a simple instance of the debate framework called feature debate and analyze the degree to which such debates track the truth. We argue that despite being very simple, feature debates nonetheless capture many aspects of practical debates such as the incentives to confuse the judge or stall to prevent losing. We then outline how these models should be generalized to analyze a wider range of debate phenomena.",2021-03-16,2022-03-11 00:06:26,2022-03-11 00:06:26,2022-03-11 00:06:26,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1911.04266,,/Users/jacquesthibodeau/Zotero/storage/BQ8UNV7R/Kovařík and Carey - 2021 - (When) Is Truth-telling Favored in AI Debate.pdf; /Users/jacquesthibodeau/Zotero/storage/CVLA9QMS/1911.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PEYVH9F5,journalArticle,2020,"Perez, Ethan; Lewis, Patrick; Yih, Wen-tau; Cho, Kyunghyun; Kiela, Douwe",Unsupervised Question Decomposition for Question Answering,arXiv:2002.09758 [cs],,,,http://arxiv.org/abs/2002.09758,"We aim to improve question answering (QA) by decomposing hard questions into simpler sub-questions that existing QA systems are capable of answering. Since labeling questions with decompositions is cumbersome, we take an unsupervised approach to produce sub-questions, also enabling us to leverage millions of questions from the internet. Specifically, we propose an algorithm for One-to-N Unsupervised Sequence transduction (ONUS) that learns to map one hard, multi-hop question to many simpler, single-hop sub-questions. We answer sub-questions with an off-the-shelf QA model and give the resulting answers to a recomposition model that combines them into a final answer. We show large QA improvements on HotpotQA over a strong baseline on the original, out-of-domain, and multi-hop dev sets. ONUS automatically learns to decompose different kinds of questions, while matching the utility of supervised and heuristic decomposition methods for QA and exceeding those methods in fluency. Qualitatively, we find that using sub-questions is promising for shedding light on why a QA system makes a prediction.",2020-10-06,2022-03-11 00:06:29,2022-03-11 00:06:29,2022-03-11 00:06:29,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2002.09758,,/Users/jacquesthibodeau/Zotero/storage/MZCSI8KF/Perez et al. - 2020 - Unsupervised Question Decomposition for Question A.pdf; /Users/jacquesthibodeau/Zotero/storage/32F7CJGY/2002.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZCCMWPQX,journalArticle,2021,"Perez, Ethan; Kiela, Douwe; Cho, Kyunghyun",Rissanen Data Analysis: Examining Dataset Characteristics via Description Length,"arXiv:2103.03872 [cs, stat]",,,,http://arxiv.org/abs/2103.03872,"We introduce a method to determine if a certain capability helps to achieve an accurate model of given data. We view labels as being generated from the inputs by a program composed of subroutines with different capabilities, and we posit that a subroutine is useful if and only if the minimal program that invokes it is shorter than the one that does not. Since minimum program length is uncomputable, we instead estimate the labels' minimum description length (MDL) as a proxy, giving us a theoretically-grounded method for analyzing dataset characteristics. We call the method Rissanen Data Analysis (RDA) after the father of MDL, and we showcase its applicability on a wide variety of settings in NLP, ranging from evaluating the utility of generating subquestions before answering a question, to analyzing the value of rationales and explanations, to investigating the importance of different parts of speech, and uncovering dataset gender bias.",2021-03-05,2022-03-11 00:06:32,2022-03-11 00:06:32,2022-03-11 00:06:31,,,,,,,Rissanen Data Analysis,,,,,,,,,,,,arXiv.org,,arXiv: 2103.03872,,/Users/jacquesthibodeau/Zotero/storage/KBEK28YX/Perez et al. - 2021 - Rissanen Data Analysis Examining Dataset Characte.pdf; /Users/jacquesthibodeau/Zotero/storage/ER72NW7P/2103.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
84S7DU6T,journalArticle,2018,"Wagstaff, Kiri L.; Lee, Jake",Interpretable Discovery in Large Image Data Sets,"arXiv:1806.08340 [cs, stat]",,,,http://arxiv.org/abs/1806.08340,"Automated detection of new, interesting, unusual, or anomalous images within large data sets has great value for applications from surveillance (e.g., airport security) to science (observations that don't fit a given theory can lead to new discoveries). Many image data analysis systems are turning to convolutional neural networks (CNNs) to represent image content due to their success in achieving high classification accuracy rates. However, CNN representations are notoriously difficult for humans to interpret. We describe a new strategy that combines novelty detection with CNN image features to achieve rapid discovery with interpretable explanations of novel image content. We applied this technique to familiar images from ImageNet as well as to a scientific image collection from planetary science.",2018-06-21,2022-03-11 00:06:44,2022-03-11 00:06:44,2022-03-11 00:06:44,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1806.08340,,/Users/jacquesthibodeau/Zotero/storage/3QAIRJS9/Wagstaff and Lee - 2018 - Interpretable Discovery in Large Image Data Sets.pdf; /Users/jacquesthibodeau/Zotero/storage/HWSNIZHR/1806.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WCMPZI9A,journalArticle,2018,"Brown, Alexander; Petrik, Marek",Interpretable Reinforcement Learning with Ensemble Methods,"arXiv:1809.06995 [cs, stat]",,,,http://arxiv.org/abs/1809.06995,"We propose to use boosted regression trees as a way to compute human-interpretable solutions to reinforcement learning problems. Boosting combines several regression trees to improve their accuracy without significantly reducing their inherent interpretability. Prior work has focused independently on reinforcement learning and on interpretable machine learning, but there has been little progress in interpretable reinforcement learning. Our experimental results show that boosted regression trees compute solutions that are both interpretable and match the quality of leading reinforcement learning methods.",2018-09-18,2022-03-11 00:06:47,2022-03-11 00:06:47,2022-03-11 00:06:47,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1809.06995,,/Users/jacquesthibodeau/Zotero/storage/NJNRSBJW/Brown and Petrik - 2018 - Interpretable Reinforcement Learning with Ensemble.pdf; /Users/jacquesthibodeau/Zotero/storage/7R5ZDJNH/1809.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MC6XR8A9,journalArticle,2018,"Papernot, Nicolas; McDaniel, Patrick","Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning","arXiv:1803.04765 [cs, stat]",,,,http://arxiv.org/abs/1803.04765,"Deep neural networks (DNNs) enable innovative applications of machine learning like image recognition, machine translation, or malware detection. However, deep learning is often criticized for its lack of robustness in adversarial settings (e.g., vulnerability to adversarial inputs) and general inability to rationalize its predictions. In this work, we exploit the structure of deep learning to enable new learning-based inference and decision strategies that achieve desirable properties such as robustness and interpretability. We take a first step in this direction and introduce the Deep k-Nearest Neighbors (DkNN). This hybrid classifier combines the k-nearest neighbors algorithm with representations of the data learned by each layer of the DNN: a test input is compared to its neighboring training points according to the distance that separates them in the representations. We show the labels of these neighboring points afford confidence estimates for inputs outside the model's training manifold, including on malicious inputs like adversarial examples--and therein provides protections against inputs that are outside the models understanding. This is because the nearest neighbors can be used to estimate the nonconformity of, i.e., the lack of support for, a prediction in the training data. The neighbors also constitute human-interpretable explanations of predictions. We evaluate the DkNN algorithm on several datasets, and show the confidence estimates accurately identify inputs outside the model, and that the explanations provided by nearest neighbors are intuitive and useful in understanding model failures.",2018-03-13,2022-03-11 00:06:49,2022-03-11 00:06:49,2022-03-11 00:06:49,,,,,,,Deep k-Nearest Neighbors,,,,,,,,,,,,arXiv.org,,arXiv: 1803.04765,,"/Users/jacquesthibodeau/Zotero/storage/RJYVXNTF/Papernot and McDaniel - 2018 - Deep k-Nearest Neighbors Towards Confident, Inter.pdf; /Users/jacquesthibodeau/Zotero/storage/V3W3Q4DX/1803.html",,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AR2YI7ZB,journalArticle,2018,"Preece, Alun; Harborne, Dan; Braines, Dave; Tomsett, Richard; Chakraborty, Supriyo",Stakeholders in Explainable AI,arXiv:1810.00184 [cs],,,,http://arxiv.org/abs/1810.00184,"There is general consensus that it is important for artificial intelligence (AI) and machine learning systems to be explainable and/or interpretable. However, there is no general consensus over what is meant by 'explainable' and 'interpretable'. In this paper, we argue that this lack of consensus is due to there being several distinct stakeholder communities. We note that, while the concerns of the individual communities are broadly compatible, they are not identical, which gives rise to different intents and requirements for explainability/interpretability. We use the software engineering distinction between validation and verification, and the epistemological distinctions between knowns/unknowns, to tease apart the concerns of the stakeholder communities and highlight the areas where their foci overlap or diverge. It is not the purpose of the authors of this paper to 'take sides' - we count ourselves as members, to varying degrees, of multiple communities - but rather to help disambiguate what stakeholders mean when they ask 'Why?' of an AI.",2018-09-29,2022-03-11 00:06:52,2022-03-11 00:06:52,2022-03-11 00:06:52,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.00184,,/Users/jacquesthibodeau/Zotero/storage/DPMXNX87/Preece et al. - 2018 - Stakeholders in Explainable AI.pdf; /Users/jacquesthibodeau/Zotero/storage/I48XICL6/1810.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MMAS3LL4,journalArticle,2019,"Jain, Sarthak; Wallace, Byron C.",Attention is not Explanation,arXiv:1902.10186 [cs],,,,http://arxiv.org/abs/1902.10186,"Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work, we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful `explanations' for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do. Code for all experiments is available at https://github.com/successar/AttentionExplanation.",2019-05-08,2022-03-11 00:07:02,2022-03-11 00:07:02,2022-03-11 00:07:02,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1902.10186,,/Users/jacquesthibodeau/Zotero/storage/CPVGIC25/Jain and Wallace - 2019 - Attention is not Explanation.pdf; /Users/jacquesthibodeau/Zotero/storage/7XYFBN47/1902.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
N8HZQWPQ,journalArticle,2020,"Adebayo, Julius; Gilmer, Justin; Muelly, Michael; Goodfellow, Ian; Hardt, Moritz; Kim, Been",Sanity Checks for Saliency Maps,"arXiv:1810.03292 [cs, stat]",,,,http://arxiv.org/abs/1810.03292,"Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings.",2020-11-06,2022-03-11 00:07:02,2022-03-11 00:07:02,2022-03-11 00:07:02,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.03292,,/Users/jacquesthibodeau/Zotero/storage/WGKXZYE9/Adebayo et al. - 2020 - Sanity Checks for Saliency Maps.pdf; /Users/jacquesthibodeau/Zotero/storage/468MMA7A/1810.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JU7ZW94W,journalArticle,2019,"Fuchs, Fabian B.; Groth, Oliver; Kosiorek, Adam R.; Bewley, Alex; Wulfmeier, Markus; Vedaldi, Andrea; Posner, Ingmar",Scrutinizing and De-Biasing Intuitive Physics with Neural Stethoscopes,"arXiv:1806.05502 [cs, stat]",,,,http://arxiv.org/abs/1806.05502,"Visually predicting the stability of block towers is a popular task in the domain of intuitive physics. While previous work focusses on prediction accuracy, a one-dimensional performance measure, we provide a broader analysis of the learned physical understanding of the final model and how the learning process can be guided. To this end, we introduce neural stethoscopes as a general purpose framework for quantifying the degree of importance of specific factors of influence in deep neural networks as well as for actively promoting and suppressing information as appropriate. In doing so, we unify concepts from multitask learning as well as training with auxiliary and adversarial losses. We apply neural stethoscopes to analyse the state-of-the-art neural network for stability prediction. We show that the baseline model is susceptible to being misled by incorrect visual cues. This leads to a performance breakdown to the level of random guessing when training on scenarios where visual cues are inversely correlated with stability. Using stethoscopes to promote meaningful feature extraction increases performance from 51% to 90% prediction accuracy. Conversely, training on an easy dataset where visual cues are positively correlated with stability, the baseline model learns a bias leading to poor performance on a harder dataset. Using an adversarial stethoscope, the network is successfully de-biased, leading to a performance increase from 66% to 88%.",2019-09-06,2022-03-11 00:07:03,2022-03-11 00:07:03,2022-03-11 00:07:03,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1806.05502,,/Users/jacquesthibodeau/Zotero/storage/I4MIQ4MC/Fuchs et al. - 2019 - Scrutinizing and De-Biasing Intuitive Physics with.pdf; /Users/jacquesthibodeau/Zotero/storage/46IPAJ5C/1806.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A9R4Y5CD,journalArticle,2019,"Gilpin, Leilani H.; Bau, David; Yuan, Ben Z.; Bajwa, Ayesha; Specter, Michael; Kagal, Lalana",Explaining Explanations: An Overview of Interpretability of Machine Learning,"arXiv:1806.00069 [cs, stat]",,,,http://arxiv.org/abs/1806.00069,"There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we provide our definition of explainability and show how it can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.",2019-02-03,2022-03-11 00:07:07,2022-03-11 00:07:07,2022-03-11 00:07:07,,,,,,,Explaining Explanations,,,,,,,,,,,,arXiv.org,,arXiv: 1806.00069,,/Users/jacquesthibodeau/Zotero/storage/R5WBGP2Q/Gilpin et al. - 2019 - Explaining Explanations An Overview of Interpreta.pdf; /Users/jacquesthibodeau/Zotero/storage/LM4PFV2M/1806.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
V6PF8CJ2,journalArticle,2021,"Déletang, Grégoire; Grau-Moya, Jordi; Martic, Miljan; Genewein, Tim; McGrath, Tom; Mikulik, Vladimir; Kunesch, Markus; Legg, Shane; Ortega, Pedro A.",Causal Analysis of Agent Behavior for AI Safety,arXiv:2103.03938 [cs],,,,http://arxiv.org/abs/2103.03938,"As machine learning systems become more powerful they also become increasingly unpredictable and opaque. Yet, finding human-understandable explanations of how they work is essential for their safe deployment. This technical report illustrates a methodology for investigating the causal mechanisms that drive the behaviour of artificial agents. Six use cases are covered, each addressing a typical question an analyst might ask about an agent. In particular, we show that each question cannot be addressed by pure observation alone, but instead requires conducting experiments with systematically chosen manipulations so as to generate the correct causal evidence.",2021-03-05,2022-03-11 00:07:17,2022-03-11 00:07:17,2022-03-11 00:07:17,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2103.03938,,/Users/jacquesthibodeau/Zotero/storage/DH69WVLP/Déletang et al. - 2021 - Causal Analysis of Agent Behavior for AI Safety.pdf; /Users/jacquesthibodeau/Zotero/storage/LMQXFRKT/2103.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VUK3E2UM,journalArticle,2019,"Mindermann, Sören; Shah, Rohin; Gleave, Adam; Hadfield-Menell, Dylan",Active Inverse Reward Design,"arXiv:1809.03060 [cs, stat]",,,,http://arxiv.org/abs/1809.03060,"Designers of AI agents often iterate on the reward function in a trial-and-error process until they get the desired behavior, but this only guarantees good behavior in the training environment. We propose structuring this process as a series of queries asking the user to compare between different reward functions. Thus we can actively select queries for maximum informativeness about the true reward. In contrast to approaches asking the designer for optimal behavior, this allows us to gather additional information by eliciting preferences between suboptimal behaviors. After each query, we need to update the posterior over the true reward function from observing the proxy reward function chosen by the designer. The recently proposed Inverse Reward Design (IRD) enables this. Our approach substantially outperforms IRD in test environments. In particular, it can query the designer about interpretable, linear reward functions and still infer non-linear ones.",2019-11-06,2022-03-11 00:10:09,2022-03-11 00:10:09,2022-03-11 00:10:09,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1809.03060,,/Users/jacquesthibodeau/Zotero/storage/KU4EL7EE/Mindermann et al. - 2019 - Active Inverse Reward Design.pdf; /Users/jacquesthibodeau/Zotero/storage/QMDKRICM/1809.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AIYWN2BB,journalArticle,2018,"Xie, Annie; Singh, Avi; Levine, Sergey; Finn, Chelsea",Few-Shot Goal Inference for Visuomotor Learning and Planning,"arXiv:1810.00482 [cs, stat]",,,,http://arxiv.org/abs/1810.00482,"Reinforcement learning and planning methods require an objective or reward function that encodes the desired behavior. Yet, in practice, there is a wide range of scenarios where an objective is difficult to provide programmatically, such as tasks with visual observations involving unknown object positions or deformable objects. In these cases, prior methods use engineered problem-specific solutions, e.g., by instrumenting the environment with additional sensors to measure a proxy for the objective. Such solutions require a significant engineering effort on a per-task basis, and make it impractical for robots to continuously learn complex skills outside of laboratory settings. We aim to find a more general and scalable solution for specifying goals for robot learning in unconstrained environments. To that end, we formulate the few-shot objective learning problem, where the goal is to learn a task objective from only a few example images of successful end states for that task. We propose a simple solution to this problem: meta-learn a classifier that can recognize new goals from a few examples. We show how this approach can be used with both model-free reinforcement learning and visual model-based planning and show results in three domains: rope manipulation from images in simulation, visual navigation in a simulated 3D environment, and object arrangement into user-specified configurations on a real robot.",2018-09-30,2022-03-11 00:10:10,2022-03-11 00:10:10,2022-03-11 00:10:10,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.00482,,/Users/jacquesthibodeau/Zotero/storage/GMXYB7BF/Xie et al. - 2018 - Few-Shot Goal Inference for Visuomotor Learning an.pdf; /Users/jacquesthibodeau/Zotero/storage/3P9SGNTU/1810.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YGKAJXS5,journalArticle,2018,"Kostrikov, Ilya; Agrawal, Kumar Krishna; Dwibedi, Debidatta; Levine, Sergey; Tompson, Jonathan",Discriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning,"arXiv:1809.02925 [cs, stat]",,,,http://arxiv.org/abs/1809.02925,"We identify two issues with the family of algorithms based on the Adversarial Imitation Learning framework. The first problem is implicit bias present in the reward functions used in these algorithms. While these biases might work well for some environments, they can also lead to sub-optimal behavior in others. Secondly, even though these algorithms can learn from few expert demonstrations, they require a prohibitively large number of interactions with the environment in order to imitate the expert for many real-world applications. In order to address these issues, we propose a new algorithm called Discriminator-Actor-Critic that uses off-policy Reinforcement Learning to reduce policy-environment interaction sample complexity by an average factor of 10. Furthermore, since our reward function is designed to be unbiased, we can apply our algorithm to many problems without making any task-specific adjustments.",2018-10-15,2022-03-11 00:10:12,2022-03-11 00:10:12,2022-03-11 00:10:12,,,,,,,Discriminator-Actor-Critic,,,,,,,,,,,,arXiv.org,,arXiv: 1809.02925,,/Users/jacquesthibodeau/Zotero/storage/QSVHPY2N/Kostrikov et al. - 2018 - Discriminator-Actor-Critic Addressing Sample Inef.pdf; /Users/jacquesthibodeau/Zotero/storage/XKD9T8VY/1809.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JRKQE3RX,journalArticle,2018,"Yu, Tianhe; Abbeel, Pieter; Levine, Sergey; Finn, Chelsea",One-Shot Hierarchical Imitation Learning of Compound Visuomotor Tasks,"arXiv:1810.11043 [cs, stat]",,,,http://arxiv.org/abs/1810.11043,"We consider the problem of learning multi-stage vision-based tasks on a real robot from a single video of a human performing the task, while leveraging demonstration data of subtasks with other objects. This problem presents a number of major challenges. Video demonstrations without teleoperation are easy for humans to provide, but do not provide any direct supervision. Learning policies from raw pixels enables full generality but calls for large function approximators with many parameters to be learned. Finally, compound tasks can require impractical amounts of demonstration data, when treated as a monolithic skill. To address these challenges, we propose a method that learns both how to learn primitive behaviors from video demonstrations and how to dynamically compose these behaviors to perform multi-stage tasks by ""watching"" a human demonstrator. Our results on a simulated Sawyer robot and real PR2 robot illustrate our method for learning a variety of order fulfillment and kitchen serving tasks with novel objects and raw pixel inputs.",2018-10-25,2022-03-11 00:10:14,2022-03-11 00:10:14,2022-03-11 00:10:14,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.11043,,/Users/jacquesthibodeau/Zotero/storage/VC8LZ59T/Yu et al. - 2018 - One-Shot Hierarchical Imitation Learning of Compou.pdf; /Users/jacquesthibodeau/Zotero/storage/23QGMQJC/1810.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YUN7N5ST,journalArticle,2019,"Behbahani, Feryal; Shiarlis, Kyriacos; Chen, Xi; Kurin, Vitaly; Kasewa, Sudhanshu; Stirbu, Ciprian; Gomes, João; Paul, Supratik; Oliehoek, Frans A.; Messias, João; Whiteson, Shimon",Learning from Demonstration in the Wild,"arXiv:1811.03516 [cs, stat]",,,,http://arxiv.org/abs/1811.03516,"Learning from demonstration (LfD) is useful in settings where hand-coding behaviour or a reward function is impractical. It has succeeded in a wide range of problems but typically relies on manually generated demonstrations or specially deployed sensors and has not generally been able to leverage the copious demonstrations available in the wild: those that capture behaviours that were occurring anyway using sensors that were already deployed for another purpose, e.g., traffic camera footage capturing demonstrations of natural behaviour of vehicles, cyclists, and pedestrians. We propose Video to Behaviour (ViBe), a new approach to learn models of behaviour from unlabelled raw video data of a traffic scene collected from a single, monocular, initially uncalibrated camera with ordinary resolution. Our approach calibrates the camera, detects relevant objects, tracks them through time, and uses the resulting trajectories to perform LfD, yielding models of naturalistic behaviour. We apply ViBe to raw videos of a traffic intersection and show that it can learn purely from videos, without additional expert knowledge.",2019-03-25,2022-03-11 00:10:16,2022-03-11 00:10:16,2022-03-11 00:10:16,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1811.03516,,/Users/jacquesthibodeau/Zotero/storage/DUMMU8DE/Behbahani et al. - 2019 - Learning from Demonstration in the Wild.pdf; /Users/jacquesthibodeau/Zotero/storage/MAJVRBGG/1811.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MAVWIY5J,journalArticle,2018,"Pereira, Ramon Fraga; Meneguzzi, Felipe",Heuristic Approaches for Goal Recognition in Incomplete Domain Models,arXiv:1804.05917 [cs],,,,http://arxiv.org/abs/1804.05917,"Recent approaches to goal recognition have progressively relaxed the assumptions about the amount and correctness of domain knowledge and available observations, yielding accurate and efficient algorithms. These approaches, however, assume completeness and correctness of the domain theory against which their algorithms match observations: this is too strong for most real-world domains. In this paper, we develop goal recognition techniques that are capable of recognizing goals using \textit{incomplete} (and possibly incorrect) domain theories. We show the efficiency and accuracy of our approaches empirically against a large dataset of goal and plan recognition problems with incomplete domains.",2018-04-16,2022-03-11 00:10:21,2022-03-11 00:10:21,2022-03-11 00:10:20,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1804.05917,,/Users/jacquesthibodeau/Zotero/storage/KYUDUWV9/Pereira and Meneguzzi - 2018 - Heuristic Approaches for Goal Recognition in Incom.pdf; /Users/jacquesthibodeau/Zotero/storage/23GREC2E/1804.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5YHSY828,journalArticle,2019,"Reddy, Siddharth; Dragan, Anca D.; Levine, Sergey",SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards,"arXiv:1905.11108 [cs, stat]",,,,http://arxiv.org/abs/1905.11108,"Learning to imitate expert behavior from demonstrations can be challenging, especially in environments with high-dimensional, continuous observations and unknown dynamics. Supervised learning methods based on behavioral cloning (BC) suffer from distribution shift: because the agent greedily imitates demonstrated actions, it can drift away from demonstrated states due to error accumulation. Recent methods based on reinforcement learning (RL), such as inverse RL and generative adversarial imitation learning (GAIL), overcome this issue by training an RL agent to match the demonstrations over a long horizon. Since the true reward function for the task is unknown, these methods learn a reward function from the demonstrations, often using complex and brittle approximation techniques that involve adversarial training. We propose a simple alternative that still uses RL, but does not require learning a reward function. The key idea is to provide the agent with an incentive to match the demonstrations over a long horizon, by encouraging it to return to demonstrated states upon encountering new, out-of-distribution states. We accomplish this by giving the agent a constant reward of r=+1 for matching the demonstrated action in a demonstrated state, and a constant reward of r=0 for all other behavior. Our method, which we call soft Q imitation learning (SQIL), can be implemented with a handful of minor modifications to any standard Q-learning or off-policy actor-critic algorithm. Theoretically, we show that SQIL can be interpreted as a regularized variant of BC that uses a sparsity prior to encourage long-horizon imitation. Empirically, we show that SQIL outperforms BC and achieves competitive results compared to GAIL, on a variety of image-based and low-dimensional tasks in Box2D, Atari, and MuJoCo.",2019-09-25,2022-03-11 00:10:31,2022-03-11 00:10:31,2022-03-11 00:10:31,,,,,,,SQIL,,,,,,,,,,,,arXiv.org,,arXiv: 1905.11108,,/Users/jacquesthibodeau/Zotero/storage/FA9668CI/Reddy et al. - 2019 - SQIL Imitation Learning via Reinforcement Learnin.pdf; /Users/jacquesthibodeau/Zotero/storage/KWRQSXZM/1905.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
V8IRKMEU,journalArticle,2018,"Carey, Ryan",Incorrigibility in the CIRL Framework,arXiv:1709.06275 [cs],,,,http://arxiv.org/abs/1709.06275,"A value learning system has incentives to follow shutdown instructions, assuming the shutdown instruction provides information (in the technical sense) about which actions lead to valuable outcomes. However, this assumption is not robust to model mis-specification (e.g., in the case of programmer errors). We demonstrate this by presenting some Supervised POMDP scenarios in which errors in the parameterized reward function remove the incentive to follow shutdown commands. These difficulties parallel those discussed by Soares et al. (2015) in their paper on corrigibility. We argue that it is important to consider systems that follow shutdown commands under some weaker set of assumptions (e.g., that one small verified module is correctly implemented; as opposed to an entire prior probability distribution and/or parameterized reward function). We discuss some difficulties with simple ways to attempt to attain these sorts of guarantees in a value learning framework.",2018-06-03,2022-03-11 00:10:35,2022-03-11 00:10:35,2022-03-11 00:10:34,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1709.06275,,/Users/jacquesthibodeau/Zotero/storage/ETZHNHZN/Carey - 2018 - Incorrigibility in the CIRL Framework.pdf; /Users/jacquesthibodeau/Zotero/storage/M2YW8ZFS/1709.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DZ33MT8Q,journalArticle,2019,"Gandhi, Sunil; Oates, Tim; Mohsenin, Tinoosh; Waytowich, Nicholas",Learning from Observations Using a Single Video Demonstration and Human Feedback,"arXiv:1909.13392 [cs, stat]",,,,http://arxiv.org/abs/1909.13392,"In this paper, we present a method for learning from video demonstrations by using human feedback to construct a mapping between the standard representation of the agent and the visual representation of the demonstration. In this way, we leverage the advantages of both these representations, i.e., we learn the policy using standard state representations, but are able to specify the expected behavior using video demonstration. We train an autonomous agent using a single video demonstration and use human feedback (using numerical similarity rating) to map the standard representation to the visual representation with a neural network. We show the effectiveness of our method by teaching a hopper agent in the MuJoCo to perform a backflip using a single video demonstration generated in MuJoCo as well as from a real-world YouTube video of a person performing a backflip. Additionally, we show that our method can transfer to new tasks, such as hopping, with very little human feedback.",2019-09-29,2022-03-11 00:10:36,2022-03-11 00:10:36,2022-03-11 00:10:36,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1909.13392,,/Users/jacquesthibodeau/Zotero/storage/IW5G8AVX/Gandhi et al. - 2019 - Learning from Observations Using a Single Video De.pdf; /Users/jacquesthibodeau/Zotero/storage/H3YCXJK7/1909.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
867E786W,journalArticle,2018,"Chitnis, Rohan; Kaelbling, Leslie Pack; Lozano-Pérez, Tomás",Learning What Information to Give in Partially Observed Domains,arXiv:1805.08263 [cs],,,,http://arxiv.org/abs/1805.08263,"In many robotic applications, an autonomous agent must act within and explore a partially observed environment that is unobserved by its human teammate. We consider such a setting in which the agent can, while acting, transmit declarative information to the human that helps them understand aspects of this unseen environment. In this work, we address the algorithmic question of how the agent should plan out what actions to take and what information to transmit. Naturally, one would expect the human to have preferences, which we model information-theoretically by scoring transmitted information based on the change it induces in weighted entropy of the human's belief state. We formulate this setting as a belief MDP and give a tractable algorithm for solving it approximately. Then, we give an algorithm that allows the agent to learn the human's preferences online, through exploration. We validate our approach experimentally in simulated discrete and continuous partially observed search-and-recover domains. Visit http://tinyurl.com/chitnis-corl-18 for a supplementary video.",2018-09-27,2022-03-11 00:10:40,2022-03-11 00:10:40,2022-03-11 00:10:40,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1805.08263,,/Users/jacquesthibodeau/Zotero/storage/MMMVAIWX/Chitnis et al. - 2018 - Learning What Information to Give in Partially Obs.pdf; /Users/jacquesthibodeau/Zotero/storage/5JJ3TVCT/1805.html,,,Computer Science - Artificial Intelligence; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PQHYG3UP,journalArticle,2018,"Lee, Kyungjae; Choi, Sungjoon; Oh, Songhwai",Maximum Causal Tsallis Entropy Imitation Learning,"arXiv:1805.08336 [cs, stat]",,,,http://arxiv.org/abs/1805.08336,"In this paper, we propose a novel maximum causal Tsallis entropy (MCTE) framework for imitation learning which can efficiently learn a sparse multi-modal policy distribution from demonstrations. We provide the full mathematical analysis of the proposed framework. First, the optimal solution of an MCTE problem is shown to be a sparsemax distribution, whose supporting set can be adjusted. The proposed method has advantages over a softmax distribution in that it can exclude unnecessary actions by assigning zero probability. Second, we prove that an MCTE problem is equivalent to robust Bayes estimation in the sense of the Brier score. Third, we propose a maximum causal Tsallis entropy imitation learning (MCTEIL) algorithm with a sparse mixture density network (sparse MDN) by modeling mixture weights using a sparsemax distribution. In particular, we show that the causal Tsallis entropy of an MDN encourages exploration and efficient mixture utilization while Boltzmann Gibbs entropy is less effective. We validate the proposed method in two simulation studies and MCTEIL outperforms existing imitation learning methods in terms of average returns and learning multi-modal policies.",2018-05-28,2022-03-11 00:10:43,2022-03-11 00:10:43,2022-03-11 00:10:43,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1805.08336,,/Users/jacquesthibodeau/Zotero/storage/XQ9B8MB3/Lee et al. - 2018 - Maximum Causal Tsallis Entropy Imitation Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/RGMP4F4J/1805.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Y6UVCAKM,journalArticle,2020,"Cabi, Serkan; Colmenarejo, Sergio Gómez; Novikov, Alexander; Konyushkova, Ksenia; Reed, Scott; Jeong, Rae; Zolna, Konrad; Aytar, Yusuf; Budden, David; Vecerik, Mel; Sushkov, Oleg; Barker, David; Scholz, Jonathan; Denil, Misha; de Freitas, Nando; Wang, Ziyu",Scaling data-driven robotics with reward sketching and batch reinforcement learning,arXiv:1909.12200 [cs],,,,http://arxiv.org/abs/1909.12200,"We present a framework for data-driven robotics that makes use of a large dataset of recorded robot experience and scales to several tasks using learned reward functions. We show how to apply this framework to accomplish three different object manipulation tasks on a real robot platform. Given demonstrations of a task together with task-agnostic recorded experience, we use a special form of human annotation as supervision to learn a reward function, which enables us to deal with real-world tasks where the reward signal cannot be acquired directly. Learned rewards are used in combination with a large dataset of experience from different tasks to learn a robot policy offline using batch RL. We show that using our approach it is possible to train agents to perform a variety of challenging manipulation tasks including stacking rigid objects and handling cloth.",2020-06-04,2022-03-11 00:10:46,2022-03-11 00:10:46,2022-03-11 00:10:46,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1909.12200,,/Users/jacquesthibodeau/Zotero/storage/99B36PA2/Cabi et al. - 2020 - Scaling data-driven robotics with reward sketching.pdf; /Users/jacquesthibodeau/Zotero/storage/BWQ55G4I/1909.html,,,Computer Science - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SK4DX5JM,journalArticle,2018,"Huang, Jessie; Wu, Fa; Precup, Doina; Cai, Yang",Learning Safe Policies with Expert Guidance,"arXiv:1805.08313 [cs, stat]",,,,http://arxiv.org/abs/1805.08313,"We propose a framework for ensuring safe behavior of a reinforcement learning agent when the reward function may be difficult to specify. In order to do this, we rely on the existence of demonstrations from expert policies, and we provide a theoretical framework for the agent to optimize in the space of rewards consistent with its existing knowledge. We propose two methods to solve the resulting optimization: an exact ellipsoid-based method and a method in the spirit of the ""follow-the-perturbed-leader"" algorithm. Our experiments demonstrate the behavior of our algorithm in both discrete and continuous problems. The trained agent safely avoids states with potential negative effects while imitating the behavior of the expert in the other states.",2018-11-21,2022-03-11 00:10:48,2022-03-11 00:10:48,2022-03-11 00:10:48,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1805.08313,,/Users/jacquesthibodeau/Zotero/storage/AVY28JIX/Huang et al. - 2018 - Learning Safe Policies with Expert Guidance.pdf; /Users/jacquesthibodeau/Zotero/storage/BII4T32N/1805.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8SILYUB7,journalArticle,2020,"Milani, Stephanie; Topin, Nicholay; Houghton, Brandon; Guss, William H.; Mohanty, Sharada P.; Nakata, Keisuke; Vinyals, Oriol; Kuno, Noboru Sean",Retrospective Analysis of the 2019 MineRL Competition on Sample Efficient Reinforcement Learning,"arXiv:2003.05012 [cs, stat]",,,,http://arxiv.org/abs/2003.05012,"To facilitate research in the direction of sample efficient reinforcement learning, we held the MineRL Competition on Sample Efficient Reinforcement Learning Using Human Priors at the Thirty-third Conference on Neural Information Processing Systems (NeurIPS 2019). The primary goal of this competition was to promote the development of algorithms that use human demonstrations alongside reinforcement learning to reduce the number of samples needed to solve complex, hierarchical, and sparse environments. We describe the competition, outlining the primary challenge, the competition design, and the resources that we provided to the participants. We provide an overview of the top solutions, each of which use deep reinforcement learning and/or imitation learning. We also discuss the impact of our organizational decisions on the competition and future directions for improvement.",2020-06-18,2022-03-11 00:10:50,2022-03-11 00:10:50,2022-03-11 00:10:50,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2003.05012,,/Users/jacquesthibodeau/Zotero/storage/94GUJR7G/Milani et al. - 2020 - Retrospective Analysis of the 2019 MineRL Competit.pdf; /Users/jacquesthibodeau/Zotero/storage/DM6P7QPW/2003.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DU98KHIJ,journalArticle,2018,"Rabinowitz, Neil C.; Perbet, Frank; Song, H. Francis; Zhang, Chiyuan; Eslami, S. M. Ali; Botvinick, Matthew",Machine Theory of Mind,arXiv:1802.07740 [cs],,,,http://arxiv.org/abs/1802.07740,"Theory of mind (ToM; Premack & Woodruff, 1978) broadly refers to humans' ability to represent the mental states of others, including their desires, beliefs, and intentions. We propose to train a machine to build such models too. We design a Theory of Mind neural network -- a ToMnet -- which uses meta-learning to build models of the agents it encounters, from observations of their behaviour alone. Through this process, it acquires a strong prior model for agents' behaviour, as well as the ability to bootstrap to richer predictions about agents' characteristics and mental states using only a small number of behavioural observations. We apply the ToMnet to agents behaving in simple gridworld environments, showing that it learns to model random, algorithmic, and deep reinforcement learning agents from varied populations, and that it passes classic ToM tasks such as the ""Sally-Anne"" test (Wimmer & Perner, 1983; Baron-Cohen et al., 1985) of recognising that others can hold false beliefs about the world. We argue that this system -- which autonomously learns how to model other agents in its world -- is an important step forward for developing multi-agent AI systems, for building intermediating technology for machine-human interaction, and for advancing the progress on interpretable AI.",2018-03-12,2022-03-11 00:10:53,2022-03-11 00:10:53,2022-03-11 00:10:53,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1802.07740,,/Users/jacquesthibodeau/Zotero/storage/ME6YSGC2/Rabinowitz et al. - 2018 - Machine Theory of Mind.pdf; /Users/jacquesthibodeau/Zotero/storage/J6N2E4NE/1802.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3NWS8W55,journalArticle,2020,"Wilder, Bryan; Horvitz, Eric; Kamar, Ece",Learning to Complement Humans,arXiv:2005.00582 [cs],,,,http://arxiv.org/abs/2005.00582,"A rising vision for AI in the open world centers on the development of systems that can complement humans for perceptual, diagnostic, and reasoning tasks. To date, systems aimed at complementing the skills of people have employed models trained to be as accurate as possible in isolation. We demonstrate how an end-to-end learning strategy can be harnessed to optimize the combined performance of human-machine teams by considering the distinct abilities of people and machines. The goal is to focus machine learning on problem instances that are difficult for humans, while recognizing instances that are difficult for the machine and seeking human input on them. We demonstrate in two real-world domains (scientific discovery and medical diagnosis) that human-machine teams built via these methods outperform the individual performance of machines and people. We then analyze conditions under which this complementarity is strongest, and which training methods amplify it. Taken together, our work provides the first systematic investigation of how machine learning systems can be trained to complement human reasoning.",2020-05-01,2022-03-11 00:10:56,2022-03-11 00:10:56,2022-03-11 00:10:56,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2005.00582,,/Users/jacquesthibodeau/Zotero/storage/7DAB2XTC/Wilder et al. - 2020 - Learning to Complement Humans.pdf; /Users/jacquesthibodeau/Zotero/storage/892PQTMS/2005.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BYSZMG73,journalArticle,2020,"Brown, Daniel S.; Coleman, Russell; Srinivasan, Ravi; Niekum, Scott",Safe Imitation Learning via Fast Bayesian Reward Inference from Preferences,"arXiv:2002.09089 [cs, stat]",,,,http://arxiv.org/abs/2002.09089,"Bayesian reward learning from demonstrations enables rigorous safety and uncertainty analysis when performing imitation learning. However, Bayesian reward learning methods are typically computationally intractable for complex control problems. We propose Bayesian Reward Extrapolation (Bayesian REX), a highly efficient Bayesian reward learning algorithm that scales to high-dimensional imitation learning problems by pre-training a low-dimensional feature encoding via self-supervised tasks and then leveraging preferences over demonstrations to perform fast Bayesian inference. Bayesian REX can learn to play Atari games from demonstrations, without access to the game score and can generate 100,000 samples from the posterior over reward functions in only 5 minutes on a personal laptop. Bayesian REX also results in imitation learning performance that is competitive with or better than state-of-the-art methods that only learn point estimates of the reward function. Finally, Bayesian REX enables efficient high-confidence policy evaluation without having access to samples of the reward function. These high-confidence performance bounds can be used to rank the performance and risk of a variety of evaluation policies and provide a way to detect reward hacking behaviors.",2020-12-17,2022-03-11 00:11:04,2022-03-11 00:11:04,2022-03-11 00:11:04,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2002.09089,,/Users/jacquesthibodeau/Zotero/storage/VMC9WQPZ/Brown et al. - 2020 - Safe Imitation Learning via Fast Bayesian Reward I.pdf; /Users/jacquesthibodeau/Zotero/storage/KAYJMDCL/2002.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LYLUIQEK,journalArticle,2021,"Barde, Paul; Roy, Julien; Jeon, Wonseok; Pineau, Joelle; Pal, Christopher; Nowrouzezahrai, Derek",Adversarial Soft Advantage Fitting: Imitation Learning without Policy Optimization,"arXiv:2006.13258 [cs, stat]",,,,http://arxiv.org/abs/2006.13258,"Adversarial Imitation Learning alternates between learning a discriminator -- which tells apart expert's demonstrations from generated ones -- and a generator's policy to produce trajectories that can fool this discriminator. This alternated optimization is known to be delicate in practice since it compounds unstable adversarial training with brittle and sample-inefficient reinforcement learning. We propose to remove the burden of the policy optimization steps by leveraging a novel discriminator formulation. Specifically, our discriminator is explicitly conditioned on two policies: the one from the previous generator's iteration and a learnable policy. When optimized, this discriminator directly learns the optimal generator's policy. Consequently, our discriminator's update solves the generator's optimization problem for free: learning a policy that imitates the expert does not require an additional optimization loop. This formulation effectively cuts by half the implementation and computational burden of Adversarial Imitation Learning algorithms by removing the Reinforcement Learning phase altogether. We show on a variety of tasks that our simpler approach is competitive to prevalent Imitation Learning methods.",2021-04-16,2022-03-11 00:11:06,2022-03-11 00:11:06,2022-03-11 00:11:06,,,,,,,Adversarial Soft Advantage Fitting,,,,,,,,,,,,arXiv.org,,arXiv: 2006.13258,,/Users/jacquesthibodeau/Zotero/storage/LQRUF8T9/Barde et al. - 2021 - Adversarial Soft Advantage Fitting Imitation Lear.pdf; /Users/jacquesthibodeau/Zotero/storage/DD4ZHZN5/2006.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
I47TWHNP,journalArticle,2021,"Spencer, Jonathan; Choudhury, Sanjiban; Venkatraman, Arun; Ziebart, Brian; Bagnell, J. Andrew",Feedback in Imitation Learning: The Three Regimes of Covariate Shift,"arXiv:2102.02872 [cs, stat]",,,,http://arxiv.org/abs/2102.02872,"Imitation learning practitioners have often noted that conditioning policies on previous actions leads to a dramatic divergence between ""held out"" error and performance of the learner in situ. Interactive approaches can provably address this divergence but require repeated querying of a demonstrator. Recent work identifies this divergence as stemming from a ""causal confound"" in predicting the current action, and seek to ablate causal aspects of current state using tools from causal inference. In this work, we argue instead that this divergence is simply another manifestation of covariate shift, exacerbated particularly by settings of feedback between decisions and input features. The learner often comes to rely on features that are strongly predictive of decisions, but are subject to strong covariate shift. Our work demonstrates a broad class of problems where this shift can be mitigated, both theoretically and practically, by taking advantage of a simulator but without any further querying of expert demonstration. We analyze existing benchmarks used to test imitation learning approaches and find that these benchmarks are realizable and simple and thus insufficient for capturing the harder regimes of error compounding seen in real-world decision making problems. We find, in a surprising contrast with previous literature, but consistent with our theory, that naive behavioral cloning provides excellent results. We detail the need for new standardized benchmarks that capture the phenomena seen in robotics problems.",2021-02-11,2022-03-11 00:11:29,2022-03-11 00:11:29,2022-03-11 00:11:29,,,,,,,Feedback in Imitation Learning,,,,,,,,,,,,arXiv.org,,arXiv: 2102.02872,,/Users/jacquesthibodeau/Zotero/storage/JDID352M/Spencer et al. - 2021 - Feedback in Imitation Learning The Three Regimes .pdf; /Users/jacquesthibodeau/Zotero/storage/K4J4CEUU/2102.html,,,Computer Science - Machine Learning; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IXNKQ9YC,journalArticle,2021,"Jonnavittula, Ananth; Losey, Dylan P.",I Know What You Meant: Learning Human Objectives by (Under)estimating Their Choice Set,arXiv:2011.06118 [cs],,,,http://arxiv.org/abs/2011.06118,"Assistive robots have the potential to help people perform everyday tasks. However, these robots first need to learn what it is their user wants them to do. Teaching assistive robots is hard for inexperienced users, elderly users, and users living with physical disabilities, since often these individuals are unable to show the robot their desired behavior. We know that inclusive learners should give human teachers credit for what they cannot demonstrate. But today's robots do the opposite: they assume every user is capable of providing any demonstration. As a result, these robots learn to mimic the demonstrated behavior, even when that behavior is not what the human really meant! Here we propose a different approach to reward learning: robots that reason about the user's demonstrations in the context of similar or simpler alternatives. Unlike prior works -- which err towards overestimating the human's capabilities -- here we err towards underestimating what the human can input (i.e., their choice set). Our theoretical analysis proves that underestimating the human's choice set is risk-averse, with better worst-case performance than overestimating. We formalize three properties to generate similar and simpler alternatives. Across simulations and a user study, our resulting algorithm better extrapolates the human's objective. See the user study here: https://youtu.be/RgbH2YULVRo",2021-04-05,2022-03-11 00:11:31,2022-03-11 00:11:31,2022-03-11 00:11:31,,,,,,,I Know What You Meant,,,,,,,,,,,,arXiv.org,,arXiv: 2011.06118,,/Users/jacquesthibodeau/Zotero/storage/INGCJRKV/Jonnavittula and Losey - 2021 - I Know What You Meant Learning Human Objectives b.pdf; /Users/jacquesthibodeau/Zotero/storage/LMBKJT75/2011.html,,,Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Z5EX63YW,journalArticle,2020,"Freire, Pedro; Gleave, Adam; Toyer, Sam; Russell, Stuart",DERAIL: Diagnostic Environments for Reward And Imitation Learning,arXiv:2012.01365 [cs],,,,http://arxiv.org/abs/2012.01365,"The objective of many real-world tasks is complex and difficult to procedurally specify. This makes it necessary to use reward or imitation learning algorithms to infer a reward or policy directly from human data. Existing benchmarks for these algorithms focus on realism, testing in complex environments. Unfortunately, these benchmarks are slow, unreliable and cannot isolate failures. As a complementary approach, we develop a suite of simple diagnostic tasks that test individual facets of algorithm performance in isolation. We evaluate a range of common reward and imitation learning algorithms on our tasks. Our results confirm that algorithm performance is highly sensitive to implementation details. Moreover, in a case-study into a popular preference-based reward learning implementation, we illustrate how the suite can pinpoint design flaws and rapidly evaluate candidate solutions. The environments are available at https://github.com/HumanCompatibleAI/seals .",2020-12-02,2022-03-11 00:11:33,2022-03-11 00:11:33,2022-03-11 00:11:33,,,,,,,DERAIL,,,,,,,,,,,,arXiv.org,,arXiv: 2012.01365,,/Users/jacquesthibodeau/Zotero/storage/JDLGXBJF/Freire et al. - 2020 - DERAIL Diagnostic Environments for Reward And Imi.pdf; /Users/jacquesthibodeau/Zotero/storage/N8RTCGS9/2012.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EUJRNI9R,journalArticle,2021,"Zellers, Rowan; Holtzman, Ari; Clark, Elizabeth; Qin, Lianhui; Farhadi, Ali; Choi, Yejin",TuringAdvice: A Generative and Dynamic Evaluation of Language Use,arXiv:2004.03607 [cs],,,,http://arxiv.org/abs/2004.03607,"We propose TuringAdvice, a new challenge task and dataset for language understanding models. Given a written situation that a real person is currently facing, a model must generate helpful advice in natural language. Our evaluation framework tests a fundamental aspect of human language understanding: our ability to use language to resolve open-ended situations by communicating with each other. Empirical results show that today's models struggle at TuringAdvice, even multibillion parameter models finetuned on 600k in-domain training examples. The best model, a finetuned T5, writes advice that is at least as helpful as human-written advice in only 14% of cases; a much larger non-finetunable GPT3 model does even worse at 4%. This low performance reveals language understanding errors that are hard to spot outside of a generative setting, showing much room for progress.",2021-04-12,2022-03-11 00:11:35,2022-03-11 00:11:35,2022-03-11 00:11:35,,,,,,,TuringAdvice,,,,,,,,,,,,arXiv.org,,arXiv: 2004.03607,,/Users/jacquesthibodeau/Zotero/storage/Q6PWCR5P/Zellers et al. - 2021 - TuringAdvice A Generative and Dynamic Evaluation .pdf; /Users/jacquesthibodeau/Zotero/storage/J86WM8PA/2004.html,,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2AE4C5I5,journalArticle,2021,"Orsini, Manu; Raichuk, Anton; Hussenot, Léonard; Vincent, Damien; Dadashi, Robert; Girgin, Sertan; Geist, Matthieu; Bachem, Olivier; Pietquin, Olivier; Andrychowicz, Marcin",What Matters for Adversarial Imitation Learning?,arXiv:2106.00672 [cs],,,,http://arxiv.org/abs/2106.00672,"Adversarial imitation learning has become a popular framework for imitation in continuous control. Over the years, several variations of its components were proposed to enhance the performance of the learned policies as well as the sample complexity of the algorithm. In practice, these choices are rarely tested all together in rigorous empirical studies. It is therefore difficult to discuss and understand what choices, among the high-level algorithmic options as well as low-level implementation details, matter. To tackle this issue, we implement more than 50 of these choices in a generic adversarial imitation learning framework and investigate their impacts in a large-scale study (>500k trained agents) with both synthetic and human-generated demonstrations. While many of our findings confirm common practices, some of them are surprising or even contradict prior work. In particular, our results suggest that artificial demonstrations are not a good proxy for human data and that the very common practice of evaluating imitation algorithms only with synthetic demonstrations may lead to algorithms which perform poorly in the more realistic scenarios with human demonstrations.",2021-06-01,2022-03-11 00:11:37,2022-03-11 00:11:37,2022-03-11 00:11:37,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2106.00672,,/Users/jacquesthibodeau/Zotero/storage/MK3UABRT/Orsini et al. - 2021 - What Matters for Adversarial Imitation Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/8Z5H574Z/2106.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BYYMMKY3,journalArticle,2021,"Mandlekar, Ajay; Xu, Danfei; Wong, Josiah; Nasiriany, Soroush; Wang, Chen; Kulkarni, Rohun; Fei-Fei, Li; Savarese, Silvio; Zhu, Yuke; Martín-Martín, Roberto",What Matters in Learning from Offline Human Demonstrations for Robot Manipulation,arXiv:2108.03298 [cs],,,,http://arxiv.org/abs/2108.03298,"Imitating human demonstrations is a promising approach to endow robots with various manipulation capabilities. While recent advances have been made in imitation learning and batch (offline) reinforcement learning, a lack of open-source human datasets and reproducible learning methods make assessing the state of the field difficult. In this paper, we conduct an extensive study of six offline learning algorithms for robot manipulation on five simulated and three real-world multi-stage manipulation tasks of varying complexity, and with datasets of varying quality. Our study analyzes the most critical challenges when learning from offline human data for manipulation. Based on the study, we derive a series of lessons including the sensitivity to different algorithmic design choices, the dependence on the quality of the demonstrations, and the variability based on the stopping criteria due to the different objectives in training and evaluation. We also highlight opportunities for learning from human datasets, such as the ability to learn proficient policies on challenging, multi-stage tasks beyond the scope of current reinforcement learning methods, and the ability to easily scale to natural, real-world manipulation scenarios where only raw sensory signals are available. We have open-sourced our datasets and all algorithm implementations to facilitate future research and fair comparisons in learning from human demonstration data. Codebase, datasets, trained models, and more available at https://arise-initiative.github.io/robomimic-web/",2021-09-24,2022-03-11 00:11:40,2022-03-11 00:11:40,2022-03-11 00:11:39,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2108.03298,,/Users/jacquesthibodeau/Zotero/storage/N64P5YH4/Mandlekar et al. - 2021 - What Matters in Learning from Offline Human Demons.pdf; /Users/jacquesthibodeau/Zotero/storage/EHKXMTQY/2108.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
J9EWASK5,journalArticle,2022,"Wei, Jason; Bosma, Maarten; Zhao, Vincent Y.; Guu, Kelvin; Yu, Adams Wei; Lester, Brian; Du, Nan; Dai, Andrew M.; Le, Quoc V.",Finetuned Language Models Are Zero-Shot Learners,arXiv:2109.01652 [cs],,,,http://arxiv.org/abs/2109.01652,"This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.",2022-02-08,2022-03-11 00:11:42,2022-03-11 00:11:42,2022-03-11 00:11:42,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2109.01652,,/Users/jacquesthibodeau/Zotero/storage/HNM4R5FW/Wei et al. - 2022 - Finetuned Language Models Are Zero-Shot Learners.pdf; /Users/jacquesthibodeau/Zotero/storage/V86IZXQ3/2109.html,,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SLKSCBIT,journalArticle,2021,"Zhong, Ruiqi; Lee, Kristy; Zhang, Zheng; Klein, Dan",Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections,arXiv:2104.04670 [cs],,,,http://arxiv.org/abs/2104.04670,"Large pre-trained language models (LMs) such as GPT-3 have acquired a surprising ability to perform zero-shot learning. For example, to classify sentiment without any training examples, we can ""prompt"" the LM with the review and the label description ""Does the user like this movie?"", and ask whether the next word is ""yes"" or ""no"". However, the next word prediction training objective is still misaligned with the target zero-shot learning objective. To address this weakness, we propose meta-tuning, which directly optimizes the zero-shot learning objective by fine-tuning pre-trained language models on a collection of datasets. We focus on classification tasks, and construct the meta-dataset by aggregating 43 existing datasets and annotating 441 label descriptions in a question-answering (QA) format. When evaluated on unseen tasks, meta-tuned models outperform a same-sized QA model and the previous SOTA zero-shot learning system based on natural language inference. Additionally, increasing parameter count from 220M to 770M improves AUC-ROC scores by 6.3%, and we forecast that even larger models would perform better. Therefore, measuring zero-shot learning performance on language models out-of-the-box might underestimate their true potential, and community-wide efforts on aggregating datasets and unifying their formats can help build models that answer prompts better.",2021-09-08,2022-03-11 00:11:44,2022-03-11 00:11:44,2022-03-11 00:11:44,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2104.04670,,/Users/jacquesthibodeau/Zotero/storage/8VHEZWHD/Zhong et al. - 2021 - Adapting Language Models for Zero-shot Learning by.pdf; /Users/jacquesthibodeau/Zotero/storage/FKDT6BMJ/2104.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZNKTN89R,journalArticle,2021,"Cruz, Christian Arzate; Igarashi, Takeo",Interactive Explanations: Diagnosis and Repair of Reinforcement Learning Based Agent Behaviors,arXiv:2105.12938 [cs],,,,http://arxiv.org/abs/2105.12938,"Reinforcement learning techniques successfully generate convincing agent behaviors, but it is still difficult to tailor the behavior to align with a user's specific preferences. What is missing is a communication method for the system to explain the behavior and for the user to repair it. In this paper, we present a novel interaction method that uses interactive explanations using templates of natural language as a communication method. The main advantage of this interaction method is that it enables a two-way communication channel between users and the agent; the bot can explain its thinking procedure to the users, and the users can communicate their behavior preferences to the bot using the same interactive explanations. In this manner, the thinking procedure of the bot is transparent, and users can provide corrections to the bot that include a suggested action to take, a goal to achieve, and the reasons behind these decisions. We tested our proposed method in a clone of the video game named \textit{Super Mario Bros.}, and the results demonstrate that our interactive explanation approach is effective at diagnosing and repairing bot behaviors.",2021-05-27,2022-03-11 00:17:19,2022-03-11 00:17:19,2022-03-11 00:17:19,,,,,,,Interactive Explanations,,,,,,,,,,,,arXiv.org,,arXiv: 2105.12938,,/Users/jacquesthibodeau/Zotero/storage/WRLG2GFY/Cruz and Igarashi - 2021 - Interactive Explanations Diagnosis and Repair of .pdf; /Users/jacquesthibodeau/Zotero/storage/3LFVBSW2/2105.html,,,Computer Science - Human-Computer Interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
S56TUWLU,journalArticle,2021,"Pauly, Leo; Agboh, Wisdom C.; Hogg, David C.; Fuentes, Raul",O2A: One-shot Observational learning with Action vectors,Frontiers in Robotics and AI,,2296-9144,10.3389/frobt.2021.686368,http://arxiv.org/abs/1810.07483,"We present O2A, a novel method for learning to perform robotic manipulation tasks from a single (one-shot) third-person demonstration video. To our knowledge, it is the first time this has been done for a single demonstration. The key novelty lies in pre-training a feature extractor for creating a perceptual representation for actions that we call 'action vectors'. The action vectors are extracted using a 3D-CNN model pre-trained as an action classifier on a generic action dataset. The distance between the action vectors from the observed third-person demonstration and trial robot executions is used as a reward for reinforcement learning of the demonstrated task. We report on experiments in simulation and on a real robot, with changes in viewpoint of observation, properties of the objects involved, scene background and morphology of the manipulator between the demonstration and the learning domains. O2A outperforms baseline approaches under different domain shifts and has comparable performance with an oracle (that uses an ideal reward function).",2021-08-02,2022-03-11 00:17:21,2022-03-11 00:17:21,2022-03-11 00:17:21,686368,,,8,,Front. Robot. AI,O2A,,,,,,,,,,,,arXiv.org,,arXiv: 1810.07483,,/Users/jacquesthibodeau/Zotero/storage/IMZEK5Z7/Pauly et al. - 2021 - O2A One-shot Observational learning with Action v.pdf; /Users/jacquesthibodeau/Zotero/storage/YCQVGP4M/1810.html,,,Computer Science - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
E5CBT6GF,journalArticle,2018,"Tucker, Aaron; Gleave, Adam; Russell, Stuart",Inverse reinforcement learning for video games,"arXiv:1810.10593 [cs, stat]",,,,http://arxiv.org/abs/1810.10593,"Deep reinforcement learning achieves superhuman performance in a range of video game environments, but requires that a designer manually specify a reward function. It is often easier to provide demonstrations of a target behavior than to design a reward function describing that behavior. Inverse reinforcement learning (IRL) algorithms can infer a reward from demonstrations in low-dimensional continuous control environments, but there has been little work on applying IRL to high-dimensional video games. In our CNN-AIRL baseline, we modify the state-of-the-art adversarial IRL (AIRL) algorithm to use CNNs for the generator and discriminator. To stabilize training, we normalize the reward and increase the size of the discriminator training dataset. We additionally learn a low-dimensional state representation using a novel autoencoder architecture tuned for video game environments. This embedding is used as input to the reward network, improving the sample efficiency of expert demonstrations. Our method achieves high-level performance on the simple Catcher video game, substantially outperforming the CNN-AIRL baseline. We also score points on the Enduro Atari racing game, but do not match expert performance, highlighting the need for further work.",2018-10-24,2022-03-11 00:17:23,2022-03-11 00:17:23,2022-03-11 00:17:23,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.10593,,/Users/jacquesthibodeau/Zotero/storage/MM4MGX6Y/Tucker et al. - 2018 - Inverse reinforcement learning for video games.pdf; /Users/jacquesthibodeau/Zotero/storage/5UG6M4YY/1810.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; I.2.6; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ULFYRANH,journalArticle,2018,"Goecks, Vinicius G.; Gremillion, Gregory M.; Lawhern, Vernon J.; Valasek, John; Waytowich, Nicholas R.",Efficiently Combining Human Demonstrations and Interventions for Safe Training of Autonomous Systems in Real-Time,arXiv:1810.11545 [cs],,,,http://arxiv.org/abs/1810.11545,"This paper investigates how to utilize different forms of human interaction to safely train autonomous systems in real-time by learning from both human demonstrations and interventions. We implement two components of the Cycle-of-Learning for Autonomous Systems, which is our framework for combining multiple modalities of human interaction. The current effort employs human demonstrations to teach a desired behavior via imitation learning, then leverages intervention data to correct for undesired behaviors produced by the imitation learner to teach novel tasks to an autonomous agent safely, after only minutes of training. We demonstrate this method in an autonomous perching task using a quadrotor with continuous roll, pitch, yaw, and throttle commands and imagery captured from a downward-facing camera in a high-fidelity simulated environment. Our method improves task completion performance for the same amount of human interaction when compared to learning from demonstrations alone, while also requiring on average 32% less data to achieve that performance. This provides evidence that combining multiple modes of human interaction can increase both the training speed and overall performance of policies for autonomous systems.",2018-11-28,2022-03-11 00:17:25,2022-03-11 00:17:25,2022-03-11 00:17:25,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.11545,,/Users/jacquesthibodeau/Zotero/storage/REII9KGZ/Goecks et al. - 2018 - Efficiently Combining Human Demonstrations and Int.pdf; /Users/jacquesthibodeau/Zotero/storage/FLM6BWV2/1810.html,,,Computer Science - Artificial Intelligence; Computer Science - Human-Computer Interaction; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IIDMEW8Z,journalArticle,2019,"Angelov, Daniel; Hristov, Yordan; Ramamoorthy, Subramanian",Using Causal Analysis to Learn Specifications from Task Demonstrations,arXiv:1903.01267 [cs],,,,http://arxiv.org/abs/1903.01267,"Learning models of user behaviour is an important problem that is broadly applicable across many application domains requiring human-robot interaction. In this work we show that it is possible to learn a generative model for distinct user behavioral types, extracted from human demonstrations, by enforcing clustering of preferred task solutions within the latent space. We use this model to differentiate between user types and to find cases with overlapping solutions. Moreover, we can alter an initially guessed solution to satisfy the preferences that constitute a particular user type by backpropagating through the learned differentiable model. An advantage of structuring generative models in this way is that it allows us to extract causal relationships between symbols that might form part of the user's specification of the task, as manifested in the demonstrations. We show that the proposed method is capable of correctly distinguishing between three user types, who differ in degrees of cautiousness in their motion, while performing the task of moving objects with a kinesthetically driven robot in a tabletop environment. Our method successfully identifies the correct type, within the specified time, in 99% [97.8 - 99.8] of the cases, which outperforms an IRL baseline. We also show that our proposed method correctly changes a default trajectory to one satisfying a particular user specification even with unseen objects. The resulting trajectory is shown to be directly implementable on a PR2 humanoid robot completing the same task.",2019-03-04,2022-03-11 00:17:27,2022-03-11 00:17:27,2022-03-11 00:17:27,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1903.01267,,/Users/jacquesthibodeau/Zotero/storage/F2H9HQWW/Angelov et al. - 2019 - Using Causal Analysis to Learn Specifications from.pdf; /Users/jacquesthibodeau/Zotero/storage/H48RB47Z/1903.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LP46NSDW,journalArticle,2019,"Goyal, Prasoon; Niekum, Scott; Mooney, Raymond J.",Using Natural Language for Reward Shaping in Reinforcement Learning,"arXiv:1903.02020 [cs, stat]",,,,http://arxiv.org/abs/1903.02020,"Recent reinforcement learning (RL) approaches have shown strong performance in complex domains such as Atari games, but are often highly sample inefficient. A common approach to reduce interaction time with the environment is to use reward shaping, which involves carefully designing reward functions that provide the agent intermediate rewards for progress towards the goal. However, designing appropriate shaping rewards is known to be difficult as well as time-consuming. In this work, we address this problem by using natural language instructions to perform reward shaping. We propose the LanguagE-Action Reward Network (LEARN), a framework that maps free-form natural language instructions to intermediate rewards based on actions taken by the agent. These intermediate language-based rewards can seamlessly be integrated into any standard reinforcement learning algorithm. We experiment with Montezuma's Revenge from the Atari Learning Environment, a popular benchmark in RL. Our experiments on a diverse set of 15 tasks demonstrate that, for the same number of interactions with the environment, language-based rewards lead to successful completion of the task 60% more often on average, compared to learning without language.",2019-05-31,2022-03-11 00:17:29,2022-03-11 00:17:29,2022-03-11 00:17:29,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1903.02020,,/Users/jacquesthibodeau/Zotero/storage/RUUCJ3J6/Goyal et al. - 2019 - Using Natural Language for Reward Shaping in Reinf.pdf; /Users/jacquesthibodeau/Zotero/storage/JSWI5CYE/1903.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ML33SSRP,journalArticle,2019,"Frye, Christopher; Feige, Ilya",Parenting: Safe Reinforcement Learning from Human Input,"arXiv:1902.06766 [cs, stat]",,,,http://arxiv.org/abs/1902.06766,"Autonomous agents trained via reinforcement learning present numerous safety concerns: reward hacking, negative side effects, and unsafe exploration, among others. In the context of near-future autonomous agents, operating in environments where humans understand the existing dangers, human involvement in the learning process has proved a promising approach to AI Safety. Here we demonstrate that a precise framework for learning from human input, loosely inspired by the way humans parent children, solves a broad class of safety problems in this context. We show that our Parenting algorithm solves these problems in the relevant AI Safety gridworlds of Leike et al. (2017), that an agent can learn to outperform its parent as it ""matures"", and that policies learnt through Parenting are generalisable to new environments.",2019-02-18,2022-03-11 00:17:31,2022-03-11 00:17:31,2022-03-11 00:17:31,,,,,,,Parenting,,,,,,,,,,,,arXiv.org,,arXiv: 1902.06766,,/Users/jacquesthibodeau/Zotero/storage/Q5CMCH8C/Frye and Feige - 2019 - Parenting Safe Reinforcement Learning from Human .pdf; /Users/jacquesthibodeau/Zotero/storage/AUTFYDC3/1902.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Y8EERDM4,journalArticle,2019,"Frazier, Spencer; Riedl, Mark",Improving Deep Reinforcement Learning in Minecraft with Action Advice,"arXiv:1908.01007 [cs, stat]",,,,http://arxiv.org/abs/1908.01007,"Training deep reinforcement learning agents complex behaviors in 3D virtual environments requires significant computational resources. This is especially true in environments with high degrees of aliasing, where many states share nearly identical visual features. Minecraft is an exemplar of such an environment. We hypothesize that interactive machine learning IML, wherein human teachers play a direct role in training through demonstrations, critique, or action advice, may alleviate agent susceptibility to aliasing. However, interactive machine learning is only practical when the number of human interactions is limited, requiring a balance between human teacher effort and agent performance. We conduct experiments with two reinforcement learning algorithms which enable human teachers to give action advice, Feedback Arbitration and Newtonian Action Advice, under visual aliasing conditions. To assess potential cognitive load per advice type, we vary the accuracy and frequency of various human action advice techniques. Training efficiency, robustness against infrequent and inaccurate advisor input, and sensitivity to aliasing are examined.",2019-08-02,2022-03-11 00:17:33,2022-03-11 00:17:33,2022-03-11 00:17:33,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1908.01007,,/Users/jacquesthibodeau/Zotero/storage/TWDSZU9T/Frazier and Riedl - 2019 - Improving Deep Reinforcement Learning in Minecraft.pdf; /Users/jacquesthibodeau/Zotero/storage/AWZVQQE9/1908.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HDY6PQ7G,journalArticle,2020,"Scheller, Christian; Schraner, Yanick; Vogel, Manfred",Sample Efficient Reinforcement Learning through Learning from Demonstrations in Minecraft,"arXiv:2003.06066 [cs, stat]",,,,http://arxiv.org/abs/2003.06066,"Sample inefficiency of deep reinforcement learning methods is a major obstacle for their use in real-world applications. In this work, we show how human demonstrations can improve final performance of agents on the Minecraft minigame ObtainDiamond with only 8M frames of environment interaction. We propose a training procedure where policy networks are first trained on human data and later fine-tuned by reinforcement learning. Using a policy exploitation mechanism, experience replay and an additional loss against catastrophic forgetting, our best agent was able to achieve a mean score of 48. Our proposed solution placed 3rd in the NeurIPS MineRL Competition for Sample-Efficient Reinforcement Learning.",2020-03-12,2022-03-11 00:17:36,2022-03-11 00:17:36,2022-03-11 00:17:36,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2003.06066,,/Users/jacquesthibodeau/Zotero/storage/7768A4MV/Scheller et al. - 2020 - Sample Efficient Reinforcement Learning through Le.pdf; /Users/jacquesthibodeau/Zotero/storage/3HM5DDV9/2003.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
M7ID57CX,journalArticle,2018,"Gleave, Adam; Habryka, Oliver",Multi-task Maximum Entropy Inverse Reinforcement Learning,"arXiv:1805.08882 [cs, stat]",,,,http://arxiv.org/abs/1805.08882,"Multi-task Inverse Reinforcement Learning (IRL) is the problem of inferring multiple reward functions from expert demonstrations. Prior work, built on Bayesian IRL, is unable to scale to complex environments due to computational constraints. This paper contributes a formulation of multi-task IRL in the more computationally efficient Maximum Causal Entropy (MCE) IRL framework. Experiments show our approach can perform one-shot imitation learning in a gridworld environment that single-task IRL algorithms need hundreds of demonstrations to solve. We outline preliminary work using meta-learning to extend our method to the function approximator setting of modern MCE IRL algorithms. Evaluating on multi-task variants of common simulated robotics benchmarks, we discover serious limitations of these IRL algorithms, and conclude with suggestions for further work.",2018-07-15,2022-03-11 00:17:38,2022-03-11 00:17:38,2022-03-11 00:17:38,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1805.08882,,/Users/jacquesthibodeau/Zotero/storage/8PJGNWY2/Gleave and Habryka - 2018 - Multi-task Maximum Entropy Inverse Reinforcement L.pdf; /Users/jacquesthibodeau/Zotero/storage/MNTKHFBJ/1805.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; I.2.6; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CXWCEBI4,journalArticle,2019,"Torabi, Faraz; Warnell, Garrett; Stone, Peter",Imitation Learning from Video by Leveraging Proprioception,"arXiv:1905.09335 [cs, stat]",,,,http://arxiv.org/abs/1905.09335,"Classically, imitation learning algorithms have been developed for idealized situations, e.g., the demonstrations are often required to be collected in the exact same environment and usually include the demonstrator's actions. Recently, however, the research community has begun to address some of these shortcomings by offering algorithmic solutions that enable imitation learning from observation (IfO), e.g., learning to perform a task from visual demonstrations that may be in a different environment and do not include actions. Motivated by the fact that agents often also have access to their own internal states (i.e., proprioception), we propose and study an IfO algorithm that leverages this information in the policy learning process. The proposed architecture learns policies over proprioceptive state representations and compares the resulting trajectories visually to the demonstration data. We experimentally test the proposed technique on several MuJoCo domains and show that it outperforms other imitation from observation algorithms by a large margin.",2019-06-18,2022-03-11 00:17:40,2022-03-11 00:17:40,2022-03-11 00:17:40,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1905.09335,,/Users/jacquesthibodeau/Zotero/storage/UJV54IYL/Torabi et al. - 2019 - Imitation Learning from Video by Leveraging Propri.pdf; /Users/jacquesthibodeau/Zotero/storage/GISGDQJB/1905.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HMC4K8IP,journalArticle,2019,"Edwards, Ashley D.; Sahni, Himanshu; Schroecker, Yannick; Isbell, Charles L.",Imitating Latent Policies from Observation,"arXiv:1805.07914 [cs, stat]",,,,http://arxiv.org/abs/1805.07914,"In this paper, we describe a novel approach to imitation learning that infers latent policies directly from state observations. We introduce a method that characterizes the causal effects of latent actions on observations while simultaneously predicting their likelihood. We then outline an action alignment procedure that leverages a small amount of environment interactions to determine a mapping between the latent and real-world actions. We show that this corrected labeling can be used for imitating the observed behavior, even though no expert actions are given. We evaluate our approach within classic control environments and a platform game and demonstrate that it performs better than standard approaches. Code for this work is available at https://github.com/ashedwards/ILPO.",2019-05-13,2022-03-11 00:17:43,2022-03-11 00:17:43,2022-03-11 00:17:42,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1805.07914,,/Users/jacquesthibodeau/Zotero/storage/AB44C2PN/Edwards et al. - 2019 - Imitating Latent Policies from Observation.pdf; /Users/jacquesthibodeau/Zotero/storage/JUE4EDQQ/1805.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LJWTJ9BU,journalArticle,2018,"Malik, Dhruv; Palaniappan, Malayandi; Fisac, Jaime F.; Hadfield-Menell, Dylan; Russell, Stuart; Dragan, Anca D.","An Efficient, Generalized Bellman Update For Cooperative Inverse Reinforcement Learning",arXiv:1806.03820 [cs],,,,http://arxiv.org/abs/1806.03820,"Our goal is for AI systems to correctly identify and act according to their human user's objectives. Cooperative Inverse Reinforcement Learning (CIRL) formalizes this value alignment problem as a two-player game between a human and robot, in which only the human knows the parameters of the reward function: the robot needs to learn them as the interaction unfolds. Previous work showed that CIRL can be solved as a POMDP, but with an action space size exponential in the size of the reward parameter space. In this work, we exploit a specific property of CIRL---the human is a full information agent---to derive an optimality-preserving modification to the standard Bellman update; this reduces the complexity of the problem by an exponential factor and allows us to relax CIRL's assumption of human rationality. We apply this update to a variety of POMDP solvers and find that it enables us to scale CIRL to non-trivial problems, with larger reward parameter spaces, and larger action spaces for both robot and human. In solutions to these larger problems, the human exhibits pedagogic (teaching) behavior, while the robot interprets it as such and attains higher value for the human.",2018-06-11,2022-03-11 00:17:45,2022-03-11 00:17:45,2022-03-11 00:17:44,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1806.03820,,"/Users/jacquesthibodeau/Zotero/storage/9X3G7DLV/Malik et al. - 2018 - An Efficient, Generalized Bellman Update For Coope.pdf; /Users/jacquesthibodeau/Zotero/storage/A9AIXFSA/1806.html",,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3TFNTENG,journalArticle,2020,"Zhi-Xuan, Tan; Mann, Jordyn L.; Silver, Tom; Tenenbaum, Joshua B.; Mansinghka, Vikash K.",Online Bayesian Goal Inference for Boundedly-Rational Planning Agents,arXiv:2006.07532 [cs],,,,http://arxiv.org/abs/2006.07532,"People routinely infer the goals of others by observing their actions over time. Remarkably, we can do so even when those actions lead to failure, enabling us to assist others when we detect that they might not achieve their goals. How might we endow machines with similar capabilities? Here we present an architecture capable of inferring an agent's goals online from both optimal and non-optimal sequences of actions. Our architecture models agents as boundedly-rational planners that interleave search with execution by replanning, thereby accounting for sub-optimal behavior. These models are specified as probabilistic programs, allowing us to represent and perform efficient Bayesian inference over an agent's goals and internal planning processes. To perform such inference, we develop Sequential Inverse Plan Search (SIPS), a sequential Monte Carlo algorithm that exploits the online replanning assumption of these models, limiting computation by incrementally extending inferred plans as new actions are observed. We present experiments showing that this modeling and inference architecture outperforms Bayesian inverse reinforcement learning baselines, accurately inferring goals from both optimal and non-optimal trajectories involving failure and back-tracking, while generalizing across domains with compositional structure and sparse rewards.",2020-10-24,2022-03-11 00:17:47,2022-03-11 00:17:47,2022-03-11 00:17:46,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2006.07532,,/Users/jacquesthibodeau/Zotero/storage/GWP9QNXU/Zhi-Xuan et al. - 2020 - Online Bayesian Goal Inference for Boundedly-Ratio.pdf; /Users/jacquesthibodeau/Zotero/storage/33KMYZ46/2006.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5GFQXFZP,journalArticle,2020,"Arenz, Oleg; Neumann, Gerhard",Non-Adversarial Imitation Learning and its Connections to Adversarial Methods,"arXiv:2008.03525 [cs, math, stat]",,,,http://arxiv.org/abs/2008.03525,"Many modern methods for imitation learning and inverse reinforcement learning, such as GAIL or AIRL, are based on an adversarial formulation. These methods apply GANs to match the expert's distribution over states and actions with the implicit state-action distribution induced by the agent's policy. However, by framing imitation learning as a saddle point problem, adversarial methods can suffer from unstable optimization, and convergence can only be shown for small policy updates. We address these problems by proposing a framework for non-adversarial imitation learning. The resulting algorithms are similar to their adversarial counterparts and, thus, provide insights for adversarial imitation learning methods. Most notably, we show that AIRL is an instance of our non-adversarial formulation, which enables us to greatly simplify its derivations and obtain stronger convergence guarantees. We also show that our non-adversarial formulation can be used to derive novel algorithms by presenting a method for offline imitation learning that is inspired by the recent ValueDice algorithm, but does not rely on small policy updates for convergence. In our simulated robot experiments, our offline method for non-adversarial imitation learning seems to perform best when using many updates for policy and discriminator at each iteration and outperforms behavioral cloning and ValueDice.",2020-08-08,2022-03-11 00:17:49,2022-03-11 00:17:49,2022-03-11 00:17:48,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2008.03525,,/Users/jacquesthibodeau/Zotero/storage/8JWASMVW/Arenz and Neumann - 2020 - Non-Adversarial Imitation Learning and its Connect.pdf; /Users/jacquesthibodeau/Zotero/storage/4YUY77YE/2008.html,,,Computer Science - Information Theory; Computer Science - Machine Learning; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A8DZDHVC,journalArticle,2020,"Cui, Yuchen; Zhang, Qiping; Allievi, Alessandro; Stone, Peter; Niekum, Scott; Knox, W. Bradley",The EMPATHIC Framework for Task Learning from Implicit Human Feedback,arXiv:2009.13649 [cs],,,,http://arxiv.org/abs/2009.13649,"Reactions such as gestures, facial expressions, and vocalizations are an abundant, naturally occurring channel of information that humans provide during interactions. A robot or other agent could leverage an understanding of such implicit human feedback to improve its task performance at no cost to the human. This approach contrasts with common agent teaching methods based on demonstrations, critiques, or other guidance that need to be attentively and intentionally provided. In this paper, we first define the general problem of learning from implicit human feedback and then propose to address this problem through a novel data-driven framework, EMPATHIC. This two-stage method consists of (1) mapping implicit human feedback to relevant task statistics such as reward, optimality, and advantage; and (2) using such a mapping to learn a task. We instantiate the first stage and three second-stage evaluations of the learned mapping. To do so, we collect a dataset of human facial reactions while participants observe an agent execute a sub-optimal policy for a prescribed training task. We train a deep neural network on this data and demonstrate its ability to (1) infer relative reward ranking of events in the training task from prerecorded human facial reactions; (2) improve the policy of an agent in the training task using live human facial reactions; and (3) transfer to a novel domain in which it evaluates robot manipulation trajectories.",2020-12-07,2022-03-11 00:17:52,2022-03-11 00:17:52,2022-03-11 00:17:52,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2009.13649,,/Users/jacquesthibodeau/Zotero/storage/IZKD27SP/Cui et al. - 2020 - The EMPATHIC Framework for Task Learning from Impl.pdf; /Users/jacquesthibodeau/Zotero/storage/UD3SXIXJ/2009.html,,,Computer Science - Human-Computer Interaction; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LCXY7LCP,journalArticle,2021,"Tangkaratt, Voot; Charoenphakdee, Nontawat; Sugiyama, Masashi",Robust Imitation Learning from Noisy Demonstrations,"arXiv:2010.10181 [cs, stat]",,,,http://arxiv.org/abs/2010.10181,"Robust learning from noisy demonstrations is a practical but highly challenging problem in imitation learning. In this paper, we first theoretically show that robust imitation learning can be achieved by optimizing a classification risk with a symmetric loss. Based on this theoretical finding, we then propose a new imitation learning method that optimizes the classification risk by effectively combining pseudo-labeling with co-training. Unlike existing methods, our method does not require additional labels or strict assumptions about noise distributions. Experimental results on continuous-control benchmarks show that our method is more robust compared to state-of-the-art methods.",2021-02-19,2022-03-11 00:17:55,2022-03-11 00:17:55,2022-03-11 00:17:55,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2010.10181,,/Users/jacquesthibodeau/Zotero/storage/RRJCQUS7/Tangkaratt et al. - 2021 - Robust Imitation Learning from Noisy Demonstration.pdf; /Users/jacquesthibodeau/Zotero/storage/L7QM75RV/2010.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CCXMUU37,journalArticle,2019,"Lin, Xiaomin; Adams, Stephen C.; Beling, Peter A.",Multi-agent Inverse Reinforcement Learning for Certain General-sum Stochastic Games,Journal of Artificial Intelligence Research,,1076-9757,10.1613/jair.1.11541,http://arxiv.org/abs/1806.09795,"This paper addresses the problem of multi-agent inverse reinforcement learning (MIRL) in a two-player general-sum stochastic game framework. Five variants of MIRL are considered: uCS-MIRL, advE-MIRL, cooE-MIRL, uCE-MIRL, and uNE-MIRL, each distinguished by its solution concept. Problem uCS-MIRL is a cooperative game in which the agents employ cooperative strategies that aim to maximize the total game value. In problem uCE-MIRL, agents are assumed to follow strategies that constitute a correlated equilibrium while maximizing total game value. Problem uNE-MIRL is similar to uCE-MIRL in total game value maximization, but it is assumed that the agents are playing a Nash equilibrium. Problems advE-MIRL and cooE-MIRL assume agents are playing an adversarial equilibrium and a coordination equilibrium, respectively. We propose novel approaches to address these five problems under the assumption that the game observer either knows or is able to accurate estimate the policies and solution concepts for players. For uCS-MIRL, we first develop a characteristic set of solutions ensuring that the observed bi-policy is a uCS and then apply a Bayesian inverse learning method. For uCE-MIRL, we develop a linear programming problem subject to constraints that define necessary and sufficient conditions for the observed policies to be correlated equilibria. The objective is to choose a solution that not only minimizes the total game value difference between the observed bi-policy and a local uCS, but also maximizes the scale of the solution. We apply a similar treatment to the problem of uNE-MIRL. The remaining two problems can be solved efficiently by taking advantage of solution uniqueness and setting up a convex optimization problem. Results are validated on various benchmark grid-world games.",2019-10-15,2022-03-11 00:18:01,2022-03-11 00:18:01,2022-03-11 00:18:01,473-502,,,66,,jair,,,,,,,,,,,,,arXiv.org,,arXiv: 1806.09795,,/Users/jacquesthibodeau/Zotero/storage/WA8AARXV/Lin et al. - 2019 - Multi-agent Inverse Reinforcement Learning for Cer.pdf; /Users/jacquesthibodeau/Zotero/storage/2FWEGVEI/1806.html,,,Computer Science - Computer Science and Game Theory; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6XLBQCLB,journalArticle,2018,"Pan, Xinlei; Ohn-Bar, Eshed; Rhinehart, Nicholas; Xu, Yan; Shen, Yilin; Kitani, Kris M.",Human-Interactive Subgoal Supervision for Efficient Inverse Reinforcement Learning,arXiv:1806.08479 [cs],,,,http://arxiv.org/abs/1806.08479,"Humans are able to understand and perform complex tasks by strategically structuring the tasks into incremental steps or subgoals. For a robot attempting to learn to perform a sequential task with critical subgoal states, such states can provide a natural opportunity for interaction with a human expert. This paper analyzes the benefit of incorporating a notion of subgoals into Inverse Reinforcement Learning (IRL) with a Human-In-The-Loop (HITL) framework. The learning process is interactive, with a human expert first providing input in the form of full demonstrations along with some subgoal states. These subgoal states define a set of subtasks for the learning agent to complete in order to achieve the final goal. The learning agent queries for partial demonstrations corresponding to each subtask as needed when the agent struggles with the subtask. The proposed Human Interactive IRL (HI-IRL) framework is evaluated on several discrete path-planning tasks. We demonstrate that subgoal-based interactive structuring of the learning task results in significantly more efficient learning, requiring only a fraction of the demonstration data needed for learning the underlying reward function with the baseline IRL model.",2018-06-21,2022-03-11 00:18:03,2022-03-11 00:18:03,2022-03-11 00:18:03,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1806.08479,,/Users/jacquesthibodeau/Zotero/storage/CJLT5WVE/Pan et al. - 2018 - Human-Interactive Subgoal Supervision for Efficien.pdf; /Users/jacquesthibodeau/Zotero/storage/N8CIB62F/1806.html,,,Computer Science - Artificial Intelligence; Computer Science - Human-Computer Interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AC4PVURJ,journalArticle,2020,"Michaud, Eric J.; Gleave, Adam; Russell, Stuart",Understanding Learned Reward Functions,arXiv:2012.05862 [cs],,,,http://arxiv.org/abs/2012.05862,"In many real-world tasks, it is not possible to procedurally specify an RL agent's reward function. In such cases, a reward function must instead be learned from interacting with and observing humans. However, current techniques for reward learning may fail to produce reward functions which accurately reflect user preferences. Absent significant advances in reward learning, it is thus important to be able to audit learned reward functions to verify whether they truly capture user preferences. In this paper, we investigate techniques for interpreting learned reward functions. In particular, we apply saliency methods to identify failure modes and predict the robustness of reward functions. We find that learned reward functions often implement surprising algorithms that rely on contingent aspects of the environment. We also discover that existing interpretability techniques often attend to irrelevant changes in reward output, suggesting that reward interpretability may need significantly different methods from policy interpretability.",2020-12-10,2022-03-11 00:18:05,2022-03-11 00:18:05,2022-03-11 00:18:05,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2012.05862,,/Users/jacquesthibodeau/Zotero/storage/SE59KWID/Michaud et al. - 2020 - Understanding Learned Reward Functions.pdf; /Users/jacquesthibodeau/Zotero/storage/C9YM3G2E/2012.html,,,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9S3TH6M6,journalArticle,2021,"Abramson, Josh; Ahuja, Arun; Barr, Iain; Brussee, Arthur; Carnevale, Federico; Cassin, Mary; Chhaparia, Rachita; Clark, Stephen; Damoc, Bogdan; Dudzik, Andrew; Georgiev, Petko; Guy, Aurelia; Harley, Tim; Hill, Felix; Hung, Alden; Kenton, Zachary; Landon, Jessica; Lillicrap, Timothy; Mathewson, Kory; Mokrá, Soňa; Muldal, Alistair; Santoro, Adam; Savinov, Nikolay; Varma, Vikrant; Wayne, Greg; Williams, Duncan; Wong, Nathaniel; Yan, Chen; Zhu, Rui",Imitating Interactive Intelligence,arXiv:2012.05672 [cs],,,,http://arxiv.org/abs/2012.05672,"A common vision from science fiction is that robots will one day inhabit our physical spaces, sense the world as we do, assist our physical labours, and communicate with us through natural language. Here we study how to design artificial agents that can interact naturally with humans using the simplification of a virtual environment. This setting nevertheless integrates a number of the central challenges of artificial intelligence (AI) research: complex visual perception and goal-directed physical control, grounded language comprehension and production, and multi-agent social interaction. To build agents that can robustly interact with humans, we would ideally train them while they interact with humans. However, this is presently impractical. Therefore, we approximate the role of the human with another learned agent, and use ideas from inverse reinforcement learning to reduce the disparities between human-human and agent-agent interactive behaviour. Rigorously evaluating our agents poses a great challenge, so we develop a variety of behavioural tests, including evaluation by humans who watch videos of agents or interact directly with them. These evaluations convincingly demonstrate that interactive training and auxiliary losses improve agent behaviour beyond what is achieved by supervised learning of actions alone. Further, we demonstrate that agent capabilities generalise beyond literal experiences in the dataset. Finally, we train evaluation models whose ratings of agents agree well with human judgement, thus permitting the evaluation of new agent models without additional effort. Taken together, our results in this virtual environment provide evidence that large-scale human behavioural imitation is a promising tool to create intelligent, interactive agents, and the challenge of reliably evaluating such agents is possible to surmount.",2021-01-20,2022-03-11 00:18:07,2022-03-11 00:18:07,2022-03-11 00:18:07,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2012.05672,,/Users/jacquesthibodeau/Zotero/storage/NI36MRW2/Abramson et al. - 2021 - Imitating Interactive Intelligence.pdf; /Users/jacquesthibodeau/Zotero/storage/UWK5ZID9/2012.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AC743544,journalArticle,2021,"Sumers, Theodore R.; Ho, Mark K.; Hawkins, Robert D.; Narasimhan, Karthik; Griffiths, Thomas L.",Learning Rewards from Linguistic Feedback,arXiv:2009.14715 [cs],,,,http://arxiv.org/abs/2009.14715,"We explore unconstrained natural language feedback as a learning signal for artificial agents. Humans use rich and varied language to teach, yet most prior work on interactive learning from language assumes a particular form of input (e.g., commands). We propose a general framework which does not make this assumption, using aspect-based sentiment analysis to decompose feedback into sentiment about the features of a Markov decision process. We then perform an analogue of inverse reinforcement learning, regressing the sentiment on the features to infer the teacher's latent reward function. To evaluate our approach, we first collect a corpus of teaching behavior in a cooperative task where both teacher and learner are human. We implement three artificial learners: sentiment-based ""literal"" and ""pragmatic"" models, and an inference network trained end-to-end to predict latent rewards. We then repeat our initial experiment and pair them with human teachers. All three successfully learn from interactive human feedback. The sentiment models outperform the inference network, with the ""pragmatic"" model approaching human performance. Our work thus provides insight into the information structure of naturalistic linguistic feedback as well as methods to leverage it for reinforcement learning.",2021-07-03,2022-03-11 00:18:10,2022-03-11 00:18:10,2022-03-11 00:18:10,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2009.14715,,/Users/jacquesthibodeau/Zotero/storage/WHV6HNJG/Sumers et al. - 2021 - Learning Rewards from Linguistic Feedback.pdf; /Users/jacquesthibodeau/Zotero/storage/DXNC24JF/2009.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SNFMQHR7,journalArticle,2019,"Waytowich, Nicholas; Barton, Sean L.; Lawhern, Vernon; Warnell, Garrett",A Narration-based Reward Shaping Approach using Grounded Natural Language Commands,arXiv:1911.00497 [cs],,,,http://arxiv.org/abs/1911.00497,"While deep reinforcement learning techniques have led to agents that are successfully able to learn to perform a number of tasks that had been previously unlearnable, these techniques are still susceptible to the longstanding problem of reward sparsity. This is especially true for tasks such as training an agent to play StarCraft II, a real-time strategy game where reward is only given at the end of a game which is usually very long. While this problem can be addressed through reward shaping, such approaches typically require a human expert with specialized knowledge. Inspired by the vision of enabling reward shaping through the more-accessible paradigm of natural-language narration, we develop a technique that can provide the benefits of reward shaping using natural language commands. Our narration-guided RL agent projects sequences of natural-language commands into the same high-dimensional representation space as corresponding goal states. We show that we can get improved performance with our method compared to traditional reward-shaping approaches. Additionally, we demonstrate the ability of our method to generalize to unseen natural-language commands.",2019-10-31,2022-03-11 00:18:12,2022-03-11 00:18:12,2022-03-11 00:18:12,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1911.00497,,/Users/jacquesthibodeau/Zotero/storage/KU6QTFDC/Waytowich et al. - 2019 - A Narration-based Reward Shaping Approach using Gr.pdf; /Users/jacquesthibodeau/Zotero/storage/PKVWYY9W/1911.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EQC7KN5G,journalArticle,2021,"Eysenbach, Benjamin; Levine, Sergey; Salakhutdinov, Ruslan",Replacing Rewards with Examples: Example-Based Policy Search via Recursive Classification,arXiv:2103.12656 [cs],,,,http://arxiv.org/abs/2103.12656,"Reinforcement learning (RL) algorithms assume that users specify tasks by manually writing down a reward function. However, this process can be laborious and demands considerable technical expertise. Can we devise RL algorithms that instead enable users to specify tasks simply by providing examples of successful outcomes? In this paper, we derive a control algorithm that maximizes the future probability of these successful outcome examples. Prior work has approached similar problems with a two-stage process, first learning a reward function and then optimizing this reward function using another RL algorithm. In contrast, our method directly learns a value function from transitions and successful outcomes, without learning this intermediate reward function. Our method therefore requires fewer hyperparameters to tune and lines of code to debug. We show that our method satisfies a new data-driven Bellman equation, where examples take the place of the typical reward function term. Experiments show that our approach outperforms prior methods that learn explicit reward functions.",2021-12-30,2022-03-11 00:18:26,2022-03-11 00:18:26,2022-03-11 00:18:26,,,,,,,Replacing Rewards with Examples,,,,,,,,,,,,arXiv.org,,arXiv: 2103.12656,,/Users/jacquesthibodeau/Zotero/storage/UINBC7L3/Eysenbach et al. - 2021 - Replacing Rewards with Examples Example-Based Pol.pdf; /Users/jacquesthibodeau/Zotero/storage/AW5LK8HW/2103.html,,,Computer Science - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XYIWZSBY,journalArticle,2019,"Torabi, Faraz; Warnell, Garrett; Stone, Peter",Generative Adversarial Imitation from Observation,"arXiv:1807.06158 [cs, stat]",,,,http://arxiv.org/abs/1807.06158,"Imitation from observation (IfO) is the problem of learning directly from state-only demonstrations without having access to the demonstrator's actions. The lack of action information both distinguishes IfO from most of the literature in imitation learning, and also sets it apart as a method that may enable agents to learn from a large set of previously inapplicable resources such as internet videos. In this paper, we propose both a general framework for IfO approaches and also a new IfO approach based on generative adversarial networks called generative adversarial imitation from observation (GAIfO). We conduct experiments in two different settings: (1) when demonstrations consist of low-dimensional, manually-defined state features, and (2) when demonstrations consist of high-dimensional, raw visual data. We demonstrate that our approach performs comparably to classical imitation learning approaches (which have access to the demonstrator's actions) and significantly outperforms existing imitation from observation methods in high-dimensional simulation environments.",2019-06-18,2022-03-11 00:18:29,2022-03-11 00:18:29,2022-03-11 00:18:29,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1807.06158,,/Users/jacquesthibodeau/Zotero/storage/QXJZYI5A/Torabi et al. - 2019 - Generative Adversarial Imitation from Observation.pdf; /Users/jacquesthibodeau/Zotero/storage/ZFYDM6EY/1807.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2LDCTX9Y,journalArticle,2021,"Garg, Divyansh; Chakraborty, Shuvam; Cundy, Chris; Song, Jiaming; Ermon, Stefano",IQ-Learn: Inverse soft-Q Learning for Imitation,arXiv:2106.12142 [cs],,,,http://arxiv.org/abs/2106.12142,"In many sequential decision-making problems (e.g., robotics control, game playing, sequential prediction), human or expert data is available containing useful information about the task. However, imitation learning (IL) from a small amount of expert data can be challenging in high-dimensional environments with complex dynamics. Behavioral cloning is a simple method that is widely used due to its simplicity of implementation and stable convergence but doesn't utilize any information involving the environment's dynamics. Many existing methods that exploit dynamics information are difficult to train in practice due to an adversarial optimization process over reward and policy approximators or biased, high variance gradient estimators. We introduce a method for dynamics-aware IL which avoids adversarial training by learning a single Q-function, implicitly representing both reward and policy. On standard benchmarks, the implicitly learned rewards show a high positive correlation with the ground-truth rewards, illustrating our method can also be used for inverse reinforcement learning (IRL). Our method, Inverse soft-Q learning (IQ-Learn) obtains state-of-the-art results in offline and online imitation learning settings, significantly outperforming existing methods both in the number of required environment interactions and scalability in high-dimensional spaces, often by more than 3x.",2021-12-01,2022-03-11 00:18:39,2022-03-11 00:18:39,2022-03-11 00:18:39,,,,,,,IQ-Learn,,,,,,,,,,,,arXiv.org,,arXiv: 2106.12142,,/Users/jacquesthibodeau/Zotero/storage/C89NSUTN/Garg et al. - 2021 - IQ-Learn Inverse soft-Q Learning for Imitation.pdf; /Users/jacquesthibodeau/Zotero/storage/52JPGFUA/2106.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
H6LBTTXY,journalArticle,2019,"Tangkaratt, Voot; Han, Bo; Khan, Mohammad Emtiyaz; Sugiyama, Masashi",VILD: Variational Imitation Learning with Diverse-quality Demonstrations,"arXiv:1909.06769 [cs, stat]",,,,http://arxiv.org/abs/1909.06769,"The goal of imitation learning (IL) is to learn a good policy from high-quality demonstrations. However, the quality of demonstrations in reality can be diverse, since it is easier and cheaper to collect demonstrations from a mix of experts and amateurs. IL in such situations can be challenging, especially when the level of demonstrators' expertise is unknown. We propose a new IL method called \underline{v}ariational \underline{i}mitation \underline{l}earning with \underline{d}iverse-quality demonstrations (VILD), where we explicitly model the level of demonstrators' expertise with a probabilistic graphical model and estimate it along with a reward function. We show that a naive approach to estimation is not suitable to large state and action spaces, and fix its issues by using a variational approach which can be easily implemented using existing reinforcement learning methods. Experiments on continuous-control benchmarks demonstrate that VILD outperforms state-of-the-art methods. Our work enables scalable and data-efficient IL under more realistic settings than before.",2019-09-15,2022-03-11 00:18:40,2022-03-11 00:18:40,2022-03-11 00:18:40,,,,,,,VILD,,,,,,,,,,,,arXiv.org,,arXiv: 1909.06769,,/Users/jacquesthibodeau/Zotero/storage/TY56XKN5/Tangkaratt et al. - 2019 - VILD Variational Imitation Learning with Diverse-.pdf; /Users/jacquesthibodeau/Zotero/storage/KBPTSQ7T/1909.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C2T527W5,journalArticle,2021,"Chan, Lawrence; Critch, Andrew; Dragan, Anca",Human irrationality: both bad and good for reward inference,arXiv:2111.06956 [cs],,,,http://arxiv.org/abs/2111.06956,"Assuming humans are (approximately) rational enables robots to infer reward functions by observing human behavior. But people exhibit a wide array of irrationalities, and our goal with this work is to better understand the effect they can have on reward inference. The challenge with studying this effect is that there are many types of irrationality, with varying degrees of mathematical formalization. We thus operationalize irrationality in the language of MDPs, by altering the Bellman optimality equation, and use this framework to study how these alterations would affect inference. We find that wrongly modeling a systematically irrational human as noisy-rational performs a lot worse than correctly capturing these biases -- so much so that it can be better to skip inference altogether and stick to the prior! More importantly, we show that an irrational human, when correctly modelled, can communicate more information about the reward than a perfectly rational human can. That is, if a robot has the correct model of a human's irrationality, it can make an even stronger inference than it ever could if the human were rational. Irrationality fundamentally helps rather than hinder reward inference, but it needs to be correctly accounted for.",2021-11-12,2022-03-11 00:18:43,2022-03-11 00:18:43,2022-03-11 00:18:43,,,,,,,Human irrationality,,,,,,,,,,,,arXiv.org,,arXiv: 2111.06956,,/Users/jacquesthibodeau/Zotero/storage/IZGJ4DLM/Chan et al. - 2021 - Human irrationality both bad and good for reward .pdf; /Users/jacquesthibodeau/Zotero/storage/QWMBWI3B/2111.html,,,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CR38PYPP,journalArticle,2019,"Gencoglu, Oguzhan; van Gils, Mark; Guldogan, Esin; Morikawa, Chamin; Süzen, Mehmet; Gruber, Mathias; Leinonen, Jussi; Huttunen, Heikki",HARK Side of Deep Learning -- From Grad Student Descent to Automated Machine Learning,arXiv:1904.07633 [cs],,,,http://arxiv.org/abs/1904.07633,"Recent advancements in machine learning research, i.e., deep learning, introduced methods that excel conventional algorithms as well as humans in several complex tasks, ranging from detection of objects in images and speech recognition to playing difficult strategic games. However, the current methodology of machine learning research and consequently, implementations of the real-world applications of such algorithms, seems to have a recurring HARKing (Hypothesizing After the Results are Known) issue. In this work, we elaborate on the algorithmic, economic and social reasons and consequences of this phenomenon. We present examples from current common practices of conducting machine learning research (e.g. avoidance of reporting negative results) and failure of generalization ability of the proposed algorithms and datasets in actual real-life usage. Furthermore, a potential future trajectory of machine learning research and development from the perspective of accountable, unbiased, ethical and privacy-aware algorithmic decision making is discussed. We would like to emphasize that with this discussion we neither claim to provide an exhaustive argumentation nor blame any specific institution or individual on the raised issues. This is simply a discussion put forth by us, insiders of the machine learning field, reflecting on us.",2019-04-16,2022-03-11 00:21:21,2022-03-11 01:38:52,2022-03-11 00:21:21,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1904.07633,,/Users/jacquesthibodeau/Zotero/storage/3ING4DCQ/Gencoglu et al. - 2019 - HARK Side of Deep Learning -- From Grad Student De.pdf; /Users/jacquesthibodeau/Zotero/storage/5EAG7L9J/Gencoglu et al. - 2019 - HARK Side of Deep Learning -- From Grad Student De.pdf; /Users/jacquesthibodeau/Zotero/storage/MNGUN86J/1904.html; /Users/jacquesthibodeau/Zotero/storage/7T2L4ZTU/1904.html,,,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
APDKJF8Y,journalArticle,2020,"Clune, Jeff","AI-GAs: AI-generating algorithms, an alternate paradigm for producing general artificial intelligence",arXiv:1905.10985 [cs],,,,http://arxiv.org/abs/1905.10985,"Perhaps the most ambitious scientific quest in human history is the creation of general artificial intelligence, which roughly means AI that is as smart or smarter than humans. The dominant approach in the machine learning community is to attempt to discover each of the pieces required for intelligence, with the implicit assumption that some future group will complete the Herculean task of figuring out how to combine all of those pieces into a complex thinking machine. I call this the ""manual AI approach"". This paper describes another exciting path that ultimately may be more successful at producing general AI. It is based on the clear trend in machine learning that hand-designed solutions eventually are replaced by more effective, learned solutions. The idea is to create an AI-generating algorithm (AI-GA), which automatically learns how to produce general AI. Three Pillars are essential for the approach: (1) meta-learning architectures, (2) meta-learning the learning algorithms themselves, and (3) generating effective learning environments. I argue that either approach could produce general AI first, and both are scientifically worthwhile irrespective of which is the fastest path. Because both are promising, yet the ML community is currently committed to the manual approach, I argue that our community should increase its research investment in the AI-GA approach. To encourage such research, I describe promising work in each of the Three Pillars. I also discuss AI-GA-specific safety and ethical considerations. Because it it may be the fastest path to general AI and because it is inherently scientifically interesting to understand the conditions in which a simple algorithm can produce general AI (as happened on Earth where Darwinian evolution produced human intelligence), I argue that the pursuit of AI-GAs should be considered a new grand challenge of computer science research.",2020-01-31,2022-03-11 00:21:25,2022-03-11 00:21:25,2022-03-11 00:21:25,,,,,,,AI-GAs,,,,,,,,,,,,arXiv.org,,arXiv: 1905.10985,,"/Users/jacquesthibodeau/Zotero/storage/FCYKE3CS/Clune - 2020 - AI-GAs AI-generating algorithms, an alternate par.pdf; /Users/jacquesthibodeau/Zotero/storage/WVHB7TJ2/1905.html",,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WPLL8LEW,journalArticle,2019,"Rusu, Andrei A.; Rao, Dushyant; Sygnowski, Jakub; Vinyals, Oriol; Pascanu, Razvan; Osindero, Simon; Hadsell, Raia",Meta-Learning with Latent Embedding Optimization,"arXiv:1807.05960 [cs, stat]",,,,http://arxiv.org/abs/1807.05960,"Gradient-based meta-learning techniques are both widely applicable and proficient at solving challenging few-shot learning and fast adaptation problems. However, they have practical difficulties when operating on high-dimensional parameter spaces in extreme low-data regimes. We show that it is possible to bypass these limitations by learning a data-dependent latent generative representation of model parameters, and performing gradient-based meta-learning in this low-dimensional latent space. The resulting approach, latent embedding optimization (LEO), decouples the gradient-based adaptation procedure from the underlying high-dimensional space of model parameters. Our evaluation shows that LEO can achieve state-of-the-art performance on the competitive miniImageNet and tieredImageNet few-shot classification tasks. Further analysis indicates LEO is able to capture uncertainty in the data, and can perform adaptation more effectively by optimizing in latent space.",2019-03-26,2022-03-11 00:21:27,2022-03-11 00:21:27,2022-03-11 00:21:27,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1807.05960,,/Users/jacquesthibodeau/Zotero/storage/9LN7MYIK/Rusu et al. - 2019 - Meta-Learning with Latent Embedding Optimization.pdf; /Users/jacquesthibodeau/Zotero/storage/GQMZXF35/1807.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KFXRLHC9,journalArticle,2018,"Chen, Boyu; Lu, Wenlian; Fokoue, Ernest",Meta-Learning with Hessian-Free Approach in Deep Neural Nets Training,"arXiv:1805.08462 [cs, stat]",,,,http://arxiv.org/abs/1805.08462,"Meta-learning is a promising method to achieve efficient training method towards deep neural net and has been attracting increases interests in recent years. But most of the current methods are still not capable to train complex neuron net model with long-time training process. In this paper, a novel second-order meta-optimizer, named Meta-learning with Hessian-Free(MLHF) approach, is proposed based on the Hessian-Free approach. Two recurrent neural networks are established to generate the damping and the precondition matrix of this Hessian-Free framework. A series of techniques to meta-train the MLHF towards stable and reinforce the meta-training of this optimizer, including the gradient calculation of $H$. Numerical experiments on deep convolution neural nets, including CUDA-convnet and ResNet18(v2), with datasets of CIFAR10 and ILSVRC2012, indicate that the MLHF shows good and continuous training performance during the whole long-time training process, i.e., both the rapid-decreasing early stage and the steadily-deceasing later stage, and so is a promising meta-learning framework towards elevating the training efficiency in real-world deep neural nets.",2018-09-07,2022-03-11 00:21:29,2022-03-11 00:21:29,2022-03-11 00:21:29,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1805.08462,,/Users/jacquesthibodeau/Zotero/storage/RBLZTRU2/Chen et al. - 2018 - Meta-Learning with Hessian-Free Approach in Deep N.pdf; /Users/jacquesthibodeau/Zotero/storage/IF8GBRQK/1805.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QSNVCFBH,journalArticle,2020,"Yu, Tianhe; Kumar, Saurabh; Gupta, Abhishek; Levine, Sergey; Hausman, Karol; Finn, Chelsea",Gradient Surgery for Multi-Task Learning,"arXiv:2001.06782 [cs, stat]",,,,http://arxiv.org/abs/2001.06782,"While deep learning and deep reinforcement learning (RL) systems have demonstrated impressive results in domains such as image classification, game playing, and robotic control, data efficiency remains a major challenge. Multi-task learning has emerged as a promising approach for sharing structure across multiple tasks to enable more efficient learning. However, the multi-task setting presents a number of optimization challenges, making it difficult to realize large efficiency gains compared to learning tasks independently. The reasons why multi-task learning is so challenging compared to single-task learning are not fully understood. In this work, we identify a set of three conditions of the multi-task optimization landscape that cause detrimental gradient interference, and develop a simple yet general approach for avoiding such interference between task gradients. We propose a form of gradient surgery that projects a task's gradient onto the normal plane of the gradient of any other task that has a conflicting gradient. On a series of challenging multi-task supervised and multi-task RL problems, this approach leads to substantial gains in efficiency and performance. Further, it is model-agnostic and can be combined with previously-proposed multi-task architectures for enhanced performance.",2020-12-21,2022-03-11 00:21:30,2022-03-11 00:21:30,2022-03-11 00:21:30,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2001.06782,,/Users/jacquesthibodeau/Zotero/storage/LBQPTYYJ/Yu et al. - 2020 - Gradient Surgery for Multi-Task Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/VYYYS3FF/2001.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QJY58CHM,journalArticle,2019,"Ortega, Pedro A.; Wang, Jane X.; Rowland, Mark; Genewein, Tim; Kurth-Nelson, Zeb; Pascanu, Razvan; Heess, Nicolas; Veness, Joel; Pritzel, Alex; Sprechmann, Pablo; Jayakumar, Siddhant M.; McGrath, Tom; Miller, Kevin; Azar, Mohammad; Osband, Ian; Rabinowitz, Neil; György, András; Chiappa, Silvia; Osindero, Simon; Teh, Yee Whye; van Hasselt, Hado; de Freitas, Nando; Botvinick, Matthew; Legg, Shane",Meta-learning of Sequential Strategies,"arXiv:1905.03030 [cs, stat]",,,,http://arxiv.org/abs/1905.03030,"In this report we review memory-based meta-learning as a tool for building sample-efficient strategies that learn from past experience to adapt to any task within a target class. Our goal is to equip the reader with the conceptual foundations of this tool for building new, scalable agents that operate on broad domains. To do so, we present basic algorithmic templates for building near-optimal predictors and reinforcement learners which behave as if they had a probabilistic model that allowed them to efficiently exploit task structure. Furthermore, we recast memory-based meta-learning within a Bayesian framework, showing that the meta-learned strategies are near-optimal because they amortize Bayes-filtered data, where the adaptation is implemented in the memory dynamics as a state-machine of sufficient statistics. Essentially, memory-based meta-learning translates the hard problem of probabilistic sequential inference into a regression problem.",2019-07-18,2022-03-11 00:21:32,2022-03-11 00:21:32,2022-03-11 00:21:32,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1905.03030,,/Users/jacquesthibodeau/Zotero/storage/U8DBK45M/Ortega et al. - 2019 - Meta-learning of Sequential Strategies.pdf; /Users/jacquesthibodeau/Zotero/storage/MR89SNXK/1905.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DK8Q24FZ,journalArticle,2019,"Zintgraf, Luisa M.; Shiarlis, Kyriacos; Kurin, Vitaly; Hofmann, Katja; Whiteson, Shimon",Fast Context Adaptation via Meta-Learning,"arXiv:1810.03642 [cs, stat]",,,,http://arxiv.org/abs/1810.03642,"We propose CAVIA for meta-learning, a simple extension to MAML that is less prone to meta-overfitting, easier to parallelise, and more interpretable. CAVIA partitions the model parameters into two parts: context parameters that serve as additional input to the model and are adapted on individual tasks, and shared parameters that are meta-trained and shared across tasks. At test time, only the context parameters are updated, leading to a low-dimensional task representation. We show empirically that CAVIA outperforms MAML for regression, classification, and reinforcement learning. Our experiments also highlight weaknesses in current benchmarks, in that the amount of adaptation needed in some cases is small.",2019-06-10,2022-03-11 00:21:35,2022-03-11 00:21:35,2022-03-11 00:21:34,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.03642,,/Users/jacquesthibodeau/Zotero/storage/7LT489CU/Zintgraf et al. - 2019 - Fast Context Adaptation via Meta-Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/Q6JC3GJ9/1810.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UHEV394W,journalArticle,2019,"Rabinowitz, Neil C.",Meta-learners' learning dynamics are unlike learners',"arXiv:1905.01320 [cs, stat]",,,,http://arxiv.org/abs/1905.01320,"Meta-learning is a tool that allows us to build sample-efficient learning systems. Here we show that, once meta-trained, LSTM Meta-Learners aren't just faster learners than their sample-inefficient deep learning (DL) and reinforcement learning (RL) brethren, but that they actually pursue fundamentally different learning trajectories. We study their learning dynamics on three sets of structured tasks for which the corresponding learning dynamics of DL and RL systems have been previously described: linear regression (Saxe et al., 2013), nonlinear regression (Rahaman et al., 2018; Xu et al., 2018), and contextual bandits (Schaul et al., 2019). In each case, while sample-inefficient DL and RL Learners uncover the task structure in a staggered manner, meta-trained LSTM Meta-Learners uncover almost all task structure concurrently, congruent with the patterns expected from Bayes-optimal inference algorithms. This has implications for research areas wherever the learning behaviour itself is of interest, such as safety, curriculum design, and human-in-the-loop machine learning.",2019-05-03,2022-03-11 00:21:36,2022-03-11 00:21:36,2022-03-11 00:21:36,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1905.01320,,/Users/jacquesthibodeau/Zotero/storage/4MDPU3UD/Rabinowitz - 2019 - Meta-learners' learning dynamics are unlike learne.pdf; /Users/jacquesthibodeau/Zotero/storage/VFPVJYMS/1905.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QNDLM3TU,journalArticle,2018,"Jamal, Muhammad Abdullah; Qi, Guo-Jun; Shah, Mubarak",Task-Agnostic Meta-Learning for Few-shot Learning,"arXiv:1805.07722 [cs, stat]",,,,http://arxiv.org/abs/1805.07722,"Meta-learning approaches have been proposed to tackle the few-shot learning problem.Typically, a meta-learner is trained on a variety of tasks in the hopes of being generalizable to new tasks. However, the generalizability on new tasks of a meta-learner could be fragile when it is over-trained on existing tasks during meta-training phase. In other words, the initial model of a meta-learner could be too biased towards existing tasks to adapt to new tasks, especially when only very few examples are available to update the model. To avoid a biased meta-learner and improve its generalizability, we propose a novel paradigm of Task-Agnostic Meta-Learning (TAML) algorithms. Specifically, we present an entropy-based approach that meta-learns an unbiased initial model with the largest uncertainty over the output labels by preventing it from over-performing in classification tasks. Alternatively, a more general inequality-minimization TAML is presented for more ubiquitous scenarios by directly minimizing the inequality of initial losses beyond the classification tasks wherever a suitable loss can be defined.Experiments on benchmarked datasets demonstrate that the proposed approaches outperform compared meta-learning algorithms in both few-shot classification and reinforcement learning tasks.",2018-05-20,2022-03-11 00:21:38,2022-03-11 00:21:38,2022-03-11 00:21:38,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1805.07722,,/Users/jacquesthibodeau/Zotero/storage/MCPE5Y7C/Jamal et al. - 2018 - Task-Agnostic Meta-Learning for Few-shot Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/JLUVXKGM/1805.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5GQRHWJI,journalArticle,2020,"Yin, Mingzhang; Tucker, George; Zhou, Mingyuan; Levine, Sergey; Finn, Chelsea",Meta-Learning without Memorization,"arXiv:1912.03820 [cs, stat]",,,,http://arxiv.org/abs/1912.03820,"The ability to learn new concepts with small amounts of data is a critical aspect of intelligence that has proven challenging for deep learning methods. Meta-learning has emerged as a promising technique for leveraging data from previous tasks to enable efficient learning of new tasks. However, most meta-learning algorithms implicitly require that the meta-training tasks be mutually-exclusive, such that no single model can solve all of the tasks at once. For example, when creating tasks for few-shot image classification, prior work uses a per-task random assignment of image classes to N-way classification labels. If this is not done, the meta-learner can ignore the task training data and learn a single model that performs all of the meta-training tasks zero-shot, but does not adapt effectively to new image classes. This requirement means that the user must take great care in designing the tasks, for example by shuffling labels or removing task identifying information from the inputs. In some domains, this makes meta-learning entirely inapplicable. In this paper, we address this challenge by designing a meta-regularization objective using information theory that places precedence on data-driven adaptation. This causes the meta-learner to decide what must be learned from the task training data and what should be inferred from the task testing input. By doing so, our algorithm can successfully use data from non-mutually-exclusive tasks to efficiently adapt to novel tasks. We demonstrate its applicability to both contextual and gradient-based meta-learning algorithms, and apply it in practical settings where applying standard meta-learning has been difficult. Our approach substantially outperforms standard meta-learning algorithms in these settings.",2020-04-27,2022-03-11 00:21:41,2022-03-11 00:21:41,2022-03-11 00:21:40,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1912.03820,,/Users/jacquesthibodeau/Zotero/storage/ANSZTYKA/Yin et al. - 2020 - Meta-Learning without Memorization.pdf; /Users/jacquesthibodeau/Zotero/storage/AU3TIN42/1912.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A78INE69,journalArticle,2020,"Beaulieu, Shawn; Frati, Lapo; Miconi, Thomas; Lehman, Joel; Stanley, Kenneth O.; Clune, Jeff; Cheney, Nick",Learning to Continually Learn,"arXiv:2002.09571 [cs, stat]",,,,http://arxiv.org/abs/2002.09571,"Continual lifelong learning requires an agent or model to learn many sequentially ordered tasks, building on previous knowledge without catastrophically forgetting it. Much work has gone towards preventing the default tendency of machine learning models to catastrophically forget, yet virtually all such work involves manually-designed solutions to the problem. We instead advocate meta-learning a solution to catastrophic forgetting, allowing AI to learn to continually learn. Inspired by neuromodulatory processes in the brain, we propose A Neuromodulated Meta-Learning Algorithm (ANML). It differentiates through a sequential learning process to meta-learn an activation-gating function that enables context-dependent selective activation within a deep neural network. Specifically, a neuromodulatory (NM) neural network gates the forward pass of another (otherwise normal) neural network called the prediction learning network (PLN). The NM network also thus indirectly controls selective plasticity (i.e. the backward pass of) the PLN. ANML enables continual learning without catastrophic forgetting at scale: it produces state-of-the-art continual learning performance, sequentially learning as many as 600 classes (over 9,000 SGD updates).",2020-03-03,2022-03-11 00:21:44,2022-03-11 00:21:44,2022-03-11 00:21:44,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2002.09571,,/Users/jacquesthibodeau/Zotero/storage/FYJKSKG8/Beaulieu et al. - 2020 - Learning to Continually Learn.pdf; /Users/jacquesthibodeau/Zotero/storage/UGAND2BI/2002.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
T8ZI6LQM,journalArticle,2020,"Triantafillou, Eleni; Zhu, Tyler; Dumoulin, Vincent; Lamblin, Pascal; Evci, Utku; Xu, Kelvin; Goroshin, Ross; Gelada, Carles; Swersky, Kevin; Manzagol, Pierre-Antoine; Larochelle, Hugo",Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples,"arXiv:1903.03096 [cs, stat]",,,,http://arxiv.org/abs/1903.03096,"Few-shot classification refers to learning a classifier for new classes given only a few examples. While a plethora of models have emerged to tackle it, we find the procedure and datasets that are used to assess their progress lacking. To address this limitation, we propose Meta-Dataset: a new benchmark for training and evaluating models that is large-scale, consists of diverse datasets, and presents more realistic tasks. We experiment with popular baselines and meta-learners on Meta-Dataset, along with a competitive method that we propose. We analyze performance as a function of various characteristics of test tasks and examine the models' ability to leverage diverse training sources for improving their generalization. We also propose a new set of baselines for quantifying the benefit of meta-learning in Meta-Dataset. Our extensive experimentation has uncovered important research challenges and we hope to inspire work in these directions.",2020-04-08,2022-03-11 00:21:46,2022-03-11 00:21:46,2022-03-11 00:21:46,,,,,,,Meta-Dataset,,,,,,,,,,,,arXiv.org,,arXiv: 1903.03096,,/Users/jacquesthibodeau/Zotero/storage/KUFEKE6U/Triantafillou et al. - 2020 - Meta-Dataset A Dataset of Datasets for Learning t.pdf; /Users/jacquesthibodeau/Zotero/storage/9T7R8TYP/1903.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7M9Y82F4,journalArticle,2021,"Yu, Tianhe; Quillen, Deirdre; He, Zhanpeng; Julian, Ryan; Narayan, Avnish; Shively, Hayden; Bellathur, Adithya; Hausman, Karol; Finn, Chelsea; Levine, Sergey",Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning,"arXiv:1910.10897 [cs, stat]",,,,http://arxiv.org/abs/1910.10897,"Meta-reinforcement learning algorithms can enable robots to acquire new skills much more quickly, by leveraging prior experience to learn how to learn. However, much of the current research on meta-reinforcement learning focuses on task distributions that are very narrow. For example, a commonly used meta-reinforcement learning benchmark uses different running velocities for a simulated robot as different tasks. When policies are meta-trained on such narrow task distributions, they cannot possibly generalize to more quickly acquire entirely new tasks. Therefore, if the aim of these methods is to enable faster acquisition of entirely new behaviors, we must evaluate them on task distributions that are sufficiently broad to enable generalization to new behaviors. In this paper, we propose an open-source simulated benchmark for meta-reinforcement learning and multi-task learning consisting of 50 distinct robotic manipulation tasks. Our aim is to make it possible to develop algorithms that generalize to accelerate the acquisition of entirely new, held-out tasks. We evaluate 7 state-of-the-art meta-reinforcement learning and multi-task learning algorithms on these tasks. Surprisingly, while each task and its variations (e.g., with different object positions) can be learned with reasonable success, these algorithms struggle to learn with multiple tasks at the same time, even with as few as ten distinct training tasks. Our analysis and open-source environments pave the way for future research in multi-task learning and meta-learning that can enable meaningful generalization, thereby unlocking the full potential of these methods.",2021-06-14,2022-03-11 00:21:48,2022-03-11 00:21:48,2022-03-11 00:21:48,,,,,,,Meta-World,,,,,,,,,,,,arXiv.org,,arXiv: 1910.10897,,/Users/jacquesthibodeau/Zotero/storage/AKSAE5M2/Yu et al. - 2021 - Meta-World A Benchmark and Evaluation for Multi-T.pdf; /Users/jacquesthibodeau/Zotero/storage/IJNB5JED/1910.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
75NGI4UC,journalArticle,2021,"Koch, Jack; Langosco, Lauro; Pfau, Jacob; Le, James; Sharkey, Lee",Objective Robustness in Deep Reinforcement Learning,arXiv:2105.14111 [cs],,,,http://arxiv.org/abs/2105.14111,"We study objective robustness failures, a type of out-of-distribution robustness failure in reinforcement learning (RL). Objective robustness failures occur when an RL agent retains its capabilities out-of-distribution yet pursues the wrong objective. This kind of failure presents different risks than the robustness problems usually considered in the literature, since it involves agents that leverage their capabilities to pursue the wrong objective rather than simply failing to do anything useful. We provide the first explicit empirical demonstrations of objective robustness failures and present a partial characterization of its causes.",2021-06-08,2022-03-11 00:22:24,2022-03-11 00:22:24,2022-03-11 00:22:24,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2105.14111,,/Users/jacquesthibodeau/Zotero/storage/5GZB85I8/Koch et al. - 2021 - Objective Robustness in Deep Reinforcement Learnin.pdf; /Users/jacquesthibodeau/Zotero/storage/4IYP7S7S/2105.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AEYFC5S4,journalArticle,2018,"Baudart, Guillaume; Hirzel, Martin; Mandel, Louis",Deep Probabilistic Programming Languages: A Qualitative Study,arXiv:1804.06458 [cs],,,,http://arxiv.org/abs/1804.06458,"Deep probabilistic programming languages try to combine the advantages of deep learning with those of probabilistic programming languages. If successful, this would be a big step forward in machine learning and programming languages. Unfortunately, as of now, this new crop of languages is hard to use and understand. This paper addresses this problem directly by explaining deep probabilistic programming languages and indirectly by characterizing their current strengths and weaknesses.",2018-04-17,2022-03-11 00:22:31,2022-03-11 00:22:31,2022-03-11 00:22:31,,,,,,,Deep Probabilistic Programming Languages,,,,,,,,,,,,arXiv.org,,arXiv: 1804.06458,,/Users/jacquesthibodeau/Zotero/storage/R6G367U3/Baudart et al. - 2018 - Deep Probabilistic Programming Languages A Qualit.pdf; /Users/jacquesthibodeau/Zotero/storage/5235IPGE/1804.html,,,Computer Science - Artificial Intelligence; Computer Science - Programming Languages,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KJSJWAGE,journalArticle,2021,"Dehghani, Mostafa; Tay, Yi; Gritsenko, Alexey A.; Zhao, Zhe; Houlsby, Neil; Diaz, Fernando; Metzler, Donald; Vinyals, Oriol",The Benchmark Lottery,arXiv:2107.07002 [cs],,,,http://arxiv.org/abs/2107.07002,"The world of empirical machine learning (ML) strongly relies on benchmarks in order to determine the relative effectiveness of different algorithms and methods. This paper proposes the notion of ""a benchmark lottery"" that describes the overall fragility of the ML benchmarking process. The benchmark lottery postulates that many factors, other than fundamental algorithmic superiority, may lead to a method being perceived as superior. On multiple benchmark setups that are prevalent in the ML community, we show that the relative performance of algorithms may be altered significantly simply by choosing different benchmark tasks, highlighting the fragility of the current paradigms and potential fallacious interpretation derived from benchmarking ML methods. Given that every benchmark makes a statement about what it perceives to be important, we argue that this might lead to biased progress in the community. We discuss the implications of the observed phenomena and provide recommendations on mitigating them using multiple machine learning domains and communities as use cases, including natural language processing, computer vision, information retrieval, recommender systems, and reinforcement learning.",2021-07-14,2022-03-11 00:22:33,2022-03-11 00:22:33,2022-03-11 00:22:33,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2107.07002,,/Users/jacquesthibodeau/Zotero/storage/8S6JFLEC/Dehghani et al. - 2021 - The Benchmark Lottery.pdf; /Users/jacquesthibodeau/Zotero/storage/GL8YTKSX/2107.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Information Retrieval; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3R2BW7HB,journalArticle,2020,"McAllester, David; Stratos, Karl",Formal Limitations on the Measurement of Mutual Information,"arXiv:1811.04251 [cs, math, stat]",,,,http://arxiv.org/abs/1811.04251,"Measuring mutual information from finite data is difficult. Recent work has considered variational methods maximizing a lower bound. In this paper, we prove that serious statistical limitations are inherent to any method of measuring mutual information. More specifically, we show that any distribution-free high-confidence lower bound on mutual information estimated from N samples cannot be larger than O(ln N ).",2020-05-20,2022-03-11 00:22:36,2022-03-11 00:22:36,2022-03-11 00:22:36,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1811.04251,,/Users/jacquesthibodeau/Zotero/storage/Q8UQEU4N/McAllester and Stratos - 2020 - Formal Limitations on the Measurement of Mutual In.pdf; /Users/jacquesthibodeau/Zotero/storage/IMSEKSAZ/1811.html,,,Computer Science - Information Theory; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
V3SZ48EP,journalArticle,2021,"Ecoffet, Adrien; Lehman, Joel",Reinforcement Learning Under Moral Uncertainty,arXiv:2006.04734 [cs],,,,http://arxiv.org/abs/2006.04734,"An ambitious goal for machine learning is to create agents that behave ethically: The capacity to abide by human moral norms would greatly expand the context in which autonomous agents could be practically and safely deployed, e.g. fully autonomous vehicles will encounter charged moral decisions that complicate their deployment. While ethical agents could be trained by rewarding correct behavior under a specific moral theory (e.g. utilitarianism), there remains widespread disagreement about the nature of morality. Acknowledging such disagreement, recent work in moral philosophy proposes that ethical behavior requires acting under moral uncertainty, i.e. to take into account when acting that one's credence is split across several plausible ethical theories. This paper translates such insights to the field of reinforcement learning, proposes two training methods that realize different points among competing desiderata, and trains agents in simple environments to act under moral uncertainty. The results illustrate (1) how such uncertainty can help curb extreme behavior from commitment to single theories and (2) several technical complications arising from attempting to ground moral philosophy in RL (e.g. how can a principled trade-off between two competing but incomparable reward functions be reached). The aim is to catalyze progress towards morally-competent agents and highlight the potential of RL to contribute towards the computational grounding of moral philosophy.",2021-07-19,2022-03-11 00:22:38,2022-03-11 00:22:38,2022-03-11 00:22:38,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2006.04734,,/Users/jacquesthibodeau/Zotero/storage/YDMWYTIH/Ecoffet and Lehman - 2021 - Reinforcement Learning Under Moral Uncertainty.pdf; /Users/jacquesthibodeau/Zotero/storage/4AIRQYHA/2006.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
M9AUTPFW,journalArticle,2018,"Yu, Han; Shen, Zhiqi; Miao, Chunyan; Leung, Cyril; Lesser, Victor R.; Yang, Qiang",Building Ethics into Artificial Intelligence,arXiv:1812.02953 [cs],,,,http://arxiv.org/abs/1812.02953,"As artificial intelligence (AI) systems become increasingly ubiquitous, the topic of AI governance for ethical decision-making by AI has captured public imagination. Within the AI research community, this topic remains less familiar to many researchers. In this paper, we complement existing surveys, which largely focused on the psychological, social and legal discussions of the topic, with an analysis of recent advances in technical solutions for AI governance. By reviewing publications in leading AI conferences including AAAI, AAMAS, ECAI and IJCAI, we propose a taxonomy which divides the field into four areas: 1) exploring ethical dilemmas; 2) individual ethical decision frameworks; 3) collective ethical decision frameworks; and 4) ethics in human-AI interactions. We highlight the intuitions and key techniques used in each approach, and discuss promising future research directions towards successful integration of ethical AI systems into human societies.",2018-12-07,2022-03-11 00:22:40,2022-03-11 00:22:40,2022-03-11 00:22:40,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1812.02953,,/Users/jacquesthibodeau/Zotero/storage/2NZ4Z6IL/Yu et al. - 2018 - Building Ethics into Artificial Intelligence.pdf; /Users/jacquesthibodeau/Zotero/storage/S35IUAF9/1812.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8S3PDAQL,journalArticle,2019,"Bussmann, Bart; Heinerman, Jacqueline; Lehman, Joel",Towards Empathic Deep Q-Learning,arXiv:1906.10918 [cs],,,,http://arxiv.org/abs/1906.10918,"As reinforcement learning (RL) scales to solve increasingly complex tasks, interest continues to grow in the fields of AI safety and machine ethics. As a contribution to these fields, this paper introduces an extension to Deep Q-Networks (DQNs), called Empathic DQN, that is loosely inspired both by empathy and the golden rule (""Do unto others as you would have them do unto you""). Empathic DQN aims to help mitigate negative side effects to other agents resulting from myopic goal-directed behavior. We assume a setting where a learning agent coexists with other independent agents (who receive unknown rewards), where some types of reward (e.g. negative rewards from physical harm) may generalize across agents. Empathic DQN combines the typical (self-centered) value with the estimated value of other agents, by imagining (by its own standards) the value of it being in the other's situation (by considering constructed states where both agents are swapped). Proof-of-concept results in two gridworld environments highlight the approach's potential to decrease collateral harms. While extending Empathic DQN to complex environments is non-trivial, we believe that this first step highlights the potential of bridge-work between machine ethics and RL to contribute useful priors for norm-abiding RL agents.",2019-06-26,2022-03-11 00:22:43,2022-03-11 00:22:43,2022-03-11 00:22:42,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1906.10918,,/Users/jacquesthibodeau/Zotero/storage/3YDENZMK/Bussmann et al. - 2019 - Towards Empathic Deep Q-Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/NC37S6L2/1906.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
J7SFG9RW,journalArticle,2018,"Wang, Baoxiang; Sun, Tongfang; Zheng, Xianjun Sam",Beyond Winning and Losing: Modeling Human Motivations and Behaviors Using Inverse Reinforcement Learning,"arXiv:1807.00366 [cs, stat]",,,,http://arxiv.org/abs/1807.00366,"In recent years, reinforcement learning (RL) methods have been applied to model gameplay with great success, achieving super-human performance in various environments, such as Atari, Go, and Poker. However, those studies mostly focus on winning the game and have largely ignored the rich and complex human motivations, which are essential for understanding different players' diverse behaviors. In this paper, we present a novel method called Multi-Motivation Behavior Modeling (MMBM) that takes the multifaceted human motivations into consideration and models the underlying value structure of the players using inverse RL. Our approach does not require the access to the dynamic of the system, making it feasible to model complex interactive environments such as massively multiplayer online games. MMBM is tested on the World of Warcraft Avatar History dataset, which recorded over 70,000 users' gameplay spanning three years period. Our model reveals the significant difference of value structures among different player groups. Using the results of motivation modeling, we also predict and explain their diverse gameplay behaviors and provide a quantitative assessment of how the redesign of the game environment impacts players' behaviors.",2018-07-05,2022-03-11 00:22:47,2022-03-11 00:22:47,2022-03-11 00:22:47,,,,,,,Beyond Winning and Losing,,,,,,,,,,,,arXiv.org,,arXiv: 1807.00366,,/Users/jacquesthibodeau/Zotero/storage/BD6PN9RF/Wang et al. - 2018 - Beyond Winning and Losing Modeling Human Motivati.pdf; /Users/jacquesthibodeau/Zotero/storage/7KWTSYUH/1807.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LGIEPNAD,journalArticle,2018,"Hristov, Yordan; Lascarides, Alex; Ramamoorthy, Subramanian",Interpretable Latent Spaces for Learning from Demonstration,arXiv:1807.06583 [cs],,,,http://arxiv.org/abs/1807.06583,"Effective human-robot interaction, such as in robot learning from human demonstration, requires the learning agent to be able to ground abstract concepts (such as those contained within instructions) in a corresponding high-dimensional sensory input stream from the world. Models such as deep neural networks, with high capacity through their large parameter spaces, can be used to compress the high-dimensional sensory data to lower dimensional representations. These low-dimensional representations facilitate symbol grounding, but may not guarantee that the representation would be human-interpretable. We propose a method which utilises the grouping of user-defined symbols and their corresponding sensory observations in order to align the learnt compressed latent representation with the semantic notions contained in the abstract labels. We demonstrate this through experiments with both simulated and real-world object data, showing that such alignment can be achieved in a process of physical symbol grounding.",2018-10-02,2022-03-11 00:22:50,2022-03-11 00:22:50,2022-03-11 00:22:49,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1807.06583,,/Users/jacquesthibodeau/Zotero/storage/PM7CKTEL/Hristov et al. - 2018 - Interpretable Latent Spaces for Learning from Demo.pdf; /Users/jacquesthibodeau/Zotero/storage/UWU527DM/1807.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GJBMZ656,journalArticle,2020,"Hoang, Lê Nguyên",A Roadmap for Robust End-to-End Alignment,arXiv:1809.01036 [cs],,,,http://arxiv.org/abs/1809.01036,"This paper discussed the {\it robust alignment} problem, that is, the problem of aligning the goals of algorithms with human preferences. It presented a general roadmap to tackle this issue. Interestingly, this roadmap identifies 5 critical steps, as well as many relevant aspects of these 5 steps. In other words, we have presented a large number of hopefully more tractable subproblems that readers are highly encouraged to tackle. Hopefully, this combination allows to better highlight the most pressing problems, how every expertise can be best used to, and how combining the solutions to subproblems might add up to solve robust alignment.",2020-02-25,2022-03-11 00:22:52,2022-03-11 00:22:52,2022-03-11 00:22:52,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1809.01036,,/Users/jacquesthibodeau/Zotero/storage/Q3QFTPUW/Hoang - 2020 - A Roadmap for Robust End-to-End Alignment.pdf; /Users/jacquesthibodeau/Zotero/storage/X4EMRJRW/1809.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MWZQMXT5,journalArticle,2018,"Garmulewicz, Michał; Michalewski, Henryk; Miłoś, Piotr",Expert-augmented actor-critic for ViZDoom and Montezumas Revenge,"arXiv:1809.03447 [cs, stat]",,,,http://arxiv.org/abs/1809.03447,"We propose an expert-augmented actor-critic algorithm, which we evaluate on two environments with sparse rewards: Montezumas Revenge and a demanding maze from the ViZDoom suite. In the case of Montezumas Revenge, an agent trained with our method achieves very good results consistently scoring above 27,000 points (in many experiments beating the first world). With an appropriate choice of hyperparameters, our algorithm surpasses the performance of the expert data. In a number of experiments, we have observed an unreported bug in Montezumas Revenge which allowed the agent to score more than 800,000 points.",2018-09-10,2022-03-11 00:22:55,2022-03-11 00:22:55,2022-03-11 00:22:54,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1809.03447,,/Users/jacquesthibodeau/Zotero/storage/ERETBZCC/Garmulewicz et al. - 2018 - Expert-augmented actor-critic for ViZDoom and Mont.pdf; /Users/jacquesthibodeau/Zotero/storage/6V6ZR5YQ/1809.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XPDIWET2,journalArticle,2018,"Pathak, Deepak; Mahmoudieh, Parsa; Luo, Guanghao; Agrawal, Pulkit; Chen, Dian; Shentu, Yide; Shelhamer, Evan; Malik, Jitendra; Efros, Alexei A.; Darrell, Trevor",Zero-Shot Visual Imitation,"arXiv:1804.08606 [cs, stat]",,,,http://arxiv.org/abs/1804.08606,"The current dominant paradigm for imitation learning relies on strong supervision of expert actions to learn both 'what' and 'how' to imitate. We pursue an alternative paradigm wherein an agent first explores the world without any expert supervision and then distills its experience into a goal-conditioned skill policy with a novel forward consistency loss. In our framework, the role of the expert is only to communicate the goals (i.e., what to imitate) during inference. The learned policy is then employed to mimic the expert (i.e., how to imitate) after seeing just a sequence of images demonstrating the desired task. Our method is 'zero-shot' in the sense that the agent never has access to expert actions during training or for the task demonstration at inference. We evaluate our zero-shot imitator in two real-world settings: complex rope manipulation with a Baxter robot and navigation in previously unseen office environments with a TurtleBot. Through further experiments in VizDoom simulation, we provide evidence that better mechanisms for exploration lead to learning a more capable policy which in turn improves end task performance. Videos, models, and more details are available at https://pathak22.github.io/zeroshot-imitation/",2018-04-23,2022-03-11 00:22:57,2022-03-11 00:22:57,2022-03-11 00:22:57,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1804.08606,,/Users/jacquesthibodeau/Zotero/storage/QLYCC974/Pathak et al. - 2018 - Zero-Shot Visual Imitation.pdf; /Users/jacquesthibodeau/Zotero/storage/8VSM6QA7/1804.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Robotics; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XVD32MFH,journalArticle,2019,"Sarafian, Elad; Tamar, Aviv; Kraus, Sarit",Constrained Policy Improvement for Safe and Efficient Reinforcement Learning,"arXiv:1805.07805 [cs, stat]",,,,http://arxiv.org/abs/1805.07805,"We propose a policy improvement algorithm for Reinforcement Learning (RL) which is called Rerouted Behavior Improvement (RBI). RBI is designed to take into account the evaluation errors of the Q-function. Such errors are common in RL when learning the $Q$-value from finite past experience data. Greedy policies or even constrained policy optimization algorithms which ignore these errors may suffer from an improvement penalty (i.e. a negative policy improvement). To minimize the improvement penalty, the RBI idea is to attenuate rapid policy changes of low probability actions which were less frequently sampled. This approach is shown to avoid catastrophic performance degradation and reduce regret when learning from a batch of past experience. Through a two-armed bandit with Gaussian distributed rewards example, we show that it also increases data efficiency when the optimal action has a high variance. We evaluate RBI in two tasks in the Atari Learning Environment: (1) learning from observations of multiple behavior policies and (2) iterative RL. Our results demonstrate the advantage of RBI over greedy policies and other constrained policy optimization algorithms as a safe learning approach and as a general data efficient learning algorithm. An anonymous Github repository of our RBI implementation is found at https://github.com/eladsar/rbi.",2019-07-10,2022-03-11 00:23:02,2022-03-11 00:23:02,2022-03-11 00:23:00,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1805.07805,,/Users/jacquesthibodeau/Zotero/storage/KXWEZZFT/Sarafian et al. - 2019 - Constrained Policy Improvement for Safe and Effici.pdf; /Users/jacquesthibodeau/Zotero/storage/R4ZBJMQZ/1805.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UFBQJEQH,journalArticle,2021,"Arora, Saurabh; Doshi, Prashant; Banerjee, Bikramjit",A Framework and Method for Online Inverse Reinforcement Learning,Autonomous Agents and Multi-Agent Systems,,"1387-2532, 1573-7454",10.1007/s10458-020-09485-4,http://arxiv.org/abs/1805.07871,"Inverse reinforcement learning (IRL) is the problem of learning the preferences of an agent from the observations of its behavior on a task. While this problem has been well investigated, the related problem of {\em online} IRL---where the observations are incrementally accrued, yet the demands of the application often prohibit a full rerun of an IRL method---has received relatively less attention. We introduce the first formal framework for online IRL, called incremental IRL (I2RL), and a new method that advances maximum entropy IRL with hidden variables, to this setting. Our formal analysis shows that the new method has a monotonically improving performance with more demonstration data, as well as probabilistically bounded error, both under full and partial observability. Experiments in a simulated robotic application of penetrating a continuous patrol under occlusion shows the relatively improved performance and speed up of the new method and validates the utility of online IRL.",2021-04,2022-03-11 00:23:05,2022-03-11 00:23:05,2022-03-11 00:23:04,4,,1,35,,Auton Agent Multi-Agent Syst,,,,,,,,,,,,,arXiv.org,,arXiv: 1805.07871,,/Users/jacquesthibodeau/Zotero/storage/VXIK6NUU/Arora et al. - 2021 - A Framework and Method for Online Inverse Reinforc.pdf; /Users/jacquesthibodeau/Zotero/storage/MRYL3D2Q/1805.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GG25PBKC,journalArticle,2019,"Justesen, Niels; Duque, Miguel Gonzalez; Jaramillo, Daniel Cabarcas; Mouret, Jean-Baptiste; Risi, Sebastian",Learning a Behavioral Repertoire from Demonstrations,arXiv:1907.03046 [cs],,,,http://arxiv.org/abs/1907.03046,"Imitation Learning (IL) is a machine learning approach to learn a policy from a dataset of demonstrations. IL can be useful to kick-start learning before applying reinforcement learning (RL) but it can also be useful on its own, e.g. to learn to imitate human players in video games. However, a major limitation of current IL approaches is that they learn only a single ""average"" policy based on a dataset that possibly contains demonstrations of numerous different types of behaviors. In this paper, we propose a new approach called Behavioral Repertoire Imitation Learning (BRIL) that instead learns a repertoire of behaviors from a set of demonstrations by augmenting the state-action pairs with behavioral descriptions. The outcome of this approach is a single neural network policy conditioned on a behavior description that can be precisely modulated. We apply this approach to train a policy on 7,777 human replays to perform build-order planning in StarCraft II. Principal Component Analysis (PCA) is applied to construct a low-dimensional behavioral space from the high-dimensional army unit composition of each demonstration. The results demonstrate that the learned policy can be effectively manipulated to express distinct behaviors. Additionally, by applying the UCB1 algorithm, we are able to adapt the behavior of the policy - in-between games - to reach a performance beyond that of the traditional IL baseline approach.",2019-07-05,2022-03-11 00:23:06,2022-03-11 00:23:06,2022-03-11 00:23:06,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1907.03046,,/Users/jacquesthibodeau/Zotero/storage/9URB9V2X/Justesen et al. - 2019 - Learning a Behavioral Repertoire from Demonstratio.pdf; /Users/jacquesthibodeau/Zotero/storage/EF3YPC9C/1907.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YWBZAMIA,journalArticle,2020,"Schneider, Johannes",Humans learn too: Better Human-AI Interaction using Optimized Human Inputs,arXiv:2009.09266 [cs],,,,http://arxiv.org/abs/2009.09266,"Humans rely more and more on systems with AI components. The AI community typically treats human inputs as a given and optimizes AI models only. This thinking is one-sided and it neglects the fact that humans can learn, too. In this work, human inputs are optimized for better interaction with an AI model while keeping the model fixed. The optimized inputs are accompanied by instructions on how to create them. They allow humans to save time and cut on errors, while keeping required changes to original inputs limited. We propose continuous and discrete optimization methods modifying samples in an iterative fashion. Our quantitative and qualitative evaluation including a human study on different hand-generated inputs shows that the generated proposals lead to lower error rates, require less effort to create and differ only modestly from the original samples.",2020-09-19,2022-03-11 00:23:09,2022-03-11 00:23:09,2022-03-11 00:23:08,,,,,,,Humans learn too,,,,,,,,,,,,arXiv.org,,arXiv: 2009.09266,,/Users/jacquesthibodeau/Zotero/storage/4HBXZ7DM/Schneider - 2020 - Humans learn too Better Human-AI Interaction usin.pdf; /Users/jacquesthibodeau/Zotero/storage/GWMQR73W/2009.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Human-Computer Interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
B3WEYDWG,journalArticle,2019,"Lütjens, Björn; Everett, Michael; How, Jonathan P.",Safe Reinforcement Learning with Model Uncertainty Estimates,arXiv:1810.08700 [cs],,,,http://arxiv.org/abs/1810.08700,"Many current autonomous systems are being designed with a strong reliance on black box predictions from deep neural networks (DNNs). However, DNNs tend to be overconfident in predictions on unseen data and can give unpredictable results for far-from-distribution test data. The importance of predictions that are robust to this distributional shift is evident for safety-critical applications, such as collision avoidance around pedestrians. Measures of model uncertainty can be used to identify unseen data, but the state-of-the-art extraction methods such as Bayesian neural networks are mostly intractable to compute. This paper uses MC-Dropout and Bootstrapping to give computationally tractable and parallelizable uncertainty estimates. The methods are embedded in a Safe Reinforcement Learning framework to form uncertainty-aware navigation around pedestrians. The result is a collision avoidance policy that knows what it does not know and cautiously avoids pedestrians that exhibit unseen behavior. The policy is demonstrated in simulation to be more robust to novel observations and take safer actions than an uncertainty-unaware baseline.",2019-03-01,2022-03-11 00:27:25,2022-03-11 00:27:25,2022-03-11 00:27:25,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.08700,,/Users/jacquesthibodeau/Zotero/storage/2B4WHVQY/Lütjens et al. - 2019 - Safe Reinforcement Learning with Model Uncertainty.pdf; /Users/jacquesthibodeau/Zotero/storage/AX5M4IK5/1810.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BHI37WRL,journalArticle,2020,"Turner, Alexander Matt; Hadfield-Menell, Dylan; Tadepalli, Prasad",Conservative Agency via Attainable Utility Preservation,"Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",,,10.1145/3375627.3375851,http://arxiv.org/abs/1902.09725,"Reward functions are easy to misspecify; although designers can make corrections after observing mistakes, an agent pursuing a misspecified reward function can irreversibly change the state of its environment. If that change precludes optimization of the correctly specified reward function, then correction is futile. For example, a robotic factory assistant could break expensive equipment due to a reward misspecification; even if the designers immediately correct the reward function, the damage is done. To mitigate this risk, we introduce an approach that balances optimization of the primary reward function with preservation of the ability to optimize auxiliary reward functions. Surprisingly, even when the auxiliary reward functions are randomly generated and therefore uninformative about the correctly specified reward function, this approach induces conservative, effective behavior.",2020-02-07,2022-03-11 00:27:28,2022-03-11 00:27:28,2022-03-11 00:27:27,385-391,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1902.09725,,/Users/jacquesthibodeau/Zotero/storage/ZABTJAMX/Turner et al. - 2020 - Conservative Agency via Attainable Utility Preserv.pdf; /Users/jacquesthibodeau/Zotero/storage/ZGXPZFAR/1902.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R2TT2S7H,journalArticle,2021,"Thananjeyan, Brijen; Balakrishna, Ashwin; Nair, Suraj; Luo, Michael; Srinivasan, Krishnan; Hwang, Minho; Gonzalez, Joseph E.; Ibarz, Julian; Finn, Chelsea; Goldberg, Ken",Recovery RL: Safe Reinforcement Learning with Learned Recovery Zones,arXiv:2010.15920 [cs],,,,http://arxiv.org/abs/2010.15920,"Safety remains a central obstacle preventing widespread use of RL in the real world: learning new tasks in uncertain environments requires extensive exploration, but safety requires limiting exploration. We propose Recovery RL, an algorithm which navigates this tradeoff by (1) leveraging offline data to learn about constraint violating zones before policy learning and (2) separating the goals of improving task performance and constraint satisfaction across two policies: a task policy that only optimizes the task reward and a recovery policy that guides the agent to safety when constraint violation is likely. We evaluate Recovery RL on 6 simulation domains, including two contact-rich manipulation tasks and an image-based navigation task, and an image-based obstacle avoidance task on a physical robot. We compare Recovery RL to 5 prior safe RL methods which jointly optimize for task performance and safety via constrained optimization or reward shaping and find that Recovery RL outperforms the next best prior method across all domains. Results suggest that Recovery RL trades off constraint violations and task successes 2 - 20 times more efficiently in simulation domains and 3 times more efficiently in physical experiments. See https://tinyurl.com/rl-recovery for videos and supplementary material.",2021-05-17,2022-03-11 00:27:29,2022-03-11 00:27:29,2022-03-11 00:27:29,,,,,,,Recovery RL,,,,,,,,,,,,arXiv.org,,arXiv: 2010.15920,,/Users/jacquesthibodeau/Zotero/storage/VY8X9J3J/Thananjeyan et al. - 2021 - Recovery RL Safe Reinforcement Learning with Lear.pdf; /Users/jacquesthibodeau/Zotero/storage/4HWNEV5H/2010.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
N66IUBBR,journalArticle,2020,"Srinivasan, Krishnan; Eysenbach, Benjamin; Ha, Sehoon; Tan, Jie; Finn, Chelsea",Learning to be Safe: Deep RL with a Safety Critic,arXiv:2010.14603 [cs],,,,http://arxiv.org/abs/2010.14603,"Safety is an essential component for deploying reinforcement learning (RL) algorithms in real-world scenarios, and is critical during the learning process itself. A natural first approach toward safe RL is to manually specify constraints on the policy's behavior. However, just as learning has enabled progress in large-scale development of AI systems, learning safety specifications may also be necessary to ensure safety in messy open-world environments where manual safety specifications cannot scale. Akin to how humans learn incrementally starting in child-safe environments, we propose to learn how to be safe in one set of tasks and environments, and then use that learned intuition to constrain future behaviors when learning new, modified tasks. We empirically study this form of safety-constrained transfer learning in three challenging domains: simulated navigation, quadruped locomotion, and dexterous in-hand manipulation. In comparison to standard deep RL techniques and prior approaches to safe RL, we find that our method enables the learning of new tasks and in new environments with both substantially fewer safety incidents, such as falling or dropping an object, and faster, more stable learning. This suggests a path forward not only for safer RL systems, but also for more effective RL systems.",2020-10-27,2022-03-11 00:27:32,2022-03-11 00:27:32,2022-03-11 00:27:32,,,,,,,Learning to be Safe,,,,,,,,,,,,arXiv.org,,arXiv: 2010.14603,,/Users/jacquesthibodeau/Zotero/storage/QAKENCAU/Srinivasan et al. - 2020 - Learning to be Safe Deep RL with a Safety Critic.pdf; /Users/jacquesthibodeau/Zotero/storage/VNCX7R62/2010.html,,,Computer Science - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5VQPHB3C,journalArticle,2021,"Giacobbe, Mirco; Hasanbeig, Mohammadhosein; Kroening, Daniel; Wijk, Hjalmar",Shielding Atari Games with Bounded Prescience,arXiv:2101.08153 [cs],,,,http://arxiv.org/abs/2101.08153,"Deep reinforcement learning (DRL) is applied in safety-critical domains such as robotics and autonomous driving. It achieves superhuman abilities in many tasks, however whether DRL agents can be shown to act safely is an open problem. Atari games are a simple yet challenging exemplar for evaluating the safety of DRL agents and feature a diverse portfolio of game mechanics. The safety of neural agents has been studied before using methods that either require a model of the system dynamics or an abstraction; unfortunately, these are unsuitable to Atari games because their low-level dynamics are complex and hidden inside their emulator. We present the first exact method for analysing and ensuring the safety of DRL agents for Atari games. Our method only requires access to the emulator. First, we give a set of 43 properties that characterise ""safe behaviour"" for 30 games. Second, we develop a method for exploring all traces induced by an agent and a game and consider a variety of sources of game non-determinism. We observe that the best available DRL agents reliably satisfy only very few properties; several critical properties are violated by all agents. Finally, we propose a countermeasure that combines a bounded explicit-state exploration with shielding. We demonstrate that our method improves the safety of all agents over multiple properties.",2021-01-22,2022-03-11 00:27:34,2022-03-11 00:27:34,2022-03-11 00:27:34,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2101.08153,,/Users/jacquesthibodeau/Zotero/storage/INALW3AF/Giacobbe et al. - 2021 - Shielding Atari Games with Bounded Prescience.pdf; /Users/jacquesthibodeau/Zotero/storage/YN9BZ4UC/2101.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GHBNC544,journalArticle,2019,"Krakovna, Victoria; Orseau, Laurent; Kumar, Ramana; Martic, Miljan; Legg, Shane",Penalizing side effects using stepwise relative reachability,"arXiv:1806.01186 [cs, stat]",,,,http://arxiv.org/abs/1806.01186,"How can we design safe reinforcement learning agents that avoid unnecessary disruptions to their environment? We show that current approaches to penalizing side effects can introduce bad incentives, e.g. to prevent any irreversible changes in the environment, including the actions of other agents. To isolate the source of such undesirable incentives, we break down side effects penalties into two components: a baseline state and a measure of deviation from this baseline state. We argue that some of these incentives arise from the choice of baseline, and others arise from the choice of deviation measure. We introduce a new variant of the stepwise inaction baseline and a new deviation measure based on relative reachability of states. The combination of these design choices avoids the given undesirable incentives, while simpler baselines and the unreachability measure fail. We demonstrate this empirically by comparing different combinations of baseline and deviation measure choices on a set of gridworld experiments designed to illustrate possible bad incentives.",2019-03-08,2022-03-11 00:27:37,2022-03-11 00:27:37,2022-03-11 00:27:37,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1806.01186,,/Users/jacquesthibodeau/Zotero/storage/4IKN8UWW/Krakovna et al. - 2019 - Penalizing side effects using stepwise relative re.pdf; /Users/jacquesthibodeau/Zotero/storage/HBZ82Z86/1806.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VCC8ID63,journalArticle,2021,"Leibo, Joel Z.; Duéñez-Guzmán, Edgar; Vezhnevets, Alexander Sasha; Agapiou, John P.; Sunehag, Peter; Koster, Raphael; Matyas, Jayd; Beattie, Charles; Mordatch, Igor; Graepel, Thore",Scalable Evaluation of Multi-Agent Reinforcement Learning with Melting Pot,arXiv:2107.06857 [cs],,,,http://arxiv.org/abs/2107.06857,"Existing evaluation suites for multi-agent reinforcement learning (MARL) do not assess generalization to novel situations as their primary objective (unlike supervised-learning benchmarks). Our contribution, Melting Pot, is a MARL evaluation suite that fills this gap, and uses reinforcement learning to reduce the human labor required to create novel test scenarios. This works because one agent's behavior constitutes (part of) another agent's environment. To demonstrate scalability, we have created over 80 unique test scenarios covering a broad range of research topics such as social dilemmas, reciprocity, resource sharing, and task partitioning. We apply these test scenarios to standard MARL training algorithms, and demonstrate how Melting Pot reveals weaknesses not apparent from training performance alone.",2021-07-14,2022-03-11 00:27:39,2022-03-11 00:27:39,2022-03-11 00:27:39,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2107.06857,,/Users/jacquesthibodeau/Zotero/storage/WLHS7YMG/Leibo et al. - 2021 - Scalable Evaluation of Multi-Agent Reinforcement L.pdf; /Users/jacquesthibodeau/Zotero/storage/6QF6MR3D/2107.html,,,Computer Science - Artificial Intelligence; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JCCGZ43G,journalArticle,2019,"Wang, Yixin; Blei, David M.",The Blessings of Multiple Causes,"arXiv:1805.06826 [cs, stat]",,,,http://arxiv.org/abs/1805.06826,"Causal inference from observational data often assumes ""ignorability,"" that all confounders are observed. This assumption is standard yet untestable. However, many scientific studies involve multiple causes, different variables whose effects are simultaneously of interest. We propose the deconfounder, an algorithm that combines unsupervised machine learning and predictive model checking to perform causal inference in multiple-cause settings. The deconfounder infers a latent variable as a substitute for unobserved confounders and then uses that substitute to perform causal inference. We develop theory for the deconfounder, and show that it requires weaker assumptions than classical causal inference. We analyze its performance in three types of studies: semi-simulated data around smoking and lung cancer, semi-simulated data around genome-wide association studies, and a real dataset about actors and movie revenue. The deconfounder provides a checkable approach to estimating closer-to-truth causal effects.",2019-04-14,2022-03-11 00:28:10,2022-03-11 00:28:10,2022-03-11 00:28:10,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1805.06826,,/Users/jacquesthibodeau/Zotero/storage/WVASPT36/Wang and Blei - 2019 - The Blessings of Multiple Causes.pdf; /Users/jacquesthibodeau/Zotero/storage/LPE4QRY3/1805.html,,,Computer Science - Machine Learning; Statistics - Machine Learning; Statistics - Methodology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ULMJWB94,journalArticle,2018,"Trazzi, Michaël; Yampolskiy, Roman V.",Building Safer AGI by introducing Artificial Stupidity,arXiv:1808.03644 [cs],,,,http://arxiv.org/abs/1808.03644,"Artificial Intelligence (AI) achieved super-human performance in a broad variety of domains. We say that an AI is made Artificially Stupid on a task when some limitations are deliberately introduced to match a human's ability to do the task. An Artificial General Intelligence (AGI) can be made safer by limiting its computing power and memory, or by introducing Artificial Stupidity on certain tasks. We survey human intellectual limits and give recommendations for which limits to implement in order to build a safe AGI.",2018-08-10,2022-03-11 00:28:14,2022-03-11 00:28:14,2022-03-11 00:28:14,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1808.03644,,/Users/jacquesthibodeau/Zotero/storage/VX3GHL26/Trazzi and Yampolskiy - 2018 - Building Safer AGI by introducing Artificial Stupi.pdf; /Users/jacquesthibodeau/Zotero/storage/AB6X3PZG/1808.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
L2JGDWI9,journalArticle,2021,"Gabriel, Iason; Ghazavi, Vafa",The Challenge of Value Alignment: from Fairer Algorithms to AI Safety,arXiv:2101.06060 [cs],,,,http://arxiv.org/abs/2101.06060,"This paper addresses the question of how to align AI systems with human values and situates it within a wider body of thought regarding technology and value. Far from existing in a vacuum, there has long been an interest in the ability of technology to 'lock-in' different value systems. There has also been considerable thought about how to align technologies with specific social values, including through participatory design-processes. In this paper we look more closely at the question of AI value alignment and suggest that the power and autonomy of AI systems gives rise to opportunities and challenges in the domain of value that have not been encountered before. Drawing important continuities between the work of the fairness, accountability, transparency and ethics community, and work being done by technical AI safety researchers, we suggest that more attention needs to be paid to the question of 'social value alignment' - that is, how to align AI systems with the plurality of values endorsed by groups of people, especially on the global level.",2021-01-18,2022-03-11 00:28:21,2022-03-11 00:28:21,2022-03-11 00:28:21,,,,,,,The Challenge of Value Alignment,,,,,,,,,,,,arXiv.org,,arXiv: 2101.06060,,/Users/jacquesthibodeau/Zotero/storage/WKC623JG/Gabriel and Ghazavi - 2021 - The Challenge of Value Alignment from Fairer Algo.pdf; /Users/jacquesthibodeau/Zotero/storage/L5QUXVDP/2101.html,,,Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EZ3D88F9,journalArticle,2021,"Gruetzemacher, Ross; Whittlestone, Jess",The Transformative Potential of Artificial Intelligence,arXiv:1912.00747 [cs],,,,http://arxiv.org/abs/1912.00747,"The terms 'human-level artificial intelligence' and 'artificial general intelligence' are widely used to refer to the possibility of advanced artificial intelligence (AI) with potentially extreme impacts on society. These terms are poorly defined and do not necessarily indicate what is most important with respect to future societal impacts. We suggest that the term 'transformative AI' is a helpful alternative, reflecting the possibility that advanced AI systems could have very large impacts on society without reaching human-level cognitive abilities. To be most useful, however, more analysis of what it means for AI to be 'transformative' is needed. In this paper, we propose three different levels on which AI might be said to be transformative, associated with different levels of societal change. We suggest that these distinctions would improve conversations between policy makers and decision makers concerning the mid- to long-term impacts of advances in AI. Further, we feel this would have a positive effect on strategic foresight efforts involving advanced AI, which we expect to illuminate paths to alternative futures. We conclude with a discussion of the benefits of our new framework and by highlighting directions for future work in this area.",2021-10-23,2022-03-11 00:28:26,2022-03-11 00:28:26,2022-03-11 00:28:26,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1912.00747,,/Users/jacquesthibodeau/Zotero/storage/2LPPT7JQ/Gruetzemacher and Whittlestone - 2021 - The Transformative Potential of Artificial Intelli.pdf; /Users/jacquesthibodeau/Zotero/storage/Q4TGQNJJ/1912.html,,,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8DCH2NZW,journalArticle,2020,"Shah, Ankit; Li, Shen; Shah, Julie",Planning With Uncertain Specifications (PUnS),IEEE Robotics and Automation Letters,,"2377-3766, 2377-3774",10.1109/LRA.2020.2977217,http://arxiv.org/abs/1906.03218,"Reward engineering is crucial to high performance in reinforcement learning systems. Prior research into reward design has largely focused on Markovian functions representing the reward. While there has been research into expressing non-Markov rewards as linear temporal logic (LTL) formulas, this has focused on task specifications directly defined by the user. However, in many real-world applications, task specifications are ambiguous, and can only be expressed as a belief over LTL formulas. In this paper, we introduce planning with uncertain specifications (PUnS), a novel formulation that addresses the challenge posed by non-Markovian specifications expressed as beliefs over LTL formulas. We present four criteria that capture the semantics of satisfying a belief over specifications for different applications, and analyze the qualitative implications of these criteria within a synthetic domain. We demonstrate the existence of an equivalent Markov decision process (MDP) for any instance of PUnS. Finally, we demonstrate our approach on the real-world task of setting a dinner table automatically with a robot that inferred task specifications from human demonstrations.",2020-04,2022-03-11 00:28:31,2022-03-11 00:28:31,2022-03-11 00:28:31,3414-3421,,2,5,,IEEE Robot. Autom. Lett.,,,,,,,,,,,,,arXiv.org,,arXiv: 1906.03218,,/Users/jacquesthibodeau/Zotero/storage/LG4CHIQ5/Shah et al. - 2020 - Planning With Uncertain Specifications (PUnS).pdf; /Users/jacquesthibodeau/Zotero/storage/BQBFGT5K/1906.html,,,Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UVBRGSUJ,journalArticle,2021,"Du, Yuqing; Tiomkin, Stas; Kiciman, Emre; Polani, Daniel; Abbeel, Pieter; Dragan, Anca",AvE: Assistance via Empowerment,arXiv:2006.14796 [cs],,,,http://arxiv.org/abs/2006.14796,"One difficulty in using artificial agents for human-assistive applications lies in the challenge of accurately assisting with a person's goal(s). Existing methods tend to rely on inferring the human's goal, which is challenging when there are many potential goals or when the set of candidate goals is difficult to identify. We propose a new paradigm for assistance by instead increasing the human's ability to control their environment, and formalize this approach by augmenting reinforcement learning with human empowerment. This task-agnostic objective preserves the person's autonomy and ability to achieve any eventual state. We test our approach against assistance based on goal inference, highlighting scenarios where our method overcomes failure modes stemming from goal ambiguity or misspecification. As existing methods for estimating empowerment in continuous domains are computationally hard, precluding its use in real time learned assistance, we also propose an efficient empowerment-inspired proxy metric. Using this, we are able to successfully demonstrate our method in a shared autonomy user study for a challenging simulated teleoperation task with human-in-the-loop training.",2021-01-07,2022-03-11 00:28:33,2022-03-11 00:28:33,2022-03-11 00:28:33,,,,,,,AvE,,,,,,,,,,,,arXiv.org,,arXiv: 2006.14796,,/Users/jacquesthibodeau/Zotero/storage/J53YANPG/Du et al. - 2021 - AvE Assistance via Empowerment.pdf; /Users/jacquesthibodeau/Zotero/storage/FRMSL5TI/2006.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DQ9TRJU9,journalArticle,2020,"Thomas, Rachel; Uminsky, David",The Problem with Metrics is a Fundamental Problem for AI,arXiv:2002.08512 [cs],,,,http://arxiv.org/abs/2002.08512,"Optimizing a given metric is a central aspect of most current AI approaches, yet overemphasizing metrics leads to manipulation, gaming, a myopic focus on short-term goals, and other unexpected negative consequences. This poses a fundamental contradiction for AI development. Through a series of real-world case studies, we look at various aspects of where metrics go wrong in practice and aspects of how our online environment and current business practices are exacerbating these failures. Finally, we propose a framework towards mitigating the harms caused by overemphasis of metrics within AI by: (1) using a slate of metrics to get a fuller and more nuanced picture, (2) combining metrics with qualitative accounts, and (3) involving a range of stakeholders, including those who will be most impacted.",2020-02-19,2022-03-11 00:28:36,2022-03-11 00:28:36,2022-03-11 00:28:36,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2002.08512,,/Users/jacquesthibodeau/Zotero/storage/MYXUEF2X/Thomas and Uminsky - 2020 - The Problem with Metrics is a Fundamental Problem .pdf; /Users/jacquesthibodeau/Zotero/storage/7GYLEEBA/2002.html,,,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MAIW3WZX,journalArticle,2019,"Dobbe, Roel; Gilbert, Thomas Krendl; Mintz, Yonatan",Hard Choices in Artificial Intelligence: Addressing Normative Uncertainty through Sociotechnical Commitments,"arXiv:1911.09005 [cs, eess]",,,,http://arxiv.org/abs/1911.09005,"As AI systems become prevalent in high stakes domains such as surveillance and healthcare, researchers now examine how to design and implement them in a safe manner. However, the potential harms caused by systems to stakeholders in complex social contexts and how to address these remains unclear. In this paper, we explain the inherent normative uncertainty in debates about the safety of AI systems. We then address this as a problem of vagueness by examining its place in the design, training, and deployment stages of AI system development. We adopt Ruth Chang's theory of intuitive comparability to illustrate the dilemmas that manifest at each stage. We then discuss how stakeholders can navigate these dilemmas by incorporating distinct forms of dissent into the development pipeline, drawing on Elizabeth Anderson's work on the epistemic powers of democratic institutions. We outline a framework of sociotechnical commitments to formal, substantive and discursive challenges that address normative uncertainty across stakeholders, and propose the cultivation of related virtues by those responsible for development.",2019-11-20,2022-03-11 00:28:40,2022-03-11 00:28:40,2022-03-11 00:28:40,,,,,,,Hard Choices in Artificial Intelligence,,,,,,,,,,,,arXiv.org,,arXiv: 1911.09005,,/Users/jacquesthibodeau/Zotero/storage/Y6WWHFR4/Dobbe et al. - 2019 - Hard Choices in Artificial Intelligence Addressin.pdf; /Users/jacquesthibodeau/Zotero/storage/VLRQSA58/1911.html,,,Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Electrical Engineering and Systems Science - Systems and Control,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JI9QN779,journalArticle,2020,"McGregor, Sean",Preventing Repeated Real World AI Failures by Cataloging Incidents: The AI Incident Database,arXiv:2011.08512 [cs],,,,http://arxiv.org/abs/2011.08512,"Mature industrial sectors (e.g., aviation) collect their real world failures in incident databases to inform safety improvements. Intelligent systems currently cause real world harms without a collective memory of their failings. As a result, companies repeatedly make the same mistakes in the design, development, and deployment of intelligent systems. A collection of intelligent system failures experienced in the real world (i.e., incidents) is needed to ensure intelligent systems benefit people and society. The AI Incident Database is an incident collection initiated by an industrial/non-profit cooperative to enable AI incident avoidance and mitigation. The database supports a variety of research and development use cases with faceted and full text search on more than 1,000 incident reports archived to date.",2020-11-17,2022-03-11 00:29:15,2022-03-11 00:29:15,2022-03-11 00:29:15,,,,,,,Preventing Repeated Real World AI Failures by Cataloging Incidents,,,,,,,,,,,,arXiv.org,,arXiv: 2011.08512,,/Users/jacquesthibodeau/Zotero/storage/4IKPVPQF/McGregor - 2020 - Preventing Repeated Real World AI Failures by Cata.pdf; /Users/jacquesthibodeau/Zotero/storage/QWR24NYR/2011.html,,,Computer Science - Computers and Society; Computer Science - Software Engineering; I.2.0; K.4.0; K.4.3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5JQCG2KN,journalArticle,2020,"Juric, Mislav; Sandic, Agneza; Brcic, Mario",AI safety: state of the field through quantitative lens,arXiv:2002.05671 [cs],,,,http://arxiv.org/abs/2002.05671,"Last decade has seen major improvements in the performance of artificial intelligence which has driven wide-spread applications. Unforeseen effects of such mass-adoption has put the notion of AI safety into the public eye. AI safety is a relatively new field of research focused on techniques for building AI beneficial for humans. While there exist survey papers for the field of AI safety, there is a lack of a quantitative look at the research being conducted. The quantitative aspect gives a data-driven insight about the emerging trends, knowledge gaps and potential areas for future research. In this paper, bibliometric analysis of the literature finds significant increase in research activity since 2015. Also, the field is so new that most of the technical issues are open, including: explainability with its long-term utility, and value alignment which we have identified as the most important long-term research topic. Equally, there is a severe lack of research into concrete policies regarding AI. As we expect AI to be the one of the main driving forces of changes in society, AI safety is the field under which we need to decide the direction of humanity's future.",2020-07-09,2022-03-11 00:29:20,2022-03-11 00:29:20,2022-03-11 00:29:20,,,,,,,AI safety,,,,,,,,,,,,arXiv.org,,arXiv: 2002.05671,,/Users/jacquesthibodeau/Zotero/storage/MQUVP8RM/Juric et al. - 2020 - AI safety state of the field through quantitative.pdf; /Users/jacquesthibodeau/Zotero/storage/ZXNJAUER/2002.html,,,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JD5VURSB,journalArticle,2021,"Agarwal, Sandhini; Krueger, Gretchen; Clark, Jack; Radford, Alec; Kim, Jong Wook; Brundage, Miles",Evaluating CLIP: Towards Characterization of Broader Capabilities and Downstream Implications,arXiv:2108.02818 [cs],,,,http://arxiv.org/abs/2108.02818,"Recently, there have been breakthroughs in computer vision (""CV"") models that are more generalizable with the advent of models such as CLIP and ALIGN. In this paper, we analyze CLIP and highlight some of the challenges such models pose. CLIP reduces the need for task specific training data, potentially opening up many niche tasks to automation. CLIP also allows its users to flexibly specify image classification classes in natural language, which we find can shift how biases manifest. Additionally, through some preliminary probes we find that CLIP can inherit biases found in prior computer vision systems. Given the wide and unpredictable domain of uses for such models, this raises questions regarding what sufficiently safe behaviour for such systems may look like. These results add evidence to the growing body of work calling for a change in the notion of a 'better' model--to move beyond simply looking at higher accuracy at task-oriented capability evaluations, and towards a broader 'better' that takes into account deployment-critical features such as different use contexts, and people who interact with the model when thinking about model deployment.",2021-08-05,2022-03-11 00:29:23,2022-03-11 00:29:23,2022-03-11 00:29:22,,,,,,,Evaluating CLIP,,,,,,,,,,,,arXiv.org,,arXiv: 2108.02818,,/Users/jacquesthibodeau/Zotero/storage/9RD9AH54/Agarwal et al. - 2021 - Evaluating CLIP Towards Characterization of Broad.pdf; /Users/jacquesthibodeau/Zotero/storage/ALLRUCF9/2108.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7JZBY3HQ,journalArticle,2019,"Everitt, Tom; Kumar, Ramana; Krakovna, Victoria; Legg, Shane",Modeling AGI Safety Frameworks with Causal Influence Diagrams,arXiv:1906.08663 [cs],,,,http://arxiv.org/abs/1906.08663,"Proposals for safe AGI systems are typically made at the level of frameworks, specifying how the components of the proposed system should be trained and interact with each other. In this paper, we model and compare the most promising AGI safety frameworks using causal influence diagrams. The diagrams show the optimization objective and causal assumptions of the framework. The unified representation permits easy comparison of frameworks and their assumptions. We hope that the diagrams will serve as an accessible and visual introduction to the main AGI safety frameworks.",2019-06-20,2022-03-11 00:29:25,2022-03-11 00:29:25,2022-03-11 00:29:25,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1906.08663,,/Users/jacquesthibodeau/Zotero/storage/CQYTJDAI/Everitt et al. - 2019 - Modeling AGI Safety Frameworks with Causal Influen.pdf; /Users/jacquesthibodeau/Zotero/storage/NEE7D275/1906.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FD9FFH2X,journalArticle,2017,"Everitt, Tom; Krakovna, Victoria; Orseau, Laurent; Hutter, Marcus; Legg, Shane",Reinforcement Learning with a Corrupted Reward Channel,"arXiv:1705.08417 [cs, stat]",,,,http://arxiv.org/abs/1705.08417,"No real-world reward function is perfect. Sensory errors and software bugs may result in RL agents observing higher (or lower) rewards than they should. For example, a reinforcement learning agent may prefer states where a sensory error gives it the maximum reward, but where the true reward is actually small. We formalise this problem as a generalised Markov Decision Problem called Corrupt Reward MDP. Traditional RL methods fare poorly in CRMDPs, even under strong simplifying assumptions and when trying to compensate for the possibly corrupt rewards. Two ways around the problem are investigated. First, by giving the agent richer data, such as in inverse reinforcement learning and semi-supervised reinforcement learning, reward corruption stemming from systematic sensory errors may sometimes be completely managed. Second, by using randomisation to blunt the agent's optimisation, reward corruption can be partially managed under some assumptions.",2017-08-19,2022-03-11 00:30:11,2022-03-11 00:30:11,2022-03-11 00:30:11,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1705.08417,,/Users/jacquesthibodeau/Zotero/storage/WI7P9VB4/Everitt et al. - 2017 - Reinforcement Learning with a Corrupted Reward Cha.pdf; /Users/jacquesthibodeau/Zotero/storage/EZ3TIG45/1705.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; I.2.6; I.2.8; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9ANIXJHD,journalArticle,2016,"Ribeiro, Marco Tulio; Singh, Sameer; Guestrin, Carlos","""Why Should I Trust You?"": Explaining the Predictions of Any Classifier","arXiv:1602.04938 [cs, stat]",,,,http://arxiv.org/abs/1602.04938,"Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.",2016-08-09,2022-03-11 00:30:12,2022-03-11 00:30:12,2022-03-11 00:30:11,,,,,,,"""Why Should I Trust You?",,,,,,,,,,,,arXiv.org,,arXiv: 1602.04938,,/Users/jacquesthibodeau/Zotero/storage/Z48JVI4D/Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf; /Users/jacquesthibodeau/Zotero/storage/AG2K5IYQ/1602.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P9MZ98YH,journalArticle,2021,"Everitt, Tom; Hutter, Marcus; Kumar, Ramana; Krakovna, Victoria",Reward Tampering Problems and Solutions in Reinforcement Learning: A Causal Influence Diagram Perspective,arXiv:1908.04734 [cs],,,,http://arxiv.org/abs/1908.04734,"Can humans get arbitrarily capable reinforcement learning (RL) agents to do their bidding? Or will sufficiently capable RL agents always find ways to bypass their intended objectives by shortcutting their reward signal? This question impacts how far RL can be scaled, and whether alternative paradigms must be developed in order to build safe artificial general intelligence. In this paper, we study when an RL agent has an instrumental goal to tamper with its reward process, and describe design principles that prevent instrumental goals for two different types of reward tampering (reward function tampering and RF-input tampering). Combined, the design principles can prevent both types of reward tampering from being instrumental goals. The analysis benefits from causal influence diagrams to provide intuitive yet precise formalizations.",2021-03-26,2022-03-11 00:30:17,2022-03-11 00:30:17,2022-03-11 00:30:17,,,,,,,Reward Tampering Problems and Solutions in Reinforcement Learning,,,,,,,,,,,,arXiv.org,,arXiv: 1908.04734,,/Users/jacquesthibodeau/Zotero/storage/ZNYDQEC2/Everitt et al. - 2021 - Reward Tampering Problems and Solutions in Reinfor.pdf; /Users/jacquesthibodeau/Zotero/storage/GX8RR8QS/1908.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JTZGCY88,journalArticle,2018,"Kusner, Matt J.; Loftus, Joshua R.; Russell, Chris; Silva, Ricardo",Counterfactual Fairness,"arXiv:1703.06856 [cs, stat]",,,,http://arxiv.org/abs/1703.06856,"Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it is the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.",2018-03-08,2022-03-11 00:30:20,2022-03-11 00:30:20,2022-03-11 00:30:20,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1703.06856,,/Users/jacquesthibodeau/Zotero/storage/8UNC2Q22/Kusner et al. - 2018 - Counterfactual Fairness.pdf; /Users/jacquesthibodeau/Zotero/storage/HAZ3ZDVQ/1703.html,,,Computer Science - Computers and Society; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DZSBDWYX,journalArticle,2020,"Mishra, Saurabh; Clark, Jack; Perrault, C. Raymond",Measurement in AI Policy: Opportunities and Challenges,arXiv:2009.09071 [cs],,,,http://arxiv.org/abs/2009.09071,"As artificial intelligence increasingly influences our world, it becomes crucial to assess its technical progress and societal impact. This paper surveys problems and opportunities in the measurement of AI systems and their impact, based on a workshop held at Stanford University in the fall of 2019. We identify six summary challenges inherent to measuring the progress and impact of AI, and summarize over 40 presentations and associated discussions from the workshop. We hope this can inspire research agendas in this crucial area.",2020-09-10,2022-03-11 00:30:24,2022-03-11 00:30:24,2022-03-11 00:30:23,,,,,,,Measurement in AI Policy,,,,,,,,,,,,arXiv.org,,arXiv: 2009.09071,,/Users/jacquesthibodeau/Zotero/storage/UEBGQ7GJ/Mishra et al. - 2020 - Measurement in AI Policy Opportunities and Challe.pdf; /Users/jacquesthibodeau/Zotero/storage/VPKHXFGW/2009.html,,,Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8M59IRFP,journalArticle,2020,"Hämäläinen, Perttu; Babadi, Amin; Ma, Xiaoxiao; Lehtinen, Jaakko",PPO-CMA: Proximal Policy Optimization with Covariance Matrix Adaptation,"arXiv:1810.02541 [cs, stat]",,,,http://arxiv.org/abs/1810.02541,"Proximal Policy Optimization (PPO) is a highly popular model-free reinforcement learning (RL) approach. However, we observe that in a continuous action space, PPO can prematurely shrink the exploration variance, which leads to slow progress and may make the algorithm prone to getting stuck in local optima. Drawing inspiration from CMA-ES, a black-box evolutionary optimization method designed for robustness in similar situations, we propose PPO-CMA, a proximal policy optimization approach that adaptively expands the exploration variance to speed up progress. With only minor changes to PPO, our algorithm considerably improves performance in Roboschool continuous control benchmarks. Our results also show that PPO-CMA, as opposed to PPO, is significantly less sensitive to the choice of hyperparameters, allowing one to use it in complex movement optimization tasks without requiring tedious tuning.",2020-11-03,2022-03-11 00:30:30,2022-03-11 00:30:30,2022-03-11 00:30:30,,,,,,,PPO-CMA,,,,,,,,,,,,arXiv.org,,arXiv: 1810.02541,,/Users/jacquesthibodeau/Zotero/storage/B4VY2NZA/Hämäläinen et al. - 2020 - PPO-CMA Proximal Policy Optimization with Covaria.pdf; /Users/jacquesthibodeau/Zotero/storage/HWWDBFTB/1810.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AI78XF9B,journalArticle,2020,"Garcez, Artur d'Avila; Lamb, Luis C.",Neurosymbolic AI: The 3rd Wave,arXiv:2012.05876 [cs],,,,http://arxiv.org/abs/2012.05876,"Current advances in Artificial Intelligence (AI) and Machine Learning (ML) have achieved unprecedented impact across research communities and industry. Nevertheless, concerns about trust, safety, interpretability and accountability of AI were raised by influential thinkers. Many have identified the need for well-founded knowledge representation and reasoning to be integrated with deep learning and for sound explainability. Neural-symbolic computing has been an active area of research for many years seeking to bring together robust learning in neural networks with reasoning and explainability via symbolic representations for network models. In this paper, we relate recent and early research results in neurosymbolic AI with the objective of identifying the key ingredients of the next wave of AI systems. We focus on research that integrates in a principled way neural network-based learning with symbolic knowledge representation and logical reasoning. The insights provided by 20 years of neural-symbolic computing are shown to shed new light onto the increasingly prominent role of trust, safety, interpretability and accountability of AI. We also identify promising directions and challenges for the next decade of AI research from the perspective of neural-symbolic systems.",2020-12-16,2022-03-11 00:30:33,2022-03-11 00:30:33,2022-03-11 00:30:33,,,,,,,Neurosymbolic AI,,,,,,,,,,,,arXiv.org,,arXiv: 2012.05876,,/Users/jacquesthibodeau/Zotero/storage/W74BGLU4/Garcez and Lamb - 2020 - Neurosymbolic AI The 3rd Wave.pdf; /Users/jacquesthibodeau/Zotero/storage/5IKSJA2A/2012.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; I.2.4; I.2.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
M9UP6XT5,journalArticle,2019,"Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina",BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,arXiv:1810.04805 [cs],,,,http://arxiv.org/abs/1810.04805,"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",2019-05-24,2022-03-11 00:30:37,2022-03-11 00:30:37,2022-03-11 00:30:37,,,,,,,BERT,,,,,,,,,,,,arXiv.org,,arXiv: 1810.04805,,/Users/jacquesthibodeau/Zotero/storage/W7MUYFB6/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf; /Users/jacquesthibodeau/Zotero/storage/ABQFIA7D/1810.html,,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BRHRJGD8,journalArticle,2019,"Ramachandran, Prajit; Parmar, Niki; Vaswani, Ashish; Bello, Irwan; Levskaya, Anselm; Shlens, Jonathon",Stand-Alone Self-Attention in Vision Models,arXiv:1906.05909 [cs],,,,http://arxiv.org/abs/1906.05909,"Convolutions are a fundamental building block of modern computer vision systems. Recent approaches have argued for going beyond convolutions in order to capture long-range dependencies. These efforts focus on augmenting convolutional models with content-based interactions, such as self-attention and non-local means, to achieve gains on a number of vision tasks. The natural question that arises is whether attention can be a stand-alone primitive for vision models instead of serving as just an augmentation on top of convolutions. In developing and testing a pure self-attention vision model, we verify that self-attention can indeed be an effective stand-alone layer. A simple procedure of replacing all instances of spatial convolutions with a form of self-attention applied to ResNet model produces a fully self-attentional model that outperforms the baseline on ImageNet classification with 12% fewer FLOPS and 29% fewer parameters. On COCO object detection, a pure self-attention model matches the mAP of a baseline RetinaNet while having 39% fewer FLOPS and 34% fewer parameters. Detailed ablation studies demonstrate that self-attention is especially impactful when used in later layers. These results establish that stand-alone self-attention is an important addition to the vision practitioner's toolbox.",2019-06-13,2022-03-11 00:30:42,2022-03-11 00:30:42,2022-03-11 00:30:42,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1906.05909,,/Users/jacquesthibodeau/Zotero/storage/SNCTRMZ4/Ramachandran et al. - 2019 - Stand-Alone Self-Attention in Vision Models.pdf; /Users/jacquesthibodeau/Zotero/storage/SAFN3FL6/1906.html,,,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PSF2D2HR,journalArticle,2021,"Tsividis, Pedro A.; Loula, Joao; Burga, Jake; Foss, Nathan; Campero, Andres; Pouncy, Thomas; Gershman, Samuel J.; Tenenbaum, Joshua B.","Human-Level Reinforcement Learning through Theory-Based Modeling, Exploration, and Planning",arXiv:2107.12544 [cs],,,,http://arxiv.org/abs/2107.12544,"Reinforcement learning (RL) studies how an agent comes to achieve reward in an environment through interactions over time. Recent advances in machine RL have surpassed human expertise at the world's oldest board games and many classic video games, but they require vast quantities of experience to learn successfully -- none of today's algorithms account for the human ability to learn so many different tasks, so quickly. Here we propose a new approach to this challenge based on a particularly strong form of model-based RL which we call Theory-Based Reinforcement Learning, because it uses human-like intuitive theories -- rich, abstract, causal models of physical objects, intentional agents, and their interactions -- to explore and model an environment, and plan effectively to achieve task goals. We instantiate the approach in a video game playing agent called EMPA (the Exploring, Modeling, and Planning Agent), which performs Bayesian inference to learn probabilistic generative models expressed as programs for a game-engine simulator, and runs internal simulations over these models to support efficient object-based, relational exploration and heuristic planning. EMPA closely matches human learning efficiency on a suite of 90 challenging Atari-style video games, learning new games in just minutes of game play and generalizing robustly to new game situations and new levels. The model also captures fine-grained structure in people's exploration trajectories and learning dynamics. Its design and behavior suggest a way forward for building more general human-like AI systems.",2021-07-26,2022-03-11 00:33:04,2022-03-11 00:33:04,2022-03-11 00:33:04,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2107.12544,,/Users/jacquesthibodeau/Zotero/storage/ICGUHKRE/Tsividis et al. - 2021 - Human-Level Reinforcement Learning through Theory-.pdf; /Users/jacquesthibodeau/Zotero/storage/23XJFZNC/2107.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Z23IGLPG,journalArticle,2019,"Achiam, Joshua; Knight, Ethan; Abbeel, Pieter",Towards Characterizing Divergence in Deep Q-Learning,arXiv:1903.08894 [cs],,,,http://arxiv.org/abs/1903.08894,"Deep Q-Learning (DQL), a family of temporal difference algorithms for control, employs three techniques collectively known as the `deadly triad' in reinforcement learning: bootstrapping, off-policy learning, and function approximation. Prior work has demonstrated that together these can lead to divergence in Q-learning algorithms, but the conditions under which divergence occurs are not well-understood. In this note, we give a simple analysis based on a linear approximation to the Q-value updates, which we believe provides insight into divergence under the deadly triad. The central point in our analysis is to consider when the leading order approximation to the deep-Q update is or is not a contraction in the sup norm. Based on this analysis, we develop an algorithm which permits stable deep Q-learning for continuous control without any of the tricks conventionally used (such as target networks, adaptive gradient optimizers, or using multiple Q functions). We demonstrate that our algorithm performs above or near state-of-the-art on standard MuJoCo benchmarks from the OpenAI Gym.",2019-03-21,2022-03-11 00:33:07,2022-03-11 00:33:07,2022-03-11 00:33:06,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1903.08894,,/Users/jacquesthibodeau/Zotero/storage/GRMW4YVF/Achiam et al. - 2019 - Towards Characterizing Divergence in Deep Q-Learni.pdf; /Users/jacquesthibodeau/Zotero/storage/EDVKUZRH/1903.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WJVN8UJY,journalArticle,2019,"Lynch, Corey; Khansari, Mohi; Xiao, Ted; Kumar, Vikash; Tompson, Jonathan; Levine, Sergey; Sermanet, Pierre",Learning Latent Plans from Play,arXiv:1903.01973 [cs],,,,http://arxiv.org/abs/1903.01973,"Acquiring a diverse repertoire of general-purpose skills remains an open challenge for robotics. In this work, we propose self-supervising control on top of human teleoperated play data as a way to scale up skill learning. Play has two properties that make it attractive compared to conventional task demonstrations. Play is cheap, as it can be collected in large quantities quickly without task segmenting, labeling, or resetting to an initial state. Play is naturally rich, covering ~4x more interaction space than task demonstrations for the same amount of collection time. To learn control from play, we introduce Play-LMP, a self-supervised method that learns to organize play behaviors in a latent space, then reuse them at test time to achieve specific goals. Combining self-supervised control with a diverse play dataset shifts the focus of skill learning from a narrow and discrete set of tasks to the full continuum of behaviors available in an environment. We find that this combination generalizes well empirically---after self-supervising on unlabeled play, our method substantially outperforms individual expert-trained policies on 18 difficult user-specified visual manipulation tasks in a simulated robotic tabletop environment. We additionally find that play-supervised models, unlike their expert-trained counterparts, are more robust to perturbations and exhibit retrying-till-success behaviors. Finally, we find that our agent organizes its latent plan space around functional tasks, despite never being trained with task labels. Videos, code and data are available at learning-from-play.github.io",2019-12-20,2022-03-11 00:33:30,2022-03-11 00:33:30,2022-03-11 00:33:30,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1903.01973,,/Users/jacquesthibodeau/Zotero/storage/8XLXFE7B/Lynch et al. - 2019 - Learning Latent Plans from Play.pdf; /Users/jacquesthibodeau/Zotero/storage/B2RIBE7N/1903.html,,,Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BQNM4NVC,journalArticle,2020,"Schrittwieser, Julian; Antonoglou, Ioannis; Hubert, Thomas; Simonyan, Karen; Sifre, Laurent; Schmitt, Simon; Guez, Arthur; Lockhart, Edward; Hassabis, Demis; Graepel, Thore; Lillicrap, Timothy; Silver, David","Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",Nature,,"0028-0836, 1476-4687",10.1038/s41586-020-03051-4,http://arxiv.org/abs/1911.08265,"Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.",2020-12-24,2022-03-11 00:33:33,2022-03-11 00:33:33,2022-03-11 00:33:33,604-609,,7839,588,,Nature,,,,,,,,,,,,,arXiv.org,,arXiv: 1911.08265,,"/Users/jacquesthibodeau/Zotero/storage/EXSBLA3L/Schrittwieser et al. - 2020 - Mastering Atari, Go, Chess and Shogi by Planning w.pdf; /Users/jacquesthibodeau/Zotero/storage/W42DTMHZ/1911.html",,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VWL3E59U,journalArticle,2018,"McAleer, Stephen; Agostinelli, Forest; Shmakov, Alexander; Baldi, Pierre",Solving the Rubik's Cube Without Human Knowledge,arXiv:1805.07470 [cs],,,,http://arxiv.org/abs/1805.07470,"A generally intelligent agent must be able to teach itself how to solve problems in complex domains with minimal human supervision. Recently, deep reinforcement learning algorithms combined with self-play have achieved superhuman proficiency in Go, Chess, and Shogi without human data or domain knowledge. In these environments, a reward is always received at the end of the game, however, for many combinatorial optimization environments, rewards are sparse and episodes are not guaranteed to terminate. We introduce Autodidactic Iteration: a novel reinforcement learning algorithm that is able to teach itself how to solve the Rubik's Cube with no human assistance. Our algorithm is able to solve 100% of randomly scrambled cubes while achieving a median solve length of 30 moves -- less than or equal to solvers that employ human domain knowledge.",2018-05-18,2022-03-11 00:33:34,2022-03-11 00:33:34,2022-03-11 00:33:34,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1805.07470,,/Users/jacquesthibodeau/Zotero/storage/TR9F8GMC/McAleer et al. - 2018 - Solving the Rubik's Cube Without Human Knowledge.pdf; /Users/jacquesthibodeau/Zotero/storage/5VNLMHPN/1805.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9R86ZKTI,journalArticle,2019,"OpenAI; Berner, Christopher; Brockman, Greg; Chan, Brooke; Cheung, Vicki; Dębiak, Przemysław; Dennison, Christy; Farhi, David; Fischer, Quirin; Hashme, Shariq; Hesse, Chris; Józefowicz, Rafal; Gray, Scott; Olsson, Catherine; Pachocki, Jakub; Petrov, Michael; Pinto, Henrique P. d O.; Raiman, Jonathan; Salimans, Tim; Schlatter, Jeremy; Schneider, Jonas; Sidor, Szymon; Sutskever, Ilya; Tang, Jie; Wolski, Filip; Zhang, Susan",Dota 2 with Large Scale Deep Reinforcement Learning,"arXiv:1912.06680 [cs, stat]",,,,http://arxiv.org/abs/1912.06680,"On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.",2019-12-13,2022-03-11 00:33:36,2022-03-11 00:33:36,2022-03-11 00:33:36,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1912.06680,,/Users/jacquesthibodeau/Zotero/storage/MFT65EEP/OpenAI et al. - 2019 - Dota 2 with Large Scale Deep Reinforcement Learnin.pdf; /Users/jacquesthibodeau/Zotero/storage/4CYUQFXY/1912.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A6CXTKX7,journalArticle,2018,"Aytar, Yusuf; Pfaff, Tobias; Budden, David; Paine, Tom Le; Wang, Ziyu; de Freitas, Nando",Playing hard exploration games by watching YouTube,"arXiv:1805.11592 [cs, stat]",,,,http://arxiv.org/abs/1805.11592,"Deep reinforcement learning methods traditionally struggle with tasks where environment rewards are particularly sparse. One successful method of guiding exploration in these domains is to imitate trajectories provided by a human demonstrator. However, these demonstrations are typically collected under artificial conditions, i.e. with access to the agent's exact environment setup and the demonstrator's action and reward trajectories. Here we propose a two-stage method that overcomes these limitations by relying on noisy, unaligned footage without access to such data. First, we learn to map unaligned videos from multiple sources to a common representation using self-supervised objectives constructed over both time and modality (i.e. vision and sound). Second, we embed a single YouTube video in this representation to construct a reward function that encourages an agent to imitate human gameplay. This method of one-shot imitation allows our agent to convincingly exceed human-level performance on the infamously hard exploration games Montezuma's Revenge, Pitfall! and Private Eye for the first time, even if the agent is not presented with any environment rewards.",2018-11-30,2022-03-11 00:33:38,2022-03-11 00:33:38,2022-03-11 00:33:38,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1805.11592,,/Users/jacquesthibodeau/Zotero/storage/KZAEZ5ET/Aytar et al. - 2018 - Playing hard exploration games by watching YouTube.pdf; /Users/jacquesthibodeau/Zotero/storage/Z8ERTZA8/1805.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FS3KHCNI,journalArticle,2021,"Evans, Charles; Kasirzadeh, Atoosa",User Tampering in Reinforcement Learning Recommender Systems,arXiv:2109.04083 [cs],,,,http://arxiv.org/abs/2109.04083,"This paper provides the first formalisation and empirical demonstration of a particular safety concern in reinforcement learning (RL)-based news and social media recommendation algorithms. This safety concern is what we call ""user tampering"" -- a phenomenon whereby an RL-based recommender system may manipulate a media user's opinions, preferences and beliefs via its recommendations as part of a policy to increase long-term user engagement. We provide a simulation study of a media recommendation problem constrained to the recommendation of political content, and demonstrate that a Q-learning algorithm consistently learns to exploit its opportunities to 'polarise' simulated 'users' with its early recommendations in order to have more consistent success with later recommendations catering to that polarisation. Finally, we argue that given our findings, designing an RL-based recommender system which cannot learn to exploit user tampering requires making the metric for the recommender's success independent of observable signals of user engagement, and thus that a media recommendation system built solely with RL is necessarily either unsafe, or almost certainly commercially unviable.",2021-09-09,2022-03-11 00:33:41,2022-03-11 00:33:41,2022-03-11 00:33:40,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2109.04083,,/Users/jacquesthibodeau/Zotero/storage/SYFSTEJ4/Evans and Kasirzadeh - 2021 - User Tampering in Reinforcement Learning Recommend.pdf; /Users/jacquesthibodeau/Zotero/storage/93SPQH6C/2109.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MVZAHE3A,journalArticle,2021,"Stray, Jonathan",Designing Recommender Systems to Depolarize,arXiv:2107.04953 [cs],,,,http://arxiv.org/abs/2107.04953,"Polarization is implicated in the erosion of democracy and the progression to violence, which makes the polarization properties of large algorithmic content selection systems (recommender systems) a matter of concern for peace and security. While algorithm-driven social media does not seem to be a primary driver of polarization at the country level, it could be a useful intervention point in polarized societies. This paper examines algorithmic depolarization interventions with the goal of conflict transformation: not suppressing or eliminating conflict but moving towards more constructive conflict. Algorithmic intervention is considered at three stages: which content is available (moderation), how content is selected and personalized (ranking), and content presentation and controls (user interface). Empirical studies of online conflict suggest that the exposure diversity intervention proposed as an antidote to ""filter bubbles"" can be improved and can even worsen polarization under some conditions. Using civility metrics in conjunction with diversity in content selection may be more effective. However, diversity-based interventions have not been tested at scale and may not work in the diverse and dynamic contexts of real platforms. Instead, intervening in platform polarization dynamics will likely require continuous monitoring of polarization metrics, such as the widely used ""feeling thermometer."" These metrics can be used to evaluate product features, and potentially engineered as algorithmic objectives. It may further prove necessary to include polarization measures in the objective functions of recommender algorithms to prevent optimization processes from creating conflict as a side effect.",2021-07-10,2022-03-11 00:33:43,2022-03-11 00:33:43,2022-03-11 00:33:43,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2107.04953,,/Users/jacquesthibodeau/Zotero/storage/9HJRYVZQ/Stray - 2021 - Designing Recommender Systems to Depolarize.pdf; /Users/jacquesthibodeau/Zotero/storage/865PUTQ9/2107.html,,,Computer Science - Computers and Society; Computer Science - Information Retrieval; Computer Science - Social and Information Networks; J.4; K.4.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7MGE4MN6,journalArticle,2021,"Milli, Smitha; Belli, Luca; Hardt, Moritz",From Optimizing Engagement to Measuring Value,"Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",,,10.1145/3442188.3445933,http://arxiv.org/abs/2008.12623,"Most recommendation engines today are based on predicting user engagement, e.g. predicting whether a user will click on an item or not. However, there is potentially a large gap between engagement signals and a desired notion of ""value"" that is worth optimizing for. We use the framework of measurement theory to (a) confront the designer with a normative question about what the designer values, (b) provide a general latent variable model approach that can be used to operationalize the target construct and directly optimize for it, and (c) guide the designer in evaluating and revising their operationalization. We implement our approach on the Twitter platform on millions of users. In line with established approaches to assessing the validity of measurements, we perform a qualitative evaluation of how well our model captures a desired notion of ""value"".",2021-03-03,2022-03-11 00:33:46,2022-03-11 00:33:46,2022-03-11 00:33:45,714-722,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2008.12623,,/Users/jacquesthibodeau/Zotero/storage/IDAKGWMV/Milli et al. - 2021 - From Optimizing Engagement to Measuring Value.pdf; /Users/jacquesthibodeau/Zotero/storage/R9VPMV64/2008.html,,,Computer Science - Machine Learning; Computer Science - Social and Information Networks; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
S6MGPEED,journalArticle,2019,"Eckersley, Peter",Impossibility and Uncertainty Theorems in AI Value Alignment (or why your AGI should not have a utility function),arXiv:1901.00064 [cs],,,,http://arxiv.org/abs/1901.00064,"Utility functions or their equivalents (value functions, objective functions, loss functions, reward functions, preference orderings) are a central tool in most current machine learning systems. These mechanisms for defining goals and guiding optimization run into practical and conceptual difficulty when there are independent, multi-dimensional objectives that need to be pursued simultaneously and cannot be reduced to each other. Ethicists have proved several impossibility theorems that stem from this origin; those results appear to show that there is no way of formally specifying what it means for an outcome to be good for a population without violating strong human ethical intuitions (in such cases, the objective function is a social welfare function). We argue that this is a practical problem for any machine learning system (such as medical decision support systems or autonomous weapons) or rigidly rule-based bureaucracy that will make high stakes decisions about human lives: such systems should not use objective functions in the strict mathematical sense. We explore the alternative of using uncertain objectives, represented for instance as partially ordered preferences, or as probability distributions over total orders. We show that previously known impossibility theorems can be transformed into uncertainty theorems in both of those settings, and prove lower bounds on how much uncertainty is implied by the impossibility results. We close by proposing two conjectures about the relationship between uncertainty in objectives and severe unintended consequences from AI systems.",2019-03-04,2022-03-11 00:33:47,2022-03-11 00:33:47,2022-03-11 00:33:47,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1901.00064,,/Users/jacquesthibodeau/Zotero/storage/SNSESZY4/Eckersley - 2019 - Impossibility and Uncertainty Theorems in AI Value.pdf; /Users/jacquesthibodeau/Zotero/storage/PRUXRYYW/1901.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2CNAP7E4,journalArticle,2020,"Ecoffet, Adrien; Clune, Jeff; Lehman, Joel",Open Questions in Creating Safe Open-ended AI: Tensions Between Control and Creativity,arXiv:2006.07495 [cs],,,,http://arxiv.org/abs/2006.07495,"Artificial life originated and has long studied the topic of open-ended evolution, which seeks the principles underlying artificial systems that innovate continually, inspired by biological evolution. Recently, interest has grown within the broader field of AI in a generalization of open-ended evolution, here called open-ended search, wherein such questions of open-endedness are explored for advancing AI, whatever the nature of the underlying search algorithm (e.g. evolutionary or gradient-based). For example, open-ended search might design new architectures for neural networks, new reinforcement learning algorithms, or most ambitiously, aim at designing artificial general intelligence. This paper proposes that open-ended evolution and artificial life have much to contribute towards the understanding of open-ended AI, focusing here in particular on the safety of open-ended search. The idea is that AI systems are increasingly applied in the real world, often producing unintended harms in the process, which motivates the growing field of AI safety. This paper argues that open-ended AI has its own safety challenges, in particular, whether the creativity of open-ended systems can be productively and predictably controlled. This paper explains how unique safety problems manifest in open-ended search, and suggests concrete contributions and research questions to explore them. The hope is to inspire progress towards creative, useful, and safe open-ended search algorithms.",2020-06-12,2022-03-11 00:33:50,2022-03-11 00:33:50,2022-03-11 00:33:50,,,,,,,Open Questions in Creating Safe Open-ended AI,,,,,,,,,,,,arXiv.org,,arXiv: 2006.07495,,/Users/jacquesthibodeau/Zotero/storage/4UQVZ9RP/Ecoffet et al. - 2020 - Open Questions in Creating Safe Open-ended AI Ten.pdf; /Users/jacquesthibodeau/Zotero/storage/Q9EDWC2Z/2006.html,,,Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZAMYM89F,journalArticle,2021,"Bommasani, Rishi; Hudson, Drew A.; Adeli, Ehsan; Altman, Russ; Arora, Simran; von Arx, Sydney; Bernstein, Michael S.; Bohg, Jeannette; Bosselut, Antoine; Brunskill, Emma; Brynjolfsson, Erik; Buch, Shyamal; Card, Dallas; Castellon, Rodrigo; Chatterji, Niladri; Chen, Annie; Creel, Kathleen; Davis, Jared Quincy; Demszky, Dora; Donahue, Chris; Doumbouya, Moussa; Durmus, Esin; Ermon, Stefano; Etchemendy, John; Ethayarajh, Kawin; Fei-Fei, Li; Finn, Chelsea; Gale, Trevor; Gillespie, Lauren; Goel, Karan; Goodman, Noah; Grossman, Shelby; Guha, Neel; Hashimoto, Tatsunori; Henderson, Peter; Hewitt, John; Ho, Daniel E.; Hong, Jenny; Hsu, Kyle; Huang, Jing; Icard, Thomas; Jain, Saahil; Jurafsky, Dan; Kalluri, Pratyusha; Karamcheti, Siddharth; Keeling, Geoff; Khani, Fereshte; Khattab, Omar; Koh, Pang Wei; Krass, Mark; Krishna, Ranjay; Kuditipudi, Rohith; Kumar, Ananya; Ladhak, Faisal; Lee, Mina; Lee, Tony; Leskovec, Jure; Levent, Isabelle; Li, Xiang Lisa; Li, Xuechen; Ma, Tengyu; Malik, Ali; Manning, Christopher D.; Mirchandani, Suvir; Mitchell, Eric; Munyikwa, Zanele; Nair, Suraj; Narayan, Avanika; Narayanan, Deepak; Newman, Ben; Nie, Allen; Niebles, Juan Carlos; Nilforoshan, Hamed; Nyarko, Julian; Ogut, Giray; Orr, Laurel; Papadimitriou, Isabel; Park, Joon Sung; Piech, Chris; Portelance, Eva; Potts, Christopher; Raghunathan, Aditi; Reich, Rob; Ren, Hongyu; Rong, Frieda; Roohani, Yusuf; Ruiz, Camilo; Ryan, Jack; Ré, Christopher; Sadigh, Dorsa; Sagawa, Shiori; Santhanam, Keshav; Shih, Andy; Srinivasan, Krishnan; Tamkin, Alex; Taori, Rohan; Thomas, Armin W.; Tramèr, Florian; Wang, Rose E.; Wang, William; Wu, Bohan; Wu, Jiajun; Wu, Yuhuai; Xie, Sang Michael; Yasunaga, Michihiro; You, Jiaxuan; Zaharia, Matei; Zhang, Michael; Zhang, Tianyi; Zhang, Xikun; Zhang, Yuhui; Zheng, Lucia; Zhou, Kaitlyn; Liang, Percy",On the Opportunities and Risks of Foundation Models,arXiv:2108.07258 [cs],,,,http://arxiv.org/abs/2108.07258,"AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.",2021-08-18,2022-03-11 00:33:53,2022-03-11 00:33:53,2022-03-11 00:33:52,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2108.07258,,/Users/jacquesthibodeau/Zotero/storage/63IM3WKS/Bommasani et al. - 2021 - On the Opportunities and Risks of Foundation Model.pdf; /Users/jacquesthibodeau/Zotero/storage/C694NVYB/2108.html,,,Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6ZA42JDT,journalArticle,2021,"Lin, Stephanie; Hilton, Jacob; Evans, Owain",TruthfulQA: Measuring How Models Mimic Human Falsehoods,arXiv:2109.07958 [cs],,,,http://arxiv.org/abs/2109.07958,"We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. For example, the 6B-parameter GPT-J model was 17% less truthful than its 125M-parameter counterpart. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.",2021-09-08,2022-03-11 00:33:55,2022-03-11 00:33:55,2022-03-11 00:33:55,,,,,,,TruthfulQA,,,,,,,,,,,,arXiv.org,,arXiv: 2109.07958,,/Users/jacquesthibodeau/Zotero/storage/ULUFHY9N/Lin et al. - 2021 - TruthfulQA Measuring How Models Mimic Human False.pdf; /Users/jacquesthibodeau/Zotero/storage/CSSSUHWN/2109.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Computers and Society; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8ZSKPUBV,journalArticle,2021,"Bae, Ho; Jang, Jaehee; Jung, Dahuin; Jang, Hyemi; Ha, Heonseok; Lee, Hyungyu; Yoon, Sungroh",Security and Privacy Issues in Deep Learning,"arXiv:1807.11655 [cs, stat]",,,,http://arxiv.org/abs/1807.11655,"To promote secure and private artificial intelligence (SPAI), we review studies on the model security and data privacy of DNNs. Model security allows system to behave as intended without being affected by malicious external influences that can compromise its integrity and efficiency. Security attacks can be divided based on when they occur: if an attack occurs during training, it is known as a poisoning attack, and if it occurs during inference (after training) it is termed an evasion attack. Poisoning attacks compromise the training process by corrupting the data with malicious examples, while evasion attacks use adversarial examples to disrupt entire classification process. Defenses proposed against such attacks include techniques to recognize and remove malicious data, train a model to be insensitive to such data, and mask the model's structure and parameters to render attacks more challenging to implement. Furthermore, the privacy of the data involved in model training is also threatened by attacks such as the model-inversion attack, or by dishonest service providers of AI applications. To maintain data privacy, several solutions that combine existing data-privacy techniques have been proposed, including differential privacy and modern cryptography techniques. In this paper, we describe the notions of some of methods, e.g., homomorphic encryption, and review their advantages and challenges when implemented in deep-learning models.",2021-03-09,2022-03-11 00:34:01,2022-03-11 00:34:01,2022-03-11 00:33:57,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1807.11655,,/Users/jacquesthibodeau/Zotero/storage/QMQ3Q4YR/Bae et al. - 2021 - Security and Privacy Issues in Deep Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/Y2CFJN2W/1807.html,,,Computer Science - Cryptography and Security; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AK9DRADR,journalArticle,2018,"Ma, Lei; Juefei-Xu, Felix; Xue, Minhui; Hu, Qiang; Chen, Sen; Li, Bo; Liu, Yang; Zhao, Jianjun; Yin, Jianxiong; See, Simon",Secure Deep Learning Engineering: A Software Quality Assurance Perspective,arXiv:1810.04538 [cs],,,,http://arxiv.org/abs/1810.04538,"Over the past decades, deep learning (DL) systems have achieved tremendous success and gained great popularity in various applications, such as intelligent machines, image processing, speech processing, and medical diagnostics. Deep neural networks are the key driving force behind its recent success, but still seem to be a magic black box lacking interpretability and understanding. This brings up many open safety and security issues with enormous and urgent demands on rigorous methodologies and engineering practice for quality enhancement. A plethora of studies have shown that the state-of-the-art DL systems suffer from defects and vulnerabilities that can lead to severe loss and tragedies, especially when applied to real-world safety-critical applications. In this paper, we perform a large-scale study and construct a paper repository of 223 relevant works to the quality assurance, security, and interpretation of deep learning. We, from a software quality assurance perspective, pinpoint challenges and future opportunities towards universal secure deep learning engineering. We hope this work and the accompanied paper repository can pave the path for the software engineering community towards addressing the pressing industrial demand of secure intelligent applications.",2018-10-10,2022-03-11 00:34:02,2022-03-11 00:34:02,2022-03-11 00:34:02,,,,,,,Secure Deep Learning Engineering,,,,,,,,,,,,arXiv.org,,arXiv: 1810.04538,,/Users/jacquesthibodeau/Zotero/storage/2RRF6S76/Ma et al. - 2018 - Secure Deep Learning Engineering A Software Quali.pdf; /Users/jacquesthibodeau/Zotero/storage/DV7EUBKW/1810.html,,,Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security; Computer Science - Machine Learning; Computer Science - Software Engineering,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FA57VAL6,journalArticle,2018,"Milli, Smitha; Schmidt, Ludwig; Dragan, Anca D.; Hardt, Moritz",Model Reconstruction from Model Explanations,"arXiv:1807.05185 [cs, stat]",,,,http://arxiv.org/abs/1807.05185,"We show through theory and experiment that gradient-based explanations of a model quickly reveal the model itself. Our results speak to a tension between the desire to keep a proprietary model secret and the ability to offer model explanations. On the theoretical side, we give an algorithm that provably learns a two-layer ReLU network in a setting where the algorithm may query the gradient of the model with respect to chosen inputs. The number of queries is independent of the dimension and nearly optimal in its dependence on the model size. Of interest not only from a learning-theoretic perspective, this result highlights the power of gradients rather than labels as a learning primitive. Complementing our theory, we give effective heuristics for reconstructing models from gradient explanations that are orders of magnitude more query-efficient than reconstruction attacks relying on prediction interfaces.",2018-07-13,2022-03-11 00:34:04,2022-03-11 00:34:04,2022-03-11 00:34:00,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1807.05185,,/Users/jacquesthibodeau/Zotero/storage/U74LY37T/Milli et al. - 2018 - Model Reconstruction from Model Explanations.pdf; /Users/jacquesthibodeau/Zotero/storage/NWBIQB53/1807.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GVBD3UR6,journalArticle,2018,"Papernot, Nicolas",A Marauder's Map of Security and Privacy in Machine Learning,arXiv:1811.01134 [cs],,,,http://arxiv.org/abs/1811.01134,"There is growing recognition that machine learning (ML) exposes new security and privacy vulnerabilities in software systems, yet the technical community's understanding of the nature and extent of these vulnerabilities remains limited but expanding. In this talk, we explore the threat model space of ML algorithms through the lens of Saltzer and Schroeder's principles for the design of secure computer systems. This characterization of the threat space prompts an investigation of current and future research directions. We structure our discussion around three of these directions, which we believe are likely to lead to significant progress. The first encompasses a spectrum of approaches to verification and admission control, which is a prerequisite to enable fail-safe defaults in machine learning systems. The second seeks to design mechanisms for assembling reliable records of compromise that would help understand the degree to which vulnerabilities are exploited by adversaries, as well as favor psychological acceptability of machine learning applications. The third pursues formal frameworks for security and privacy in machine learning, which we argue should strive to align machine learning goals such as generalization with security and privacy desiderata like robustness or privacy. Key insights resulting from these three directions pursued both in the ML and security communities are identified and the effectiveness of approaches are related to structural elements of ML algorithms and the data used to train them. We conclude by systematizing best practices in our community.",2018-11-02,2022-03-11 00:34:05,2022-03-11 00:34:05,2022-03-11 00:34:04,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1811.01134,,/Users/jacquesthibodeau/Zotero/storage/XRIBUKJX/Papernot - 2018 - A Marauder's Map of Security and Privacy in Machin.pdf; /Users/jacquesthibodeau/Zotero/storage/G392P8MA/1811.html,,,Computer Science - Cryptography and Security,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6RWQ2TWN,journalArticle,2021,"Carlini, Nicholas; Tramer, Florian; Wallace, Eric; Jagielski, Matthew; Herbert-Voss, Ariel; Lee, Katherine; Roberts, Adam; Brown, Tom; Song, Dawn; Erlingsson, Ulfar; Oprea, Alina; Raffel, Colin",Extracting Training Data from Large Language Models,arXiv:2012.07805 [cs],,,,http://arxiv.org/abs/2012.07805,"It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. We demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. We comprehensively evaluate our extraction attack to understand the factors that contribute to its success. Worryingly, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.",2021-06-15,2022-03-11 00:34:16,2022-03-11 00:34:16,2022-03-11 00:34:16,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2012.07805,,/Users/jacquesthibodeau/Zotero/storage/XVPFZRYY/Carlini et al. - 2021 - Extracting Training Data from Large Language Model.pdf; /Users/jacquesthibodeau/Zotero/storage/SXBX6TTA/2012.html,,,Computer Science - Computation and Language; Computer Science - Cryptography and Security; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VC6RD9SS,journalArticle,2021,"Pittman, Jason M.; Espinoza, Jesus P.; Crosby, Courtney",Stovepiping and Malicious Software: A Critical Review of AGI Containment,arXiv:1811.03653 [cs],,,,http://arxiv.org/abs/1811.03653,"Awareness of the possible impacts associated with artificial intelligence has risen in proportion to progress in the field. While there are tremendous benefits to society, many argue that there are just as many, if not more, concerns related to advanced forms of artificial intelligence. Accordingly, research into methods to develop artificial intelligence safely is increasingly important. In this paper, we provide an overview of one such safety paradigm: containment with a critical lens aimed toward generative adversarial networks and potentially malicious artificial intelligence. Additionally, we illuminate the potential for a developmental blindspot in the stovepiping of containment mechanisms.",2021-08-01,2022-03-11 00:34:19,2022-03-11 00:34:19,2022-03-11 00:34:19,,,,,,,Stovepiping and Malicious Software,,,,,,,,,,,,arXiv.org,,arXiv: 1811.03653,,/Users/jacquesthibodeau/Zotero/storage/QMM8NDK3/Pittman et al. - 2021 - Stovepiping and Malicious Software A Critical Rev.pdf; /Users/jacquesthibodeau/Zotero/storage/3Q5PKANJ/1811.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GVBY8D7W,journalArticle,2021,"Jain, Arushi; Khetarpal, Khimya; Precup, Doina",Safe Option-Critic: Learning Safety in the Option-Critic Architecture,The Knowledge Engineering Review,,"0269-8889, 1469-8005",10.1017/S0269888921000035,http://arxiv.org/abs/1807.08060,"Designing hierarchical reinforcement learning algorithms that exhibit safe behaviour is not only vital for practical applications but also, facilitates a better understanding of an agent's decisions. We tackle this problem in the options framework, a particular way to specify temporally abstract actions which allow an agent to use sub-policies with start and end conditions. We consider a behaviour as safe that avoids regions of state-space with high uncertainty in the outcomes of actions. We propose an optimization objective that learns safe options by encouraging the agent to visit states with higher behavioural consistency. The proposed objective results in a trade-off between maximizing the standard expected return and minimizing the effect of model uncertainty in the return. We propose a policy gradient algorithm to optimize the constrained objective function. We examine the quantitative and qualitative behaviour of the proposed approach in a tabular grid-world, continuous-state puddle-world, and three games from the Arcade Learning Environment: Ms.Pacman, Amidar, and Q*Bert. Our approach achieves a reduction in the variance of return, boosts performance in environments with intrinsic variability in the reward structure, and compares favorably both with primitive actions as well as with risk-neutral options.",2021,2022-03-11 00:34:22,2022-03-11 00:34:22,2022-03-11 00:34:22,e4,,,36,,The Knowledge Engineering Review,Safe Option-Critic,,,,,,,,,,,,arXiv.org,,arXiv: 1807.08060,,/Users/jacquesthibodeau/Zotero/storage/2SSUE59U/Jain et al. - 2021 - Safe Option-Critic Learning Safety in the Option-.pdf; /Users/jacquesthibodeau/Zotero/storage/7723WXZE/1807.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VJI56G4K,journalArticle,2020,"Miret, Santiago; Majumdar, Somdeb; Wainwright, Carroll",Safety Aware Reinforcement Learning (SARL),arXiv:2010.02846 [cs],,,,http://arxiv.org/abs/2010.02846,"As reinforcement learning agents become increasingly integrated into complex, real-world environments, designing for safety becomes a critical consideration. We specifically focus on researching scenarios where agents can cause undesired side effects while executing a policy on a primary task. Since one can define multiple tasks for a given environment dynamics, there are two important challenges. First, we need to abstract the concept of safety that applies broadly to that environment independent of the specific task being executed. Second, we need a mechanism for the abstracted notion of safety to modulate the actions of agents executing different policies to minimize their side-effects. In this work, we propose Safety Aware Reinforcement Learning (SARL) - a framework where a virtual safe agent modulates the actions of a main reward-based agent to minimize side effects. The safe agent learns a task-independent notion of safety for a given environment. The main agent is then trained with a regularization loss given by the distance between the native action probabilities of the two agents. Since the safe agent effectively abstracts a task-independent notion of safety via its action probabilities, it can be ported to modulate multiple policies solving different tasks within the given environment without further training. We contrast this with solutions that rely on task-specific regularization metrics and test our framework on the SafeLife Suite, based on Conway's Game of Life, comprising a number of complex tasks in dynamic environments. We show that our solution is able to match the performance of solutions that rely on task-specific side-effect penalties on both the primary and safety objectives while additionally providing the benefit of generalizability and portability.",2020-10-06,2022-03-11 00:34:24,2022-03-11 00:34:24,2022-03-11 00:34:24,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2010.02846,,/Users/jacquesthibodeau/Zotero/storage/CE3FJP9R/Miret et al. - 2020 - Safety Aware Reinforcement Learning (SARL).pdf; /Users/jacquesthibodeau/Zotero/storage/KIDAAJI7/2010.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CA64YTYY,journalArticle,2020,"Turner, Alexander Matt; Ratzlaff, Neale; Tadepalli, Prasad",Avoiding Side Effects in Complex Environments,arXiv:2006.06547 [cs],,,,http://arxiv.org/abs/2006.06547,"Reward function specification can be difficult. Rewarding the agent for making a widget may be easy, but penalizing the multitude of possible negative side effects is hard. In toy environments, Attainable Utility Preservation (AUP) avoided side effects by penalizing shifts in the ability to achieve randomly generated goals. We scale this approach to large, randomly generated environments based on Conway's Game of Life. By preserving optimal value for a single randomly generated reward function, AUP incurs modest overhead while leading the agent to complete the specified task and avoid many side effects. Videos and code are available at https://avoiding-side-effects.github.io/.",2020-10-22,2022-03-11 00:34:26,2022-03-11 00:34:26,2022-03-11 00:34:26,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2006.06547,,/Users/jacquesthibodeau/Zotero/storage/KN2YSGU8/Turner et al. - 2020 - Avoiding Side Effects in Complex Environments.pdf; /Users/jacquesthibodeau/Zotero/storage/HJ662BZN/2006.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4LZJLPCI,journalArticle,2021,"Saisubramanian, Sandhya; Zilberstein, Shlomo; Kamar, Ece",Avoiding Negative Side Effects due to Incomplete Knowledge of AI Systems,arXiv:2008.12146 [cs],,,,http://arxiv.org/abs/2008.12146,"Autonomous agents acting in the real-world often operate based on models that ignore certain aspects of the environment. The incompleteness of any given model -- handcrafted or machine acquired -- is inevitable due to practical limitations of any modeling technique for complex real-world settings. Due to the limited fidelity of its model, an agent's actions may have unexpected, undesirable consequences during execution. Learning to recognize and avoid such negative side effects of an agent's actions is critical to improve the safety and reliability of autonomous systems. Mitigating negative side effects is an emerging research topic that is attracting increased attention due to the rapid growth in the deployment of AI systems and their broad societal impacts. This article provides a comprehensive overview of different forms of negative side effects and the recent research efforts to address them. We identify key characteristics of negative side effects, highlight the challenges in avoiding negative side effects, and discuss recently developed approaches, contrasting their benefits and limitations. The article concludes with a discussion of open questions and suggestions for future research directions.",2021-10-18,2022-03-11 00:34:29,2022-03-11 00:34:29,2022-03-11 00:34:29,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2008.12146,,/Users/jacquesthibodeau/Zotero/storage/NJWN9TV6/Saisubramanian et al. - 2021 - Avoiding Negative Side Effects due to Incomplete K.pdf; /Users/jacquesthibodeau/Zotero/storage/HDW668C6/2008.html,,,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZHPSD9YD,journalArticle,2019,"Jansen, Nils; Könighofer, Bettina; Junges, Sebastian; Serban, Alexandru C.; Bloem, Roderick",Safe Reinforcement Learning via Probabilistic Shields,arXiv:1807.06096 [cs],,,,http://arxiv.org/abs/1807.06096,"This paper targets the efficient construction of a safety shield for decision making in scenarios that incorporate uncertainty. Markov decision processes (MDPs) are prominent models to capture such planning problems. Reinforcement learning (RL) is a machine learning technique to determine near-optimal policies in MDPs that may be unknown prior to exploring the model. However, during exploration, RL is prone to induce behavior that is undesirable or not allowed in safety- or mission-critical contexts. We introduce the concept of a probabilistic shield that enables decision-making to adhere to safety constraints with high probability. In a separation of concerns, we employ formal verification to efficiently compute the probabilities of critical decisions within a safety-relevant fragment of the MDP. We use these results to realize a shield that is applied to an RL algorithm which then optimizes the actual performance objective. We discuss tradeoffs between sufficient progress in exploration of the environment and ensuring safety. In our experiments, we demonstrate on the arcade game PAC-MAN and on a case study involving service robots that the learning efficiency increases as the learning needs orders of magnitude fewer episodes.",2019-11-25,2022-03-11 00:34:36,2022-03-11 00:34:36,2022-03-11 00:34:35,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1807.06096,,/Users/jacquesthibodeau/Zotero/storage/9XWY8QLJ/Jansen et al. - 2019 - Safe Reinforcement Learning via Probabilistic Shie.pdf; /Users/jacquesthibodeau/Zotero/storage/GGVG8MNT/1807.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PW5CB7B2,journalArticle,2018,"Leike, Jan; Krueger, David; Everitt, Tom; Martic, Miljan; Maini, Vishal; Legg, Shane",Scalable agent alignment via reward modeling: a research direction,"arXiv:1811.07871 [cs, stat]",,,,http://arxiv.org/abs/1811.07871,"One obstacle to applying reinforcement learning algorithms to real-world problems is the lack of suitable reward functions. Designing such reward functions is difficult in part because the user only has an implicit understanding of the task objective. This gives rise to the agent alignment problem: how do we create agents that behave in accordance with the user's intentions? We outline a high-level research direction to solve the agent alignment problem centered around reward modeling: learning a reward function from interaction with the user and optimizing the learned reward function with reinforcement learning. We discuss the key challenges we expect to face when scaling reward modeling to complex and general domains, concrete approaches to mitigate these challenges, and ways to establish trust in the resulting agents.",2018-11-19,2022-03-11 00:42:06,2022-03-11 00:42:06,2022-03-11 00:42:05,,,,,,,Scalable agent alignment via reward modeling,,,,,,,,,,,,arXiv.org,,arXiv: 1811.07871,,/Users/jacquesthibodeau/Zotero/storage/AIECHVYQ/Leike et al. - 2018 - Scalable agent alignment via reward modeling a re.pdf; /Users/jacquesthibodeau/Zotero/storage/NNLDE4SI/1811.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VA98D7T8,journalArticle,2016,"Amodei, Dario; Olah, Chris; Steinhardt, Jacob; Christiano, Paul; Schulman, John; Mané, Dan",Concrete Problems in AI Safety,arXiv:1606.06565 [cs],,,,http://arxiv.org/abs/1606.06565,"Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function (""avoiding side effects"" and ""avoiding reward hacking""), an objective function that is too expensive to evaluate frequently (""scalable supervision""), or undesirable behavior during the learning process (""safe exploration"" and ""distributional shift""). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.",2016-07-25,2022-03-11 00:42:26,2022-03-11 00:42:26,2022-03-11 00:42:25,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1606.06565,,/Users/jacquesthibodeau/Zotero/storage/R4EVMHFA/Amodei et al. - 2016 - Concrete Problems in AI Safety.pdf; /Users/jacquesthibodeau/Zotero/storage/V4DD97CN/1606.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4EWDEG7I,journalArticle,2022,"Everitt, Tom; Ortega, Pedro A.; Barnes, Elizabeth; Legg, Shane",Understanding Agent Incentives using Causal Influence Diagrams. Part I: Single Action Settings,arXiv:1902.09980 [cs],,,,http://arxiv.org/abs/1902.09980,"Agents are systems that optimize an objective function in an environment. Together, the goal and the environment induce secondary objectives, incentives. Modeling the agent-environment interaction using causal influence diagrams, we can answer two fundamental questions about an agent's incentives directly from the graph: (1) which nodes can the agent have an incentivize to observe, and (2) which nodes can the agent have an incentivize to control? The answers tell us which information and influence points need extra protection. For example, we may want a classifier for job applications to not use the ethnicity of the candidate, and a reinforcement learning agent not to take direct control of its reward mechanism. Different algorithms and training paradigms can lead to different causal influence diagrams, so our method can be used to identify algorithms with problematic incentives and help in designing algorithms with better incentives.",2022-01-20,2022-03-11 00:42:33,2022-03-11 00:42:33,2022-03-11 00:42:33,,,,,,,Understanding Agent Incentives using Causal Influence Diagrams. Part I,,,,,,,,,,,,arXiv.org,,arXiv: 1902.09980,,/Users/jacquesthibodeau/Zotero/storage/E5RDDPHQ/Everitt et al. - 2022 - Understanding Agent Incentives using Causal Influe.pdf; /Users/jacquesthibodeau/Zotero/storage/RSCHXQRY/1902.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; I.2.6; I.2.8,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FRMHYJ6G,journalArticle,2020,"Krakovna, Victoria; Orseau, Laurent; Ngo, Richard; Martic, Miljan; Legg, Shane",Avoiding Side Effects By Considering Future Tasks,arXiv:2010.07877 [cs],,,,http://arxiv.org/abs/2010.07877,"Designing reward functions is difficult: the designer has to specify what to do (what it means to complete the task) as well as what not to do (side effects that should be avoided while completing the task). To alleviate the burden on the reward designer, we propose an algorithm to automatically generate an auxiliary reward function that penalizes side effects. This auxiliary objective rewards the ability to complete possible future tasks, which decreases if the agent causes side effects during the current task. The future task reward can also give the agent an incentive to interfere with events in the environment that make future tasks less achievable, such as irreversible actions by other agents. To avoid this interference incentive, we introduce a baseline policy that represents a default course of action (such as doing nothing), and use it to filter out future tasks that are not achievable by default. We formally define interference incentives and show that the future task approach with a baseline policy avoids these incentives in the deterministic case. Using gridworld environments that test for side effects and interference, we show that our method avoids interference and is more effective for avoiding side effects than the common approach of penalizing irreversible actions.",2020-10-15,2022-03-11 00:42:35,2022-03-11 00:42:35,2022-03-11 00:42:35,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2010.07877,,/Users/jacquesthibodeau/Zotero/storage/33MTNX49/Krakovna et al. - 2020 - Avoiding Side Effects By Considering Future Tasks.pdf; /Users/jacquesthibodeau/Zotero/storage/VH5L2V4Q/2010.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6GHG39AE,journalArticle,2017,"Eysenbach, Benjamin; Gu, Shixiang; Ibarz, Julian; Levine, Sergey",Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning,arXiv:1711.06782 [cs],,,,http://arxiv.org/abs/1711.06782,"Deep reinforcement learning algorithms can learn complex behavioral skills, but real-world application of these methods requires a large amount of experience to be collected by the agent. In practical settings, such as robotics, this involves repeatedly attempting a task, resetting the environment between each attempt. However, not all tasks are easily or automatically reversible. In practice, this learning process requires extensive human intervention. In this work, we propose an autonomous method for safe and efficient reinforcement learning that simultaneously learns a forward and reset policy, with the reset policy resetting the environment for a subsequent attempt. By learning a value function for the reset policy, we can automatically determine when the forward policy is about to enter a non-reversible state, providing for uncertainty-aware safety aborts. Our experiments illustrate that proper use of the reset policy can greatly reduce the number of manual resets required to learn a task, can reduce the number of unsafe actions that lead to non-reversible states, and can automatically induce a curriculum.",2017-11-17,2022-03-11 00:42:46,2022-03-11 00:42:46,2022-03-11 00:42:45,,,,,,,Leave no Trace,,,,,,,,,,,,arXiv.org,,arXiv: 1711.06782,,/Users/jacquesthibodeau/Zotero/storage/QA657MFR/Eysenbach et al. - 2017 - Leave no Trace Learning to Reset for Safe and Aut.pdf; /Users/jacquesthibodeau/Zotero/storage/LDBYGCQ9/1711.html,,,Computer Science - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NYNU6YJM,journalArticle,2017,"Armstrong, Stuart; Levinstein, Benjamin",Low Impact Artificial Intelligences,arXiv:1705.10720 [cs],,,,http://arxiv.org/abs/1705.10720,"There are many goals for an AI that could become dangerous if the AI becomes superintelligent or otherwise powerful. Much work on the AI control problem has been focused on constructing AI goals that are safe even for such AIs. This paper looks at an alternative approach: defining a general concept of `low impact'. The aim is to ensure that a powerful AI which implements low impact will not modify the world extensively, even if it is given a simple or dangerous goal. The paper proposes various ways of defining and grounding low impact, and discusses methods for ensuring that the AI can still be allowed to have a (desired) impact despite the restriction. The end of the paper addresses known issues with this approach and avenues for future research.",2017-05-30,2022-03-11 00:42:48,2022-03-11 00:42:48,2022-03-11 00:42:48,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1705.10720,,/Users/jacquesthibodeau/Zotero/storage/G36NEVU8/Armstrong and Levinstein - 2017 - Low Impact Artificial Intelligences.pdf; /Users/jacquesthibodeau/Zotero/storage/5J3V26QV/1705.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KNICDEFZ,journalArticle,2021,"Turner, Alexander Matt; Smith, Logan; Shah, Rohin; Critch, Andrew; Tadepalli, Prasad",Optimal Policies Tend to Seek Power,arXiv:1912.01683 [cs],,,,http://arxiv.org/abs/1912.01683,"Some researchers speculate that intelligent reinforcement learning (RL) agents would be incentivized to seek resources and power in pursuit of their objectives. Other researchers point out that RL agents need not have human-like power-seeking instincts. To clarify this discussion, we develop the first formal theory of the statistical tendencies of optimal policies. In the context of Markov decision processes, we prove that certain environmental symmetries are sufficient for optimal policies to tend to seek power over the environment. These symmetries exist in many environments in which the agent can be shut down or destroyed. We prove that in these environments, most reward functions make it optimal to seek power by keeping a range of options available and, when maximizing average reward, by navigating towards larger sets of potential terminal states.",2021-12-03,2022-03-11 00:42:51,2022-03-11 00:42:51,2022-03-11 00:42:51,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1912.01683,,/Users/jacquesthibodeau/Zotero/storage/73K4TGAT/Turner et al. - 2021 - Optimal Policies Tend to Seek Power.pdf; /Users/jacquesthibodeau/Zotero/storage/SQQ73FKH/1912.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
63NDGA9I,journalArticle,2017,"Mhamdi, El Mahdi El; Guerraoui, Rachid; Hendrikx, Hadrien; Maurer, Alexandre",Dynamic Safe Interruptibility for Decentralized Multi-Agent Reinforcement Learning,"arXiv:1704.02882 [cs, stat]",,,,http://arxiv.org/abs/1704.02882,"In reinforcement learning, agents learn by performing actions and observing their outcomes. Sometimes, it is desirable for a human operator to \textit{interrupt} an agent in order to prevent dangerous situations from happening. Yet, as part of their learning process, agents may link these interruptions, that impact their reward, to specific states and deliberately avoid them. The situation is particularly challenging in a multi-agent context because agents might not only learn from their own past interruptions, but also from those of other agents. Orseau and Armstrong defined \emph{safe interruptibility} for one learner, but their work does not naturally extend to multi-agent systems. This paper introduces \textit{dynamic safe interruptibility}, an alternative definition more suited to decentralized learning problems, and studies this notion in two learning frameworks: \textit{joint action learners} and \textit{independent learners}. We give realistic sufficient conditions on the learning algorithm to enable dynamic safe interruptibility in the case of joint action learners, yet show that these conditions are not sufficient for independent learners. We show however that if agents can detect interruptions, it is possible to prune the observations to ensure dynamic safe interruptibility even for independent learners.",2017-05-22,2022-03-11 00:42:57,2022-03-11 00:42:57,2022-03-11 00:42:57,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1704.02882,,/Users/jacquesthibodeau/Zotero/storage/HR9VLUER/Mhamdi et al. - 2017 - Dynamic Safe Interruptibility for Decentralized Mu.pdf; /Users/jacquesthibodeau/Zotero/storage/KDS44NFT/1704.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Multiagent Systems; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QGB25IIX,journalArticle,2017,"Hadfield-Menell, Dylan; Dragan, Anca; Abbeel, Pieter; Russell, Stuart",The Off-Switch Game,arXiv:1611.08219 [cs],,,,http://arxiv.org/abs/1611.08219,"It is clear that one of the primary tools we can use to mitigate the potential risk from a misbehaving AI system is the ability to turn the system off. As the capabilities of AI systems improve, it is important to ensure that such systems do not adopt subgoals that prevent a human from switching them off. This is a challenge because many formulations of rational agents create strong incentives for self-preservation. This is not caused by a built-in instinct, but because a rational agent will maximize expected utility and cannot achieve whatever objective it has been given if it is dead. Our goal is to study the incentives an agent has to allow itself to be switched off. We analyze a simple game between a human H and a robot R, where H can press R's off switch but R can disable the off switch. A traditional agent takes its reward function for granted: we show that such agents have an incentive to disable the off switch, except in the special case where H is perfectly rational. Our key insight is that for R to want to preserve its off switch, it needs to be uncertain about the utility associated with the outcome, and to treat H's actions as important observations about that utility. (R also has no incentive to switch itself off in this setting.) We conclude that giving machines an appropriate level of uncertainty about their objectives leads to safer designs, and we argue that this setting is a useful generalization of the classical AI paradigm of rational agents.",2017-06-15,2022-03-11 00:42:59,2022-03-11 00:42:59,2022-03-11 00:42:59,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1611.08219,,/Users/jacquesthibodeau/Zotero/storage/7WJ7VJ7Y/Hadfield-Menell et al. - 2017 - The Off-Switch Game.pdf; /Users/jacquesthibodeau/Zotero/storage/L2J85TR7/1611.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5HNCC3GW,journalArticle,2019,"Lehman, Joel; Clune, Jeff; Misevic, Dusan; Adami, Christoph; Altenberg, Lee; Beaulieu, Julie; Bentley, Peter J.; Bernard, Samuel; Beslon, Guillaume; Bryson, David M.; Chrabaszcz, Patryk; Cheney, Nick; Cully, Antoine; Doncieux, Stephane; Dyer, Fred C.; Ellefsen, Kai Olav; Feldt, Robert; Fischer, Stephan; Forrest, Stephanie; Frénoy, Antoine; Gagné, Christian; Goff, Leni Le; Grabowski, Laura M.; Hodjat, Babak; Hutter, Frank; Keller, Laurent; Knibbe, Carole; Krcah, Peter; Lenski, Richard E.; Lipson, Hod; MacCurdy, Robert; Maestre, Carlos; Miikkulainen, Risto; Mitri, Sara; Moriarty, David E.; Mouret, Jean-Baptiste; Nguyen, Anh; Ofria, Charles; Parizeau, Marc; Parsons, David; Pennock, Robert T.; Punch, William F.; Ray, Thomas S.; Schoenauer, Marc; Shulte, Eric; Sims, Karl; Stanley, Kenneth O.; Taddei, François; Tarapore, Danesh; Thibault, Simon; Weimer, Westley; Watson, Richard; Yosinski, Jason",The Surprising Creativity of Digital Evolution: A Collection of Anecdotes from the Evolutionary Computation and Artificial Life Research Communities,arXiv:1803.03453 [cs],,,,http://arxiv.org/abs/1803.03453,"Biological evolution provides a creative fount of complex and subtle adaptations, often surprising the scientists who discover them. However, because evolution is an algorithmic process that transcends the substrate in which it occurs, evolution's creativity is not limited to nature. Indeed, many researchers in the field of digital evolution have observed their evolving algorithms and organisms subverting their intentions, exposing unrecognized bugs in their code, producing unexpected adaptations, or exhibiting outcomes uncannily convergent with ones in nature. Such stories routinely reveal creativity by evolution in these digital worlds, but they rarely fit into the standard scientific narrative. Instead they are often treated as mere obstacles to be overcome, rather than results that warrant study in their own right. The stories themselves are traded among researchers through oral tradition, but that mode of information transmission is inefficient and prone to error and outright loss. Moreover, the fact that these stories tend to be shared only among practitioners means that many natural scientists do not realize how interesting and lifelike digital organisms are and how natural their evolution can be. To our knowledge, no collection of such anecdotes has been published before. This paper is the crowd-sourced product of researchers in the fields of artificial life and evolutionary computation who have provided first-hand accounts of such cases. It thus serves as a written, fact-checked collection of scientifically important and even entertaining stories. In doing so we also present here substantial evidence that the existence and importance of evolutionary surprises extends beyond the natural world, and may indeed be a universal property of all complex evolving systems.",2019-11-21,2022-03-11 00:43:07,2022-03-11 00:43:07,2022-03-11 00:43:07,,,,,,,The Surprising Creativity of Digital Evolution,,,,,,,,,,,,arXiv.org,,arXiv: 1803.03453,,/Users/jacquesthibodeau/Zotero/storage/69SMNNDL/Lehman et al. - 2019 - The Surprising Creativity of Digital Evolution A .pdf; /Users/jacquesthibodeau/Zotero/storage/5FZ57XHE/1803.html,,,Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RQ75MUDN,journalArticle,2016,"Everitt, Tom; Hutter, Marcus",Avoiding Wireheading with Value Reinforcement Learning,arXiv:1605.03143 [cs],,,,http://arxiv.org/abs/1605.03143,"How can we design good goals for arbitrarily intelligent agents? Reinforcement learning (RL) is a natural approach. Unfortunately, RL does not work well for generally intelligent agents, as RL agents are incentivised to shortcut the reward sensor for maximum reward -- the so-called wireheading problem. In this paper we suggest an alternative to RL called value reinforcement learning (VRL). In VRL, agents use the reward signal to learn a utility function. The VRL setup allows us to remove the incentive to wirehead by placing a constraint on the agent's actions. The constraint is defined in terms of the agent's belief distributions, and does not require an explicit specification of which actions constitute wireheading.",2016-05-10,2022-03-11 00:43:17,2022-03-11 00:43:17,2022-03-11 00:43:17,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1605.03143,,/Users/jacquesthibodeau/Zotero/storage/US83UTF9/Everitt and Hutter - 2016 - Avoiding Wireheading with Value Reinforcement Lear.pdf; /Users/jacquesthibodeau/Zotero/storage/JJVC8DQC/1605.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WQRJXT3M,journalArticle,2018,"Fisac, Jaime F.; Gates, Monica A.; Hamrick, Jessica B.; Liu, Chang; Hadfield-Menell, Dylan; Palaniappan, Malayandi; Malik, Dhruv; Sastry, S. Shankar; Griffiths, Thomas L.; Dragan, Anca D.",Pragmatic-Pedagogic Value Alignment,arXiv:1707.06354 [cs],,,,http://arxiv.org/abs/1707.06354,"As intelligent systems gain autonomy and capability, it becomes vital to ensure that their objectives match those of their human users; this is known as the value-alignment problem. In robotics, value alignment is key to the design of collaborative robots that can integrate into human workflows, successfully inferring and adapting to their users' objectives as they go. We argue that a meaningful solution to value alignment must combine multi-agent decision theory with rich mathematical models of human cognition, enabling robots to tap into people's natural collaborative capabilities. We present a solution to the cooperative inverse reinforcement learning (CIRL) dynamic game based on well-established cognitive models of decision making and theory of mind. The solution captures a key reciprocity relation: the human will not plan her actions in isolation, but rather reason pedagogically about how the robot might learn from them; the robot, in turn, can anticipate this and interpret the human's actions pragmatically. To our knowledge, this work constitutes the first formal analysis of value alignment grounded in empirically validated cognitive models.",2018-02-05,2022-03-11 00:43:31,2022-03-11 00:43:31,2022-03-11 00:43:31,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1707.06354,,/Users/jacquesthibodeau/Zotero/storage/X6H3CUGX/Fisac et al. - 2018 - Pragmatic-Pedagogic Value Alignment.pdf; /Users/jacquesthibodeau/Zotero/storage/JG5LLJD7/1707.html,,,68T05; Computer Science - Artificial Intelligence; Computer Science - Human-Computer Interaction; Computer Science - Machine Learning; Computer Science - Robotics; I.2.0; I.2.6; I.2.8; I.2.9,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UHKCNLGW,journalArticle,2017,"Milli, Smitha; Hadfield-Menell, Dylan; Dragan, Anca; Russell, Stuart",Should Robots be Obedient?,arXiv:1705.09990 [cs],,,,http://arxiv.org/abs/1705.09990,"Intuitively, obedience -- following the order that a human gives -- seems like a good property for a robot to have. But, we humans are not perfect and we may give orders that are not best aligned to our preferences. We show that when a human is not perfectly rational then a robot that tries to infer and act according to the human's underlying preferences can always perform better than a robot that simply follows the human's literal order. Thus, there is a tradeoff between the obedience of a robot and the value it can attain for its owner. We investigate how this tradeoff is impacted by the way the robot infers the human's preferences, showing that some methods err more on the side of obedience than others. We then analyze how performance degrades when the robot has a misspecified model of the features that the human cares about or the level of rationality of the human. Finally, we study how robots can start detecting such model misspecification. Overall, our work suggests that there might be a middle ground in which robots intelligently decide when to obey human orders, but err on the side of obedience.",2017-05-28,2022-03-11 00:43:34,2022-03-11 00:43:34,2022-03-11 00:43:34,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1705.09990,,/Users/jacquesthibodeau/Zotero/storage/39JYHBQ3/Milli et al. - 2017 - Should Robots be Obedient.pdf; /Users/jacquesthibodeau/Zotero/storage/NZVTKACD/1705.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
I5CVN6W5,journalArticle,2016,"Hadfield-Menell, Dylan; Dragan, Anca; Abbeel, Pieter; Russell, Stuart",Cooperative Inverse Reinforcement Learning,arXiv:1606.03137 [cs],,,,http://arxiv.org/abs/1606.03137,"For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (CIRL). A CIRL problem is a cooperative, partial-information game with two agents, human and robot; both are rewarded according to the human's reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is suboptimal in CIRL, and derive an approximate CIRL algorithm.",2016-11-12,2022-03-11 00:43:37,2022-03-11 00:43:37,2022-03-11 00:43:37,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1606.03137,,/Users/jacquesthibodeau/Zotero/storage/GM9JBIWP/Hadfield-Menell et al. - 2016 - Cooperative Inverse Reinforcement Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/HDDUUGTH/1606.html,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
L23N649D,journalArticle,2015,"Evans, Owain; Stuhlmueller, Andreas; Goodman, Noah D.","Learning the Preferences of Ignorant, Inconsistent Agents",arXiv:1512.05832 [cs],,,,http://arxiv.org/abs/1512.05832,"An important use of machine learning is to learn what people value. What posts or photos should a user be shown? Which jobs or activities would a person find rewarding? In each case, observations of people's past choices can inform our inferences about their likes and preferences. If we assume that choices are approximately optimal according to some utility function, we can treat preference inference as Bayesian inverse planning. That is, given a prior on utility functions and some observed choices, we invert an optimal decision-making process to infer a posterior distribution on utility functions. However, people often deviate from approximate optimality. They have false beliefs, their planning is sub-optimal, and their choices may be temporally inconsistent due to hyperbolic discounting and other biases. We demonstrate how to incorporate these deviations into algorithms for preference inference by constructing generative models of planning for agents who are subject to false beliefs and time inconsistency. We explore the inferences these models make about preferences, beliefs, and biases. We present a behavioral experiment in which human subjects perform preference inference given the same observations of choices as our model. Results show that human subjects (like our model) explain choices in terms of systematic deviations from optimal behavior and suggest that they take such deviations into account when inferring preferences.",2015-12-17,2022-03-11 00:43:55,2022-03-11 00:43:55,2022-03-11 00:43:55,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1512.05832,,"/Users/jacquesthibodeau/Zotero/storage/E8RI8QP6/Evans et al. - 2015 - Learning the Preferences of Ignorant, Inconsistent.pdf; /Users/jacquesthibodeau/Zotero/storage/YEKCHHXP/1512.html",,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YCKXJDSL,journalArticle,2018,"Christiano, Paul; Shlegeris, Buck; Amodei, Dario",Supervising strong learners by amplifying weak experts,"arXiv:1810.08575 [cs, stat]",,,,http://arxiv.org/abs/1810.08575,"Many real world learning tasks involve complex or hard-to-specify objectives, and using an easier-to-specify proxy can lead to poor performance or misaligned behavior. One solution is to have humans provide a training signal by demonstrating or judging performance, but this approach fails if the task is too complicated for a human to directly evaluate. We propose Iterated Amplification, an alternative training strategy which progressively builds up a training signal for difficult problems by combining solutions to easier subproblems. Iterated Amplification is closely related to Expert Iteration (Anthony et al., 2017; Silver et al., 2017), except that it uses no external reward function. We present results in algorithmic environments, showing that Iterated Amplification can efficiently learn complex behaviors.",2018-10-19,2022-03-11 00:43:58,2022-03-11 00:43:58,2022-03-11 00:43:58,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1810.08575,,/Users/jacquesthibodeau/Zotero/storage/W4VA7R8S/Christiano et al. - 2018 - Supervising strong learners by amplifying weak exp.pdf; /Users/jacquesthibodeau/Zotero/storage/89JT3M6T/1810.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QJGNXPNN,journalArticle,2017,"Christiano, Paul; Leike, Jan; Brown, Tom B.; Martic, Miljan; Legg, Shane; Amodei, Dario",Deep reinforcement learning from human preferences,"arXiv:1706.03741 [cs, stat]",,,,http://arxiv.org/abs/1706.03741,"For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.",2017-07-13,2022-03-11 00:44:19,2022-03-11 00:44:19,2022-03-11 00:44:19,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1706.03741,,/Users/jacquesthibodeau/Zotero/storage/MI43DNB4/Christiano et al. - 2017 - Deep reinforcement learning from human preferences.pdf; /Users/jacquesthibodeau/Zotero/storage/8R8GAI69/1706.html,,,Computer Science - Artificial Intelligence; Computer Science - Human-Computer Interaction; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3WT89LZU,journalArticle,2016,"Critch, Andrew","Parametric Bounded L\""ob's Theorem and Robust Cooperation of Bounded Agents",arXiv:1602.04184 [cs],,,,http://arxiv.org/abs/1602.04184,"L\""ob's theorem and G\""odel's theorems make predictions about the behavior of systems capable of self-reference with unbounded computational resources with which to write and evaluate proofs. However, in the real world, systems capable of self-reference will have limited memory and processing speed, so in this paper we introduce an effective version of L\""ob's theorem which is applicable given such bounded resources. These results have powerful implications for the game theory of bounded agents who are able to write proofs about themselves and one another, including the capacity to out-perform classical Nash equilibria and correlated equilibria, attaining mutually cooperative program equilibrium in the Prisoner's Dilemma. Previous cooperative program equilibria studied by Tennenholtz (2004) and Fortnow (2009) have depended on tests for program equality, a fragile condition, whereas ""L\""obian"" cooperation is much more robust and agnostic of the opponent's implementation.",2016-08-24,2022-03-11 00:44:28,2022-03-11 00:44:28,2022-03-11 00:44:28,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1602.04184,,/Users/jacquesthibodeau/Zotero/storage/C3IBEIN7/Critch - 2016 - Parametric Bounded Lob's Theorem and Robust Coop.pdf; /Users/jacquesthibodeau/Zotero/storage/WY8SWMEB/1602.html,,,Computer Science - Computer Science and Game Theory; Computer Science - Logic in Computer Science,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NJMNAAYK,journalArticle,2017,"Babcock, James; Kramar, Janos; Yampolskiy, Roman V.",Guidelines for Artificial Intelligence Containment,arXiv:1707.08476 [cs],,,,http://arxiv.org/abs/1707.08476,"With almost daily improvements in capabilities of artificial intelligence it is more important than ever to develop safety software for use by the AI research community. Building on our previous work on AI Containment Problem we propose a number of guidelines which should help AI safety researchers to develop reliable sandboxing software for intelligent programs of all levels. Such safety container software will make it possible to study and analyze intelligent artificial agent while maintaining certain level of safety against information leakage, social engineering attacks and cyberattacks from within the container.",2017-07-24,2022-03-11 00:44:34,2022-03-11 00:44:34,2022-03-11 00:44:34,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1707.08476,,/Users/jacquesthibodeau/Zotero/storage/G4Q86FPE/Babcock et al. - 2017 - Guidelines for Artificial Intelligence Containment.pdf; /Users/jacquesthibodeau/Zotero/storage/3ZXKW6RX/1707.html,,,Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KEJHL6CV,journalArticle,2017,"Leike, Jan; Martic, Miljan; Krakovna, Victoria; Ortega, Pedro A.; Everitt, Tom; Lefrancq, Andrew; Orseau, Laurent; Legg, Shane",AI Safety Gridworlds,arXiv:1711.09883 [cs],,,,http://arxiv.org/abs/1711.09883,"We present a suite of reinforcement learning environments illustrating various safety properties of intelligent agents. These problems include safe interruptibility, avoiding side effects, absent supervisor, reward gaming, safe exploration, as well as robustness to self-modification, distributional shift, and adversaries. To measure compliance with the intended safe behavior, we equip each environment with a performance function that is hidden from the agent. This allows us to categorize AI safety problems into robustness and specification problems, depending on whether the performance function corresponds to the observed reward function. We evaluate A2C and Rainbow, two recent deep reinforcement learning agents, on our environments and show that they are not able to solve them satisfactorily.",2017-11-28,2022-03-11 00:44:42,2022-03-11 00:44:42,2022-03-11 00:44:42,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1711.09883,,/Users/jacquesthibodeau/Zotero/storage/UL2GIZZX/Leike et al. - 2017 - AI Safety Gridworlds.pdf; /Users/jacquesthibodeau/Zotero/storage/VZLHCUDR/1711.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9NQ6YV92,journalArticle,2020,"Hernandez, Danny; Brown, Tom B.",Measuring the Algorithmic Efficiency of Neural Networks,"arXiv:2005.04305 [cs, stat]",,,,http://arxiv.org/abs/2005.04305,"Three factors drive the advance of AI: algorithmic innovation, data, and the amount of compute available for training. Algorithmic progress has traditionally been more difficult to quantify than compute and data. In this work, we argue that algorithmic progress has an aspect that is both straightforward to measure and interesting: reductions over time in the compute needed to reach past capabilities. We show that the number of floating-point operations required to train a classifier to AlexNet-level performance on ImageNet has decreased by a factor of 44x between 2012 and 2019. This corresponds to algorithmic efficiency doubling every 16 months over a period of 7 years. By contrast, Moore's Law would only have yielded an 11x cost improvement. We observe that hardware and algorithmic efficiency gains multiply and can be on a similar scale over meaningful horizons, which suggests that a good model of AI progress should integrate measures from both.",2020-05-08,2022-03-11 00:44:54,2022-03-11 01:36:58,2022-03-11 00:44:54,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2005.04305,,/Users/jacquesthibodeau/Zotero/storage/JC7RS7J2/Hernandez and Brown - 2020 - Measuring the Algorithmic Efficiency of Neural Net.pdf; /Users/jacquesthibodeau/Zotero/storage/Z3HDZ9IE/2005.html; /Users/jacquesthibodeau/Zotero/storage/BY8J4GU4/Hernandez and Brown - 2020 - Measuring the Algorithmic Efficiency of Neural Net.pdf; /Users/jacquesthibodeau/Zotero/storage/6WIJZCB8/2005.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EL6UNISZ,journalArticle,2014,"Kingma, Diederik P.; Welling, Max",Auto-Encoding Variational Bayes,"arXiv:1312.6114 [cs, stat]",,,,http://arxiv.org/abs/1312.6114,"How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.",2014-05-01,2022-03-11 00:46:12,2022-03-11 00:46:12,2022-03-11 00:46:12,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1312.6114,,/Users/jacquesthibodeau/Zotero/storage/UZPG9FA5/Kingma and Welling - 2014 - Auto-Encoding Variational Bayes.pdf; /Users/jacquesthibodeau/Zotero/storage/CAIAKN8Q/1312.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NA8XG89R,journalArticle,2013,"Mikolov, Tomas; Sutskever, Ilya; Chen, Kai; Corrado, Greg; Dean, Jeffrey",Distributed Representations of Words and Phrases and their Compositionality,"arXiv:1310.4546 [cs, stat]",,,,http://arxiv.org/abs/1310.4546,"The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of ""Canada"" and ""Air"" cannot be easily combined to obtain ""Air Canada"". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",2013-10-16,2022-03-11 00:46:13,2022-03-11 00:46:13,2022-03-11 00:46:13,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1310.4546,,/Users/jacquesthibodeau/Zotero/storage/2N29Q6YH/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases a.pdf; /Users/jacquesthibodeau/Zotero/storage/BLK42UCN/1310.html,,,Computer Science - Computation and Language; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MQ8ELTQC,journalArticle,2014,"Goodfellow, Ian J.; Pouget-Abadie, Jean; Mirza, Mehdi; Xu, Bing; Warde-Farley, David; Ozair, Sherjil; Courville, Aaron; Bengio, Yoshua",Generative Adversarial Networks,"arXiv:1406.2661 [cs, stat]",,,,http://arxiv.org/abs/1406.2661,"We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",2014-06-10,2022-03-11 00:46:15,2022-03-11 00:46:15,2022-03-11 00:46:15,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1406.2661,,/Users/jacquesthibodeau/Zotero/storage/HVWCFA35/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf; /Users/jacquesthibodeau/Zotero/storage/SP78GDXY/1406.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3WHTA2X9,journalArticle,2016,"Bahdanau, Dzmitry; Cho, Kyunghyun; Bengio, Yoshua",Neural Machine Translation by Jointly Learning to Align and Translate,"arXiv:1409.0473 [cs, stat]",,,,http://arxiv.org/abs/1409.0473,"Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",2016-05-19,2022-03-11 00:46:17,2022-03-11 00:46:17,2022-03-11 00:46:17,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1409.0473,,/Users/jacquesthibodeau/Zotero/storage/5P4IAMPV/Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to .pdf; /Users/jacquesthibodeau/Zotero/storage/HJ65LWP9/1409.html,,,Computer Science - Computation and Language; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BUX5IU2E,journalArticle,2017,"Kingma, Diederik P.; Ba, Jimmy",Adam: A Method for Stochastic Optimization,arXiv:1412.6980 [cs],,,,http://arxiv.org/abs/1412.6980,"We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",2017-01-29,2022-03-11 00:46:19,2022-03-11 00:46:19,2022-03-11 00:46:19,,,,,,,Adam,,,,,,,,,,,,arXiv.org,,arXiv: 1412.6980,,/Users/jacquesthibodeau/Zotero/storage/NC9XZNEX/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf; /Users/jacquesthibodeau/Zotero/storage/VQKWXJ43/1412.html,,,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KJU95QAX,journalArticle,2017,"Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N.; Kaiser, Lukasz; Polosukhin, Illia",Attention Is All You Need,arXiv:1706.03762 [cs],,,,http://arxiv.org/abs/1706.03762,"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",2017-12-05,2022-03-11 00:46:22,2022-03-11 00:46:22,2022-03-11 00:46:21,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1706.03762,,/Users/jacquesthibodeau/Zotero/storage/JZJ8T8EL/Vaswani et al. - 2017 - Attention Is All You Need.pdf; /Users/jacquesthibodeau/Zotero/storage/8UIKQ785/1706.html,,,Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QV84CAEJ,journalArticle,2016,"Wu, Yonghui; Schuster, Mike; Chen, Zhifeng; Le, Quoc V.; Norouzi, Mohammad; Macherey, Wolfgang; Krikun, Maxim; Cao, Yuan; Gao, Qin; Macherey, Klaus; Klingner, Jeff; Shah, Apurva; Johnson, Melvin; Liu, Xiaobing; Kaiser, Łukasz; Gouws, Stephan; Kato, Yoshikiyo; Kudo, Taku; Kazawa, Hideto; Stevens, Keith; Kurian, George; Patil, Nishant; Wang, Wei; Young, Cliff; Smith, Jason; Riesa, Jason; Rudnick, Alex; Vinyals, Oriol; Corrado, Greg; Hughes, Macduff; Dean, Jeffrey",Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation,arXiv:1609.08144 [cs],,,,http://arxiv.org/abs/1609.08144,"Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (""wordpieces"") for both input and output. This method provides a good balance between the flexibility of ""character""-delimited models and the efficiency of ""word""-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system.",2016-10-08,2022-03-11 00:46:23,2022-03-11 00:46:23,2022-03-11 00:46:23,,,,,,,Google's Neural Machine Translation System,,,,,,,,,,,,arXiv.org,,arXiv: 1609.08144,,/Users/jacquesthibodeau/Zotero/storage/NE29WJV2/Wu et al. - 2016 - Google's Neural Machine Translation System Bridgi.pdf; /Users/jacquesthibodeau/Zotero/storage/UWQ7F2HF/1609.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VGKIB34A,journalArticle,2017,"Zoph, Barret; Le, Quoc V.",Neural Architecture Search with Reinforcement Learning,arXiv:1611.01578 [cs],,,,http://arxiv.org/abs/1611.01578,"Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.",2017-02-15,2022-03-11 00:46:25,2022-03-11 00:46:25,2022-03-11 00:46:25,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1611.01578,,/Users/jacquesthibodeau/Zotero/storage/HE8SNP2I/Zoph and Le - 2017 - Neural Architecture Search with Reinforcement Lear.pdf; /Users/jacquesthibodeau/Zotero/storage/ZKD6S2JZ/1611.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
STARDH43,journalArticle,2017,"Chollet, François",Xception: Deep Learning with Depthwise Separable Convolutions,arXiv:1610.02357 [cs],,,,http://arxiv.org/abs/1610.02357,"We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.",2017-04-04,2022-03-11 00:46:28,2022-03-11 00:46:28,2022-03-11 00:46:28,,,,,,,Xception,,,,,,,,,,,,arXiv.org,,arXiv: 1610.02357,,/Users/jacquesthibodeau/Zotero/storage/QAZ54WG6/Chollet - 2017 - Xception Deep Learning with Depthwise Separable C.pdf; /Users/jacquesthibodeau/Zotero/storage/EFBWMIG5/1610.html,,,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EDJYWEF7,journalArticle,2015,"Amodei, Dario; Anubhai, Rishita; Battenberg, Eric; Case, Carl; Casper, Jared; Catanzaro, Bryan; Chen, Jingdong; Chrzanowski, Mike; Coates, Adam; Diamos, Greg; Elsen, Erich; Engel, Jesse; Fan, Linxi; Fougner, Christopher; Han, Tony; Hannun, Awni; Jun, Billy; LeGresley, Patrick; Lin, Libby; Narang, Sharan; Ng, Andrew; Ozair, Sherjil; Prenger, Ryan; Raiman, Jonathan; Satheesh, Sanjeev; Seetapun, David; Sengupta, Shubho; Wang, Yi; Wang, Zhiqian; Wang, Chong; Xiao, Bo; Yogatama, Dani; Zhan, Jun; Zhu, Zhenyao",Deep Speech 2: End-to-End Speech Recognition in English and Mandarin,arXiv:1512.02595 [cs],,,,http://arxiv.org/abs/1512.02595,"We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech--two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.",2015-12-08,2022-03-11 00:46:31,2022-03-11 00:46:31,2022-03-11 00:46:30,,,,,,,Deep Speech 2,,,,,,,,,,,,arXiv.org,,arXiv: 1512.02595,,/Users/jacquesthibodeau/Zotero/storage/GCW6SYTS/Amodei et al. - 2015 - Deep Speech 2 End-to-End Speech Recognition in En.pdf; /Users/jacquesthibodeau/Zotero/storage/R524APC3/1512.html,,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9MWJA2PZ,journalArticle,2015,"Simonyan, Karen; Zisserman, Andrew",Very Deep Convolutional Networks for Large-Scale Image Recognition,arXiv:1409.1556 [cs],,,,http://arxiv.org/abs/1409.1556,"In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.",2015-04-10,2022-03-11 00:46:42,2022-03-11 00:46:42,2022-03-11 00:46:42,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1409.1556,,/Users/jacquesthibodeau/Zotero/storage/IU8LRRIT/Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf; /Users/jacquesthibodeau/Zotero/storage/GJ3DHSB9/1409.html,,,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BNWXRCHF,journalArticle,2014,"Sutskever, Ilya; Vinyals, Oriol; Le, Quoc V.",Sequence to Sequence Learning with Neural Networks,arXiv:1409.3215 [cs],,,,http://arxiv.org/abs/1409.3215,"Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.",2014-12-14,2022-03-11 00:46:45,2022-03-11 00:46:45,2022-03-11 00:46:44,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1409.3215,,/Users/jacquesthibodeau/Zotero/storage/XIY9LPCW/Sutskever et al. - 2014 - Sequence to Sequence Learning with Neural Networks.pdf; /Users/jacquesthibodeau/Zotero/storage/R2YMG2T3/1409.html,,,Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2CM3NXAW,journalArticle,2013,"Mnih, Volodymyr; Kavukcuoglu, Koray; Silver, David; Graves, Alex; Antonoglou, Ioannis; Wierstra, Daan; Riedmiller, Martin",Playing Atari with Deep Reinforcement Learning,arXiv:1312.5602 [cs],,,,http://arxiv.org/abs/1312.5602,"We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.",2013-12-19,2022-03-11 00:46:48,2022-03-11 00:46:48,2022-03-11 00:46:47,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1312.5602,,/Users/jacquesthibodeau/Zotero/storage/8RGV3UHT/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/6P98LQKJ/1312.html,,,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DYKT55PJ,journalArticle,2013,"Zeiler, Matthew D.; Fergus, Rob",Visualizing and Understanding Convolutional Networks,arXiv:1311.2901 [cs],,,,http://arxiv.org/abs/1311.2901,"Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky \etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.",2013-11-28,2022-03-11 00:46:49,2022-03-11 00:46:49,2022-03-11 00:46:49,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1311.2901,,/Users/jacquesthibodeau/Zotero/storage/RKFMSM5J/Zeiler and Fergus - 2013 - Visualizing and Understanding Convolutional Networ.pdf; /Users/jacquesthibodeau/Zotero/storage/URZ59DSU/1311.html,,,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PPY7NANL,journalArticle,2012,"Hinton, Geoffrey E.; Srivastava, Nitish; Krizhevsky, Alex; Sutskever, Ilya; Salakhutdinov, Ruslan R.",Improving neural networks by preventing co-adaptation of feature detectors,arXiv:1207.0580 [cs],,,,http://arxiv.org/abs/1207.0580,"When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This ""overfitting"" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random ""dropout"" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.",2012-07-03,2022-03-11 00:46:51,2022-03-11 00:46:51,2022-03-11 00:46:51,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1207.0580,,/Users/jacquesthibodeau/Zotero/storage/RKKHRNR9/Hinton et al. - 2012 - Improving neural networks by preventing co-adaptat.pdf; /Users/jacquesthibodeau/Zotero/storage/MQSGQ5TF/1207.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R87A9SW2,journalArticle,2020,"Hubinger, Evan",An overview of 11 proposals for building safe advanced AI,arXiv:2012.07532 [cs],,,,http://arxiv.org/abs/2012.07532,"This paper analyzes and compares 11 different proposals for building safe advanced AI under the current machine learning paradigm, including major contenders such as iterated amplification, AI safety via debate, and recursive reward modeling. Each proposal is evaluated on the four components of outer alignment, inner alignment, training competitiveness, and performance competitiveness, of which the distinction between the latter two is introduced in this paper. While prior literature has primarily focused on analyzing individual proposals, or primarily focused on outer alignment at the expense of inner alignment, this analysis seeks to take a comparative look at a wide range of proposals including a comparative analysis across all four previously mentioned components.",2020-12-04,2022-03-11 00:47:00,2022-03-11 00:47:00,2022-03-11 00:46:59,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2012.07532,,/Users/jacquesthibodeau/Zotero/storage/ATHV7RAM/Hubinger - 2020 - An overview of 11 proposals for building safe adva.pdf; /Users/jacquesthibodeau/Zotero/storage/HBRXGPYV/2012.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DPTBSH82,journalArticle,2022,"Gathani, Sneha; Hulsebos, Madelon; Gale, James; Haas, Peter J.; Demiralp, Çağatay",Augmenting Decision Making via Interactive What-If Analysis,arXiv:2109.06160 [cs],,,,http://arxiv.org/abs/2109.06160,"The fundamental goal of business data analysis is to improve business decisions using data. Business users often make decisions to achieve key performance indicators (KPIs) such as increasing customer retention or sales, or decreasing costs. To discover the relationship between data attributes hypothesized to be drivers and those corresponding to KPIs of interest, business users currently need to perform lengthy exploratory analyses. This involves considering multitudes of combinations and scenarios and performing slicing, dicing, and transformations on the data accordingly, e.g., analyzing customer retention across quarters of the year or suggesting optimal media channels across strata of customers. However, the increasing complexity of datasets combined with the cognitive limitations of humans makes it challenging to carry over multiple hypotheses, even for simple datasets. Therefore mentally performing such analyses is hard. Existing commercial tools either provide partial solutions or fail to cater to business users altogether. Here we argue for four functionalities to enable business users to interactively learn and reason about the relationships between sets of data attributes thereby facilitating data-driven decision making. We implement these functionalities in SystemD, an interactive visual data analysis system enabling business users to experiment with the data by asking what-if questions. We evaluate the system through three business use cases: marketing mix modeling, customer retention analysis, and deal closing analysis, and report on feedback from multiple business users. Users find the SystemD functionalities highly useful for quick testing and validation of their hypotheses around their KPIs of interest, addressing their unmet analysis needs. The feedback also suggests that the UX design can be enhanced to further improve the understandability of these functionalities.",2022-02-08,2022-03-11 00:54:22,2022-03-11 00:54:22,2022-03-11 00:54:22,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2109.06160,,/Users/jacquesthibodeau/Zotero/storage/AX2U4F9X/Gathani et al. - 2022 - Augmenting Decision Making via Interactive What-If.pdf; /Users/jacquesthibodeau/Zotero/storage/HBYRB7W9/2109.html,,,Computer Science - Databases; Computer Science - Human-Computer Interaction; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IXJ7BGT7,journalArticle,2021,"Pearce, Hammond; Ahmad, Baleegh; Tan, Benjamin; Dolan-Gavitt, Brendan; Karri, Ramesh",Asleep at the Keyboard? Assessing the Security of GitHub Copilot's Code Contributions,arXiv:2108.09293 [cs],,,,http://arxiv.org/abs/2108.09293,"There is burgeoning interest in designing AI-based systems to assist humans in designing computing systems, including tools that automatically generate computer code. The most notable of these comes in the form of the first self-described `AI pair programmer', GitHub Copilot, a language model trained over open-source GitHub code. However, code often contains bugs - and so, given the vast quantity of unvetted code that Copilot has processed, it is certain that the language model will have learned from exploitable, buggy code. This raises concerns on the security of Copilot's code contributions. In this work, we systematically investigate the prevalence and conditions that can cause GitHub Copilot to recommend insecure code. To perform this analysis we prompt Copilot to generate code in scenarios relevant to high-risk CWEs (e.g. those from MITRE's ""Top 25"" list). We explore Copilot's performance on three distinct code generation axes -- examining how it performs given diversity of weaknesses, diversity of prompts, and diversity of domains. In total, we produce 89 different scenarios for Copilot to complete, producing 1,689 programs. Of these, we found approximately 40% to be vulnerable.",2021-12-16,2022-03-11 00:54:25,2022-03-11 00:54:25,2022-03-11 00:54:24,,,,,,,Asleep at the Keyboard?,,,,,,,,,,,,arXiv.org,,arXiv: 2108.09293,,/Users/jacquesthibodeau/Zotero/storage/BHUSEKZ7/Pearce et al. - 2021 - Asleep at the Keyboard Assessing the Security of .pdf; /Users/jacquesthibodeau/Zotero/storage/BIAL94TY/2108.html,,,Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3FVBGZZD,journalArticle,2022,"Rahtz, Matthew; Varma, Vikrant; Kumar, Ramana; Kenton, Zachary; Legg, Shane; Leike, Jan",Safe Deep RL in 3D Environments using Human Feedback,arXiv:2201.08102 [cs],,,,http://arxiv.org/abs/2201.08102,"Agents should avoid unsafe behaviour during both training and deployment. This typically requires a simulator and a procedural specification of unsafe behaviour. Unfortunately, a simulator is not always available, and procedurally specifying constraints can be difficult or impossible for many real-world tasks. A recently introduced technique, ReQueST, aims to solve this problem by learning a neural simulator of the environment from safe human trajectories, then using the learned simulator to efficiently learn a reward model from human feedback. However, it is yet unknown whether this approach is feasible in complex 3D environments with feedback obtained from real humans - whether sufficient pixel-based neural simulator quality can be achieved, and whether the human data requirements are viable in terms of both quantity and quality. In this paper we answer this question in the affirmative, using ReQueST to train an agent to perform a 3D first-person object collection task using data entirely from human contractors. We show that the resulting agent exhibits an order of magnitude reduction in unsafe behaviour compared to standard reinforcement learning.",2022-01-21,2022-03-11 00:54:50,2022-03-11 00:54:50,2022-03-11 00:54:50,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2201.08102,,/Users/jacquesthibodeau/Zotero/storage/9B34QRJG/Rahtz et al. - 2022 - Safe Deep RL in 3D Environments using Human Feedba.pdf; /Users/jacquesthibodeau/Zotero/storage/K8AJLZ9H/2201.html,,,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NRYCFH8M,journalArticle,2022,"Pan, Alexander; Bhatia, Kush; Steinhardt, Jacob",The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models,"arXiv:2201.03544 [cs, stat]",,,,http://arxiv.org/abs/2201.03544,"Reward hacking -- where RL agents exploit gaps in misspecified reward functions -- has been widely observed, but not yet systematically studied. To understand how reward hacking arises, we construct four RL environments with misspecified rewards. We investigate reward hacking as a function of agent capabilities: model capacity, action space resolution, observation space noise, and training time. More capable agents often exploit reward misspecifications, achieving higher proxy reward and lower true reward than less capable agents. Moreover, we find instances of phase transitions: capability thresholds at which the agent's behavior qualitatively shifts, leading to a sharp decrease in the true reward. Such phase transitions pose challenges to monitoring the safety of ML systems. To address this, we propose an anomaly detection task for aberrant policies and offer several baseline detectors.",2022-02-14,2022-03-11 00:56:03,2022-03-11 00:56:03,2022-03-11 00:56:03,,,,,,,The Effects of Reward Misspecification,,,,,,,,,,,,arXiv.org,,arXiv: 2201.03544,,/Users/jacquesthibodeau/Zotero/storage/53BDMGWR/Pan et al. - 2022 - The Effects of Reward Misspecification Mapping an.pdf; /Users/jacquesthibodeau/Zotero/storage/EGP4DIZ6/2201.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ADBMHHNP,journalArticle,2022,"Du, Xuefeng; Wang, Zhaoning; Cai, Mu; Li, Yixuan",VOS: Learning What You Don't Know by Virtual Outlier Synthesis,arXiv:2202.01197 [cs],,,,http://arxiv.org/abs/2202.01197,"Out-of-distribution (OOD) detection has received much attention lately due to its importance in the safe deployment of neural networks. One of the key challenges is that models lack supervision signals from unknown data, and as a result, can produce overconfident predictions on OOD data. Previous approaches rely on real outlier datasets for model regularization, which can be costly and sometimes infeasible to obtain in practice. In this paper, we present VOS, a novel framework for OOD detection by adaptively synthesizing virtual outliers that can meaningfully regularize the model's decision boundary during training. Specifically, VOS samples virtual outliers from the low-likelihood region of the class-conditional distribution estimated in the feature space. Alongside, we introduce a novel unknown-aware training objective, which contrastively shapes the uncertainty space between the ID data and synthesized outlier data. VOS achieves state-of-the-art performance on both object detection and image classification models, reducing the FPR95 by up to 7.87% compared to the previous best method. Code is available at https://github.com/deeplearning-wisc/vos.",2022-02-04,2022-03-11 00:56:21,2022-03-11 01:37:55,2022-03-11 00:56:21,,,,,,,VOS,,,,,,,,,,,,arXiv.org,,arXiv: 2202.01197,,/Users/jacquesthibodeau/Zotero/storage/Z3WME7GW/Du et al. - 2022 - VOS Learning What You Don't Know by Virtual Outli.pdf; /Users/jacquesthibodeau/Zotero/storage/KP6NEVTF/Du et al. - 2022 - VOS Learning What You Don't Know by Virtual Outli.pdf; /Users/jacquesthibodeau/Zotero/storage/NKYHICJN/2202.html; /Users/jacquesthibodeau/Zotero/storage/R5WB6WPU/2202.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BQXFH99W,journalArticle,2021,"Meinke, Alexander; Bitterwolf, Julian; Hein, Matthias",Provably Robust Detection of Out-of-distribution Data (almost) for free,arXiv:2106.04260 [cs],,,,http://arxiv.org/abs/2106.04260,"When applying machine learning in safety-critical systems, a reliable assessment of the uncertainy of a classifier is required. However, deep neural networks are known to produce highly overconfident predictions on out-of-distribution (OOD) data and even if trained to be non-confident on OOD data one can still adversarially manipulate OOD data so that the classifer again assigns high confidence to the manipulated samples. In this paper we propose a novel method where from first principles we combine a certifiable OOD detector with a standard classifier into an OOD aware classifier. In this way we achieve the best of two worlds: certifiably adversarially robust OOD detection, even for OOD samples close to the in-distribution, without loss in prediction accuracy and close to state-of-the-art OOD detection performance for non-manipulated OOD data. Moreover, due to the particular construction our classifier provably avoids the asymptotic overconfidence problem of standard neural networks.",2021-06-08,2022-03-11 00:56:43,2022-03-11 00:56:43,2022-03-11 00:56:43,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2106.04260,,/Users/jacquesthibodeau/Zotero/storage/DZCQQRR8/Meinke et al. - 2021 - Provably Robust Detection of Out-of-distribution D.pdf; /Users/jacquesthibodeau/Zotero/storage/DMYN2EYN/2106.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HK525QH9,journalArticle,2021,"Yoon, Jinsung; Sohn, Kihyuk; Li, Chun-Liang; Arik, Sercan O.; Lee, Chen-Yu; Pfister, Tomas","Self-Supervise, Refine, Repeat: Improving Unsupervised Anomaly Detection",,,,,https://openreview.net/forum?id=Nct9j3BVswZ,"Anomaly detection (AD) - separating anomalies from normal data - has many applications across domains, from manufacturing to healthcare. While most previous works have been shown to be effective...",2021-09-29,2022-03-11 00:57:29,2022-03-11 00:57:29,2022-03-11 00:57:29,,,,,,,"Self-Supervise, Refine, Repeat",,,,,,,en,,,,,openreview.net,,,,"/Users/jacquesthibodeau/Zotero/storage/ANY6VSD7/Yoon et al. - 2021 - Self-Supervise, Refine, Repeat Improving Unsuperv.pdf; /Users/jacquesthibodeau/Zotero/storage/GTIQ5XJ2/forum.html",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PU8QPVJ2,journalArticle,2022,"Wortsman, Mitchell; Ilharco, Gabriel; Kim, Jong Wook; Li, Mike; Kornblith, Simon; Roelofs, Rebecca; Lopes, Raphael Gontijo; Hajishirzi, Hannaneh; Farhadi, Ali; Namkoong, Hongseok; Schmidt, Ludwig",Robust fine-tuning of zero-shot models,arXiv:2109.01903 [cs],,,,http://arxiv.org/abs/2109.01903,"Large pre-trained models such as CLIP or ALIGN offer consistent accuracy across a range of data distributions when performing zero-shot inference (i.e., without fine-tuning on a specific dataset). Although existing fine-tuning methods substantially improve accuracy on a given target distribution, they often reduce robustness to distribution shifts. We address this tension by introducing a simple and effective method for improving robustness while fine-tuning: ensembling the weights of the zero-shot and fine-tuned models (WiSE-FT). Compared to standard fine-tuning, WiSE-FT provides large accuracy improvements under distribution shift, while preserving high accuracy on the target distribution. On ImageNet and five derived distribution shifts, WiSE-FT improves accuracy under distribution shift by 4 to 6 percentage points (pp) over prior work while increasing ImageNet accuracy by 1.6 pp. WiSE-FT achieves similarly large robustness gains (2 to 23 pp) on a diverse set of six further distribution shifts, and accuracy gains of 0.8 to 3.3 pp compared to standard fine-tuning on seven commonly used transfer learning datasets. These improvements come at no additional computational cost during fine-tuning or inference.",2022-02-24,2022-03-11 00:58:29,2022-03-11 00:58:29,2022-03-11 00:58:29,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2109.01903,,/Users/jacquesthibodeau/Zotero/storage/ZPIXAEPG/Wortsman et al. - 2022 - Robust fine-tuning of zero-shot models.pdf; /Users/jacquesthibodeau/Zotero/storage/74PFL6FD/2109.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LHDSLAVF,journalArticle,2020,"Guo, Weiyu; Ouyang, Yidong",Robust Learning with Frequency Domain Regularization,"arXiv:2007.03244 [cs, stat]",,,,http://arxiv.org/abs/2007.03244,"Convolution neural networks have achieved remarkable performance in many tasks of computing vision. However, CNN tends to bias to low frequency components. They prioritize capturing low frequency patterns which lead them fail when suffering from application scenario transformation. While adversarial example implies the model is very sensitive to high frequency perturbations. In this paper, we introduce a new regularization method by constraining the frequency spectra of the filter of the model. Different from band-limit training, our method considers the valid frequency range probably entangles in different layers rather than continuous and trains the valid frequency range end-to-end by backpropagation. We demonstrate the effectiveness of our regularization by (1) defensing to adversarial perturbations; (2) reducing the generalization gap in different architecture; (3) improving the generalization ability in transfer learning scenario without fine-tune.",2020-07-07,2022-03-11 00:59:35,2022-03-11 00:59:35,2022-03-11 00:59:35,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2007.03244,,/Users/jacquesthibodeau/Zotero/storage/TXPXRMAB/Guo and Ouyang - 2020 - Robust Learning with Frequency Domain Regularizati.pdf; /Users/jacquesthibodeau/Zotero/storage/3ZLQBLGC/2007.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
M6G2KFV3,journalArticle,2021,"Salman, Hadi; Jain, Saachi; Wong, Eric; Mądry, Aleksander",Certified Patch Robustness via Smoothed Vision Transformers,arXiv:2110.07719 [cs],,,,http://arxiv.org/abs/2110.07719,"Certified patch defenses can guarantee robustness of an image classifier to arbitrary changes within a bounded contiguous region. But, currently, this robustness comes at a cost of degraded standard accuracies and slower inference times. We demonstrate how using vision transformers enables significantly better certified patch robustness that is also more computationally efficient and does not incur a substantial drop in standard accuracy. These improvements stem from the inherent ability of the vision transformer to gracefully handle largely masked images. Our code is available at https://github.com/MadryLab/smoothed-vit.",2021-10-11,2022-03-11 00:59:50,2022-03-11 00:59:50,2022-03-11 00:59:50,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2110.07719,,/Users/jacquesthibodeau/Zotero/storage/24FM6UTF/Salman et al. - 2021 - Certified Patch Robustness via Smoothed Vision Tra.pdf; /Users/jacquesthibodeau/Zotero/storage/E69HPRYS/2110.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R9X3L95V,journalArticle,2021,"Zhang, Marvin Mengxin; Levine, Sergey; Finn, Chelsea",Test Time Robustification of Deep Models via Adaptation and Augmentation,,,,,https://openreview.net/forum?id=J1uOGgf-bP,"While deep neural networks can attain good accuracy on in-distribution test points, many applications require robustness even in the face of unexpected perturbations in the input, changes in the...",2021-09-29,2022-03-11 01:00:09,2022-03-11 01:00:09,2022-03-11 01:00:09,,,,,,,,,,,,,,en,,,,,openreview.net,,,,/Users/jacquesthibodeau/Zotero/storage/FMDPIXDJ/Zhang et al. - 2021 - Test Time Robustification of Deep Models via Adapt.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EPQFF5J7,journalArticle,2022,"Zhang, Marvin; Levine, Sergey; Finn, Chelsea",MEMO: Test Time Robustness via Adaptation and Augmentation,arXiv:2110.09506 [cs],,,,http://arxiv.org/abs/2110.09506,"While deep neural networks can attain good accuracy on in-distribution test points, many applications require robustness even in the face of unexpected perturbations in the input, changes in the domain, or other sources of distribution shift. We study the problem of test time robustification, i.e., using the test input to improve model robustness. Recent prior works have proposed methods for test time adaptation, however, they each introduce additional assumptions, such as access to multiple test points, that prevent widespread adoption. In this work, we aim to study and devise methods that make no assumptions about the model training process and are broadly applicable at test time. We propose a simple approach that can be used in any test setting where the model is probabilistic and adaptable: when presented with a test example, perform different data augmentations on the data point, and then adapt (all of) the model parameters by minimizing the entropy of the model's average, or marginal, output distribution across the augmentations. Intuitively, this objective encourages the model to make the same prediction across different augmentations, thus enforcing the invariances encoded in these augmentations, while also maintaining confidence in its predictions. In our experiments, we evaluate two baseline ResNet models, two robust ResNet-50 models, and a robust vision transformer model, and we demonstrate that this approach achieves accuracy gains of 1-8\% over standard model evaluation and also generally outperforms prior augmentation and adaptation strategies. For the setting in which only one test point is available, we achieve state-of-the-art results on the ImageNet-C, ImageNet-R, and, among ResNet-50 models, ImageNet-A distribution shift benchmarks.",2022-01-24,2022-03-11 01:00:14,2022-03-11 01:00:14,2022-03-11 01:00:14,,,,,,,MEMO,,,,,,,,,,,,arXiv.org,,arXiv: 2110.09506,,/Users/jacquesthibodeau/Zotero/storage/XHMLXPLS/Zhang et al. - 2022 - MEMO Test Time Robustness via Adaptation and Augm.pdf; /Users/jacquesthibodeau/Zotero/storage/NNWE2FCS/2110.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SKVTSGIG,journalArticle,2021,"Mao, Chengzhi; Jiang, Lu; Dehghani, Mostafa; Vondrick, Carl; Sukthankar, Rahul; Essa, Irfan",Discrete Representations Strengthen Vision Transformer Robustness,arXiv:2111.10493 [cs],,,,http://arxiv.org/abs/2111.10493,"Vision Transformer (ViT) is emerging as the state-of-the-art architecture for image recognition. While recent studies suggest that ViTs are more robust than their convolutional counterparts, our experiments find that ViTs are overly reliant on local features (e.g., nuisances and texture) and fail to make adequate use of global context (e.g., shape and structure). As a result, ViTs fail to generalize to out-of-distribution, real-world data. To address this deficiency, we present a simple and effective architecture modification to ViT's input layer by adding discrete tokens produced by a vector-quantized encoder. Different from the standard continuous pixel tokens, discrete tokens are invariant under small perturbations and contain less information individually, which promote ViTs to learn global information that is invariant. Experimental results demonstrate that adding discrete representation on four architecture variants strengthens ViT robustness by up to 12% across seven ImageNet robustness benchmarks while maintaining the performance on ImageNet.",2021-11-19,2022-03-11 01:00:31,2022-03-11 01:00:31,2022-03-11 01:00:31,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2111.10493,,/Users/jacquesthibodeau/Zotero/storage/H25YPYY2/Mao et al. - 2021 - Discrete Representations Strengthen Vision Transfo.pdf; /Users/jacquesthibodeau/Zotero/storage/QS6L6XT6/2111.html,,,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VJ37T7YV,journalArticle,2021,"Tang, Leonard; Ke, Elizabeth; Singh, Nikhil; Verma, Nakul; Drori, Iddo",Solving Probability and Statistics Problems by Program Synthesis,arXiv:2111.08267 [cs],,,,http://arxiv.org/abs/2111.08267,"We solve university level probability and statistics questions by program synthesis using OpenAI's Codex, a Transformer trained on text and fine-tuned on code. We transform course problems from MIT's 18.05 Introduction to Probability and Statistics and Harvard's STAT110 Probability into programming tasks. We then execute the generated code to get a solution. Since these course questions are grounded in probability, we often aim to have Codex generate probabilistic programs that simulate a large number of probabilistic dependencies to compute its solution. Our approach requires prompt engineering to transform the question from its original form to an explicit, tractable form that results in a correct program and solution. To estimate the amount of work needed to translate an original question into its tractable form, we measure the similarity between original and transformed questions. Our work is the first to introduce a new dataset of university-level probability and statistics problems and solve these problems in a scalable fashion using the program synthesis capabilities of large language models.",2021-11-16,2022-03-11 01:00:35,2022-03-11 01:00:35,2022-03-11 01:00:35,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2111.08267,,/Users/jacquesthibodeau/Zotero/storage/SY7Y8EE9/Tang et al. - 2021 - Solving Probability and Statistics Problems by Pro.pdf; /Users/jacquesthibodeau/Zotero/storage/57AFMQM2/2111.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Machine Learning; Computer Science - Programming Languages,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R5VFGPBS,journalArticle,2021,"Trabucco, Brandon; Kumar, Aviral; Geng, Xinyang; Levine, Sergey",Conservative Objective Models for Effective Offline Model-Based Optimization,arXiv:2107.06882 [cs],,,,http://arxiv.org/abs/2107.06882,"Computational design problems arise in a number of settings, from synthetic biology to computer architectures. In this paper, we aim to solve data-driven model-based optimization (MBO) problems, where the goal is to find a design input that maximizes an unknown objective function provided access to only a static dataset of prior experiments. Such data-driven optimization procedures are the only practical methods in many real-world domains where active data collection is expensive (e.g., when optimizing over proteins) or dangerous (e.g., when optimizing over aircraft designs). Typical methods for MBO that optimize the design against a learned model suffer from distributional shift: it is easy to find a design that ""fools"" the model into predicting a high value. To overcome this, we propose conservative objective models (COMs), a method that learns a model of the objective function that lower bounds the actual value of the ground-truth objective on out-of-distribution inputs, and uses it for optimization. Structurally, COMs resemble adversarial training methods used to overcome adversarial examples. COMs are simple to implement and outperform a number of existing methods on a wide range of MBO problems, including optimizing protein sequences, robot morphologies, neural network weights, and superconducting materials.",2021-07-14,2022-03-11 01:00:39,2022-03-11 01:00:39,2022-03-11 01:00:39,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2107.06882,,/Users/jacquesthibodeau/Zotero/storage/C5EGTGJS/Trabucco et al. - 2021 - Conservative Objective Models for Effective Offlin.pdf; /Users/jacquesthibodeau/Zotero/storage/GNWT39N2/2107.html,,,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KF6A3HRJ,journalArticle,2021,"Grinsztajn, Nathan; Ferret, Johan; Pietquin, Olivier; Preux, Philippe; Geist, Matthieu",There Is No Turning Back: A Self-Supervised Approach for Reversibility-Aware Reinforcement Learning,arXiv:2106.04480 [cs],,,,http://arxiv.org/abs/2106.04480,"We propose to learn to distinguish reversible from irreversible actions for better informed decision-making in Reinforcement Learning (RL). From theoretical considerations, we show that approximate reversibility can be learned through a simple surrogate task: ranking randomly sampled trajectory events in chronological order. Intuitively, pairs of events that are always observed in the same order are likely to be separated by an irreversible sequence of actions. Conveniently, learning the temporal order of events can be done in a fully self-supervised way, which we use to estimate the reversibility of actions from experience, without any priors. We propose two different strategies that incorporate reversibility in RL agents, one strategy for exploration (RAE) and one strategy for control (RAC). We demonstrate the potential of reversibility-aware agents in several environments, including the challenging Sokoban game. In synthetic tasks, we show that we can learn control policies that never fail and reduce to zero the side-effects of interactions, even without access to the reward function.",2021-10-29,2022-03-11 01:00:53,2022-03-11 01:00:53,2022-03-11 01:00:53,,,,,,,There Is No Turning Back,,,,,,,,,,,,arXiv.org,,arXiv: 2106.04480,,/Users/jacquesthibodeau/Zotero/storage/LHDD4ENB/Grinsztajn et al. - 2021 - There Is No Turning Back A Self-Supervised Approa.pdf; /Users/jacquesthibodeau/Zotero/storage/EH4DTHSK/2106.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9KHBCIAM,journalArticle,2022,"Papantoniou, Katerina; Papadakos, Panagiotis; Flouris, Giorgos; Plexousakis, Dimitris",Linguistic Cues of Deception in a Multilingual April Fools' Day Context,arXiv:2111.03913 [cs],,,,http://arxiv.org/abs/2111.03913,"In this work we consider the collection of deceptive April Fools' Day(AFD) news articles as a useful addition in existing datasets for deception detection tasks. Such collections have an established ground truth and are relatively easy to construct across languages. As a result, we introduce a corpus that includes diachronic AFD and normal articles from Greek newspapers and news websites. On top of that, we build a rich linguistic feature set, and analyze and compare its deception cues with the only AFD collection currently available, which is in English. Following a current research thread, we also discuss the individualism/collectivism dimension in deception with respect to these two datasets. Lastly, we build classifiers by testing various monolingual and crosslingual settings. The results showcase that AFD datasets can be helpful in deception detection studies, and are in alignment with the observations of other deception detection works.",2022-02-28,2022-03-11 01:02:16,2022-03-11 01:02:16,2022-03-11 01:02:16,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2111.03913,,/Users/jacquesthibodeau/Zotero/storage/RY7TMNC4/Papantoniou et al. - 2022 - Linguistic Cues of Deception in a Multilingual Apr.pdf; /Users/jacquesthibodeau/Zotero/storage/SUXTGAER/2111.html,,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P4BJVT2D,journalArticle,2021,"Jung, Sanghun; Lee, Jungsoo; Gwak, Daehoon; Choi, Sungha; Choo, Jaegul",Standardized Max Logits: A Simple yet Effective Approach for Identifying Unexpected Road Obstacles in Urban-Scene Segmentation,arXiv:2107.11264 [cs],,,,http://arxiv.org/abs/2107.11264,"Identifying unexpected objects on roads in semantic segmentation (e.g., identifying dogs on roads) is crucial in safety-critical applications. Existing approaches use images of unexpected objects from external datasets or require additional training (e.g., retraining segmentation networks or training an extra network), which necessitate a non-trivial amount of labor intensity or lengthy inference time. One possible alternative is to use prediction scores of a pre-trained network such as the max logits (i.e., maximum values among classes before the final softmax layer) for detecting such objects. However, the distribution of max logits of each predicted class is significantly different from each other, which degrades the performance of identifying unexpected objects in urban-scene segmentation. To address this issue, we propose a simple yet effective approach that standardizes the max logits in order to align the different distributions and reflect the relative meanings of max logits within each predicted class. Moreover, we consider the local regions from two different perspectives based on the intuition that neighboring pixels share similar semantic information. In contrast to previous approaches, our method does not utilize any external datasets or require additional training, which makes our method widely applicable to existing pre-trained segmentation models. Such a straightforward approach achieves a new state-of-the-art performance on the publicly available Fishyscapes Lost & Found leaderboard with a large margin. Our code is publicly available at this $\href{https://github.com/shjung13/Standardized-max-logits}{link}$.",2021-10-11,2022-03-11 01:02:18,2022-03-11 01:02:18,2022-03-11 01:02:18,,,,,,,Standardized Max Logits,,,,,,,,,,,,arXiv.org,,arXiv: 2107.11264,,/Users/jacquesthibodeau/Zotero/storage/PZIWZIZU/Jung et al. - 2021 - Standardized Max Logits A Simple yet Effective Ap.pdf; /Users/jacquesthibodeau/Zotero/storage/APA8JUSF/2107.html,,,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
D696L88R,journalArticle,2021,"Besnier, Victor; Bursuc, Andrei; Picard, David; Briot, Alexandre",Triggering Failures: Out-Of-Distribution detection by learning from local adversarial attacks in Semantic Segmentation,arXiv:2108.01634 [cs],,,,http://arxiv.org/abs/2108.01634,"In this paper, we tackle the detection of out-of-distribution (OOD) objects in semantic segmentation. By analyzing the literature, we found that current methods are either accurate or fast but not both which limits their usability in real world applications. To get the best of both aspects, we propose to mitigate the common shortcomings by following four design principles: decoupling the OOD detection from the segmentation task, observing the entire segmentation network instead of just its output, generating training data for the OOD detector by leveraging blind spots in the segmentation network and focusing the generated data on localized regions in the image to simulate OOD objects. Our main contribution is a new OOD detection architecture called ObsNet associated with a dedicated training scheme based on Local Adversarial Attacks (LAA). We validate the soundness of our approach across numerous ablation studies. We also show it obtains top performances both in speed and accuracy when compared to ten recent methods of the literature on three different datasets.",2021-08-03,2022-03-11 01:02:20,2022-03-11 01:02:20,2022-03-11 01:02:20,,,,,,,Triggering Failures,,,,,,,,,,,,arXiv.org,,arXiv: 2108.01634,,/Users/jacquesthibodeau/Zotero/storage/GLNSWEXX/Besnier et al. - 2021 - Triggering Failures Out-Of-Distribution detection.pdf; /Users/jacquesthibodeau/Zotero/storage/632WXETD/2108.html,,,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UW2MWSM2,journalArticle,2021,"Zimmermann, Roland S.; Borowski, Judy; Geirhos, Robert; Bethge, Matthias; Wallis, Thomas S. A.; Brendel, Wieland",How Well do Feature Visualizations Support Causal Understanding of CNN Activations?,arXiv:2106.12447 [cs],,,,http://arxiv.org/abs/2106.12447,"A precise understanding of why units in an artificial network respond to certain stimuli would constitute a big step towards explainable artificial intelligence. One widely used approach towards this goal is to visualize unit responses via activation maximization. These synthetic feature visualizations are purported to provide humans with precise information about the image features that cause a unit to be activated - an advantage over other alternatives like strongly activating natural dataset samples. If humans indeed gain causal insight from visualizations, this should enable them to predict the effect of an intervention, such as how occluding a certain patch of the image (say, a dog's head) changes a unit's activation. Here, we test this hypothesis by asking humans to decide which of two square occlusions causes a larger change to a unit's activation. Both a large-scale crowdsourced experiment and measurements with experts show that on average the extremely activating feature visualizations by Olah et al. (2017) indeed help humans on this task ($68 \pm 4$% accuracy; baseline performance without any visualizations is $60 \pm 3$%). However, they do not provide any substantial advantage over other visualizations (such as e.g. dataset samples), which yield similar performance ($66\pm3$% to $67 \pm3$% accuracy). Taken together, we propose an objective psychophysical task to quantify the benefit of unit-level interpretability methods for humans, and find no evidence that a widely-used feature visualization method provides humans with better ""causal understanding"" of unit activations than simple alternative visualizations.",2021-11-12,2022-03-11 01:02:22,2022-03-11 01:02:22,2022-03-11 01:02:21,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2106.12447,,/Users/jacquesthibodeau/Zotero/storage/NJ7L54YY/Zimmermann et al. - 2021 - How Well do Feature Visualizations Support Causal .pdf; /Users/jacquesthibodeau/Zotero/storage/IGG727ZB/2106.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Human-Computer Interaction; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7SXHW6NB,journalArticle,2021,"Zhao, Bingchen; Yu, Shaozuo; Ma, Wufei; Yu, Mingxin; Mei, Shenxiao; Wang, Angtian; He, Ju; Yuille, Alan; Kortylewski, Adam",ROBIN : A Benchmark for Robustness to Individual Nuisances in Real-World Out-of-Distribution Shifts,arXiv:2111.14341 [cs],,,,http://arxiv.org/abs/2111.14341,"Enhancing the robustness in real-world scenarios has been proven very challenging. One reason is that existing robustness benchmarks are limited, as they either rely on synthetic data or they simply measure robustness as generalization between datasets and hence ignore the effects of individual nuisance factors. In this work, we introduce ROBIN, a benchmark dataset for diagnosing the robustness of vision algorithms to individual nuisances in real-world images. ROBIN builds on 10 rigid categories from the PASCAL VOC 2012 and ImageNet datasets and includes out-of-distribution examples of the objects 3D pose, shape, texture, context and weather conditions. ROBIN is richly annotated to enable benchmark models for image classification, object detection, and 3D pose estimation. We provide results for a number of popular baselines and make several interesting observations: 1. Some nuisance factors have a much stronger negative effect on the performance compared to others. Moreover, the negative effect of an OODnuisance depends on the downstream vision task. 2. Current approaches to enhance OOD robustness using strong data augmentation have only marginal effects in real-world OOD scenarios, and sometimes even reduce the OOD performance. 3. We do not observe any significant differences between convolutional and transformer architectures in terms of OOD robustness. We believe our dataset provides a rich testbed to study the OOD robustness of vision algorithms and will help to significantly push forward research in this area.",2021-12-02,2022-03-11 01:02:24,2022-03-11 01:02:24,2022-03-11 01:02:24,,,,,,,ROBIN,,,,,,,,,,,,arXiv.org,,arXiv: 2111.14341,,/Users/jacquesthibodeau/Zotero/storage/5LTTPXN7/Zhao et al. - 2021 - ROBIN  A Benchmark for Robustness to Individual N.pdf; /Users/jacquesthibodeau/Zotero/storage/ACW5XG3M/2111.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BBSEGLAK,journalArticle,2022,"Wang, Boxin; Xu, Chejian; Wang, Shuohang; Gan, Zhe; Cheng, Yu; Gao, Jianfeng; Awadallah, Ahmed Hassan; Li, Bo",Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models,arXiv:2111.02840 [cs],,,,http://arxiv.org/abs/2111.02840,"Large-scale pre-trained language models have achieved tremendous success across a wide range of natural language understanding (NLU) tasks, even surpassing human performance. However, recent studies reveal that the robustness of these models can be challenged by carefully crafted textual adversarial examples. While several individual datasets have been proposed to evaluate model robustness, a principled and comprehensive benchmark is still missing. In this paper, we present Adversarial GLUE (AdvGLUE), a new multi-task benchmark to quantitatively and thoroughly explore and evaluate the vulnerabilities of modern large-scale language models under various types of adversarial attacks. In particular, we systematically apply 14 textual adversarial attack methods to GLUE tasks to construct AdvGLUE, which is further validated by humans for reliable annotations. Our findings are summarized as follows. (i) Most existing adversarial attack algorithms are prone to generating invalid or ambiguous adversarial examples, with around 90% of them either changing the original semantic meanings or misleading human annotators as well. Therefore, we perform a careful filtering process to curate a high-quality benchmark. (ii) All the language models and robust training methods we tested perform poorly on AdvGLUE, with scores lagging far behind the benign accuracy. We hope our work will motivate the development of new adversarial attacks that are more stealthy and semantic-preserving, as well as new robust language models against sophisticated adversarial attacks. AdvGLUE is available at https://adversarialglue.github.io.",2022-01-10,2022-03-11 01:02:45,2022-03-11 01:02:45,2022-03-11 01:02:45,,,,,,,Adversarial GLUE,,,,,,,,,,,,arXiv.org,,arXiv: 2111.02840,,/Users/jacquesthibodeau/Zotero/storage/ICNPCDMG/Wang et al. - 2022 - Adversarial GLUE A Multi-Task Benchmark for Robus.pdf; /Users/jacquesthibodeau/Zotero/storage/BYPTVT8I/2111.html,,,Computer Science - Computation and Language; Computer Science - Cryptography and Security; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7VARQKSN,journalArticle,2021,"Wallace, Eric; Williams, Adina; Jia, Robin; Kiela, Douwe",Analyzing Dynamic Adversarial Training Data in the Limit,arXiv:2110.08514 [cs],,,,http://arxiv.org/abs/2110.08514,"To create models that are robust across a wide range of test inputs, training datasets should include diverse examples that span numerous phenomena. Dynamic adversarial data collection (DADC), where annotators craft examples that challenge continually improving models, holds promise as an approach for generating such diverse training sets. Prior work has shown that running DADC over 1-3 rounds can help models fix some error types, but it does not necessarily lead to better generalization beyond adversarial test data. We argue that running DADC over many rounds maximizes its training-time benefits, as the different rounds can together cover many of the task-relevant phenomena. We present the first study of longer-term DADC, where we collect 20 rounds of NLI examples for a small set of premise paragraphs, with both adversarial and non-adversarial approaches. Models trained on DADC examples make 26% fewer errors on our expert-curated test set compared to models trained on non-adversarial data. Our analysis shows that DADC yields examples that are more difficult, more lexically and syntactically diverse, and contain fewer annotation artifacts compared to non-adversarial examples.",2021-10-16,2022-03-11 01:02:47,2022-03-11 01:02:47,2022-03-11 01:02:47,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2110.08514,,/Users/jacquesthibodeau/Zotero/storage/MVBXYNDE/Wallace et al. - 2021 - Analyzing Dynamic Adversarial Training Data in the.pdf; /Users/jacquesthibodeau/Zotero/storage/R3MSXIM9/2110.html,,,Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UQQSUQMM,journalArticle,2021,"Herrmann, Charles; Sargent, Kyle; Jiang, Lu; Zabih, Ramin; Chang, Huiwen; Liu, Ce; Krishnan, Dilip; Sun, Deqing",Pyramid Adversarial Training Improves ViT Performance,arXiv:2111.15121 [cs],,,,http://arxiv.org/abs/2111.15121,"Aggressive data augmentation is a key component of the strong generalization capabilities of Vision Transformer (ViT). One such data augmentation technique is adversarial training; however, many prior works have shown that this often results in poor clean accuracy. In this work, we present Pyramid Adversarial Training, a simple and effective technique to improve ViT's overall performance. We pair it with a ""matched"" Dropout and stochastic depth regularization, which adopts the same Dropout and stochastic depth configuration for the clean and adversarial samples. Similar to the improvements on CNNs by AdvProp (not directly applicable to ViT), our Pyramid Adversarial Training breaks the trade-off between in-distribution accuracy and out-of-distribution robustness for ViT and related architectures. It leads to $1.82\%$ absolute improvement on ImageNet clean accuracy for the ViT-B model when trained only on ImageNet-1K data, while simultaneously boosting performance on $7$ ImageNet robustness metrics, by absolute numbers ranging from $1.76\%$ to $11.45\%$. We set a new state-of-the-art for ImageNet-C (41.4 mCE), ImageNet-R ($53.92\%$), and ImageNet-Sketch ($41.04\%$) without extra data, using only the ViT-B/16 backbone and our Pyramid Adversarial Training. Our code will be publicly available upon acceptance.",2021-11-29,2022-03-11 01:02:49,2022-03-11 01:02:49,2022-03-11 01:02:49,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2111.15121,,/Users/jacquesthibodeau/Zotero/storage/EJIU9S9U/Herrmann et al. - 2021 - Pyramid Adversarial Training Improves ViT Performa.pdf; /Users/jacquesthibodeau/Zotero/storage/YPLHXMN9/2111.html,,,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TFUIX2ZG,journalArticle,2021,"Yang, Tsung-Yen; Hu, Michael; Chow, Yinlam; Ramadge, Peter J.; Narasimhan, Karthik",Safe Reinforcement Learning with Natural Language Constraints,arXiv:2010.05150 [cs],,,,http://arxiv.org/abs/2010.05150,"While safe reinforcement learning (RL) holds great promise for many practical applications like robotics or autonomous cars, current approaches require specifying constraints in mathematical form. Such specifications demand domain expertise, limiting the adoption of safe RL. In this paper, we propose learning to interpret natural language constraints for safe RL. To this end, we first introduce HazardWorld, a new multi-task benchmark that requires an agent to optimize reward while not violating constraints specified in free-form text. We then develop an agent with a modular architecture that can interpret and adhere to such textual constraints while learning new tasks. Our model consists of (1) a constraint interpreter that encodes textual constraints into spatial and temporal representations of forbidden states, and (2) a policy network that uses these representations to produce a policy achieving minimal constraint violations during training. Across different domains in HazardWorld, we show that our method achieves higher rewards (up to11x) and fewer constraint violations (by 1.8x) compared to existing approaches. However, in terms of absolute performance, HazardWorld still poses significant challenges for agents to learn efficiently, motivating the need for future work.",2021-08-03,2022-03-11 01:03:02,2022-03-11 01:03:02,2022-03-11 01:03:02,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2010.05150,,/Users/jacquesthibodeau/Zotero/storage/PAB7GCUJ/Yang et al. - 2021 - Safe Reinforcement Learning with Natural Language .pdf; /Users/jacquesthibodeau/Zotero/storage/WDEUWHPV/2010.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IRSRXTZA,journalArticle,2022,"Yu, Haonan; Xu, Wei; Zhang, Haichao",Towards Safe Reinforcement Learning with a Safety Editor Policy,arXiv:2201.12427 [cs],,,,http://arxiv.org/abs/2201.12427,"We consider the safe reinforcement learning (RL) problem of maximizing utility while satisfying provided constraints. Since we do not assume any prior knowledge or pre-training of the safety concept, we are interested in asymptotic constraint satisfaction. A popular approach in this line of research is to combine the Lagrangian method with a model-free RL algorithm to adjust the weight of the constraint reward dynamically. It relies on a single policy to handle the conflict between utility and constraint rewards, which is often challenging. Inspired by the safety layer design (Dalal et al., 2018), we propose to separately learn a safety editor policy that transforms potentially unsafe actions output by a utility maximizer policy into safe ones. The safety editor is trained to maximize the constraint reward while minimizing a hinge loss of the utility Q values of actions before and after the edit. On 12 custom Safety Gym (Ray et al., 2019) tasks and 2 safe racing tasks with very harsh constraint thresholds, our approach demonstrates outstanding utility performance while complying with the constraints. Ablation studies reveal that our two-policy design is critical. Simply doubling the model capacity of typical single-policy approaches will not lead to comparable results. The Q hinge loss is also important in certain circumstances, and replacing it with the usual L2 distance could fail badly.",2022-01-28,2022-03-11 01:03:18,2022-03-11 01:38:19,2022-03-11 01:03:18,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2201.12427,,/Users/jacquesthibodeau/Zotero/storage/9C9VJF2I/Yu et al. - 2022 - Towards Safe Reinforcement Learning with a Safety .pdf; /Users/jacquesthibodeau/Zotero/storage/IK4X6IGS/Yu et al. - 2022 - Towards Safe Reinforcement Learning with a Safety .pdf; /Users/jacquesthibodeau/Zotero/storage/LIDV2Y77/2201.html; /Users/jacquesthibodeau/Zotero/storage/SL4D3MH8/2201.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PIW2SID7,journalArticle,2022,"Thomas, Garrett; Luo, Yuping; Ma, Tengyu",Safe Reinforcement Learning by Imagining the Near Future,arXiv:2202.07789 [cs],,,,http://arxiv.org/abs/2202.07789,"Safe reinforcement learning is a promising path toward applying reinforcement learning algorithms to real-world problems, where suboptimal behaviors may lead to actual negative consequences. In this work, we focus on the setting where unsafe states can be avoided by planning ahead a short time into the future. In this setting, a model-based agent with a sufficiently accurate model can avoid unsafe states. We devise a model-based algorithm that heavily penalizes unsafe trajectories, and derive guarantees that our algorithm can avoid unsafe states under certain assumptions. Experiments demonstrate that our algorithm can achieve competitive rewards with fewer safety violations in several continuous control tasks.",2022-02-15,2022-03-11 01:03:43,2022-03-11 01:03:43,2022-03-11 01:03:43,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2202.07789,,/Users/jacquesthibodeau/Zotero/storage/BPZXUX3A/Thomas et al. - 2022 - Safe Reinforcement Learning by Imagining the Near .pdf; /Users/jacquesthibodeau/Zotero/storage/93SZGJUK/2202.html,,,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RNVV4SR2,journalArticle,2022,"Ouyang, Long; Wu, Jeff; Jiang, Xu; Almeida, Diogo; Wainwright, Carroll L.; Mishkin, Pamela; Zhang, Chong; Agarwal, Sandhini; Slama, Katarina; Ray, Alex; Schulman, John; Hilton, Jacob; Kelton, Fraser; Miller, Luke; Simens, Maddie; Askell, Amanda; Welinder, Peter; Christiano, Paul; Leike, Jan; Lowe, Ryan",Training language models to follow instructions with human feedback,arXiv:2203.02155 [cs],,,,http://arxiv.org/abs/2203.02155,"Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",2022-03-04,2022-03-11 01:03:58,2022-03-11 01:03:58,2022-03-11 01:03:58,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2203.02155,,/Users/jacquesthibodeau/Zotero/storage/PADEXKSX/Ouyang et al. - 2022 - Training language models to follow instructions wi.pdf; /Users/jacquesthibodeau/Zotero/storage/TWCAF3MA/2203.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
G4KC3SAZ,journalArticle,2021,"Lee, Kimin; Smith, Laura; Dragan, Anca; Abbeel, Pieter",B-Pref: Benchmarking Preference-Based Reinforcement Learning,arXiv:2111.03026 [cs],,,,http://arxiv.org/abs/2111.03026,"Reinforcement learning (RL) requires access to a reward function that incentivizes the right behavior, but these are notoriously hard to specify for complex tasks. Preference-based RL provides an alternative: learning policies using a teacher's preferences without pre-defined rewards, thus overcoming concerns associated with reward engineering. However, it is difficult to quantify the progress in preference-based RL due to the lack of a commonly adopted benchmark. In this paper, we introduce B-Pref: a benchmark specially designed for preference-based RL. A key challenge with such a benchmark is providing the ability to evaluate candidate algorithms quickly, which makes relying on real human input for evaluation prohibitive. At the same time, simulating human input as giving perfect preferences for the ground truth reward function is unrealistic. B-Pref alleviates this by simulating teachers with a wide array of irrationalities, and proposes metrics not solely for performance but also for robustness to these potential irrationalities. We showcase the utility of B-Pref by using it to analyze algorithmic design choices, such as selecting informative queries, for state-of-the-art preference-based RL algorithms. We hope that B-Pref can serve as a common starting point to study preference-based RL more systematically. Source code is available at https://github.com/rll-research/B-Pref.",2021-11-04,2022-03-11 01:04:21,2022-03-11 01:04:21,2022-03-11 01:04:21,,,,,,,B-Pref,,,,,,,,,,,,arXiv.org,,arXiv: 2111.03026,,/Users/jacquesthibodeau/Zotero/storage/LZZIN2MY/Lee et al. - 2021 - B-Pref Benchmarking Preference-Based Reinforcemen.pdf; /Users/jacquesthibodeau/Zotero/storage/7R2FJCXT/2111.html,,,Computer Science - Artificial Intelligence; Computer Science - Human-Computer Interaction; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4QFVRC4R,journalArticle,2022,"Meng, Kevin; Bau, David; Andonian, Alex; Belinkov, Yonatan",Locating and Editing Factual Knowledge in GPT,arXiv:2202.05262 [cs],,,,http://arxiv.org/abs/2202.05262,"We investigate the mechanisms underlying factual knowledge recall in autoregressive transformer language models. First, we develop a causal intervention for identifying neuron activations capable of altering a model's factual predictions. Within large GPT-style models, this reveals two distinct sets of neurons that we hypothesize correspond to knowing an abstract fact and saying a concrete word, respectively. This insight inspires the development of ROME, a novel method for editing facts stored in model weights. For evaluation, we assemble CounterFact, a dataset of over twenty thousand counterfactuals and tools to facilitate sensitive measurements of knowledge editing. Using CounterFact, we confirm the distinction between saying and knowing neurons, and we find that ROME achieves state-of-the-art performance in knowledge editing compared to other methods. An interactive demo notebook, full code implementation, and the dataset are available at https://rome.baulab.info/.",2022-02-10,2022-03-11 01:04:40,2022-03-11 01:04:40,2022-03-11 01:04:40,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2202.05262,,/Users/jacquesthibodeau/Zotero/storage/FVG2NMDT/Meng et al. - 2022 - Locating and Editing Factual Knowledge in GPT.pdf; /Users/jacquesthibodeau/Zotero/storage/FIPFTGUT/2202.html,,,Computer Science - Computation and Language; Computer Science - Machine Learning; I.2.7,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DIQ5IJQ8,journalArticle,2020,"Liu, Hanmeng; Cui, Leyang; Liu, Jian; Zhang, Yue",Natural Language Inference in Context -- Investigating Contextual Reasoning over Long Texts,arXiv:2011.04864 [cs],,,,http://arxiv.org/abs/2011.04864,"Natural language inference (NLI) is a fundamental NLP task, investigating the entailment relationship between two texts. Popular NLI datasets present the task at sentence-level. While adequate for testing semantic representations, they fall short for testing contextual reasoning over long texts, which is a natural part of the human inference process. We introduce ConTRoL, a new dataset for ConTextual Reasoning over Long texts. Consisting of 8,325 expert-designed ""context-hypothesis"" pairs with gold labels, ConTRoL is a passage-level NLI dataset with a focus on complex contextual reasoning types such as logical reasoning. It is derived from competitive selection and recruitment test (verbal reasoning test) for police recruitment, with expert level quality. Compared with previous NLI benchmarks, the materials in ConTRoL are much more challenging, involving a range of reasoning types. Empirical results show that state-of-the-art language models perform by far worse than educated humans. Our dataset can also serve as a testing-set for downstream tasks like Checking Factual Correctness of Summaries.",2020-11-09,2022-03-11 01:04:42,2022-03-11 01:04:42,2022-03-11 01:04:42,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2011.04864,,/Users/jacquesthibodeau/Zotero/storage/6J29LTVY/Liu et al. - 2020 - Natural Language Inference in Context -- Investiga.pdf; /Users/jacquesthibodeau/Zotero/storage/JMU6K4XF/2011.html,,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PIG42Y6E,journalArticle,2021,"Minderer, Matthias; Djolonga, Josip; Romijnders, Rob; Hubis, Frances; Zhai, Xiaohua; Houlsby, Neil; Tran, Dustin; Lucic, Mario",Revisiting the Calibration of Modern Neural Networks,arXiv:2106.07998 [cs],,,,http://arxiv.org/abs/2106.07998,"Accurate estimation of predictive uncertainty (model calibration) is essential for the safe application of neural networks. Many instances of miscalibration in modern neural networks have been reported, suggesting a trend that newer, more accurate models produce poorly calibrated predictions. Here, we revisit this question for recent state-of-the-art image classification models. We systematically relate model calibration and accuracy, and find that the most recent models, notably those not using convolutions, are among the best calibrated. Trends observed in prior model generations, such as decay of calibration with distribution shift or model size, are less pronounced in recent architectures. We also show that model size and amount of pretraining do not fully explain these differences, suggesting that architecture is a major determinant of calibration properties.",2021-10-26,2022-03-11 01:04:52,2022-03-11 01:04:52,2022-03-11 01:04:52,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2106.07998,,/Users/jacquesthibodeau/Zotero/storage/V74X8RRV/Minderer et al. - 2021 - Revisiting the Calibration of Modern Neural Networ.pdf; /Users/jacquesthibodeau/Zotero/storage/Z9XNYP9T/2106.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
I4WMRJ5M,journalArticle,2021,"Karandikar, Archit; Cain, Nicholas; Tran, Dustin; Lakshminarayanan, Balaji; Shlens, Jonathon; Mozer, Michael C.; Roelofs, Becca",Soft Calibration Objectives for Neural Networks,arXiv:2108.00106 [cs],,,,http://arxiv.org/abs/2108.00106,"Optimal decision making requires that classifiers produce uncertainty estimates consistent with their empirical accuracy. However, deep neural networks are often under- or over-confident in their predictions. Consequently, methods have been developed to improve the calibration of their predictive uncertainty both during training and post-hoc. In this work, we propose differentiable losses to improve calibration based on a soft (continuous) version of the binning operation underlying popular calibration-error estimators. When incorporated into training, these soft calibration losses achieve state-of-the-art single-model ECE across multiple datasets with less than 1% decrease in accuracy. For instance, we observe an 82% reduction in ECE (70% relative to the post-hoc rescaled ECE) in exchange for a 0.7% relative decrease in accuracy relative to the cross entropy baseline on CIFAR-100. When incorporated post-training, the soft-binning-based calibration error objective improves upon temperature scaling, a popular recalibration method. Overall, experiments across losses and datasets demonstrate that using calibration-sensitive procedures yield better uncertainty estimates under dataset shift than the standard practice of using a cross entropy loss and post-hoc recalibration methods.",2021-12-07,2022-03-11 01:05:04,2022-03-11 01:05:04,2022-03-11 01:05:04,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2108.00106,,/Users/jacquesthibodeau/Zotero/storage/R5PT9IM3/Karandikar et al. - 2021 - Soft Calibration Objectives for Neural Networks.pdf; /Users/jacquesthibodeau/Zotero/storage/CQRA3RSD/2108.html,,,Computer Science - Artificial Intelligence; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QRRZ4CQE,journalArticle,2022,"Yu, Yaodong; Yang, Zitong; Wei, Alexander; Ma, Yi; Steinhardt, Jacob",Predicting Out-of-Distribution Error with the Projection Norm,"arXiv:2202.05834 [cs, stat]",,,,http://arxiv.org/abs/2202.05834,"We propose a metric -- Projection Norm -- to predict a model's performance on out-of-distribution (OOD) data without access to ground truth labels. Projection Norm first uses model predictions to pseudo-label test samples and then trains a new model on the pseudo-labels. The more the new model's parameters differ from an in-distribution model, the greater the predicted OOD error. Empirically, our approach outperforms existing methods on both image and text classification tasks and across different network architectures. Theoretically, we connect our approach to a bound on the test error for overparameterized linear models. Furthermore, we find that Projection Norm is the only approach that achieves non-trivial detection performance on adversarial examples. Our code is available at https://github.com/yaodongyu/ProjNorm.",2022-02-11,2022-03-11 01:05:16,2022-03-11 01:05:16,2022-03-11 01:05:16,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2202.05834,,/Users/jacquesthibodeau/Zotero/storage/2588K5QW/Yu et al. - 2022 - Predicting Out-of-Distribution Error with the Proj.pdf; /Users/jacquesthibodeau/Zotero/storage/2HT6DBJK/2202.html,,,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R5DUJUZS,journalArticle,2022,"Ypsilantis, Nikolaos-Antonios; Garcia, Noa; Han, Guangxing; Ibrahimi, Sarah; Van Noord, Nanne; Tolias, Giorgos",The Met Dataset: Instance-level Recognition for Artworks,arXiv:2202.01747 [cs],,,,http://arxiv.org/abs/2202.01747,"This work introduces a dataset for large-scale instance-level recognition in the domain of artworks. The proposed benchmark exhibits a number of different challenges such as large inter-class similarity, long tail distribution, and many classes. We rely on the open access collection of The Met museum to form a large training set of about 224k classes, where each class corresponds to a museum exhibit with photos taken under studio conditions. Testing is primarily performed on photos taken by museum guests depicting exhibits, which introduces a distribution shift between training and testing. Testing is additionally performed on a set of images not related to Met exhibits making the task resemble an out-of-distribution detection problem. The proposed benchmark follows the paradigm of other recent datasets for instance-level recognition on different domains to encourage research on domain independent approaches. A number of suitable approaches are evaluated to offer a testbed for future comparisons. Self-supervised and supervised contrastive learning are effectively combined to train the backbone which is used for non-parametric classification that is shown as a promising direction. Dataset webpage: http://cmp.felk.cvut.cz/met/",2022-02-03,2022-03-11 01:05:29,2022-03-11 01:05:29,2022-03-11 01:05:29,,,,,,,The Met Dataset,,,,,,,,,,,,arXiv.org,,arXiv: 2202.01747,,/Users/jacquesthibodeau/Zotero/storage/DRGJVC3M/Ypsilantis et al. - 2022 - The Met Dataset Instance-level Recognition for Ar.pdf; /Users/jacquesthibodeau/Zotero/storage/SYB6FK9M/2202.html,,,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9ICPFS4N,journalArticle,2021,"Sun, Yiyou; Guo, Chuan; Li, Yixuan",ReAct: Out-of-distribution Detection With Rectified Activations,arXiv:2111.12797 [cs],,,,http://arxiv.org/abs/2111.12797,"Out-of-distribution (OOD) detection has received much attention lately due to its practical importance in enhancing the safe deployment of neural networks. One of the primary challenges is that models often produce highly confident predictions on OOD data, which undermines the driving principle in OOD detection that the model should only be confident about in-distribution samples. In this work, we propose ReAct--a simple and effective technique for reducing model overconfidence on OOD data. Our method is motivated by novel analysis on internal activations of neural networks, which displays highly distinctive signature patterns for OOD distributions. Our method can generalize effectively to different network architectures and different OOD detection scores. We empirically demonstrate that ReAct achieves competitive detection performance on a comprehensive suite of benchmark datasets, and give theoretical explication for our method's efficacy. On the ImageNet benchmark, ReAct reduces the false positive rate (FPR95) by 25.05% compared to the previous best method.",2021-11-24,2022-03-11 01:05:39,2022-03-11 01:05:39,2022-03-11 01:05:39,,,,,,,ReAct,,,,,,,,,,,,arXiv.org,,arXiv: 2111.12797,,/Users/jacquesthibodeau/Zotero/storage/UR9UMZSS/Sun et al. - 2021 - ReAct Out-of-distribution Detection With Rectifie.pdf; /Users/jacquesthibodeau/Zotero/storage/JX4WRFTR/2111.html,,,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IKMB48ME,journalArticle,2022,"Hendrycks, Dan; Basart, Steven; Mazeika, Mantas; Zou, Andy; Kwon, Joe; Mostajabi, Mohammadreza; Steinhardt, Jacob; Song, Dawn",Scaling Out-of-Distribution Detection for Real-World Settings,arXiv:1911.11132 [cs],,,,http://arxiv.org/abs/1911.11132,"Detecting out-of-distribution examples is important for safety-critical machine learning applications such as detecting novel biological phenomena and self-driving cars. However, existing research mainly focuses on simple small-scale settings. To set the stage for more realistic out-of-distribution detection, we depart from small-scale settings and explore large-scale multiclass and multi-label settings with high-resolution images and thousands of classes. To make future work in real-world settings possible, we create new benchmarks for three large-scale settings. To test ImageNet multiclass anomaly detectors, we introduce the Species dataset containing over 700,000 images and over a thousand anomalous species. We leverage ImageNet-21K to evaluate PASCAL VOC and COCO multilabel anomaly detectors. Third, we introduce a new benchmark for anomaly segmentation by introducing a segmentation benchmark with road anomalies. We conduct extensive experiments in these more realistic settings for out-of-distribution detection and find that a surprisingly simple detector based on the maximum logit outperforms prior methods in all the large-scale multi-class, multi-label, and segmentation tasks, establishing a simple new baseline for future work.",2022-02-07,2022-03-11 01:05:54,2022-03-11 01:05:54,2022-03-11 01:05:54,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 1911.11132,,/Users/jacquesthibodeau/Zotero/storage/CBAXQ3U5/Hendrycks et al. - 2022 - Scaling Out-of-Distribution Detection for Real-Wor.pdf; /Users/jacquesthibodeau/Zotero/storage/GV4HFVE2/1911.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
D42L7Z7U,journalArticle,2022,"Foley, Harrison; Fowl, Liam; Goldstein, Tom; Taylor, Gavin",Execute Order 66: Targeted Data Poisoning for Reinforcement Learning,arXiv:2201.00762 [cs],,,,http://arxiv.org/abs/2201.00762,"Data poisoning for reinforcement learning has historically focused on general performance degradation, and targeted attacks have been successful via perturbations that involve control of the victim's policy and rewards. We introduce an insidious poisoning attack for reinforcement learning which causes agent misbehavior only at specific target states - all while minimally modifying a small fraction of training observations without assuming any control over policy or reward. We accomplish this by adapting a recent technique, gradient alignment, to reinforcement learning. We test our method and demonstrate success in two Atari games of varying difficulty.",2022-01-03,2022-03-11 01:06:03,2022-03-11 01:06:03,2022-03-11 01:06:03,,,,,,,Execute Order 66,,,,,,,,,,,,arXiv.org,,arXiv: 2201.00762,,/Users/jacquesthibodeau/Zotero/storage/DCWYVM9Q/Foley et al. - 2022 - Execute Order 66 Targeted Data Poisoning for Rein.pdf; /Users/jacquesthibodeau/Zotero/storage/DR45MNB4/2201.html,,,Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JQG3F593,journalArticle,2022,"Shi, Bowen; Hsu, Wei-Ning; Mohamed, Abdelrahman",Robust Self-Supervised Audio-Visual Speech Recognition,"arXiv:2201.01763 [cs, eess]",,,,http://arxiv.org/abs/2201.01763,"Audio-based automatic speech recognition (ASR) degrades significantly in noisy environments and is particularly vulnerable to interfering speech, as the model cannot determine which speaker to transcribe. Audio-visual speech recognition (AVSR) systems improve robustness by complementing the audio stream with the visual information that is invariant to noise and helps the model focus on the desired speaker. However, previous AVSR work focused solely on the supervised learning setup; hence the progress was hindered by the amount of labeled data available. In this work, we present a self-supervised AVSR framework built upon Audio-Visual HuBERT (AV-HuBERT), a state-of-the-art audio-visual speech representation learning model. On the largest available AVSR benchmark dataset LRS3, our approach outperforms prior state-of-the-art by ~50% (28.0% vs. 14.1%) using less than 10% of labeled data (433hr vs. 30hr) in the presence of babble noise, while reducing the WER of an audio-based model by over 75% (25.8% vs. 5.8%) on average.",2022-01-05,2022-03-11 01:06:23,2022-03-11 01:06:23,2022-03-11 01:06:23,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2201.01763,,/Users/jacquesthibodeau/Zotero/storage/59BG2ZWZ/Shi et al. - 2022 - Robust Self-Supervised Audio-Visual Speech Recogni.pdf; /Users/jacquesthibodeau/Zotero/storage/P73PM5N8/2201.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Sound; Electrical Engineering and Systems Science - Audio and Speech Processing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
35ZECVN7,journalArticle,2022,"Kar, Oğuzhan Fatih; Yeo, Teresa; Atanov, Andrei; Zamir, Amir",3D Common Corruptions and Data Augmentation,arXiv:2203.01441 [cs],,,,http://arxiv.org/abs/2203.01441,"We introduce a set of image transformations that can be used as `corruptions' to evaluate the robustness of models as well as `data augmentation' mechanisms for training neural networks. The primary distinction of the proposed transformations is that, unlike existing approaches such as Common Corruptions, the geometry of the scene is incorporated in the transformations -- thus leading to corruptions that are more likely to occur in the real world. We show these transformations are `efficient' (can be computed on-the-fly), `extendable' (can be applied on most datasets of real images), expose vulnerability of existing models, and can effectively make models more robust when employed as `3D data augmentation' mechanisms. Our evaluations performed on several tasks and datasets suggest incorporating 3D information into robustness benchmarking and training opens up a promising direction for robustness research.",2022-03-02,2022-03-11 01:06:33,2022-03-11 01:06:33,2022-03-11 01:06:33,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2203.01441,,/Users/jacquesthibodeau/Zotero/storage/5JKJMPKB/Kar et al. - 2022 - 3D Common Corruptions and Data Augmentation.pdf; /Users/jacquesthibodeau/Zotero/storage/7AXV3UB7/2203.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TW5ZP9IV,journalArticle,2022,"Long, Alexander; Yin, Wei; Ajanthan, Thalaiyasingam; Nguyen, Vu; Purkait, Pulak; Garg, Ravi; Blair, Alan; Shen, Chunhua; Hengel, Anton van den",Retrieval Augmented Classification for Long-Tail Visual Recognition,arXiv:2202.11233 [cs],,,,http://arxiv.org/abs/2202.11233,"We introduce Retrieval Augmented Classification (RAC), a generic approach to augmenting standard image classification pipelines with an explicit retrieval module. RAC consists of a standard base image encoder fused with a parallel retrieval branch that queries a non-parametric external memory of pre-encoded images and associated text snippets. We apply RAC to the problem of long-tail classification and demonstrate a significant improvement over previous state-of-the-art on Places365-LT and iNaturalist-2018 (14.5% and 6.7% respectively), despite using only the training datasets themselves as the external information source. We demonstrate that RAC's retrieval module, without prompting, learns a high level of accuracy on tail classes. This, in turn, frees the base encoder to focus on common classes, and improve its performance thereon. RAC represents an alternative approach to utilizing large, pretrained models without requiring fine-tuning, as well as a first step towards more effectively making use of external memory within common computer vision architectures.",2022-02-22,2022-03-11 01:06:44,2022-03-11 01:06:44,2022-03-11 01:06:44,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2202.11233,,/Users/jacquesthibodeau/Zotero/storage/GNXXXSFR/Long et al. - 2022 - Retrieval Augmented Classification for Long-Tail V.pdf; /Users/jacquesthibodeau/Zotero/storage/GRHJ4W6F/2202.html,,,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3BM2QHXN,journalArticle,2022,"Kaplun, Gal; Ghosh, Nikhil; Garg, Saurabh; Barak, Boaz; Nakkiran, Preetum",Deconstructing Distributions: A Pointwise Framework of Learning,"arXiv:2202.09931 [cs, stat]",,,,http://arxiv.org/abs/2202.09931,"In machine learning, we traditionally evaluate the performance of a single model, averaged over a collection of test inputs. In this work, we propose a new approach: we measure the performance of a collection of models when evaluated on a $\textit{single input point}$. Specifically, we study a point's $\textit{profile}$: the relationship between models' average performance on the test distribution and their pointwise performance on this individual point. We find that profiles can yield new insights into the structure of both models and data -- in and out-of-distribution. For example, we empirically show that real data distributions consist of points with qualitatively different profiles. On one hand, there are ""compatible"" points with strong correlation between the pointwise and average performance. On the other hand, there are points with weak and even $\textit{negative}$ correlation: cases where improving overall model accuracy actually $\textit{hurts}$ performance on these inputs. We prove that these experimental observations are inconsistent with the predictions of several simplified models of learning proposed in prior work. As an application, we use profiles to construct a dataset we call CIFAR-10-NEG: a subset of CINIC-10 such that for standard models, accuracy on CIFAR-10-NEG is $\textit{negatively correlated}$ with accuracy on CIFAR-10 test. This illustrates, for the first time, an OOD dataset that completely inverts ""accuracy-on-the-line"" (Miller, Taori, Raghunathan, Sagawa, Koh, Shankar, Liang, Carmon, and Schmidt 2021)",2022-02-20,2022-03-11 01:06:55,2022-03-11 01:06:55,2022-03-11 01:06:55,,,,,,,Deconstructing Distributions,,,,,,,,,,,,arXiv.org,,arXiv: 2202.09931,,/Users/jacquesthibodeau/Zotero/storage/M4MFP3EB/Kaplun et al. - 2022 - Deconstructing Distributions A Pointwise Framewor.pdf; /Users/jacquesthibodeau/Zotero/storage/KEFGW4PR/2202.html,,,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EWMKLQRL,journalArticle,2022,"Weber, Maurice; Li, Linyi; Wang, Boxin; Zhao, Zhikuan; Li, Bo; Zhang, Ce",Certifying Out-of-Domain Generalization for Blackbox Functions,arXiv:2202.01679 [cs],,,,http://arxiv.org/abs/2202.01679,"Certifying the robustness of model performance under bounded data distribution shifts has recently attracted intensive interests under the umbrella of distributional robustness. However, existing techniques either make strong assumptions on the model class and loss functions that can be certified, such as smoothness expressed via Lipschitz continuity of gradients, or require to solve complex optimization problems. As a result, the wider application of these techniques is currently limited by its scalability and flexibility -- these techniques often do not scale to large-scale datasets with modern deep neural networks or cannot handle loss functions which may be non-smooth, such as the 0-1 loss. In this paper, we focus on the problem of certifying distributional robustness for black box models and bounded losses, without other assumptions. We propose a novel certification framework given bounded distance of mean and variance of two distributions. Our certification technique scales to ImageNet-scale datasets, complex models, and a diverse range of loss functions. We then focus on one specific application enabled by such scalability and flexibility, i.e., certifying out-of-domain generalization for large neural networks and loss functions such as accuracy and AUC. We experimentally validate our certification method on a number of datasets, ranging from ImageNet, where we provide the first non-vacuous certified out-of-domain generalization, to smaller classification tasks where we are able to compare with the state-of-the-art and show that our method performs considerably better.",2022-02-03,2022-03-11 01:07:06,2022-03-11 01:07:06,2022-03-11 01:07:06,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2202.01679,,/Users/jacquesthibodeau/Zotero/storage/5CJR9VJI/Weber et al. - 2022 - Certifying Out-of-Domain Generalization for Blackb.pdf; /Users/jacquesthibodeau/Zotero/storage/QUDAFBNS/2202.html,,,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EYMUMTND,journalArticle,2022,"Kumar, Aounon; Levine, Alexander; Goldstein, Tom; Feizi, Soheil",Certifying Model Accuracy under Distribution Shifts,arXiv:2201.12440 [cs],,,,http://arxiv.org/abs/2201.12440,"Certified robustness in machine learning has primarily focused on adversarial perturbations of the input with a fixed attack budget for each point in the data distribution. In this work, we present provable robustness guarantees on the accuracy of a model under bounded Wasserstein shifts of the data distribution. We show that a simple procedure that randomizes the input of the model within a transformation space is provably robust to distributional shifts under the transformation. Our framework allows the datum-specific perturbation size to vary across different points in the input distribution and is general enough to include fixed-sized perturbations as well. Our certificates produce guaranteed lower bounds on the performance of the model for any (natural or adversarial) shift of the input distribution within a Wasserstein ball around the original distribution. We apply our technique to: (i) certify robustness against natural (non-adversarial) transformations of images such as color shifts, hue shifts and changes in brightness and saturation, (ii) certify robustness against adversarial shifts of the input distribution, and (iii) show provable lower bounds (hardness results) on the performance of models trained on so-called ""unlearnable"" datasets that have been poisoned to interfere with model training.",2022-01-28,2022-03-11 01:07:17,2022-03-11 01:07:17,2022-03-11 01:07:17,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2201.12440,,/Users/jacquesthibodeau/Zotero/storage/FPGPLJLI/Kumar et al. - 2022 - Certifying Model Accuracy under Distribution Shift.pdf; /Users/jacquesthibodeau/Zotero/storage/57NNXGYH/2201.html,,,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BCHQQL7G,journalArticle,2021,"Sun, Jiachen; Mehra, Akshay; Kailkhura, Bhavya; Chen, Pin-Yu; Hendrycks, Dan; Hamm, Jihun; Mao, Z. Morley",Certified Adversarial Defenses Meet Out-of-Distribution Corruptions: Benchmarking Robustness and Simple Baselines,arXiv:2112.00659 [cs],,,,http://arxiv.org/abs/2112.00659,"Certified robustness guarantee gauges a model's robustness to test-time attacks and can assess the model's readiness for deployment in the real world. In this work, we critically examine how the adversarial robustness guarantees from randomized smoothing-based certification methods change when state-of-the-art certifiably robust models encounter out-of-distribution (OOD) data. Our analysis demonstrates a previously unknown vulnerability of these models to low-frequency OOD data such as weather-related corruptions, rendering these models unfit for deployment in the wild. To alleviate this issue, we propose a novel data augmentation scheme, FourierMix, that produces augmentations to improve the spectral coverage of the training data. Furthermore, we propose a new regularizer that encourages consistent predictions on noise perturbations of the augmented data to improve the quality of the smoothed models. We find that FourierMix augmentations help eliminate the spectral bias of certifiably robust models enabling them to achieve significantly better robustness guarantees on a range of OOD benchmarks. Our evaluation also uncovers the inability of current OOD benchmarks at highlighting the spectral biases of the models. To this end, we propose a comprehensive benchmarking suite that contains corruptions from different regions in the spectral domain. Evaluation of models trained with popular augmentation methods on the proposed suite highlights their spectral biases and establishes the superiority of FourierMix trained models at achieving better-certified robustness guarantees under OOD shifts over the entire frequency spectrum.",2021-12-01,2022-03-11 01:07:27,2022-03-11 01:07:27,2022-03-11 01:07:27,,,,,,,Certified Adversarial Defenses Meet Out-of-Distribution Corruptions,,,,,,,,,,,,arXiv.org,,arXiv: 2112.00659,,/Users/jacquesthibodeau/Zotero/storage/6HF2MDV3/Sun et al. - 2021 - Certified Adversarial Defenses Meet Out-of-Distrib.pdf; /Users/jacquesthibodeau/Zotero/storage/CJWSQXUZ/2112.html,,,Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
U4N2XYSI,journalArticle,2022,"Perez, Ethan; Huang, Saffron; Song, Francis; Cai, Trevor; Ring, Roman; Aslanides, John; Glaese, Amelia; McAleese, Nat; Irving, Geoffrey",Red Teaming Language Models with Language Models,arXiv:2202.03286 [cs],,,,http://arxiv.org/abs/2202.03286,"Language Models (LMs) often cannot be deployed because of their potential to harm users in hard-to-predict ways. Prior work identifies harmful behaviors before deployment by using human annotators to hand-write test cases. However, human annotation is expensive, limiting the number and diversity of test cases. In this work, we automatically find cases where a target LM behaves in a harmful way, by generating test cases (""red teaming"") using another LM. We evaluate the target LM's replies to generated test questions using a classifier trained to detect offensive content, uncovering tens of thousands of offensive replies in a 280B parameter LM chatbot. We explore several methods, from zero-shot generation to reinforcement learning, for generating test cases with varying levels of diversity and difficulty. Furthermore, we use prompt engineering to control LM-generated test cases to uncover a variety of other harms, automatically finding groups of people that the chatbot discusses in offensive ways, personal and hospital phone numbers generated as the chatbot's own contact info, leakage of private training data in generated text, and harms that occur over the course of a conversation. Overall, LM-based red teaming is one promising tool (among many needed) for finding and fixing diverse, undesirable LM behaviors before impacting users.",2022-02-07,2022-03-11 01:07:38,2022-03-11 01:07:38,2022-03-11 01:07:37,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2202.03286,,/Users/jacquesthibodeau/Zotero/storage/JHI3DJ3D/Perez et al. - 2022 - Red Teaming Language Models with Language Models.pdf; /Users/jacquesthibodeau/Zotero/storage/XQL524J2/2202.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Cryptography and Security; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AN7HZJTD,journalArticle,2020,"Li, Linyang; Ma, Ruotian; Guo, Qipeng; Xue, Xiangyang; Qiu, Xipeng",BERT-ATTACK: Adversarial Attack Against BERT Using BERT,arXiv:2004.09984 [cs],,,,http://arxiv.org/abs/2004.09984,"Adversarial attacks for discrete data (such as texts) have been proved significantly more challenging than continuous data (such as images) since it is difficult to generate adversarial samples with gradient-based methods. Current successful attack methods for texts usually adopt heuristic replacement strategies on the character or word level, which remains challenging to find the optimal solution in the massive space of possible combinations of replacements while preserving semantic consistency and language fluency. In this paper, we propose \textbf{BERT-Attack}, a high-quality and effective method to generate adversarial samples using pre-trained masked language models exemplified by BERT. We turn BERT against its fine-tuned models and other deep neural models in downstream tasks so that we can successfully mislead the target models to predict incorrectly. Our method outperforms state-of-the-art attack strategies in both success rate and perturb percentage, while the generated adversarial samples are fluent and semantically preserved. Also, the cost of calculation is low, thus possible for large-scale generations. The code is available at https://github.com/LinyangLee/BERT-Attack.",2020-10-01,2022-03-11 01:07:50,2022-03-11 01:07:50,2022-03-11 01:07:49,,,,,,,BERT-ATTACK,,,,,,,,,,,,arXiv.org,,arXiv: 2004.09984,,/Users/jacquesthibodeau/Zotero/storage/5PYEYRB4/Li et al. - 2020 - BERT-ATTACK Adversarial Attack Against BERT Using.pdf; /Users/jacquesthibodeau/Zotero/storage/XK76QFYP/2004.html,,,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CF4WKV4F,journalArticle,2022,"Xie, Zhouhang; Brophy, Jonathan; Noack, Adam; You, Wencong; Asthana, Kalyani; Perkins, Carter; Reis, Sabrina; Singh, Sameer; Lowd, Daniel",Identifying Adversarial Attacks on Text Classifiers,arXiv:2201.08555 [cs],,,,http://arxiv.org/abs/2201.08555,"The landscape of adversarial attacks against text classifiers continues to grow, with new attacks developed every year and many of them available in standard toolkits, such as TextAttack and OpenAttack. In response, there is a growing body of work on robust learning, which reduces vulnerability to these attacks, though sometimes at a high cost in compute time or accuracy. In this paper, we take an alternate approach -- we attempt to understand the attacker by analyzing adversarial text to determine which methods were used to create it. Our first contribution is an extensive dataset for attack detection and labeling: 1.5~million attack instances, generated by twelve adversarial attacks targeting three classifiers trained on six source datasets for sentiment analysis and abuse detection in English. As our second contribution, we use this dataset to develop and benchmark a number of classifiers for attack identification -- determining if a given text has been adversarially manipulated and by which attack. As a third contribution, we demonstrate the effectiveness of three classes of features for these tasks: text properties, capturing content and presentation of text; language model properties, determining which tokens are more or less probable throughout the input; and target model properties, representing how the text classifier is influenced by the attack, including internal node activations. Overall, this represents a first step towards forensics for adversarial attacks against text classifiers.",2022-01-21,2022-03-11 01:08:10,2022-03-11 01:08:10,2022-03-11 01:08:10,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2201.08555,,/Users/jacquesthibodeau/Zotero/storage/VNQ3JHFR/Xie et al. - 2022 - Identifying Adversarial Attacks on Text Classifier.pdf; /Users/jacquesthibodeau/Zotero/storage/U6D5XGZ8/2201.html,,,Computer Science - Computation and Language; Computer Science - Cryptography and Security; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YPD6EZNY,journalArticle,2021,"Guo, Chuan; Sablayrolles, Alexandre; Jégou, Hervé; Kiela, Douwe",Gradient-based Adversarial Attacks against Text Transformers,arXiv:2104.13733 [cs],,,,http://arxiv.org/abs/2104.13733,"We propose the first general-purpose gradient-based attack against transformer models. Instead of searching for a single adversarial example, we search for a distribution of adversarial examples parameterized by a continuous-valued matrix, hence enabling gradient-based optimization. We empirically demonstrate that our white-box attack attains state-of-the-art attack performance on a variety of natural language tasks. Furthermore, we show that a powerful black-box transfer attack, enabled by sampling from the adversarial distribution, matches or exceeds existing methods, while only requiring hard-label outputs.",2021-04-15,2022-03-11 01:08:10,2022-03-11 01:08:10,2022-03-11 01:08:10,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2104.13733,,/Users/jacquesthibodeau/Zotero/storage/VDXYSSZU/Guo et al. - 2021 - Gradient-based Adversarial Attacks against Text Tr.pdf; /Users/jacquesthibodeau/Zotero/storage/DMMIPIUP/2104.html,,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Cryptography and Security; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BW76HKK7,journalArticle,2021,"Rebuffi, Sylvestre-Alvise; Gowal, Sven; Calian, Dan A.; Stimberg, Florian; Wiles, Olivia; Mann, Timothy",Data Augmentation Can Improve Robustness,"arXiv:2111.05328 [cs, stat]",,,,http://arxiv.org/abs/2111.05328,"Adversarial training suffers from robust overfitting, a phenomenon where the robust test accuracy starts to decrease during training. In this paper, we focus on reducing robust overfitting by using common data augmentation schemes. We demonstrate that, contrary to previous findings, when combined with model weight averaging, data augmentation can significantly boost robust accuracy. Furthermore, we compare various augmentations techniques and observe that spatial composition techniques work the best for adversarial training. Finally, we evaluate our approach on CIFAR-10 against $\ell_\infty$ and $\ell_2$ norm-bounded perturbations of size $\epsilon = 8/255$ and $\epsilon = 128/255$, respectively. We show large absolute improvements of +2.93% and +2.16% in robust accuracy compared to previous state-of-the-art methods. In particular, against $\ell_\infty$ norm-bounded perturbations of size $\epsilon = 8/255$, our model reaches 60.07% robust accuracy without using any external data. We also achieve a significant performance boost with this approach while using other architectures and datasets such as CIFAR-100, SVHN and TinyImageNet.",2021-11-09,2022-03-11 01:08:28,2022-03-11 01:08:28,2022-03-11 01:08:28,,,,,,,,,,,,,,,,,,,arXiv.org,,arXiv: 2111.05328,,/Users/jacquesthibodeau/Zotero/storage/7JIV2ZL9/Rebuffi et al. - 2021 - Data Augmentation Can Improve Robustness.pdf; /Users/jacquesthibodeau/Zotero/storage/Q9ZA9DBT/2111.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8L7V5878,journalArticle,2021,"Hendrycks, Dan; Zou, Andy; Mazeika, Mantas; Tang, Leonard; Li, Bo; Song, Dawn; Steinhardt, Jacob",PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures,arXiv:2112.05135 [cs],,,,http://arxiv.org/abs/2112.05135,"In real-world applications of machine learning, reliable and safe systems must consider measures of performance beyond standard test set accuracy. These other goals include out-of-distribution (OOD) robustness, prediction consistency, resilience to adversaries, calibrated uncertainty estimates, and the ability to detect anomalous inputs. However, improving performance towards these goals is often a balancing act that today's methods cannot achieve without sacrificing performance on other safety axes. For instance, adversarial training improves adversarial robustness but sharply degrades other classifier performance metrics. Similarly, strong data augmentation and regularization techniques often improve OOD robustness but harm anomaly detection, raising the question of whether a Pareto improvement on all existing safety measures is possible. To meet this challenge, we design a new data augmentation strategy utilizing the natural structural complexity of pictures such as fractals, which outperforms numerous baselines, is near Pareto-optimal, and roundly improves safety measures.",2021-12-11,2022-03-11 01:08:31,2022-03-11 01:08:31,2022-03-11 01:08:31,,,,,,,PixMix,,,,,,,,,,,,arXiv.org,,arXiv: 2112.05135,,/Users/jacquesthibodeau/Zotero/storage/ZKTIUZ67/Hendrycks et al. - 2021 - PixMix Dreamlike Pictures Comprehensively Improve.pdf; /Users/jacquesthibodeau/Zotero/storage/T5KZ92QK/2112.html,,,Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JUMWFE49,journalArticle,2018,"Everitt, Tom; Lea, Gary; Hutter, Marcus",AGI Safety Literature Review,,,,10.48550/arXiv.1805.01109,https://arxiv.org/abs/1805.01109v2,"The development of Artificial General Intelligence (AGI) promises to be a major event. Along with its many potential benefits, it also raises serious safety concerns (Bostrom, 2014). The intention of this paper is to provide an easily accessible and up-to-date collection of references for the emerging field of AGI safety. A significant number of safety problems for AGI have been identified. We list these, and survey recent research on solving them. We also cover works on how best to think of AGI from the limited knowledge we have today, predictions for when AGI will first be created, and what will happen after its creation. Finally, we review the current public policy on AGI.",2018-05-03,2022-03-11 01:24:37,2022-03-11 01:24:37,2022-03-11 01:24:37,,,,,,,,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/DKMM7ZFQ/Everitt et al. - 2018 - AGI Safety Literature Review.pdf; /Users/jacquesthibodeau/Zotero/storage/LQ33DGVC/1805.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QTBCAPFF,journalArticle,2021,"Hendrycks, Dan; Carlini, Nicholas; Schulman, John; Steinhardt, Jacob",Unsolved Problems in ML Safety,,,,10.48550/arXiv.2109.13916,https://arxiv.org/abs/2109.13916v3,"Machine learning (ML) systems are rapidly increasing in size, are acquiring new capabilities, and are increasingly deployed in high-stakes settings. As with other powerful technologies, safety for ML should be a leading research priority. In response to emerging safety challenges in ML, such as those introduced by recent large-scale models, we provide a new roadmap for ML Safety and refine the technical problems that the field needs to address. We present four problems ready for research, namely withstanding hazards (""Robustness""), identifying hazards (""Monitoring""), steering ML systems (""Alignment""), and reducing deployment hazards (""External Safety""). Throughout, we clarify each problem's motivation and provide concrete research directions.",2021-09-28,2022-03-11 01:24:59,2022-03-11 01:24:59,2022-03-11 01:24:59,,,,,,,,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/5G22C7B7/Hendrycks et al. - 2021 - Unsolved Problems in ML Safety.pdf; /Users/jacquesthibodeau/Zotero/storage/BZ7SK5PB/2109.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LYYL3QVX,journalArticle,2020,"Critch, Andrew; Krueger, David",AI Research Considerations for Human Existential Safety (ARCHES),,,,10.48550/arXiv.2006.04948,https://arxiv.org/abs/2006.04948v1,"Framed in positive terms, this report examines how technical AI research might be steered in a manner that is more attentive to humanity's long-term prospects for survival as a species. In negative terms, we ask what existential risks humanity might face from AI development in the next century, and by what principles contemporary technical research might be directed to address those risks. A key property of hypothetical AI technologies is introduced, called \emph{prepotence}, which is useful for delineating a variety of potential existential risks from artificial intelligence, even as AI paradigms might shift. A set of \auxref{dirtot} contemporary research \directions are then examined for their potential benefit to existential safety. Each research direction is explained with a scenario-driven motivation, and examples of existing work from which to build. The research directions present their own risks and benefits to society that could occur at various scales of impact, and in particular are not guaranteed to benefit existential safety if major developments in them are deployed without adequate forethought and oversight. As such, each direction is accompanied by a consideration of potentially negative side effects.",2020-05-30,2022-03-11 01:25:00,2022-03-11 01:25:00,2022-03-11 01:25:00,,,,,,,,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/CRAVZTQY/Critch and Krueger - 2020 - AI Research Considerations for Human Existential S.pdf; /Users/jacquesthibodeau/Zotero/storage/ZPDQZXMM/2006.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
73ZHAJ4U,journalArticle,2021,"Hammond, Lewis; Fox, James; Everitt, Tom; Abate, Alessandro; Wooldridge, Michael",Equilibrium Refinements for Multi-Agent Influence Diagrams: Theory and Practice,,,,10.48550/arXiv.2102.05008,https://arxiv.org/abs/2102.05008v1,"Multi-agent influence diagrams (MAIDs) are a popular form of graphical model that, for certain classes of games, have been shown to offer key complexity and explainability advantages over traditional extensive form game (EFG) representations. In this paper, we extend previous work on MAIDs by introducing the concept of a MAID subgame, as well as subgame perfect and trembling hand perfect equilibrium refinements. We then prove several equivalence results between MAIDs and EFGs. Finally, we describe an open source implementation for reasoning about MAIDs and computing their equilibria.",2021-02-09,2022-03-11 01:25:05,2022-03-11 01:25:05,2022-03-11 01:25:05,,,,,,,Equilibrium Refinements for Multi-Agent Influence Diagrams,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/QNF83N3L/Hammond et al. - 2021 - Equilibrium Refinements for Multi-Agent Influence .pdf; /Users/jacquesthibodeau/Zotero/storage/W5ITAFL9/2102.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RQ7TJV39,journalArticle,2020,"Dafoe, Allan; Hughes, Edward; Bachrach, Yoram; Collins, Tantum; McKee, Kevin R.; Leibo, Joel Z.; Larson, Kate; Graepel, Thore",Open Problems in Cooperative AI,,,,10.48550/arXiv.2012.08630,https://arxiv.org/abs/2012.08630v1,"Problems of cooperation--in which agents seek ways to jointly improve their welfare--are ubiquitous and important. They can be found at scales ranging from our daily routines--such as driving on highways, scheduling meetings, and working collaboratively--to our global challenges--such as peace, commerce, and pandemic preparedness. Arguably, the success of the human species is rooted in our ability to cooperate. Since machines powered by artificial intelligence are playing an ever greater role in our lives, it will be important to equip them with the capabilities necessary to cooperate and to foster cooperation. We see an opportunity for the field of artificial intelligence to explicitly focus effort on this class of problems, which we term Cooperative AI. The objective of this research would be to study the many aspects of the problems of cooperation and to innovate in AI to contribute to solving these problems. Central goals include building machine agents with the capabilities needed for cooperation, building tools to foster cooperation in populations of (machine and/or human) agents, and otherwise conducting AI research for insight relevant to problems of cooperation. This research integrates ongoing work on multi-agent systems, game theory and social choice, human-machine interaction and alignment, natural-language processing, and the construction of social tools and platforms. However, Cooperative AI is not the union of these existing areas, but rather an independent bet about the productivity of specific kinds of conversations that involve these and other areas. We see opportunity to more explicitly focus on the problem of cooperation, to construct unified theory and vocabulary, and to build bridges with adjacent communities working on cooperation, including in the natural, social, and behavioural sciences.",2020-12-15,2022-03-11 01:25:06,2022-03-11 01:25:06,2022-03-11 01:25:06,,,,,,,,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/QY8XGVLI/Dafoe et al. - 2020 - Open Problems in Cooperative AI.pdf; /Users/jacquesthibodeau/Zotero/storage/LFH6FRBJ/2012.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SYS2URXD,journalArticle,2021,"Cohen, Michael K.; Hutter, Marcus; Nanda, Neel",Fully General Online Imitation Learning,,,,10.48550/arXiv.2102.08686,https://arxiv.org/abs/2102.08686v1,"In imitation learning, imitators and demonstrators are policies for picking actions given past interactions with the environment. If we run an imitator, we probably want events to unfold similarly to the way they would have if the demonstrator had been acting the whole time. No existing work provides formal guidance in how this might be accomplished, instead restricting focus to environments that restart, making learning unusually easy, and conveniently limiting the significance of any mistake. We address a fully general setting, in which the (stochastic) environment and demonstrator never reset, not even for training purposes. Our new conservative Bayesian imitation learner underestimates the probabilities of each available action, and queries for more data with the remaining probability. Our main result: if an event would have been unlikely had the demonstrator acted the whole time, that event's likelihood can be bounded above when running the (initially totally ignorant) imitator instead. Meanwhile, queries to the demonstrator rapidly diminish in frequency.",2021-02-17,2022-03-11 01:25:07,2022-03-11 01:25:07,2022-03-11 01:25:07,,,,,,,,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/7HUD5XGM/Cohen et al. - 2021 - Fully General Online Imitation Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/6TXX3RFN/2102.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GM37CFF7,journalArticle,2021,"Klinova, Katya; Korinek, Anton",AI and Shared Prosperity,,,,10.1145/3461702.3462619,https://arxiv.org/abs/2105.08475v1,"Future advances in AI that automate away human labor may have stark implications for labor markets and inequality. This paper proposes a framework to analyze the effects of specific types of AI systems on the labor market, based on how much labor demand they will create versus displace, while taking into account that productivity gains also make society wealthier and thereby contribute to additional labor demand. This analysis enables ethically-minded companies creating or deploying AI systems as well as researchers and policymakers to take into account the effects of their actions on labor markets and inequality, and therefore to steer progress in AI in a direction that advances shared prosperity and an inclusive economic future for all of humanity.",2021-05-18,2022-03-11 01:25:14,2022-03-11 01:36:20,2022-03-11 01:25:14,,,,,,,,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/APLZI8BN/Klinova and Korinek - 2021 - AI and Shared Prosperity.pdf; /Users/jacquesthibodeau/Zotero/storage/NSDK44MG/Klinova and Korinek - 2021 - AI and Shared Prosperity.pdf; /Users/jacquesthibodeau/Zotero/storage/CVL2FGE4/2105.html; /Users/jacquesthibodeau/Zotero/storage/8GKBMWRN/2105.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WHA6VWZL,journalArticle,2021,"Filan, Daniel; Casper, Stephen; Hod, Shlomi; Wild, Cody; Critch, Andrew; Russell, Stuart",Clusterability in Neural Networks,,,,10.48550/arXiv.2103.03386,https://arxiv.org/abs/2103.03386v1,"The learned weights of a neural network have often been considered devoid of scrutable internal structure. In this paper, however, we look for structure in the form of clusterability: how well a network can be divided into groups of neurons with strong internal connectivity but weak external connectivity. We find that a trained neural network is typically more clusterable than randomly initialized networks, and often clusterable relative to random networks with the same distribution of weights. We also exhibit novel methods to promote clusterability in neural network training, and find that in multi-layer perceptrons they lead to more clusterable networks with little reduction in accuracy. Understanding and controlling the clusterability of neural networks will hopefully render their inner workings more interpretable to engineers by facilitating partitioning into meaningful clusters.",2021-03-04,2022-03-11 01:25:21,2022-03-11 01:25:21,2022-03-11 01:25:21,,,,,,,,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/QBKW4JPH/Filan et al. - 2021 - Clusterability in Neural Networks.pdf; /Users/jacquesthibodeau/Zotero/storage/VDE8SL52/2103.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
K5PD6SSU,journalArticle,2021,"Zhuang, Simon; Hadfield-Menell, Dylan",Consequences of Misaligned AI,,,,10.48550/arXiv.2102.03896,https://arxiv.org/abs/2102.03896v1,AI systems often rely on two key components: a specified goal or reward function and an optimization algorithm to compute the optimal behavior for that goal. This approach is intended to provide value for a principal: the user on whose behalf the agent acts. The objectives given to these agents often refer to a partial specification of the principal's goals. We consider the cost of this incompleteness by analyzing a model of a principal and an agent in a resource constrained world where the $L$ attributes of the state correspond to different sources of utility for the principal. We assume that the reward function given to the agent only has support on $J < L$ attributes. The contributions of our paper are as follows: 1) we propose a novel model of an incomplete principal-agent problem from artificial intelligence; 2) we provide necessary and sufficient conditions under which indefinitely optimizing for any incomplete proxy objective leads to arbitrarily low overall utility; and 3) we show how modifying the setup to allow reward functions that reference the full state or allowing the principal to update the proxy objective over time can lead to higher utility solutions. The results in this paper argue that we should view the design of reward functions as an interactive and dynamic process and identifies a theoretical scenario where some degree of interactivity is desirable.,2021-02-07,2022-03-11 01:26:50,2022-03-11 01:26:50,2022-03-11 01:26:50,,,,,,,,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/6D3TAWRW/Zhuang and Hadfield-Menell - 2021 - Consequences of Misaligned AI.pdf; /Users/jacquesthibodeau/Zotero/storage/AB3SPPGB/2102.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2D3UTUMI,journalArticle,2021,"Shah, Rohin; Wild, Cody; Wang, Steven H.; Alex, Neel; Houghton, Brandon; Guss, William; Mohanty, Sharada; Kanervisto, Anssi; Milani, Stephanie; Topin, Nicholay; Abbeel, Pieter; Russell, Stuart; Dragan, Anca",The MineRL BASALT Competition on Learning from Human Feedback,,,,10.48550/arXiv.2107.01969,https://arxiv.org/abs/2107.01969v1,"The last decade has seen a significant increase of interest in deep learning research, with many public successes that have demonstrated its potential. As such, these systems are now being incorporated into commercial products. With this comes an additional challenge: how can we build AI systems that solve tasks where there is not a crisp, well-defined specification? While multiple solutions have been proposed, in this competition we focus on one in particular: learning from human feedback. Rather than training AI systems using a predefined reward function or using a labeled dataset with a predefined set of categories, we instead train the AI system using a learning signal derived from some form of human feedback, which can evolve over time as the understanding of the task changes, or as the capabilities of the AI system improve. The MineRL BASALT competition aims to spur forward research on this important class of techniques. We design a suite of four tasks in Minecraft for which we expect it will be hard to write down hardcoded reward functions. These tasks are defined by a paragraph of natural language: for example, ""create a waterfall and take a scenic picture of it"", with additional clarifying details. Participants must train a separate agent for each task, using any method they want. Agents are then evaluated by humans who have read the task description. To help participants get started, we provide a dataset of human demonstrations on each of the four tasks, as well as an imitation learning baseline that leverages these demonstrations. Our hope is that this competition will improve our ability to build AI systems that do what their designers intend them to do, even when the intent cannot be easily formalized. Besides allowing AI to solve more tasks, this can also enable more effective regulation of AI systems, as well as making progress on the value alignment problem.",2021-07-05,2022-03-11 01:26:52,2022-03-11 01:26:52,2022-03-11 01:26:52,,,,,,,,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/YCQJBT2C/Shah et al. - 2021 - The MineRL BASALT Competition on Learning from Hum.pdf; /Users/jacquesthibodeau/Zotero/storage/G287MGGG/2107.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WY2UBYEQ,journalArticle,2021,"Lindner, David; Shah, Rohin; Abbeel, Pieter; Dragan, Anca",Learning What To Do by Simulating the Past,,,,10.48550/arXiv.2104.03946,https://arxiv.org/abs/2104.03946v2,"Since reward functions are hard to specify, recent work has focused on learning policies from human feedback. However, such approaches are impeded by the expense of acquiring such feedback. Recent work proposed that agents have access to a source of information that is effectively free: in any environment that humans have acted in, the state will already be optimized for human preferences, and thus an agent can extract information about what humans want from the state. Such learning is possible in principle, but requires simulating all possible past trajectories that could have led to the observed state. This is feasible in gridworlds, but how do we scale it to complex tasks? In this work, we show that by combining a learned feature encoder with learned inverse models, we can enable agents to simulate human actions backwards in time to infer what they must have done. The resulting algorithm is able to reproduce a specific skill in MuJoCo environments given a single state sampled from the optimal policy for that skill.",2021-04-08,2022-03-11 01:27:04,2022-03-11 01:27:04,2022-03-11 01:27:04,,,,,,,,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/9Q8T8D43/Lindner et al. - 2021 - Learning What To Do by Simulating the Past.pdf; /Users/jacquesthibodeau/Zotero/storage/4TS3RJ62/2104.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DQWAINS4,journalArticle,2019,"Shah, Rohin; Krasheninnikov, Dmitrii; Alexander, Jordan; Abbeel, Pieter; Dragan, Anca",Preferences Implicit in the State of the World,,,,10.48550/arXiv.1902.04198,https://arxiv.org/abs/1902.04198v2,"Reinforcement learning (RL) agents optimize only the features specified in a reward function and are indifferent to anything left out inadvertently. This means that we must not only specify what to do, but also the much larger space of what not to do. It is easy to forget these preferences, since these preferences are already satisfied in our environment. This motivates our key insight: when a robot is deployed in an environment that humans act in, the state of the environment is already optimized for what humans want. We can therefore use this implicit preference information from the state to fill in the blanks. We develop an algorithm based on Maximum Causal Entropy IRL and use it to evaluate the idea in a suite of proof-of-concept environments designed to show its properties. We find that information from the initial state can be used to infer both side effects that should be avoided as well as preferences for how the environment should be organized. Our code can be found at https://github.com/HumanCompatibleAI/rlsp.",2019-02-12,2022-03-11 01:27:05,2022-03-11 01:27:05,2022-03-11 01:27:05,,,,,,,,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/NB55YFZS/Shah et al. - 2019 - Preferences Implicit in the State of the World.pdf; /Users/jacquesthibodeau/Zotero/storage/PKKXP2Q5/1902.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
69WEZGBQ,journalArticle,2021,"Lee, Kimin; Smith, Laura; Abbeel, Pieter",PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training,,,,10.48550/arXiv.2106.05091,https://arxiv.org/abs/2106.05091v1,"Conveying complex objectives to reinforcement learning (RL) agents can often be difficult, involving meticulous design of reward functions that are sufficiently informative yet easy enough to provide. Human-in-the-loop RL methods allow practitioners to instead interactively teach agents through tailored feedback; however, such approaches have been challenging to scale since human feedback is very expensive. In this work, we aim to make this process more sample- and feedback-efficient. We present an off-policy, interactive RL algorithm that capitalizes on the strengths of both feedback and off-policy learning. Specifically, we learn a reward model by actively querying a teacher's preferences between two clips of behavior and use it to train an agent. To enable off-policy learning, we relabel all the agent's past experience when its reward model changes. We additionally show that pre-training our agents with unsupervised exploration substantially increases the mileage of its queries. We demonstrate that our approach is capable of learning tasks of higher complexity than previously considered by human-in-the-loop methods, including a variety of locomotion and robotic manipulation skills. We also show that our method is able to utilize real-time human feedback to effectively prevent reward exploitation and learn new behaviors that are difficult to specify with standard reward functions.",2021-06-09,2022-03-11 01:27:08,2022-03-11 01:27:08,2022-03-11 01:27:08,,,,,,,PEBBLE,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/KIY5YDLG/Lee et al. - 2021 - PEBBLE Feedback-Efficient Interactive Reinforceme.pdf; /Users/jacquesthibodeau/Zotero/storage/9BGQI398/2106.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FGUDILHF,journalArticle,2021,"Andrus, McKane; Dean, Sarah; Gilbert, Thomas Krendl; Lambert, Nathan; Zick, Tom",AI Development for the Public Interest: From Abstraction Traps to Sociotechnical Risks,,,,10.48550/arXiv.2102.04255,https://arxiv.org/abs/2102.04255v1,"Despite interest in communicating ethical problems and social contexts within the undergraduate curriculum to advance Public Interest Technology (PIT) goals, interventions at the graduate level remain largely unexplored. This may be due to the conflicting ways through which distinct Artificial Intelligence (AI) research tracks conceive of their interface with social contexts. In this paper we track the historical emergence of sociotechnical inquiry in three distinct subfields of AI research: AI Safety, Fair Machine Learning (Fair ML) and Human-in-the-Loop (HIL) Autonomy. We show that for each subfield, perceptions of PIT stem from the particular dangers faced by past integration of technical systems within a normative social order. We further interrogate how these histories dictate the response of each subfield to conceptual traps, as defined in the Science and Technology Studies literature. Finally, through a comparative analysis of these currently siloed fields, we present a roadmap for a unified approach to sociotechnical graduate pedagogy in AI.",2021-02-04,2022-03-11 01:27:19,2022-03-11 01:36:22,2022-03-11 01:27:19,,,,,,,AI Development for the Public Interest,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/QKNPJEBP/Andrus et al. - 2021 - AI Development for the Public Interest From Abstr.pdf; /Users/jacquesthibodeau/Zotero/storage/UZW8GAVV/Andrus et al. - 2021 - AI Development for the Public Interest From Abstr.pdf; /Users/jacquesthibodeau/Zotero/storage/HT327V55/2102.html; /Users/jacquesthibodeau/Zotero/storage/6TX9QIFH/2102.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FWCKT7XU,journalArticle,2021,"Garrabrant, Scott",Temporal Inference with Finite Factored Sets,,,,10.48550/arXiv.2109.11513,https://arxiv.org/abs/2109.11513v1,"We propose a new approach to temporal inference, inspired by the Pearlian causal inference paradigm - though quite different from Pearl's approach formally. Rather than using directed acyclic graphs, we make use of factored sets, which are sets expressed as Cartesian products. We show that finite factored sets are powerful tools for inferring temporal relations. We introduce an analog of d-separation for factored sets, conditional orthogonality, and we demonstrate that this notion is equivalent to conditional independence in all probability distributions on a finite factored set.",2021-09-23,2022-03-11 01:27:21,2022-03-11 01:27:21,2022-03-11 01:27:21,,,,,,,,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/3BX2P4WW/Garrabrant - 2021 - Temporal Inference with Finite Factored Sets.pdf; /Users/jacquesthibodeau/Zotero/storage/6745K6FU/2109.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MZMFN98H,journalArticle,2021,"Chen, Mark; Tworek, Jerry; Jun, Heewoo; Yuan, Qiming; Pinto, Henrique Ponde de Oliveira; Kaplan, Jared; Edwards, Harri; Burda, Yuri; Joseph, Nicholas; Brockman, Greg; Ray, Alex; Puri, Raul; Krueger, Gretchen; Petrov, Michael; Khlaaf, Heidy; Sastry, Girish; Mishkin, Pamela; Chan, Brooke; Gray, Scott; Ryder, Nick; Pavlov, Mikhail; Power, Alethea; Kaiser, Lukasz; Bavarian, Mohammad; Winter, Clemens; Tillet, Philippe; Such, Felipe Petroski; Cummings, Dave; Plappert, Matthias; Chantzis, Fotios; Barnes, Elizabeth; Herbert-Voss, Ariel; Guss, William Hebgen; Nichol, Alex; Paino, Alex; Tezak, Nikolas; Tang, Jie; Babuschkin, Igor; Balaji, Suchir; Jain, Shantanu; Saunders, William; Hesse, Christopher; Carr, Andrew N.; Leike, Jan; Achiam, Josh; Misra, Vedant; Morikawa, Evan; Radford, Alec; Knight, Matthew; Brundage, Miles; Murati, Mira; Mayer, Katie; Welinder, Peter; McGrew, Bob; Amodei, Dario; McCandlish, Sam; Sutskever, Ilya; Zaremba, Wojciech",Evaluating Large Language Models Trained on Code,,,,10.48550/arXiv.2107.03374,https://arxiv.org/abs/2107.03374v2,"We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.",2021-07-07,2022-03-11 01:27:24,2022-03-11 01:27:24,2022-03-11 01:27:24,,,,,,,,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/GLFVN6RB/Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf; /Users/jacquesthibodeau/Zotero/storage/G8J7P6ZA/2107.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BZM73QCS,journalArticle,2021,"Welbl, Johannes; Glaese, Amelia; Uesato, Jonathan; Dathathri, Sumanth; Mellor, John; Hendricks, Lisa Anne; Anderson, Kirsty; Kohli, Pushmeet; Coppin, Ben; Huang, Po-Sen",Challenges in Detoxifying Language Models,,,,10.48550/arXiv.2109.07445,https://arxiv.org/abs/2109.07445v1,"Large language models (LM) generate remarkably fluent text and can be efficiently adapted across NLP tasks. Measuring and guaranteeing the quality of generated text in terms of safety is imperative for deploying LMs in the real world; to this end, prior work often relies on automatic evaluation of LM toxicity. We critically discuss this approach, evaluate several toxicity mitigation strategies with respect to both automatic and human evaluation, and analyze consequences of toxicity mitigation in terms of model bias and LM quality. We demonstrate that while basic intervention strategies can effectively optimize previously established automatic metrics on the RealToxicityPrompts dataset, this comes at the cost of reduced LM coverage for both texts about, and dialects of, marginalized groups. Additionally, we find that human raters often disagree with high automatic toxicity scores after strong toxicity reduction interventions -- highlighting further the nuances involved in careful evaluation of LM toxicity.",2021-09-15,2022-03-11 01:27:27,2022-03-11 01:27:27,2022-03-11 01:27:27,,,,,,,,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/YVVZ3C32/Welbl et al. - 2021 - Challenges in Detoxifying Language Models.pdf; /Users/jacquesthibodeau/Zotero/storage/RUFXR8I7/2109.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VEMYGUZD,journalArticle,2021,"Gabriel, Iason",Towards a Theory of Justice for Artificial Intelligence,,,,10.48550/arXiv.2110.14419,https://arxiv.org/abs/2110.14419v1,"This paper explores the relationship between artificial intelligence and principles of distributive justice. Drawing upon the political philosophy of John Rawls, it holds that the basic structure of society should be understood as a composite of socio-technical systems, and that the operation of these systems is increasingly shaped and influenced by AI. As a consequence, egalitarian norms of justice apply to the technology when it is deployed in these contexts. These norms entail that the relevant AI systems must meet a certain standard of public justification, support citizens rights, and promote substantively fair outcomes -- something that requires specific attention be paid to the impact they have on the worst-off members of society.",2021-10-27,2022-03-11 01:27:30,2022-03-11 01:27:30,2022-03-11 01:27:30,,,,,,,,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/GLV244ND/Gabriel - 2021 - Towards a Theory of Justice for Artificial Intelli.pdf; /Users/jacquesthibodeau/Zotero/storage/6RN8GS45/2110.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
28FUQKDY,journalArticle,2022,"Ganguli, Deep; Hernandez, Danny; Lovitt, Liane; DasSarma, Nova; Henighan, Tom; Jones, Andy; Joseph, Nicholas; Kernion, Jackson; Mann, Ben; Askell, Amanda; Bai, Yuntao; Chen, Anna; Conerly, Tom; Drain, Dawn; Elhage, Nelson; Showk, Sheer El; Fort, Stanislav; Hatfield-Dodds, Zac; Johnston, Scott; Kravec, Shauna; Nanda, Neel; Ndousse, Kamal; Olsson, Catherine; Amodei, Daniela; Amodei, Dario; Brown, Tom; Kaplan, Jared; McCandlish, Sam; Olah, Chris; Clark, Jack",Predictability and Surprise in Large Generative Models,,,,10.48550/arXiv.2202.07785,https://arxiv.org/abs/2202.07785v1,"Large-scale pre-training has recently emerged as a technique for creating capable, general purpose, generative models such as GPT-3, Megatron-Turing NLG, Gopher, and many others. In this paper, we highlight a counterintuitive property of such models and discuss the policy implications of this property. Namely, these generative models have an unusual combination of predictable loss on a broad training distribution (as embodied in their ""scaling laws""), and unpredictable specific capabilities, inputs, and outputs. We believe that the high-level predictability and appearance of useful capabilities drives rapid development of such models, while the unpredictable qualities make it difficult to anticipate the consequences of model deployment. We go through examples of how this combination can lead to socially harmful behavior with examples from the literature and real world observations, and we also perform two novel experiments to illustrate our point about harms from unpredictability. Furthermore, we analyze how these conflicting properties combine to give model developers various motivations for deploying these models, and challenges that can hinder deployment. We conclude with a list of possible interventions the AI community may take to increase the chance of these models having a beneficial impact. We intend this paper to be useful to policymakers who want to understand and regulate AI systems, technologists who care about the potential policy impact of their work, and academics who want to analyze, critique, and potentially develop large generative models.",2022-02-15,2022-03-11 01:28:44,2022-03-11 01:28:44,2022-03-11 01:28:44,,,,,,,,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/8J6ZWACU/Ganguli et al. - 2022 - Predictability and Surprise in Large Generative Mo.pdf; /Users/jacquesthibodeau/Zotero/storage/PJB8A842/2202.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RA7RKHJW,journalArticle,2020,"Mishra, Saurabh; Clark, Jack; Perrault, C. Raymond",Measurement in AI Policy: Opportunities and Challenges,,,,10.48550/arXiv.2009.09071,https://arxiv.org/abs/2009.09071v1,"As artificial intelligence increasingly influences our world, it becomes crucial to assess its technical progress and societal impact. This paper surveys problems and opportunities in the measurement of AI systems and their impact, based on a workshop held at Stanford University in the fall of 2019. We identify six summary challenges inherent to measuring the progress and impact of AI, and summarize over 40 presentations and associated discussions from the workshop. We hope this can inspire research agendas in this crucial area.",2020-09-10,2022-03-11 01:28:55,2022-03-11 01:28:55,2022-03-11 01:28:55,,,,,,,Measurement in AI Policy,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/D28GLISV/Mishra et al. - 2020 - Measurement in AI Policy Opportunities and Challe.pdf; /Users/jacquesthibodeau/Zotero/storage/NUXCBT74/2009.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2X9B74NK,journalArticle,2021,"Zhang, Daniel; Mishra, Saurabh; Brynjolfsson, Erik; Etchemendy, John; Ganguli, Deep; Grosz, Barbara; Lyons, Terah; Manyika, James; Niebles, Juan Carlos; Sellitto, Michael; Shoham, Yoav; Clark, Jack; Perrault, Raymond",The AI Index 2021 Annual Report,,,,10.48550/arXiv.2103.06312,https://arxiv.org/abs/2103.06312v1,"Welcome to the fourth edition of the AI Index Report. This year we significantly expanded the amount of data available in the report, worked with a broader set of external organizations to calibrate our data, and deepened our connections with the Stanford Institute for Human-Centered Artificial Intelligence (HAI). The AI Index Report tracks, collates, distills, and visualizes data related to artificial intelligence. Its mission is to provide unbiased, rigorously vetted, and globally sourced data for policymakers, researchers, executives, journalists, and the general public to develop intuitions about the complex field of AI. The report aims to be the most credible and authoritative source for data and insights about AI in the world.",2021-03-09,2022-03-11 01:28:58,2022-03-11 01:28:58,2022-03-11 01:28:58,,,,,,,,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/Y6EL6VPQ/Zhang et al. - 2021 - The AI Index 2021 Annual Report.pdf; /Users/jacquesthibodeau/Zotero/storage/MJ99X8U3/2103.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PQNRR2B6,journalArticle,2021,"Evans, Owain; Cotton-Barratt, Owen; Finnveden, Lukas; Bales, Adam; Balwit, Avital; Wills, Peter; Righetti, Luca; Saunders, William",Truthful AI: Developing and governing AI that does not lie,,,,10.48550/arXiv.2110.06674,https://arxiv.org/abs/2110.06674v1,"In many contexts, lying -- the use of verbal falsehoods to deceive -- is harmful. While lying has traditionally been a human affair, AI systems that make sophisticated verbal statements are becoming increasingly prevalent. This raises the question of how we should limit the harm caused by AI ""lies"" (i.e. falsehoods that are actively selected for). Human truthfulness is governed by social norms and by laws (against defamation, perjury, and fraud). Differences between AI and humans present an opportunity to have more precise standards of truthfulness for AI, and to have these standards rise over time. This could provide significant benefits to public epistemics and the economy, and mitigate risks of worst-case AI futures. Establishing norms or laws of AI truthfulness will require significant work to: (1) identify clear truthfulness standards; (2) create institutions that can judge adherence to those standards; and (3) develop AI systems that are robustly truthful. Our initial proposals for these areas include: (1) a standard of avoiding ""negligent falsehoods"" (a generalisation of lies that is easier to assess); (2) institutions to evaluate AI systems before and after real-world deployment; and (3) explicitly training AI systems to be truthful via curated datasets and human interaction. A concerning possibility is that evaluation mechanisms for eventual truthfulness standards could be captured by political interests, leading to harmful censorship and propaganda. Avoiding this might take careful attention. And since the scale of AI speech acts might grow dramatically over the coming decades, early truthfulness standards might be particularly important because of the precedents they set.",2021-10-13,2022-03-11 01:29:04,2022-03-11 01:29:04,2022-03-11 01:29:04,,,,,,,Truthful AI,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/KFWCLI5U/Evans et al. - 2021 - Truthful AI Developing and governing AI that does.pdf; /Users/jacquesthibodeau/Zotero/storage/A32SJDCF/2110.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HS2LIKSF,journalArticle,2021,"Koch, Jack; Langosco, Lauro; Pfau, Jacob; Le, James; Sharkey, Lee",Objective Robustness in Deep Reinforcement Learning,,,,10.48550/arXiv.2105.14111,https://arxiv.org/abs/2105.14111v2,"We study objective robustness failures, a type of out-of-distribution robustness failure in reinforcement learning (RL). Objective robustness failures occur when an RL agent retains its capabilities out-of-distribution yet pursues the wrong objective. This kind of failure presents different risks than the robustness problems usually considered in the literature, since it involves agents that leverage their capabilities to pursue the wrong objective rather than simply failing to do anything useful. We provide the first explicit empirical demonstrations of objective robustness failures and present a partial characterization of its causes.",2021-05-28,2022-03-11 01:29:10,2022-03-11 01:29:10,2022-03-11 01:29:10,,,,,,,,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/CQHLL55C/Koch et al. - 2021 - Objective Robustness in Deep Reinforcement Learnin.pdf; /Users/jacquesthibodeau/Zotero/storage/CGDL2DKR/2105.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
S68RSNVJ,journalArticle,2021,"Jiang, Liwei; Hwang, Jena D.; Bhagavatula, Chandra; Bras, Ronan Le; Forbes, Maxwell; Borchardt, Jon; Liang, Jenny; Etzioni, Oren; Sap, Maarten; Choi, Yejin",Delphi: Towards Machine Ethics and Norms,,,,10.48550/arXiv.2110.07574,https://arxiv.org/abs/2110.07574v1,"What would it take to teach a machine to behave ethically? While broad ethical rules may seem straightforward to state (""thou shalt not kill""), applying such rules to real-world situations is far more complex. For example, while ""helping a friend"" is generally a good thing to do, ""helping a friend spread fake news"" is not. We identify four underlying challenges towards machine ethics and norms: (1) an understanding of moral precepts and social norms; (2) the ability to perceive real-world situations visually or by reading natural language descriptions; (3) commonsense reasoning to anticipate the outcome of alternative actions in different contexts; (4) most importantly, the ability to make ethical judgments given the interplay between competing values and their grounding in different contexts (e.g., the right to freedom of expression vs. preventing the spread of fake news). Our paper begins to address these questions within the deep learning paradigm. Our prototype model, Delphi, demonstrates strong promise of language-based commonsense moral reasoning, with up to 92.1% accuracy vetted by humans. This is in stark contrast to the zero-shot performance of GPT-3 of 52.3%, which suggests that massive scale alone does not endow pre-trained neural language models with human values. Thus, we present Commonsense Norm Bank, a moral textbook customized for machines, which compiles 1.7M examples of people's ethical judgments on a broad spectrum of everyday situations. In addition to the new resources and baseline performances for future research, our study provides new insights that lead to several important open research questions: differentiating between universal human values and personal values, modeling different moral frameworks, and explainable, consistent approaches to machine ethics.",2021-10-14,2022-03-11 01:29:13,2022-03-11 01:29:13,2022-03-11 01:29:13,,,,,,,Delphi,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/CDRKZDXA/Jiang et al. - 2021 - Delphi Towards Machine Ethics and Norms.pdf; /Users/jacquesthibodeau/Zotero/storage/N5CP6QQ9/2110.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Y2X87DB2,journalArticle,2020,"Hendrycks, Dan; Burns, Collin; Basart, Steven; Critch, Andrew; Li, Jerry; Song, Dawn; Steinhardt, Jacob",Aligning AI With Shared Human Values,,,,10.48550/arXiv.2008.02275,https://arxiv.org/abs/2008.02275v5,"We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the ETHICS dataset, we find that current language models have a promising but incomplete ability to predict basic human ethical judgements. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values.",2020-08-05,2022-03-11 01:29:15,2022-03-11 01:29:15,2022-03-11 01:29:15,,,,,,,,,,,,,,en,,,,,arxiv.org,,,,/Users/jacquesthibodeau/Zotero/storage/BBJJGAUR/Hendrycks et al. - 2020 - Aligning AI With Shared Human Values.pdf; /Users/jacquesthibodeau/Zotero/storage/X45A9WE8/2008.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WZLKJKLU,journalArticle,2020,"Fischer, Ian",The Conditional Entropy Bottleneck,Entropy,,,,http://arxiv.org/abs/2002.05379,"Much of the field of Machine Learning exhibits a prominent set of failure modes, including vulnerability to adversarial examples, poor out-of-distribution (OoD) detection, miscalibration, and willingness to memorize random labelings of datasets. We characterize these as failures of robust generalization, which extends the traditional measure of generalization as accuracy or related metrics on a held-out set. We hypothesize that these failures to robustly generalize are due to the learning systems retaining too much information about the training data. To test this hypothesis, we propose the Minimum Necessary Information (MNI) criterion for evaluating the quality of a model. In order to train models that perform well with respect to the MNI criterion, we present a new objective function, the Conditional Entropy Bottleneck (CEB), which is closely related to the Information Bottleneck (IB). We experimentally test our hypothesis by comparing the performance of CEB models with deterministic models and Variational Information Bottleneck (VIB) models on a variety of different datasets and robustness challenges. We find strong empirical evidence supporting our hypothesis that MNI models improve on these problems of robust generalization.",2020-02-13,2020-09-05 18:45,2020-12-21 18:06,2020-09-05 18:45,,,9,22,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000004  arXiv: 2002.05379,,/Users/angelica/Zotero/storage/AT4UIMH9/Fischer - 2020 - The Conditional Entropy Bottleneck.pdf; /Users/angelica/Zotero/storage/CBDF6ZG3/2002.html,,Other-org; TechSafety; AmbiguosSafety,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"While I've categorized this paper under robustness because it can apply to most forms of training, I'll talk about it specifically in the context of unsupervised learning (and in particular its relation to Contrastive Predictive Coding (CPC), summarized in the highlights).

One potential problem with deep learning is that there might be too _much_ information in the input, causing the model to learn spurious correlations that do not actually generalize well (see <@Causal Confusion in Imitation Learning@> as an example). The idea with CEB is to penalize the model for learning irrelevant information, using a form of _information bottleneck_.

We consider a setting where we want to learn a representation **Z** of some input data **X** in order to predict some downstream data **Y**. In CPC, **X** would be the inputs from time 1 to t, **Z** would be the latent representation **z_t**, and **Y** would be the future data **x_{t+k}**. Then, we want **Z** to capture the **minimum necessary information** needed for **Z** to predict **Y** as best as possible. The _necessary_ information is **I(Y; Z)**, that is, the mutual information between **Z** and **Y**: we want to maximize this to maximize our accuracy at predicting **Y**. Since **Y** depends on **X** and **Z** is computed from **X**, any information about **Y** must come through mutual information between **X** and **Z**. Maximizing just this **I(Y; Z)** term gives us Contrastive Predictive Coding.

However, we don't want to capture any extra irrelevant information (the minimality criterion), which means that **Z** shouldn't capture any _more_ information about **X** beyond what it captured to maximize **I(Y; Z)**. In information-theoretic terms, we want to _minimize_ **I(X; Z | Y)**. Thus, we have the CEB objective: minimizing **I(X; Z | Y) - γ I(Y; Z)**, where **γ** is a hyperparameter controlling the tradeoff between the two terms. The authors then use some fairly straightforward math to reduce the objective to simpler terms which can be bounded using variational approximations, leading to an algorithm that can work in practice.

The authors perform experiments on Fashion MNIST and CIFAR10 (where Y corresponds to the labels for the images, so we're in the supervised learning setting). Since the main benefit of CEB is to remove unnecessary information from the model, they evaluate adversarial robustness and out-of-distribution detection in addition to standard performance checks. They find that models trained with CEB perform better than ones trained with a variational information bottleneck, or ones trained with vanilla SGD."
VCQ53PLA,journalArticle,2020,"Li, Mingwei; Zhao, Zhenge; Scheidegger, Carlos",Visualizing Neural Networks with the Grand Tour,Distill,,2476-0757,10.23915/distill.00025,https://distill.pub/2020/grand-tour,"By focusing on linear dimensionality reduction, we show how to visualize many dynamic phenomena in neural networks.",2020-03-16,2020-09-05 17:37,2020-12-21 18:17,2020-09-05 17:37,e25,,3,5,,Distill,,,,,,,,en,,,,,distill.pub,,ZSCC: 0000000,,/Users/angelica/Zotero/storage/RRJ646FX/grand-tour.html,,Other-org; NotSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Visualizing a complete dataset instead of single input examples is helpful when we want to analyze the relationships between different input examples and how their classification changes during training, as we can do so by looking at a single video.

The authors use an example on MNIST in which the network learns to classify the numbers 1 and 7 in an almost discrete fashion during particular epochs to compare different methods for visualizing how the dataset is classified. They find that one problem with nonlinear dimensionality reduction like t-SNE and UMAPs is that changes to a subset of the dataset can strongly affect how unchanged data points are represented. Then they compare this to the Grand Tour, a classical technique that projects the data into two dimensions from varying points of view. As projections are linear in the input variables, it is rather easy to reason about how changes in the data affect this visualization and the times the classes 1 and 7 are learnt are indeed quite salient in their example. Another advantage of this method is that confusion between two specific classes can be identified more easily, as the corresponding data points will be projected onto the line connecting the clusters for these classes. A similar approach can be taken on a network's hidden layers to identify the layer in which different classes become clearly distinguishable. They find that they can identify adversarial examples generated by FGSM by looking at the second to last layer, where the adversarial examples form a cluster distinct from the real images.

As the Grand Tour involves varying rotations, it is basically unaffected by rotations of the data. The authors argue that this is a feature, as rotations are small changes to the data and should not have a large effect on the visualization."
DXSJMY7V,journalArticle,2020,"Hilton, Jacob; Cammarata, Nick; Carter, Shan; Goh, Gabriel; Olah, Chris",Understanding RL Vision,Distill,,2476-0757,10.23915/distill.00029,https://distill.pub/2020/understanding-rl-vision,"With diverse environments, we can analyze, diagnose and edit deep reinforcement learning models using attribution.",2020-11-17,2020-12-18 15:18,2020-12-20 15:20,2020-12-18 15:18,e29,,11,5,,Distill,,,,,,,,en,,,,,distill.pub,,ZSCC: 0000000,,/Users/angelica/Zotero/storage/BLZ7VWI8/understanding-rl-vision.html,,NotSafety; AmbiguosSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This work presents an interface for interpreting the vision of a reinforcement learning agent trained with PPO on the CoinRun game. This game is procedurally generated, which means the levels are different in every episode of playing. The interface primarily uses attribution from a hidden layer to the output of the value function. This interface is used in several ways.

First, they use the interface to dissect failed trajectories of the policy (it fails in 1 out of 200 levels). They're able to understand why the failures occurred using their interface: for example, in one case the view of the agent at the top of its jump means it can't see any platforms below it, so doesn't move to the right fast enough to reach the platform it was jumping for, leading it to miss the platform and fail the level. Second, they use the interface to discover ""hallucinations"", where the value function mistakes one element of the environment for another, causing its value to drop or rise significantly. Often these hallucinations only last a single time-step, so they don't affect performance.

Finally, they use the attributions specifically to hand-edit the weights of the model to make it ""blind"" to buzzsaws (one of the hazards) by zeroing the feature which recognises them. After doing this, they show that the edited agent fails a lot more from buzzsaw failures but no more from other types of failures, which gives a quantitative justification for their interpretation of the feature as buzzsaw-recognising.

From using this interface, they propose the **diversity hypothesis:** _Interpretable features tend to arise (at a given level of abstraction) if and only if the training distribution is diverse enough (at that level of abstraction)._ This is based on the fact that interpretable features arise more when the agent is trained on a wider variety of levels. There also seems to be a qualitative link to generalisation - a wider distribution of training levels leads to better interpretability (measured qualitatively) and better generalisation (measured quantitatively)."
,journalArticle,2020,"Mordvintsev, Alexander; Randazzo, Ettore; Niklasson, Eyvind; Levin, Michael",Growing Neural Cellular Automata,Distill,,2476-0757,10.23915/distill.00023,https://distill.pub/2020/growing-ca,"Training an end-to-end differentiable, self-organising cellular automata model of morphogenesis, able to both grow and regenerate specific patterns.",2020-02-11,2020-08-31 18:44,2020-12-21 18:21,2020-08-31 18:44,e23,,2,5,,Distill,,,,,,,,en,,,,,distill.pub,,ZSCC: 0000002,,/Users/angelica/Zotero/storage/XE438DGP/growing-ca.html,,Other-org; NotSafety; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The process of an organism's shape development (morphogensis) is an active area of research. One central problem is determining how cells decide how to grow and when to stop. One popular model for investigating this is Cellular Automata (CA). These model cells as living on a grid and interacting with each other via rules generated by looking at their nearest neighbors. The authors contribute to this research direction by introducing rule-sets that depend continuously on their local surroundings. The central insight connecting CA and deep learning is that because the rule-sets are constant the update rules work similarly to a convolutional filter. This allows the authors to take advantage of methods available to train neural networks to simulate CA. Using this insight, the authors train CA that can form into images that are resistant to perturbations and deletions. In other words, the CA are capable of regeneration."
XXX,journalArticle,2013,"Carl G. Wagner, Bruce Tonn",Constructing Lower Probabilities,,,,,http://arxiv.org/abs/1303.1516v1,"An elaboration of Dempster's method of constructing belief functions suggests
a broadly applicable strategy for constructing lower probabilities under a
variety of evidentiary constraints.",2022-11-13 15:58:06,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2009,Ji Han,A Rational Decision Maker with Ordinal Utility under Uncertainty: Optimism and Pessimism,,,,,http://arxiv.org/abs/0912.5073v2,"In game theory and artificial intelligence, decision making models often
involve maximizing expected utility, which does not respect ordinal invariance.
In this paper, the author discusses the possibility of preserving ordinal
invariance and still making a rational decision under uncertainty.",2022-11-13 15:58:06,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2011,"Tor Lattimore, Marcus Hutter",Asymptotically Optimal Agents,"Proc. 22nd International Conf. on Algorithmic Learning Theory
  (ALT-2011) pages 368-382",,,,http://arxiv.org/abs/1107.5537v1,"Artificial general intelligence aims to create agents capable of learning to
solve arbitrary interesting problems. We define two versions of asymptotic
optimality and prove that no agent can satisfy the strong version while in some
cases, depending on discounting, there does exist a non-computable weak
asymptotically optimal agent.",2022-11-13 15:58:07,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2013,"Paul E. Lehner, Azar Sadigh",Two Procedures for Compiling Influence Diagrams,,,,,http://arxiv.org/abs/1303.1494v1,"Two algorithms are presented for ""compiling"" influence diagrams into a set of
simple decision rules. These decision rules define simple-to-execute, complete,
consistent, and near-optimal decision procedures. These compilation algorithms
can be used to derive decision procedures for human teams solving time
constrained decision problems.",2022-11-13 15:58:07,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2013,"Ross D. Shachter, David Heckerman",A Backwards View for Assessment,,,,,http://arxiv.org/abs/1304.3107v1,"Much artificial intelligence research focuses on the problem of deducing the
validity of unobservable propositions or hypotheses from observable evidence.!
Many of the knowledge representation techniques designed for this problem
encode the relationship between evidence and hypothesis in a directed manner.
Moreover, the direction in which evidence is stored is typically from evidence
to hypothesis.",2022-11-13 15:58:08,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2015,George Konidaris,Constructing Abstraction Hierarchies Using a Skill-Symbol Loop,,,,,http://arxiv.org/abs/1509.07582v1,"We describe a framework for building abstraction hierarchies whereby an agent
alternates skill- and representation-acquisition phases to construct a sequence
of increasingly abstract Markov decision processes. Our formulation builds on
recent results showing that the appropriate abstract representation of a
problem is specified by the agent's skills. We describe how such a hierarchy
can be used for fast planning, and illustrate the construction of an
appropriate hierarchy for the Taxi domain.",2022-11-13 15:58:08,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2015,Toby Walsh,Turing's Red Flag,,,,,http://arxiv.org/abs/1510.09033v1,"Sometime in the future we will have to deal with the impact of AI's being
mistaken for humans. For this reason, I propose that any autonomous system
should be designed so that it is unlikely to be mistaken for anything besides
an autonomous sysem, and should identify itself at the start of any interaction
with another agent.",2022-11-13 15:58:09,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,Roman V. Yampolskiy,The Singularity May Be Near,,,,,http://arxiv.org/abs/1706.01303v1,"Toby Walsh in 'The Singularity May Never Be Near' gives six arguments to
support his point of view that technological singularity may happen but that it
is unlikely. In this paper, we provide analysis of each one of his arguments
and arrive at similar conclusions, but with more weight given to the 'likely to
happen' probability.",2022-11-13 15:58:09,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"James D. Miller, Roman Yampolskiy",An AGI with Time-Inconsistent Preferences,,,,,http://arxiv.org/abs/1906.10536v1,"This paper reveals a trap for artificial general intelligence (AGI) theorists
who use economists' standard method of discounting. This trap is implicitly and
falsely assuming that a rational AGI would have time-consistent preferences. An
agent with time-inconsistent preferences knows that its future self will
disagree with its current self concerning intertemporal decision making. Such
an agent cannot automatically trust its future self to carry out plans that its
current self considers optimal.",2022-11-13 15:58:10,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Guillaume Escamocher, Barry O'Sullivan",Solving Logic Grid Puzzles with an Algorithm that Imitates Human Behavior,,,,,http://arxiv.org/abs/1910.06636v1,"We present in this paper our solver for logic grid puzzles. The approach used
by our algorithm mimics the way a human would try to solve the same problem.
Every progress made during the solving process is accompanied by a detailed
explanation of our program's reasoning. Since this reasoning is based on the
same heuristics that a human would employ, the user can easily follow the given
explanation.",2022-11-13 15:58:11,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,Adnan Darwiche,Three Modern Roles for Logic in AI,,,,10.1145/3375395.3389131,http://arxiv.org/abs/2004.08599v1,"We consider three modern roles for logic in artificial intelligence, which
are based on the theory of tractable Boolean circuits: (1) logic as a basis for
computation, (2) logic for learning from a combination of data and knowledge,
and (3) logic for reasoning about the behavior of machine learning systems.",2022-11-13 15:58:11,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,Roman V. Yampolskiy,AI Risk Skepticism,,,,,http://arxiv.org/abs/2105.02704v3,"In this work, we survey skepticism regarding AI risk and show parallels with
other types of scientific skepticism. We start by classifying different types
of AI Risk skepticism and analyze their root causes. We conclude by suggesting
some intervention approaches, which may be successful in reducing AI risk
skepticism, at least amongst artificial intelligence researchers.",2022-11-13 15:58:12,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Ajay Krishnan, Niranj Jyothish, Xun Jia",Using reinforcement learning to design an AI assistantfor a satisfying co-op experience,,,,,http://arxiv.org/abs/2105.03414v1,"In this project, we designed an intelligent assistant player for the
single-player game Space Invaders with the aim to provide a satisfying co-op
experience. The agent behaviour was designed using reinforcement learning
techniques and evaluated based on several criteria. We validate the hypothesis
that an AI-driven computer player can provide a satisfying co-op experience.",2022-11-13 15:58:12,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Domonkos Czifra, Endre Csóka, Zsolt Zombori, Géza Makay",Towards solving the 7-in-a-row game,,,,,http://arxiv.org/abs/2107.05363v1,"Our paper explores the game theoretic value of the 7-in-a-row game. We reduce
the problem to solving a finite board game, which we target using Proof Number
Search. We present a number of heuristic improvements to Proof Number Search
and examine their effect within the context of this particular game. Although
our paper does not solve the 7-in-a-row game, our experiments indicate that we
have made significant progress towards it.",2022-11-13 15:58:13,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Jinxin Ding, Yuxin Huang, Keyang Ni, Xueyao Wang, Yinxiao Wang, Yucheng Wang",Intellectual Property Evaluation Utilizing Machine Learning,,,,,http://arxiv.org/abs/2208.08611v1,"Intellectual properties is increasingly important in the economic
development. To solve the pain points by traditional methods in IP evaluation,
we are developing a new technology with machine learning as the core. We have
built an online platform and will expand our business in the Greater Bay Area
with plans.",2022-11-13 15:58:13,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,1995,"C. G. Giraud-Carrier, T. R. Martinez",An Integrated Framework for Learning and Reasoning,"Journal of Artificial Intelligence Research, Vol 3, (1995),
  147-185",,,,http://arxiv.org/abs/cs/9508102v1,"Learning and reasoning are both aspects of what is considered to be
intelligence. Their studies within AI have been separated historically,
learning being the topic of machine learning and neural networks, and reasoning
falling under classical (or symbolic) AI. However, learning and reasoning are
in many ways interdependent. This paper discusses the nature of some of these
interdependencies and proposes a general framework called FLARE, that combines
inductive learning using prior knowledge together with reasoning in a
propositional setting. Several examples that test the framework are presented,
including classical induction, many important reasoning protocols and two
simple expert systems.",2022-11-13 15:58:14,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2003,"Ulla Bergsten, Johan Schubert, Per Svensson",Beslutstödssystemet Dezzy - en översikt,"in Dokumentation 7 juni av Seminarium och fackutst\""allning om
  samband, sensorer och datorer f\""or ledningssystem till f\""orsvaret
  (MILINF'89), pp. 07B2:19-31, Enk\""oping, June 1989, Telub AB, V\""axj\""o, 1989",,,,http://arxiv.org/abs/cs/0305033v1,"Within the scope of the three-year ANTI-SUBMARINE WARFARE project of the
National Defence Research Establishment, the INFORMATION SYSTEMS subproject has
developed the demonstration prototype Dezzy for handling and analysis of
intelligence reports concerning foreign underwater activities.
  -----
  Inom ramen f\""or FOA:s tre{\aa}riga huvudprojekt UB{\AA}TSSKYDD har
delprojekt INFORMATIONSSYSTEM utvecklat demonstrationsprototypen Dezzy till ett
beslutsst\""odsystem f\""or hantering och analys av underr\""attelser om
fr\""ammande undervattensverksamhet.",2022-11-13 15:58:14,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2003,"Joseph Y. Halpern, Riccardo Pucella",A logic for reasoning about upper probabilities,"Journal of AI Research 17, 2001, pp. 57-81",,,,http://arxiv.org/abs/cs/0307069v1,"We present a propositional logic %which can be used to reason about the
uncertainty of events, where the uncertainty is modeled by a set of probability
measures assigning an interval of probability to each event. We give a sound
and complete axiomatization for the logic, and show that the satisfiability
problem is NP-complete, no harder than satisfiability for propositional logic.",2022-11-13 15:58:15,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2003,"Marcello Balduccini, Michael Gelfond",Diagnostic reasoning with A-Prolog,TPLP Vol 3(4&5) (2003) 425-461,,,,http://arxiv.org/abs/cs/0312040v1,"In this paper we suggest an architecture for a software agent which operates
a physical device and is capable of making observations and of testing and
repairing the device's components. We present simplified definitions of the
notions of symptom, candidate diagnosis, and diagnosis which are based on the
theory of action language ${\cal AL}$. The definitions allow one to give a
simple account of the agent's behavior in which many of the agent's tasks are
reduced to computing stable models of logic programs.",2022-11-13 15:58:15,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2006,"Shane Legg, Marcus Hutter",A Formal Measure of Machine Intelligence,"Proc. 15th Annual Machine Learning Conference of {B}elgium and The
  Netherlands (Benelearn 2006) pages 73-80",,,,http://arxiv.org/abs/cs/0605024v1,"A fundamental problem in artificial intelligence is that nobody really knows
what intelligence is. The problem is especially acute when we need to consider
artificial systems which are significantly different to humans. In this paper
we approach this problem in the following way: We take a number of well known
informal definitions of human intelligence that have been given by experts, and
extract their essential features. These are then mathematically formalised to
produce a general measure of intelligence for arbitrary machines. We believe
that this measure formally captures the concept of machine intelligence in the
broadest reasonable sense.",2022-11-13 15:58:16,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2009,"Eugen Staab, Martin Caminada",Assessing the Impact of Informedness on a Consultant's Profit,,,,,http://arxiv.org/abs/0909.0901v1,"We study the notion of informedness in a client-consultant setting. Using a
software simulator, we examine the extent to which it pays off for consultants
to provide their clients with advice that is well-informed, or with advice that
is merely meant to appear to be well-informed. The latter strategy is
beneficial in that it costs less resources to keep up-to-date, but carries the
risk of a decreased reputation if the clients discover the low level of
informedness of the consultant. Our experimental results indicate that under
different circumstances, different strategies yield the optimal results (net
profit) for the consultants.",2022-11-13 15:58:16,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2011,"J. E. Laird, R. E. Wray",An Architectural Approach to Ensuring Consistency in Hierarchical Execution,"Journal Of Artificial Intelligence Research, Volume 19, pages
  355-398, 2003",,,10.1613/jair.1142,http://arxiv.org/abs/1106.4871v1,"Hierarchical task decomposition is a method used in many agent systems to
organize agent knowledge. This work shows how the combination of a hierarchy
and persistent assertions of knowledge can lead to difficulty in maintaining
logical consistency in asserted knowledge. We explore the problematic
consequences of persistent assumptions in the reasoning process and introduce
novel potential solutions. Having implemented one of the possible solutions,
Dynamic Hierarchical Justification, its effectiveness is demonstrated with an
empirical analysis.",2022-11-13 15:58:17,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2012,"Mauricio Araya, Olivier Buffet, Vincent Thomas",Near-Optimal BRL using Optimistic Local Transitions,,,,,http://arxiv.org/abs/1206.4613v1,"Model-based Bayesian Reinforcement Learning (BRL) allows a found
formalization of the problem of acting optimally while facing an unknown
environment, i.e., avoiding the exploration-exploitation dilemma. However,
algorithms explicitly addressing BRL suffer from such a combinatorial explosion
that a large body of work relies on heuristic algorithms. This paper introduces
BOLT, a simple and (almost) deterministic heuristic algorithm for BRL which is
optimistic about the transition function. We analyze BOLT's sample complexity,
and show that under certain parameters, the algorithm is near-optimal in the
Bayesian sense with high probability. Then, experimental results highlight the
key differences of this method compared to previous work.",2022-11-13 15:58:17,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2012,"Nitin Yadav, Sebastian Sardina",Reasoning about Agent Programs using ATL-like Logics,"In Proceedings of the European Conference on Logics in Artificial
  Intelligence (JELIA), volume 7519 of LNCS, pages 437-449, 2012",,,,http://arxiv.org/abs/1207.3874v1,"We propose a variant of Alternating-time Temporal Logic (ATL) grounded in the
agents' operational know-how, as defined by their libraries of abstract plans.
Inspired by ATLES, a variant itself of ATL, it is possible in our logic to
explicitly refer to ""rational"" strategies for agents developed under the
Belief-Desire-Intention agent programming paradigm. This allows us to express
and verify properties of BDI systems using ATL-type logical frameworks.",2022-11-13 15:58:18,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2012,Michael Maher,Relative Expressiveness of Defeasible Logics,"Theory and Practice of Logic Programming 12 (4-5), 793--810, 2012",,,,http://arxiv.org/abs/1210.1785v1,"We address the relative expressiveness of defeasible logics in the framework
DL. Relative expressiveness is formulated as the ability to simulate the
reasoning of one logic within another logic. We show that such simulations must
be modular, in the sense that they also work if applied only to part of a
theory, in order to achieve a useful notion of relative expressiveness. We
present simulations showing that logics in DL with and without the capability
of team defeat are equally expressive. We also show that logics that handle
ambiguity differently -- ambiguity blocking versus ambiguity propagating --
have distinct expressiveness, with neither able to simulate the other under a
different formulation of expressiveness.",2022-11-13 15:58:19,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2012,"Antonio Pisasale, Domenico Cantone",An Experiment on the Connection between the DLs' Family DL<ForAllPiZero> and the Real World,,,,,http://arxiv.org/abs/1211.4957v2,"This paper describes the analysis of a selected testbed of Semantic Web
ontologies, by a SPARQL query, which determines those ontologies that can be
related to the description logic DL<ForAllPiZero>, introduced in [4] and
studied in [9]. We will see that a reasonable number of them is expressible
within such computationally efficient language. We expect that, in a long-term
view, a temporalization of description logics, and consequently, of OWL(2), can
open new perspectives for the inclusion in this language of a greater number of
ontologies of the testbed and, hopefully, of the ""real world"".",2022-11-13 15:58:20,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2013,"Paul J. Krause, John Fox, Philip Judson",Is There a Role for Qualitative Risk Assessment?,,,,,http://arxiv.org/abs/1302.4970v1,"Classically, risk is characterized by a point value probability indicating
the likelihood of occurrence of an adverse effect. However, there are domains
where the attainability of objective numerical risk characterizations is
increasingly being questioned. This paper reviews the arguments in favour of
extending classical techniques of risk assessment to incorporate meaningful
qualitative and weak quantitative risk characterizations. A technique in which
linguistic uncertainty terms are defined in terms of patterns of argument is
then proposed. The technique is demonstrated using a prototype computer-based
system for predicting the carcinogenic risk due to novel chemical compounds.",2022-11-13 15:58:20,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2013,Michael Pittarelli,Anytime Decision Making with Imprecise Probabilities,,,,,http://arxiv.org/abs/1302.6837v1,"This paper examines methods of decision making that are able to accommodate
limitations on both the form in which uncertainty pertaining to a decision
problem can be realistically represented and the amount of computing time
available before a decision must be made. The methods are anytime algorithms in
the sense of Boddy and Dean 1991. Techniques are presented for use with Frisch
and Haddawy's [1992] anytime deduction system, with an anytime adaptation of
Nilsson's [1986] probabilistic logic, and with a probabilistic database model.",2022-11-13 15:58:21,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2013,"Lisa J. Burnell, Eric J. Horvitz",A Synthesis of Logical and Probabilistic Reasoning for Program Understanding and Debugging,,,,,http://arxiv.org/abs/1303.1488v1,"We describe the integration of logical and uncertain reasoning methods to
identify the likely source and location of software problems. To date, software
engineers have had few tools for identifying the sources of error in complex
software packages. We describe a method for diagnosing software problems
through combining logical and uncertain reasoning analyses. Our preliminary
results suggest that such methods can be of value in directing the attention of
software engineers to paths of an algorithm that have the highest likelihood of
harboring a programming error.",2022-11-13 15:58:21,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2013,Paul E. Lehner,Inference Policies,,,,,http://arxiv.org/abs/1304.1516v1,"It is suggested that an AI inference system should reflect an inference
policy that is tailored to the domain of problems to which it is applied -- and
furthermore that an inference policy need not conform to any general theory of
rational inference or induction. We note, for instance, that Bayesian reasoning
about the probabilistic characteristics of an inference domain may result in
the specification of an nonBayesian procedure for reasoning within the
inference domain. In this paper, the idea of an inference policy is explored in
some detail. To support this exploration, the characteristics of some standard
and nonstandard inference policies are examined.",2022-11-13 15:58:22,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2013,David Heckerman,An Empirical Comparison of Three Inference Methods,,,,,http://arxiv.org/abs/1304.2357v2,"In this paper, an empirical evaluation of three inference methods for
uncertain reasoning is presented in the context of Pathfinder, a large expert
system for the diagnosis of lymph-node pathology. The inference procedures
evaluated are (1) Bayes' theorem, assuming evidence is conditionally
independent given each hypothesis; (2) odds-likelihood updating, assuming
evidence is conditionally independent given each hypothesis and given the
negation of each hypothesis; and (3) a inference method related to the
Dempster-Shafer theory of belief. Both expert-rating and decision-theoretic
metrics are used to compare the diagnostic accuracy of the inference methods.",2022-11-13 15:58:23,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2013,Daniel Hunter,Dempster-Shafer vs. Probabilistic Logic,,,,,http://arxiv.org/abs/1304.2713v1,"The combination of evidence in Dempster-Shafer theory is compared with the
combination of evidence in probabilistic logic. Sufficient conditions are
stated for these two methods to agree. It is then shown that these conditions
are minimal in the sense that disagreement can occur when any one of them is
removed. An example is given in which the traditional assumption of conditional
independence of evidence on hypotheses holds and a uniform prior is assumed,
but probabilistic logic and Dempster's rule give radically different results
for the combination of two evidence events.",2022-11-13 15:58:23,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2013,"John S. Breese, Edison Tse",Integrating Logical and Probabilistic Reasoning for Decision Making,,,,,http://arxiv.org/abs/1304.2751v1,"We describe a representation and a set of inference methods that combine
logic programming techniques with probabilistic network representations for
uncertainty (influence diagrams). The techniques emphasize the dynamic
construction and solution of probabilistic and decision-theoretic models for
complex and uncertain domains. Given a query, a logical proof is produced if
possible; if not, an influence diagram based on the query and the knowledge of
the decision domain is produced and subsequently solved. A uniform declarative,
first-order, knowledge representation is combined with a set of integrated
inference procedures for logical, probabilistic, and decision-theoretic
reasoning.",2022-11-13 15:58:24,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2013,Dimiter Dobrev,Giving the AI definition a form suitable for the engineer,,,,,http://arxiv.org/abs/1312.5713v2,"Artificial Intelligence - what is this? That is the question! In earlier
papers we already gave a formal definition for AI, but if one desires to build
an actual AI implementation, the following issues require attention and are
treated here: the data format to be used, the idea of Undef and Nothing
symbols, various ways for defining the ""meaning of life"", and finally, a new
notion of ""incorrect move"". These questions are of minor importance in the
theoretical discussion, but we already know the answer of the question ""Does AI
exist?"" Now we want to make the next step and to create this program.",2022-11-13 15:58:24,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2014,"Joseph Y. Halpern, Riccardo Pucella",A Logic for Reasoning about Upper Probabilities,,,,,http://arxiv.org/abs/1408.1485v1,"We present a propositional logic to reason about the uncertainty of events,
where the uncertainty is modeled by a set of probability measures assigning an
interval of probability to each event. We give a sound and complete
axiomatization for the logic, and show that the satisfiability problem is
NP-complete, no harder than satisfiability for propositional logic.",2022-11-13 15:58:25,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2015,"Nate Soares, Benja Fallenstein",Toward Idealized Decision Theory,,,,,http://arxiv.org/abs/1507.01986v1,"This paper motivates the study of decision theory as necessary for aligning
smarter-than-human artificial systems with human interests. We discuss the
shortcomings of two standard formulations of decision theory, and demonstrate
that they cannot be used to describe an idealized decision procedure suitable
for approximation by artificial systems. We then explore the notions of policy
selection and logical counterfactuals, two recent insights into decision theory
that point the way toward promising paths for future research.",2022-11-13 15:58:26,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2016,Amnon H. Eden,"The Singularity Controversy, Part I: Lessons Learned and Open Questions: Conclusions from the Battle on the Legitimacy of the Debate",,,,10.13140/RG.2.1.3416.6809,http://arxiv.org/abs/1601.05977v2,"This report seeks to inform policy makers on the nature and the merit of the
arguments for and against the concerns associated with a potential
technological singularity.
  Part I describes the lessons learned from our investigation of the subject,
separating the argu-ments of merit from the fallacies and misconceptions that
confuse the debate and undermine its rational resolution.",2022-11-13 15:58:26,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2016,"Stuart Russell, Daniel Dewey, Max Tegmark",Research Priorities for Robust and Beneficial Artificial Intelligence,AI Magazine 36:4 (2015),,,,http://arxiv.org/abs/1602.03506v1,"Success in the quest for artificial intelligence has the potential to bring
unprecedented benefits to humanity, and it is therefore worthwhile to
investigate how to maximize these benefits while avoiding potential pitfalls.
This article gives numerous examples (which should by no means be construed as
an exhaustive list) of such worthwhile research aimed at ensuring that AI
remains robust and beneficial.",2022-11-13 15:58:27,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2016,"Carissa Schoenick, Peter Clark, Oyvind Tafjord, Peter Turney, Oren Etzioni",Moving Beyond the Turing Test with the Allen AI Science Challenge,,,,,http://arxiv.org/abs/1604.04315v3,"Given recent successes in AI (e.g., AlphaGo's victory against Lee Sedol in
the game of GO), it's become increasingly important to assess: how close are AI
systems to human-level intelligence? This paper describes the Allen AI Science
Challenge---an approach towards that goal which led to a unique Kaggle
Competition, its results, the lessons learned, and our next steps.",2022-11-13 15:58:28,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2016,"Tom Everitt, Marcus Hutter",Avoiding Wireheading with Value Reinforcement Learning,,,,,http://arxiv.org/abs/1605.03143v1,"How can we design good goals for arbitrarily intelligent agents?
Reinforcement learning (RL) is a natural approach. Unfortunately, RL does not
work well for generally intelligent agents, as RL agents are incentivised to
shortcut the reward sensor for maximum reward -- the so-called wireheading
problem. In this paper we suggest an alternative to RL called value
reinforcement learning (VRL). In VRL, agents use the reward signal to learn a
utility function. The VRL setup allows us to remove the incentive to wirehead
by placing a constraint on the agent's actions. The constraint is defined in
terms of the agent's belief distributions, and does not require an explicit
specification of which actions constitute wireheading.",2022-11-13 15:58:28,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,Sebastian Benthall,Don't Fear the Reaper: Refuting Bostrom's Superintelligence Argument,,,,,http://arxiv.org/abs/1702.08495v2,"In recent years prominent intellectuals have raised ethical concerns about
the consequences of artificial intelligence. One concern is that an autonomous
agent might modify itself to become ""superintelligent"" and, in supremely
effective pursuit of poorly specified goals, destroy all of humanity. This
paper considers and rejects the possibility of this outcome. We argue that this
scenario depends on an agent's ability to rapidly improve its ability to
predict its environment through self-modification. Using a Bayesian model of a
reasoning agent, we show that there are important limitations to how an agent
may improve its predictive ability through self-modification alone. We conclude
that concern about this artificial intelligence outcome is misplaced and better
directed at policy questions around data access and storage.",2022-11-13 15:58:29,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,Christopher A. Tucker,A proposal for ethically traceable artificial intelligence,,,,,http://arxiv.org/abs/1703.01908v2,"Although the problem of a critique of robotic behavior in near-unanimous
agreement to human norms seems intractable, a starting point of such an
ambition is a framework of the collection of knowledge a priori and experience
a posteriori categorized as a set of synthetical judgments available to the
intelligence, translated into computer code. If such a proposal were
successful, an algorithm with ethically traceable behavior and cogent
equivalence to human cognition is established. This paper will propose the
application of Kant's critique of reason to current programming constructs of
an autonomous intelligent system.",2022-11-13 15:58:29,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,"Stuart Armstrong, Benjamin Levinstein",Low Impact Artificial Intelligences,,,,,http://arxiv.org/abs/1705.10720v1,"There are many goals for an AI that could become dangerous if the AI becomes
superintelligent or otherwise powerful. Much work on the AI control problem has
been focused on constructing AI goals that are safe even for such AIs. This
paper looks at an alternative approach: defining a general concept of `low
impact'. The aim is to ensure that a powerful AI which implements low impact
will not modify the world extensively, even if it is given a simple or
dangerous goal. The paper proposes various ways of defining and grounding low
impact, and discusses methods for ensuring that the AI can still be allowed to
have a (desired) impact despite the restriction. The end of the paper addresses
known issues with this approach and avenues for future research.",2022-11-13 15:58:30,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,Virginia Dignum,Responsible Autonomy,,,,,http://arxiv.org/abs/1706.02513v1,"As intelligent systems are increasingly making decisions that directly affect
society, perhaps the most important upcoming research direction in AI is to
rethink the ethical implications of their actions. Means are needed to
integrate moral, societal and legal values with technological developments in
AI, both during the design process as well as part of the deliberation
algorithms employed by these systems. In this paper, we describe leading ethics
theories and propose alternative ways to ensure ethical behavior by artificial
systems. Given that ethics are dependent on the socio-cultural context and are
often only implicit in deliberation processes, methodologies are needed to
elicit the values held by designers and stakeholders, and to make these
explicit leading to better understanding and trust on artificial autonomous
systems.",2022-11-13 15:58:30,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,"James Babcock, Janos Kramar, Roman V. Yampolskiy",Guidelines for Artificial Intelligence Containment,,,,,http://arxiv.org/abs/1707.08476v1,"With almost daily improvements in capabilities of artificial intelligence it
is more important than ever to develop safety software for use by the AI
research community. Building on our previous work on AI Containment Problem we
propose a number of guidelines which should help AI safety researchers to
develop reliable sandboxing software for intelligent programs of all levels.
Such safety container software will make it possible to study and analyze
intelligent artificial agent while maintaining certain level of safety against
information leakage, social engineering attacks and cyberattacks from within
the container.",2022-11-13 15:58:31,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,"Pavel Naumov, Jia Tao",Together We Know How to Achieve: An Epistemic Logic of Know-How (Extended Abstract),"EPTCS 251, 2017, pp. 441-453",,,10.4204/EPTCS.251.32,http://arxiv.org/abs/1707.08759v1,"The existence of a coalition strategy to achieve a goal does not necessarily
mean that the coalition has enough information to know how to follow the
strategy. Neither does it mean that the coalition knows that such a strategy
exists. The paper studies an interplay between the distributed knowledge,
coalition strategies, and coalition ""know-how"" strategies. The main technical
result is a sound and complete trimodal logical system that describes the
properties of this interplay.",2022-11-13 15:58:31,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,"Robert B. Allen, Eunsang Yang, Tatsawan Timakum",A Foundry of Human Activities and Infrastructures,,,,,http://arxiv.org/abs/1711.01927v1,"Direct representation knowledgebases can enhance and even provide an
alternative to document-centered digital libraries. Here we consider realist
semantic modeling of everyday activities and infrastructures in such
knowledgebases. Because we want to integrate a wide variety of topics, a
collection of ontologies (a foundry) and a range of other knowledge resources
are needed. We first consider modeling the routine procedures that support
human activities and technologies. Next, we examine the interactions of
technologies with aspects of social organization. Then, we consider approaches
and issues for developing and validating explanations of the relationships
among various entities.",2022-11-13 15:58:32,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,"Yueh-Hua Wu, Shou-De Lin",A Low-Cost Ethics Shaping Approach for Designing Reinforcement Learning Agents,,,,,http://arxiv.org/abs/1712.04172v2,"This paper proposes a low-cost, easily realizable strategy to equip a
reinforcement learning (RL) agent the capability of behaving ethically. Our
model allows the designers of RL agents to solely focus on the task to achieve,
without having to worry about the implementation of multiple trivial ethical
patterns to follow. Based on the assumption that the majority of human
behavior, regardless which goals they are achieving, is ethical, our design
integrates human policy with the RL policy to achieve the target objective with
less chance of violating the ethical code that human beings normally obey.",2022-11-13 15:58:32,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,Atrisha Sarkar,A Brandom-ian view of Reinforcement Learning towards strong-AI,,,,,http://arxiv.org/abs/1803.02912v1,"The analytic philosophy of Robert Brandom, based on the ideas of pragmatism,
paints a picture of sapience, through inferentialism. In this paper, we present
a theory, that utilizes essential elements of Brandom's philosophy, towards the
objective of achieving strong-AI. We do this by connecting the constitutive
elements of reinforcement learning and the Game Of Giving and Asking For
Reasons. Further, following Brandom's prescriptive thoughts, we restructure the
popular reinforcement learning algorithm A3C, and show that RL algorithms can
be tuned towards the objective of strong-AI.",2022-11-13 15:58:33,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,"Christoph Benzmüller, Xavier Parent",First Experiments with a Flexible Infrastructure for Normative Reasoning,,,,,http://arxiv.org/abs/1804.02929v1,"A flexible infrastructure for normative reasoning is outlined. A small-scale
demonstrator version of the envisioned system has been implemented in the proof
assistant Isabelle/HOL by utilising the first authors universal logical
reasoning approach based on shallow semantical embeddings in meta-logic HOL.
The need for such a flexible reasoning infrastructure is motivated and
illustrated with a contrary-to-duty example scenario selected from the General
Data Protection Regulation.",2022-11-13 15:58:34,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,Dimiter Dobrev,The IQ of Artificial Intelligence,"Serdica Journal of Computing, Vol. 13, Number 1-2, 2019, pp.41-70",,,,http://arxiv.org/abs/1806.04915v1,"All it takes to identify the computer programs which are Artificial
Intelligence is to give them a test and award AI to those that pass the test.
Let us say that the scores they earn at the test will be called IQ. We cannot
pinpoint a minimum IQ threshold that a program has to cover in order to be AI,
however, we will choose a certain value. Thus, our definition for AI will be
any program the IQ of which is above the chosen value. While this idea has
already been implemented in [3], here we will revisit this construct in order
to introduce certain improvements.",2022-11-13 15:58:34,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,Daniele Funaro,Understanding the Meaning of Understanding,,,,,http://arxiv.org/abs/1806.05234v2,"Can we train a machine to detect if another machine has understood a concept?
In principle, this is possible by conducting tests on the subject of that
concept. However we want this procedure to be done by avoiding direct
questions. In other words, we would like to isolate the absolute meaning of an
abstract idea by putting it into a class of equivalence, hence without adopting
straight definitions or showing how this idea ""works"" in practice. We discuss
the metaphysical implications hidden in the above question, with the aim of
providing a plausible reference framework.",2022-11-13 15:58:35,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,John Hooker,Truly Autonomous Machines Are Ethical,,,,,http://arxiv.org/abs/1812.02217v1,"While many see the prospect of autonomous machines as threatening, autonomy
may be exactly what we want in a superintelligent machine. There is a sense of
autonomy, deeply rooted in the ethical literature, in which an autonomous
machine is necessarily an ethical one. Development of the theory underlying
this idea not only reveals the advantages of autonomy, but it sheds light on a
number of issues in the ethics of artificial intelligence. It helps us to
understand what sort of obligations we owe to machines, and what obligations
they owe to us. It clears up the issue of assigning responsibility to machines
or their creators. More generally, a concept of autonomy that is adequate to
both human and artificial intelligence can lead to a more adequate ethical
theory for both.",2022-11-13 15:58:35,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,Tshilidzi Marwala,The limit of artificial intelligence: Can machines be rational?,,,,,http://arxiv.org/abs/1812.06510v1,"This paper studies the question on whether machines can be rational. It
observes the existing reasons why humans are not rational which is due to
imperfect and limited information, limited and inconsistent processing power
through the brain and the inability to optimize decisions and achieve maximum
utility. It studies whether these limitations of humans are transferred to the
limitations of machines. The conclusion reached is that even though machines
are not rational advances in technological developments make these machines
more rational. It also concludes that machines can be more rational than
humans.",2022-11-13 15:58:36,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Jobst Landgrebe, Barry Smith",Making AI meaningful again,,,,,http://arxiv.org/abs/1901.02918v3,"Artificial intelligence (AI) research enjoyed an initial period of enthusiasm
in the 1970s and 80s. But this enthusiasm was tempered by a long interlude of
frustration when genuinely useful AI applications failed to be forthcoming.
Today, we are experiencing once again a period of enthusiasm, fired above all
by the successes of the technology of deep neural networks or deep machine
learning. In this paper we draw attention to what we take to be serious
problems underlying current views of artificial intelligence encouraged by
these successes, especially in the domain of language processing. We then show
an alternative approach to language-centric AI, in which we identify a role for
philosophy.",2022-11-13 15:58:37,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Risto Miikkulainen, Bret Greenstein, Babak Hodjat, Jerry Smith",Better Future through AI: Avoiding Pitfalls and Guiding AI Towards its Full Potential,,,,,http://arxiv.org/abs/1905.13178v1,"Artificial Intelligence (AI) technology is rapidly changing many areas of
society. While there is tremendous potential in this transition, there are
several pitfalls as well. Using the history of computing and the world-wide web
as a guide, in this article we identify those pitfalls and actions that lead AI
development to its full potential. If done right, AI will be instrumental in
achieving the goals we set for economy, society, and the world in general.",2022-11-13 15:58:37,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Arthur Szlam, Jonathan Gray, Kavya Srinet, Yacine Jernite, Armand Joulin, Gabriel Synnaeve, Douwe Kiela, Haonan Yu, Zhuoyuan Chen, Siddharth Goyal, Demi Guo, Danielle Rothermel, C. Lawrence Zitnick, Jason Weston",Why Build an Assistant in Minecraft?,,,,,http://arxiv.org/abs/1907.09273v2,"In this document we describe a rationale for a research program aimed at
building an open ""assistant"" in the game Minecraft, in order to make progress
on the problems of natural language understanding and learning from dialogue.",2022-11-13 15:58:38,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,Vaishak Belle,The Quest for Interpretable and Responsible Artificial Intelligence,,,,,http://arxiv.org/abs/1910.04527v1,"Artificial Intelligence (AI) provides many opportunities to improve private
and public life. Discovering patterns and structures in large troves of data in
an automated manner is a core component of data science, and currently drives
applications in computational biology, finance, law and robotics. However, such
a highly positive impact is coupled with significant challenges: How do we
understand the decisions suggested by these systems in order that we can trust
them? How can they be held accountable for those decisions?
  In this short survey, we cover some of the motivations and trends in the area
that attempt to address such questions.",2022-11-13 15:58:38,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Constantine Goulimis, Gastón Simone",Can ML predict the solution value for a difficult combinatorial problem?,,,,,http://arxiv.org/abs/2003.03181v1,"We look at whether machine learning can predict the final objective function
value of a difficult combinatorial optimisation problem from the input. Our
context is the pattern reduction problem, one industrially important but
difficult aspect of the cutting stock problem. Machine learning appears to have
higher prediction accuracy than a na\""ive model, reducing mean absolute
percentage error (MAPE) from 12.0% to 8.7%.",2022-11-13 15:58:39,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Evan Piermont, Peio Zuazo-Garin",Failures of Contingent Thinking,,,,,http://arxiv.org/abs/2007.07703v1,"In this paper, we provide a theoretical framework to analyze an agent who
misinterprets or misperceives the true decision problem she faces. Within this
framework, we show that a wide range of behavior observed in experimental
settings manifest as failures to perceive implications, in other words, to
properly account for the logical relationships between various payoff relevant
contingencies. We present behavioral characterizations corresponding to several
benchmarks of logical sophistication and show how it is possible to identify
which implications the agent fails to perceive. Thus, our framework delivers
both a methodology for assessing an agent's level of contingent thinking and a
strategy for identifying her beliefs in the absence full rationality.",2022-11-13 15:58:39,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Vinod Muthusamy, Merve Unuvar, Hagen Völzer, Justin D. Weisz",Do's and Don'ts for Human and Digital Worker Integration,,,,,http://arxiv.org/abs/2010.07738v1,"Robotic process automation (RPA) and its next evolutionary stage, intelligent
process automation, promise to drive improvements in efficiencies and process
outcomes. However, how can business leaders evaluate how to integrate
intelligent automation into business processes? What is an appropriate division
of labor between humans and machines? How should combined human-AI teams be
evaluated? For RPA, often the human labor cost and the robotic labor cost are
directly compared to make an automation decision. In this position paper, we
argue for a broader view that incorporates the potential for multiple levels of
autonomy and human involvement, as well as a wider range of metrics beyond
productivity when integrating digital workers into a business process",2022-11-13 15:58:40,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,Joar Skalse,A General Counterexample to Any Decision Theory and Some Responses,,,,,http://arxiv.org/abs/2101.00280v1,"In this paper I present an argument and a general schema which can be used to
construct a problem case for any decision theory, in a way that could be taken
to show that one cannot formulate a decision theory that is never outperformed
by any other decision theory. I also present and discuss a number of possible
responses to this argument. One of these responses raises the question of what
it means for two decision problems to be ""equivalent"" in the relevant sense,
and gives an answer to this question which would invalidate the first argument.
However, this position would have further consequences for how we compare
different decision theories in decision problems already discussed in the
literature (including e.g. Newcomb's problem).",2022-11-13 15:58:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Zachary Kenton, Tom Everitt, Laura Weidinger, Iason Gabriel, Vladimir Mikulik, Geoffrey Irving",Alignment of Language Agents,,,,,http://arxiv.org/abs/2103.14659v1,"For artificial intelligence to be beneficial to humans the behaviour of AI
agents needs to be aligned with what humans want. In this paper we discuss some
behavioural issues for language agents, arising from accidental
misspecification by the system designer. We highlight some ways that
misspecification can occur and discuss some behavioural issues that could arise
from misspecification, including deceptive or manipulative language, and review
some approaches for avoiding these issues.",2022-11-13 15:58:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,Bin Liu,"""Weak AI"" is Likely to Never Become ""Strong AI"", So What is its Greatest Value for us?",,,,,http://arxiv.org/abs/2103.15294v1,"AI has surpassed humans across a variety of tasks such as image
classification, playing games (e.g., go, ""Starcraft"" and poker), and protein
structure prediction. However, at the same time, AI is also bearing serious
controversies. Many researchers argue that little substantial progress has been
made for AI in recent decades. In this paper, the author (1) explains why
controversies about AI exist; (2) discriminates two paradigms of AI research,
termed ""weak AI"" and ""strong AI"" (a.k.a. artificial general intelligence); (3)
clarifies how to judge which paradigm a research work should be classified
into; (4) discusses what is the greatest value of ""weak AI"" if it has no chance
to develop into ""strong AI"".",2022-11-13 15:58:42,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Lu Cheng, Ahmadreza Mosallanezhad, Paras Sheth, Huan Liu",Causal Learning for Socially Responsible AI,,,,,http://arxiv.org/abs/2104.12278v2,"There have been increasing concerns about Artificial Intelligence (AI) due to
its unfathomable potential power. To make AI address ethical challenges and
shun undesirable outcomes, researchers proposed to develop socially responsible
AI (SRAI). One of these approaches is causal learning (CL). We survey
state-of-the-art methods of CL for SRAI. We begin by examining the seven CL
tools to enhance the social responsibility of AI, then review how existing
works have succeeded using these tools to tackle issues in developing SRAI such
as fairness. The goal of this survey is to bring forefront the potentials and
promises of CL for SRAI.",2022-11-13 15:58:42,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,Leopoldo Bertossi,"Reasoning about Counterfactuals and Explanations: Problems, Results and Directions",,,,,http://arxiv.org/abs/2108.11004v1,"There are some recent approaches and results about the use of answer-set
programming for specifying counterfactual interventions on entities under
classification, and reasoning about them. These approaches are flexible and
modular in that they allow the seamless addition of domain knowledge. Reasoning
is enabled by query answering from the answer-set program. The programs can be
used to specify and compute responsibility-based numerical scores as
attributive explanations for classification results.",2022-11-13 15:58:43,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Florian Ellsaesser, Guido Fioretti",Deciding Not To Decide,,,,,http://arxiv.org/abs/2201.05818v1,"Sometimes unexpected, novel, unconceivable events enter our lives. The
cause-effect mappings that usually guide our behaviour are destroyed. Surprised
and shocked by possibilities that we had never imagined, we are unable to make
any decision beyond mere routine. Among them there are decisions, such as
making investments, that are essential for the long-term survival of businesses
as well as the economy at large. We submit that the standard machinery of
utility maximization does not apply, but we propose measures inspired by
scenario planning and graph analysis, pointing to solutions being explored in
machine learning.",2022-11-13 15:58:43,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Harald Rueß, Simon Burton",Safe AI -- How is this Possible?,,,,,http://arxiv.org/abs/2201.10436v2,"Ttraditional safety engineering is coming to a turning point moving from
deterministic, non-evolving systems operating in well-defined contexts to
increasingly autonomous and learning-enabled AI systems which are acting in
largely unpredictable operating contexts. We outline some of underlying
challenges of safe AI and suggest a rigorous engineering framework for
minimizing uncertainty, thereby increasing confidence, up to tolerable levels,
in the safe behavior of AI systems.",2022-11-13 15:58:44,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Olivia Brown, Brad Dillman",Proceedings of the Robust Artificial Intelligence System Assurance (RAISA) Workshop 2022,,,,,http://arxiv.org/abs/2202.04787v1,"The Robust Artificial Intelligence System Assurance (RAISA) workshop will
focus on research, development and application of robust artificial
intelligence (AI) and machine learning (ML) systems. Rather than studying
robustness with respect to particular ML algorithms, our approach will be to
explore robustness assurance at the system architecture level, during both
development and deployment, and within the human-machine teaming context. While
the research community is converging on robust solutions for individual AI
models in specific scenarios, the problem of evaluating and assuring the
robustness of an AI system across its entire life cycle is much more complex.
Moreover, the operational context in which AI systems are deployed necessitates
consideration of robustness and its relation to principles of fairness,
privacy, and explainability.",2022-11-13 15:58:44,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Sebastian Farquhar, Ryan Carey, Tom Everitt",Path-Specific Objectives for Safer Agent Incentives,,,,,http://arxiv.org/abs/2204.10018v1,"We present a general framework for training safe agents whose naive
incentives are unsafe. As an example, manipulative or deceptive behaviour can
improve rewards but should be avoided. Most approaches fail here: agents
maximize expected return by any means necessary. We formally describe settings
with 'delicate' parts of the state which should not be used as a means to an
end. We then train agents to maximize the causal effect of actions on the
expected return which is not mediated by the delicate parts of state, using
Causal Influence Diagram analysis. The resulting agents have no incentive to
control the delicate state. We further show how our framework unifies and
generalizes existing proposals.",2022-11-13 15:58:45,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,Travis LaCroix,"The Linguistic Blind Spot of Value-Aligned Agency, Natural and Artificial",,,,,http://arxiv.org/abs/2207.00868v1,"The value-alignment problem for artificial intelligence (AI) asks how we can
ensure that the 'values' (i.e., objective functions) of artificial systems are
aligned with the values of humanity. In this paper, I argue that linguistic
communication (natural language) is a necessary condition for robust value
alignment. I discuss the consequences that the truth of this claim would have
for research programmes that attempt to ensure value alignment for AI systems;
or, more loftily, designing robustly beneficial or ethical artificial agents.",2022-11-13 15:58:46,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Susmit Jha, John Rushby",Inferring and Conveying Intentionality: Beyond Numerical Rewards to Logical Intentions,,,,,http://arxiv.org/abs/2207.05058v2,"Shared intentionality is a critical component in developing conscious AI
agents capable of collaboration, self-reflection, deliberation, and reasoning.
We formulate inference of shared intentionality as an inverse reinforcement
learning problem with logical reward specifications. We show how the approach
can infer task descriptions from demonstrations. We also extend our approach to
actively convey intentionality. We demonstrate the approach on a simple
grid-world example.",2022-11-13 15:58:47,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,Kyle Dent,Ethical Considerations for AI Researchers,,,,,http://arxiv.org/abs/2006.07558v1,"Use of artificial intelligence is growing and expanding into applications
that impact people's lives. People trust their technology without really
understanding it or its limitations. There is the potential for harm and we are
already seeing examples of that in the world. AI researchers have an obligation
to consider the impact of intelligent applications they work on. While the
ethics of AI is not clear-cut, there are guidelines we can consider to minimize
the harm we might introduce.",2022-11-13 15:58:47,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2000,"Joseph Y. Halpern, Gerhard Lakemeyer",Multi-Agent Only Knowing,,,,,http://arxiv.org/abs/cs/0001015v1,"Levesque introduced a notion of ``only knowing'', with the goal of capturing
certain types of nonmonotonic reasoning. Levesque's logic dealt with only the
case of a single agent. Recently, both Halpern and Lakemeyer independently
attempted to extend Levesque's logic to the multi-agent case. Although there
are a number of similarities in their approaches, there are some significant
differences. In this paper, we reexamine the notion of only knowing, going back
to first principles. In the process, we simplify Levesque's completeness proof,
and point out some problems with the earlier definitions. This leads us to
reconsider what the properties of only knowing ought to be. We provide an axiom
system that captures our desiderata, and show that it has a semantics that
corresponds to it. The axiom system has an added feature of interest: it
includes a modal operator for satisfiability, and thus provides a complete
axiomatization for satisfiability in the logic K45.",2022-11-13 15:58:48,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2000,Marcus Hutter,A Theory of Universal Artificial Intelligence based on Algorithmic Complexity,,,,,http://arxiv.org/abs/cs/0004001v1,"Decision theory formally solves the problem of rational agents in uncertain
worlds if the true environmental prior probability distribution is known.
Solomonoff's theory of universal induction formally solves the problem of
sequence prediction for unknown prior distribution. We combine both ideas and
get a parameterless theory of universal Artificial Intelligence. We give strong
arguments that the resulting AIXI model is the most intelligent unbiased agent
possible. We outline for a number of problem classes, including sequence
prediction, strategic games, function minimization, reinforcement and
supervised learning, how the AIXI model can formally solve them. The major
drawback of the AIXI model is that it is uncomputable. To overcome this
problem, we construct a modified algorithm AIXI-tl, which is still effectively
more intelligent than any other time t and space l bounded agent. The
computation time of AIXI-tl is of the order tx2^l. Other discussed topics are
formal definitions of intelligence order relations, the horizon problem and
relations of the AIXI theory to other AI approaches.",2022-11-13 15:58:48,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2002,"Pedrito Maynard-Reid II, Daniel Lehmann",Representing and Aggregating Conflicting Beliefs,"Proceedings of the Seventh International Conference on Principles
  of Knowledge Representation and Reasoning (KR 2000), April 2000, pp. 153-164",,,,http://arxiv.org/abs/cs/0203013v1,"We consider the two-fold problem of representing collective beliefs and
aggregating these beliefs. We propose modular, transitive relations for
collective beliefs. They allow us to represent conflicting opinions and they
have a clear semantics. We compare them with the quasi-transitive relations
often used in Social Choice. Then, we describe a way to construct the belief
state of an agent informed by a set of sources of varying degrees of
reliability. This construction circumvents Arrow's Impossibility Theorem in a
satisfactory manner. Finally, we give a simple set-theory-based operator for
combining the information of multiple agents. We show that this operator
satisfies the desirable invariants of idempotence, commutativity, and
associativity, and, thus, is well-behaved when iterated, and we describe a
computationally effective way of computing the resulting belief state.",2022-11-13 15:58:49,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2002,Larry Wos,A Spectrum of Applications of Automated Reasoning,,,,,http://arxiv.org/abs/cs/0205078v1,"The likelihood of an automated reasoning program being of substantial
assistance for a wide spectrum of applications rests with the nature of the
options and parameters it offers on which to base needed strategies and
methodologies. This article focuses on such a spectrum, featuring W. McCune's
program OTTER, discussing widely varied successes in answering open questions,
and touching on some of the strategies and methodologies that played a key
role. The applications include finding a first proof, discovering single
axioms, locating improved axiom systems, and simplifying existing proofs. The
last application is directly pertinent to the recently found (by R. Thiele)
Hilbert's twenty-fourth problem--which is extremely amenable to attack with the
appropriate automated reasoning program--a problem concerned with proof
simplification. The methodologies include those for seeking shorter proofs and
for finding proofs that avoid unwanted lemmas or classes of term, a specific
option for seeking proofs with smaller equational or formula complexity, and a
different option to address the variable richness of a proof. The type of proof
one obtains with the use of OTTER is Hilbert-style axiomatic, including details
that permit one sometimes to gain new insights. We include questions still open
and challenges that merit consideration.",2022-11-13 15:58:49,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2005,"Joseph Y. Halpern, Riccardo Pucella",Evidence with Uncertain Likelihoods,,,,,http://arxiv.org/abs/cs/0510079v2,"An agent often has a number of hypotheses, and must choose among them based
on observations, or outcomes of experiments. Each of these observations can be
viewed as providing evidence for or against various hypotheses. All the
attempts to formalize this intuition up to now have assumed that associated
with each hypothesis h there is a likelihood function \mu_h, which is a
probability measure that intuitively describes how likely each observation is,
conditional on h being the correct hypothesis. We consider an extension of this
framework where there is uncertainty as to which of a number of likelihood
functions is appropriate, and discuss how one formal approach to defining
evidence, which views evidence as a function from priors to posteriors, can be
generalized to accommodate this uncertainty.",2022-11-13 15:58:50,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2007,Marcus Hutter,Universal Algorithmic Intelligence: A mathematical top->down approach,"In Artificial General Intelligence, Springer (2007) 227-290",,,,http://arxiv.org/abs/cs/0701125v1,"Sequential decision theory formally solves the problem of rational agents in
uncertain worlds if the true environmental prior probability distribution is
known. Solomonoff's theory of universal induction formally solves the problem
of sequence prediction for unknown prior distribution. We combine both ideas
and get a parameter-free theory of universal Artificial Intelligence. We give
strong arguments that the resulting AIXI model is the most intelligent unbiased
agent possible. We outline how the AIXI model can formally solve a number of
problem classes, including sequence prediction, strategic games, function
minimization, reinforcement and supervised learning. The major drawback of the
AIXI model is that it is uncomputable. To overcome this problem, we construct a
modified algorithm AIXItl that is still effectively more intelligent than any
other time t and length l bounded agent. The computation time of AIXItl is of
the order t x 2^l. The discussion includes formal definitions of intelligence
order relations, the horizon problem and relations of the AIXI theory to other
AI approaches.",2022-11-13 15:58:50,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2009,"Joseph Y. Halpern, Leandro Rego",Reasoning About Knowledge of Unawareness Revisited,,,,,http://arxiv.org/abs/0906.4321v1,"In earlier work, we proposed a logic that extends the Logic of General
Awareness of Fagin and Halpern [1988] by allowing quantification over primitive
propositions. This makes it possible to express the fact that an agent knows
that there are some facts of which he is unaware. In that logic, it is not
possible to model an agent who is uncertain about whether he is aware of all
formulas. To overcome this problem, we keep the syntax of the earlier paper,
but allow models where, with each world, a possibly different language is
associated. We provide a sound and complete axiomatization for this logic and
show that, under natural assumptions, the quantifier-free fragment of the logic
is characterized by exactly the same axioms as the logic of Heifetz, Meier, and
Schipper [2008].",2022-11-13 15:58:51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2010,"David Tolpin, Solomon Eyal Shimony",Rational Value of Information Estimation for Measurement Selection,,,,,http://arxiv.org/abs/1003.5305v2,"Computing value of information (VOI) is a crucial task in various aspects of
decision-making under uncertainty, such as in meta-reasoning for search; in
selecting measurements to make, prior to choosing a course of action; and in
managing the exploration vs. exploitation tradeoff. Since such applications
typically require numerous VOI computations during a single run, it is
essential that VOI be computed efficiently. We examine the issue of anytime
estimation of VOI, as frequently it suffices to get a crude estimate of the
VOI, thus saving considerable computational resources. As a case study, we
examine VOI estimation in the measurement selection problem. Empirical
evaluation of the proposed scheme in this domain shows that computational
resources can indeed be significantly reduced, at little cost in expected
rewards achieved in the overall decision problem.",2022-11-13 15:58:51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2010,"Jan Feyereisl, Uwe Aickelin",ToLeRating UR-STD,"Proceedings of the 2nd International Conference on Emerging
  Security Information, Systems and Technologies, Cap Esterel, France, p
  287-293, 2008",,,,http://arxiv.org/abs/1006.1563v1,"A new emerging paradigm of Uncertain Risk of Suspicion, Threat and Danger,
observed across the field of information security, is described. Based on this
paradigm a novel approach to anomaly detection is presented. Our approach is
based on a simple yet powerful analogy from the innate part of the human immune
system, the Toll-Like Receptors. We argue that such receptors incorporated as
part of an anomaly detector enhance the detector's ability to distinguish
normal and anomalous behaviour. In addition we propose that Toll-Like Receptors
enable the classification of detected anomalies based on the types of attacks
that perpetrate the anomalous behaviour. Classification of such type is either
missing in existing literature or is not fit for the purpose of reducing the
burden of an administrator of an intrusion detection system. For our model to
work, we propose the creation of a taxonomy of the digital Acytota, based on
which our receptors are created.",2022-11-13 15:58:52,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2010,"Alejandra Gonzalez-Beltran, Ben Tagger, Anthony Finkelstein",Ontology-based Queries over Cancer Data,,,,,http://arxiv.org/abs/1012.5506v1,"The ever-increasing amount of data in biomedical research, and in cancer
research in particular, needs to be managed to support efficient data access,
exchange and integration. Existing software infrastructures, such caGrid,
support access to distributed information annotated with a domain ontology.
However, caGrid's current querying functionality depends on the structure of
individual data resources without exploiting the semantic annotations. In this
paper, we present the design and development of an ontology-based querying
functionality that consists of: the generation of OWL2 ontologies from the
underlying data resources metadata and a query rewriting and translation
process based on reasoning, which converts a query at the domain ontology level
into queries at the software infrastructure level. We present a detailed
analysis of our approach as well as an extensive performance evaluation. While
the implementation and evaluation was performed for the caGrid infrastructure,
the approach could be applicable to other model and metadata-driven
environments for data sharing.",2022-11-13 15:58:52,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2010,Wan Ahmad Tajuddin Wan Abdullah,Looking for plausibility,,,,,http://arxiv.org/abs/1012.5705v1,"In the interpretation of experimental data, one is actually looking for
plausible explanations. We look for a measure of plausibility, with which we
can compare different possible explanations, and which can be combined when
there are different sets of data. This is contrasted to the conventional
measure for probabilities as well as to the proposed measure of possibilities.
We define what characteristics this measure of plausibility should have.
  In getting to the conception of this measure, we explore the relation of
plausibility to abductive reasoning, and to Bayesian probabilities. We also
compare with the Dempster-Schaefer theory of evidence, which also has its own
definition for plausibility. Abduction can be associated with biconditionality
in inference rules, and this provides a platform to relate to the
Collins-Michalski theory of plausibility. Finally, using a formalism for wiring
logic onto Hopfield neural networks, we ask if this is relevant in obtaining
this measure.",2022-11-13 15:58:53,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2011,Peter de Blanc,Ontological Crises in Artificial Agents' Value Systems,,,,,http://arxiv.org/abs/1105.3821v1,"Decision-theoretic agents predict and evaluate the results of their actions
using a model, or ontology, of their environment. An agent's goal, or utility
function, may also be specified in terms of the states of, or entities within,
its ontology. If the agent may upgrade or replace its ontology, it faces a
crisis: the agent's original goal may not be well-defined with respect to its
new ontology. This crisis must be resolved before the agent can make plans
towards achieving its goals.
  We discuss in this paper which sorts of agents will undergo ontological
crises and why we may want to create such agents. We present some concrete
examples, and argue that a well-defined procedure for resolving ontological
crises is needed. We point to some possible approaches to solving this problem,
and evaluate these methods on our examples.",2022-11-13 15:58:54,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2011,M. A. Walker,An Application of Reinforcement Learning to Dialogue Strategy Selection in a Spoken Dialogue System for Email,"Journal Of Artificial Intelligence Research, Volume 12, pages
  387-416, 2000",,,10.1613/jair.713,http://arxiv.org/abs/1106.0241v1,"This paper describes a novel method by which a spoken dialogue system can
learn to choose an optimal dialogue strategy from its experience interacting
with human users. The method is based on a combination of reinforcement
learning and performance modeling of spoken dialogue systems. The reinforcement
learning component applies Q-learning (Watkins, 1989), while the performance
modeling component applies the PARADISE evaluation framework (Walker et al.,
1997) to learn the performance function (reward) used in reinforcement
learning. We illustrate the method with a spoken dialogue system named ELVIS
(EmaiL Voice Interactive System), that supports access to email over the phone.
We conduct a set of experiments for training an optimal dialogue strategy on a
corpus of 219 dialogues in which human users interact with ELVIS over the
phone. We then test that strategy on a corpus of 18 dialogues. We show that
ELVIS can learn to optimize its strategy selection for agent initiative, for
reading messages, and for summarizing email folders.",2022-11-13 15:58:54,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2012,"Joseph Y. Halpern, Willemien Kets",Ambiguous Language and Differences in Beliefs,,,,,http://arxiv.org/abs/1203.0699v1,"Standard models of multi-agent modal logic do not capture the fact that
information is often ambiguous, and may be interpreted in different ways by
different agents. We propose a framework that can model this, and consider
different semantics that capture different assumptions about the agents'
beliefs regarding whether or not there is ambiguity. We consider the impact of
ambiguity on a seminal result in economics: Aumann's result saying that agents
with a common prior cannot agree to disagree. This result is known not to hold
if agents do not have a common prior; we show that it also does not hold in the
presence of ambiguity. We then consider the tradeoff between assuming a common
interpretation (i.e., no ambiguity) and a common prior (i.e., shared initial
beliefs).",2022-11-13 15:58:55,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2012,Dimiter Dobrev,Formal Definition of AI,"International Journal ""Information Theories & Applications"",
  vol.12, Number 3, 2005, pp.277-285",,,,http://arxiv.org/abs/1209.4838v1,"A definition of Artificial Intelligence was proposed in [1] but this
definition was not absolutely formal at least because the word ""Human"" was
used. In this paper we will formalize the definition from [1]. The biggest
problem in this definition was that the level of intelligence of AI is compared
to the intelligence of a human being. In order to change this we will introduce
some parameters to which AI will depend. One of this parameters will be the
level of intelligence and we will define one AI to each level of intelligence.
We assume that for some level of intelligence the respective AI will be more
intelligent than a human being. Nevertheless, we cannot say which is this level
because we cannot calculate its exact value.",2022-11-13 15:58:55,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2013,"Eric J. Horvitz, Andy Jacobs, David Hovel",Attention-Sensitive Alerting,,,,,http://arxiv.org/abs/1301.6707v1,"We introduce utility-directed procedures for mediating the flow of
potentially distracting alerts and communications to computer users. We present
models and inference procedures that balance the context-sensitive costs of
deferring alerts with the cost of interruption. We describe the challenge of
reasoning about such costs under uncertainty via an analysis of user activity
and the content of notifications. After introducing principles of
attention-sensitive alerting, we focus on the problem of guiding alerts about
email messages. We dwell on the problem of inferring the expected criticality
of email and discuss work on the Priorities system, centering on prioritizing
email by criticality and modulating the communication of notifications to users
about the presence and nature of incoming email.",2022-11-13 15:58:56,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2013,Lonnie Chrisman,Independence with Lower and Upper Probabilities,,,,,http://arxiv.org/abs/1302.3568v1,"It is shown that the ability of the interval probability representation to
capture epistemological independence is severely limited. Two events are
epistemologically independent if knowledge of the first event does not alter
belief (i.e., probability bounds) about the second. However, independence in
this form can only exist in a 2-monotone probability function in degenerate
cases i.e., if the prior bounds are either point probabilities or entirely
vacuous. Additional limitations are characterized for other classes of lower
probabilities as well. It is argued that these phenomena are simply a matter of
interpretation. They appear to be limitations when one interprets probability
bounds as a measure of epistemological indeterminacy (i.e., uncertainty arising
from a lack of knowledge), but are exactly as one would expect when probability
intervals are interpreted as representations of ontological indeterminacy
(indeterminacy introduced by structural approximations). The ontological
interpretation is introduced and discussed.",2022-11-13 15:58:56,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2013,David L. Poole,Exploiting the Rule Structure for Decision Making within the Independent Choice Logic,,,,,http://arxiv.org/abs/1302.4978v1,"This paper introduces the independent choice logic, and in particular the
""single agent with nature"" instance of the independent choice logic, namely
ICLdt. This is a logical framework for decision making uncertainty that extends
both logic programming and stochastic models such as influence diagrams. This
paper shows how the representation of a decision problem within the independent
choice logic can be exploited to cut down the combinatorics of dynamic
programming. One of the main problems with influence diagram evaluation
techniques is the need to optimise a decision for all values of the 'parents'
of a decision variable. In this paper we show how the rule based nature of the
ICLdt can be exploited so that we only make distinctions in the values of the
information available for a decision that will make a difference to utility.",2022-11-13 15:58:57,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2013,Gregory M. Provan,Tradeoffs in Constructing and Evaluating Temporal Influence Diagrams,,,,,http://arxiv.org/abs/1303.1458v1,"This paper addresses the tradeoffs which need to be considered in reasoning
using probabilistic network representations, such as Influence Diagrams (IDs).
In particular, we examine the tradeoffs entailed in using Temporal Influence
Diagrams (TIDs) which adequately capture the temporal evolution of a dynamic
system without prohibitive data and computational requirements. Three
approaches for TID construction which make different tradeoffs are examined:
(1) tailoring the network at each time interval to the data available (rather
then just copying the original Bayes Network for all time intervals); (2)
modeling the evolution of a parsimonious subset of variables (rather than all
variables); and (3) model selection approaches, which seek to minimize some
measure of the predictive accuracy of the model without introducing too many
parameters, which might cause ""overfitting"" of the model. Methods of evaluating
the accuracy/efficiency of the tradeoffs are proposed.",2022-11-13 15:58:57,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2013,"Adam J. Grove, Daphne Koller",Probability Estimation in Face of Irrelevant Information,,,,,http://arxiv.org/abs/1303.5719v1,"In this paper, we consider one aspect of the problem of applying decision
theory to the design of agents that learn how to make decisions under
uncertainty. This aspect concerns how an agent can estimate probabilities for
the possible states of the world, given that it only makes limited observations
before committing to a decision. We show that the naive application of
statistical tools can be improved upon if the agent can determine which of his
observations are truly relevant to the estimation problem at hand. We give a
framework in which such determinations can be made, and define an estimation
procedure to use them. Our framework also suggests several extensions, which
show how additional knowledge can be used to improve tile estimation procedure
still further.",2022-11-13 15:58:58,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2013,"David Heckerman, Eric J. Horvitz, Blackford Middleton",An Approximate Nonmyopic Computation for Value of Information,,,,,http://arxiv.org/abs/1303.5720v2,"Value-of-information analyses provide a straightforward means for selecting
the best next observation to make, and for determining whether it is better to
gather additional information or to act immediately. Determining the next best
test to perform, given a state of uncertainty about the world, requires a
consideration of the value of making all possible sequences of observations. In
practice, decision analysts and expert-system designers have avoided the
intractability of exact computation of the value of information by relying on a
myopic approximation. Myopic analyses are based on the assumption that only one
additional test will be performed, even when there is an opportunity to make a
large number of observations. We present a nonmyopic approximation for value of
information that bypasses the traditional myopic analyses by exploiting the
statistical properties of large samples.",2022-11-13 15:58:59,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2013,"Paul E. Lehner, Theresa M. Mullin, Marvin S. Cohen",When Should a Decision Maker Ignore the Advice of a Decision Aid?,,,,,http://arxiv.org/abs/1304.1515v1,"This paper argues that the principal difference between decision aids and
most other types of information systems is the greater reliance of decision
aids on fallible algorithms--algorithms that sometimes generate incorrect
advice. It is shown that interactive problem solving with a decision aid that
is based on a fallible algorithm can easily result in aided performance which
is poorer than unaided performance, even if the algorithm, by itself, performs
significantly better than the unaided decision maker. This suggests that unless
certain conditions are satisfied, using a decision aid as an aid is
counterproductive. Some conditions under which a decision aid is best used as
an aid are derived.",2022-11-13 15:58:59,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2013,Spencer Star,Generating Decision Structures and Causal Explanations for Decision Making,,,,,http://arxiv.org/abs/1304.2376v1,"This paper examines two related problems that are central to developing an
autonomous decision-making agent, such as a robot. Both problems require
generating structured representafions from a database of unstructured
declarative knowledge that includes many facts and rules that are irrelevant in
the problem context. The first problem is how to generate a well structured
decision problem from such a database. The second problem is how to generate,
from the same database, a well-structured explanation of why some possible
world occurred. In this paper it is shown that the problem of generating the
appropriate decision structure or explanation is intractable without
introducing further constraints on the knowledge in the database. The paper
proposes that the problem search space can be constrained by adding knowledge
to the database about causal relafions between events. In order to determine
the causal knowledge that would be most useful, causal theories for
deterministic and indeterministic universes are proposed. A program that uses
some of these causal constraints has been used to generate explanations about
faulty plans. The program shows the expected increase in efficiency as the
causal constraints are introduced.",2022-11-13 15:59:00,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2013,Eric J. Horvitz,Reasoning About Beliefs and Actions Under Computational Resource Constraints,,,,,http://arxiv.org/abs/1304.2759v1,"Although many investigators affirm a desire to build reasoning systems that
behave consistently with the axiomatic basis defined by probability theory and
utility theory, limited resources for engineering and computation can make a
complete normative analysis impossible. We attempt to move discussion beyond
the debate over the scope of problems that can be handled effectively to cases
where it is clear that there are insufficient computational resources to
perform an analysis deemed as complete. Under these conditions, we stress the
importance of considering the expected costs and benefits of applying
alternative approximation procedures and heuristics for computation and
knowledge acquisition. We discuss how knowledge about the structure of user
utility can be used to control value tradeoffs for tailoring inference to
alternative contexts. We address the notion of real-time rationality, focusing
on the application of knowledge about the expected timewise-refinement
abilities of reasoning strategies to balance the benefits of additional
computation with the costs of acting with a partial result. We discuss the
benefits of applying decision theory to control the solution of difficult
problems given limitations and uncertainty in reasoning resources.",2022-11-13 15:59:00,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2013,"Trong Nghia Hoang, Kian Hsiang Low",Interactive POMDP Lite: Towards Practical Planning to Predict and Exploit Intentions for Interacting with Self-Interested Agents,,,,,http://arxiv.org/abs/1304.5159v1,"A key challenge in non-cooperative multi-agent systems is that of developing
efficient planning algorithms for intelligent agents to interact and perform
effectively among boundedly rational, self-interested agents (e.g., humans).
The practicality of existing works addressing this challenge is being
undermined due to either the restrictive assumptions of the other agents'
behavior, the failure in accounting for their rationality, or the prohibitively
expensive cost of modeling and predicting their intentions. To boost the
practicality of research in this field, we investigate how intention prediction
can be efficiently exploited and made practical in planning, thereby leading to
efficient intention-aware planning frameworks capable of predicting the
intentions of other agents and acting optimally with respect to their predicted
intentions. We show that the performance losses incurred by the resulting
planning policies are linearly bounded by the error of intention prediction.
Empirical evaluations through a series of stochastic games demonstrate that our
policies can achieve better and more robust performance than the
state-of-the-art algorithms.",2022-11-13 15:59:01,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2013,Ernest Davis,The Relevance of Proofs of the Rationality of Probability Theory to Automated Reasoning and Cognitive Models,,,,,http://arxiv.org/abs/1310.1328v1,"A number of well-known theorems, such as Cox's theorem and de Finetti's
theorem. prove that any model of reasoning with uncertain information that
satisfies specified conditions of ""rationality"" must satisfy the axioms of
probability theory. I argue here that these theorems do not in themselves
demonstrate that probabilistic models are in fact suitable for any specific
task in automated reasoning or plausible for cognitive models. First, the
theorems only establish that there exists some probabilistic model; they do not
establish that there exists a useful probabilistic model, i.e. one with a
tractably small number of numerical parameters and a large number of
independence assumptions. Second, there are in general many different
probabilistic models for a given situation, many of which may be far more
irrational, in the usual sense of the term, than a model that violates the
axioms of probability theory. I illustrate this second point with an extended
examples of two tasks of induction, of a similar structure, where the
reasonable probabilistic models are very different.",2022-11-13 15:59:01,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2013,"Christoph Salge, Cornelius Glackin, Daniel Polani",Empowerment -- an Introduction,,,,,http://arxiv.org/abs/1310.1863v2,"This book chapter is an introduction to and an overview of the
information-theoretic, task independent utility function ""Empowerment"", which
is defined as the channel capacity between an agent's actions and an agent's
sensors. It quantifies how much influence and control an agent has over the
world it can perceive. This book chapter discusses the general idea behind
empowerment as an intrinsic motivation and showcases several previous
applications of empowerment to demonstrate how empowerment can be applied to
different sensor-motor configuration, and how the same formalism can lead to
different observed behaviors. Furthermore, we also present a fast approximation
for empowerment in the continuous domain.",2022-11-13 15:59:02,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2013,"Jie Fan, Yanjing Wang, Hans van Ditmarsch",Knowing Whether,,,,,http://arxiv.org/abs/1312.0144v3,"Knowing whether a proposition is true means knowing that it is true or
knowing that it is false. In this paper, we study logics with a modal operator
Kw for knowing whether but without a modal operator K for knowing that. This
logic is not a normal modal logic, because we do not have Kw (phi -> psi) ->
(Kw phi -> Kw psi). Knowing whether logic cannot define many common frame
properties, and its expressive power less than that of basic modal logic over
classes of models without reflexivity. These features make axiomatizing knowing
whether logics non-trivial. We axiomatize knowing whether logic over various
frame classes. We also present an extension of knowing whether logic with
public announcement operators and we give corresponding reduction axioms for
that. We compare our work in detail to two recent similar proposals.",2022-11-13 15:59:02,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2014,"Dongmo Zhang, Michael Thielsher",Representing and Reasoning about Game Strategies,,,,,http://arxiv.org/abs/1407.5380v1,"As a contribution to the challenge of building game-playing AI systems, we
develop and analyse a formal language for representing and reasoning about
strategies. Our logical language builds on the existing general Game
Description Language (GDL) and extends it by a standard modality for linear
time along with two dual connectives to express preferences when combining
strategies. The semantics of the language is provided by a standard
state-transition model. As such, problems that require reasoning about games
can be solved by the standard methods for reasoning about actions and change.
We also endow the language with a specific semantics by which strategy formulas
are understood as move recommendations for a player. To illustrate how our
formalism supports automated reasoning about strategies, we demonstrate two
example methods of implementation\/: first, we formalise the semantic
interpretation of our language in conjunction with game rules and strategy
rules in the Situation Calculus; second, we show how the reasoning problem can
be solved with Answer Set Programming.",2022-11-13 15:59:03,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2014,"Joseph Y. Halpern, Riccardo Pucella",Evidence with Uncertain Likelihoods,,,,,http://arxiv.org/abs/1407.7189v1,"An agent often has a number of hypotheses, and must choose among them based
on observations, or outcomes of experiments. Each of these observations can be
viewed as providing evidence for or against various hypotheses. All the
attempts to formalize this intuition up to now have assumed that associated
with each hypothesis h there is a likelihood function {\mu}h, which is a
probability measure that intuitively describes how likely each observation is,
conditional on h being the correct hypothesis. We consider an extension of this
framework where there is uncertainty as to which of a number of likelihood
functions is appropriate, and discuss how one formal approach to defining
evidence, which views evidence as a function from priors to posteriors, can be
generalized to accommodate this uncertainty.",2022-11-13 15:59:03,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2014,Brian Tomasik,Do Artificial Reinforcement-Learning Agents Matter Morally?,,,,,http://arxiv.org/abs/1410.8233v1,"Artificial reinforcement learning (RL) is a widely used technique in
artificial intelligence that provides a general method for training agents to
perform a wide variety of behaviours. RL as used in computer science has
striking parallels to reward and punishment learning in animal and human
brains. I argue that present-day artificial RL agents have a very small but
nonzero degree of ethical importance. This is particularly plausible for views
according to which sentience comes in degrees based on the abilities and
complexities of minds, but even binary views on consciousness should assign
nonzero probability to RL programs having morally relevant experiences. While
RL programs are not a top ethical priority today, they may become more
significant in the coming decades as RL is increasingly applied to industry,
robotics, video games, and other areas. I encourage scientists, philosophers,
and citizens to begin a conversation about our ethical duties to reduce the
harm that we inflict on powerless, voiceless RL agents.",2022-11-13 15:59:04,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2015,"Louise A. Dennis, Michael Fisher, Alan F. T. Winfield",Towards Verifiably Ethical Robot Behaviour,,,,,http://arxiv.org/abs/1504.03592v1,"Ensuring that autonomous systems work ethically is both complex and
difficult. However, the idea of having an additional `governor' that assesses
options the system has, and prunes them to select the most ethical choices is
well understood. Recent work has produced such a governor consisting of a
`consequence engine' that assesses the likely future outcomes of actions then
applies a Safety/Ethical logic to select actions. Although this is appealing,
it is impossible to be certain that the most ethical options are actually
taken. In this paper we extend and apply a well-known agent verification
approach to our consequence engine, allowing us to verify the correctness of
its ethical decision-making.",2022-11-13 15:59:04,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2015,"Christopher H. Lin, Andrey Kolobov, Ece Kamar, Eric Horvitz",Metareasoning for Planning Under Uncertainty,,,,,http://arxiv.org/abs/1505.00399v1,"The conventional model for online planning under uncertainty assumes that an
agent can stop and plan without incurring costs for the time spent planning.
However, planning time is not free in most real-world settings. For example, an
autonomous drone is subject to nature's forces, like gravity, even while it
thinks, and must either pay a price for counteracting these forces to stay in
place, or grapple with the state change caused by acquiescing to them. Policy
optimization in these settings requires metareasoning---a process that trades
off the cost of planning and the potential policy improvement that can be
achieved. We formalize and analyze the metareasoning problem for Markov
Decision Processes (MDPs). Our work subsumes previously studied special cases
of metareasoning and shows that in the general case, metareasoning is at most
polynomially harder than solving MDPs with any given algorithm that disregards
the cost of thinking. For reasons we discuss, optimal general metareasoning
turns out to be impractical, motivating approximations. We present approximate
metareasoning procedures which rely on special properties of the BRTDP planning
algorithm and explore the effectiveness of our methods on a variety of
problems.",2022-11-13 15:59:05,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2015,"Daniel Raggi, Alan Bundy, Gudmund Grov, Alison Pease",Automating change of representation for proofs in discrete mathematics,,,,,http://arxiv.org/abs/1505.02449v1,"Representation determines how we can reason about a specific problem.
Sometimes one representation helps us find a proof more easily than others.
Most current automated reasoning tools focus on reasoning within one
representation. There is, therefore, a need for the development of better tools
to mechanise and automate formal and logically sound changes of representation.
  In this paper we look at examples of representational transformations in
discrete mathematics, and show how we have used Isabelle's Transfer tool to
automate the use of these transformations in proofs. We give a brief overview
of a general theory of transformations that we consider appropriate for
thinking about the matter, and we explain how it relates to the Transfer
package. We show our progress towards developing a general tactic that
incorporates the automatic search for representation within the proving
process.",2022-11-13 15:59:05,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2015,"Andreas Falkner, Anna Ryabokon, Gottfried Schenner, Kostyantyn Shchekotykhin",OOASP: Connecting Object-oriented and Logic Programming,,,,,http://arxiv.org/abs/1508.03032v1,"Most of contemporary software systems are implemented using an
object-oriented approach. Modeling phases -- during which software engineers
analyze requirements to the future system using some modeling language -- are
an important part of the development process, since modeling errors are often
hard to recognize and correct.
  In this paper we present a framework which allows the integration of Answer
Set Programming into the object-oriented software development process. OOASP
supports reasoning about object-oriented software models and their
instantiations. Preliminary results of the OOASP application in CSL Studio,
which is a Siemens internal modeling environment for product configurators,
show that it can be used as a lightweight approach to verify, create and
transform instantiations of object models at runtime and to support the
software development process during design and testing.",2022-11-13 15:59:06,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2015,Miles Brundage,Modeling Progress in AI,,,,,http://arxiv.org/abs/1512.05849v1,"Participants in recent discussions of AI-related issues ranging from
intelligence explosion to technological unemployment have made diverse claims
about the nature, pace, and drivers of progress in AI. However, these theories
are rarely specified in enough detail to enable systematic evaluation of their
assumptions or to extrapolate progress quantitatively, as is often done with
some success in other technological domains. After reviewing relevant
literatures and justifying the need for more rigorous modeling of AI progress,
this paper contributes to that research program by suggesting ways to account
for the relationship between hardware speed increases and algorithmic
improvements in AI, the role of human inputs in enabling AI capabilities, and
the relationships between different sub-fields of AI. It then outlines ways of
tailoring AI progress models to generate insights on the specific issue of
technological unemployment, and outlines future directions for research on AI
progress.",2022-11-13 15:59:07,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2015,"Alexander Kott, Michael Ownby",Toward a Research Agenda in Adversarial Reasoning: Computational Approaches to Anticipating the Opponent's Intent and Actions,,,,,http://arxiv.org/abs/1512.07943v1,"This paper defines adversarial reasoning as computational approaches to
inferring and anticipating an enemy's perceptions, intents and actions. It
argues that adversarial reasoning transcends the boundaries of game theory and
must also leverage such disciplines as cognitive modeling, control theory, AI
planning and others. To illustrate the challenges of applying adversarial
reasoning to real-world problems, the paper explores the lessons learned in the
CADET - a battle planning system that focuses on brigade-level ground
operations and involves adversarial reasoning. From this example of current
capabilities, the paper proceeds to describe RAID - a DARPA program that aims
to build capabilities in adversarial reasoning, and how such capabilities would
address practical requirements in Defense and other application areas.",2022-11-13 15:59:07,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2016,Toby Walsh,The Singularity May Never Be Near,,,,,http://arxiv.org/abs/1602.06462v1,"There is both much optimism and pessimism around artificial intelligence (AI)
today. The optimists are investing millions of dollars, and even in some cases
billions of dollars into AI. The pessimists, on the other hand, predict that AI
will end many things: jobs, warfare, and even the human race. Both the
optimists and the pessimists often appeal to the idea of a technological
singularity, a point in time where machine intelligence starts to run away, and
a new, more intelligent species starts to inhabit the earth. If the optimists
are right, this will be a moment that fundamentally changes our economy and our
society. If the pessimists are right, this will be a moment that also
fundamentally changes our economy and our society. It is therefore very
worthwhile spending some time deciding if either of them might be right.",2022-11-13 15:59:08,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2016,David J. Jilk,Limits to Verification and Validation of Agentic Behavior,,,,,http://arxiv.org/abs/1604.06963v2,"Verification and validation of agentic behavior have been suggested as
important research priorities in efforts to reduce risks associated with the
creation of general artificial intelligence (Russell et al 2015). In this paper
we question the appropriateness of using language of certainty with respect to
efforts to manage that risk. We begin by establishing a very general formalism
to characterize agentic behavior and to describe standards of acceptable
behavior. We show that determination of whether an agent meets any particular
standard is not computable. We discuss the extent of the burden associated with
verification by manual proof and by automated behavioral governance. We show
that to ensure decidability of the behavioral standard itself, one must further
limit the capabilities of the agent. We then demonstrate that if our concerns
relate to outcomes in the physical world, attempts at validation are futile.
Finally, we show that layered architectures aimed at making these challenges
tractable mistakenly equate intentions with actions or outcomes, thereby
failing to provide any guarantees. We conclude with a discussion of why
language of certainty should be eradicated from the conversation about the
safety of general artificial intelligence.",2022-11-13 15:59:09,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2016,"Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, Dan Mané",Concrete Problems in AI Safety,,,,,http://arxiv.org/abs/1606.06565v2,"Rapid progress in machine learning and artificial intelligence (AI) has
brought increasing attention to the potential impacts of AI technologies on
society. In this paper we discuss one such potential impact: the problem of
accidents in machine learning systems, defined as unintended and harmful
behavior that may emerge from poor design of real-world AI systems. We present
a list of five practical research problems related to accident risk,
categorized according to whether the problem originates from having the wrong
objective function (""avoiding side effects"" and ""avoiding reward hacking""), an
objective function that is too expensive to evaluate frequently (""scalable
supervision""), or undesirable behavior during the learning process (""safe
exploration"" and ""distributional shift""). We review previous work in these
areas as well as suggesting research directions with a focus on relevance to
cutting-edge AI systems. Finally, we consider the high-level question of how to
think most productively about the safety of forward-looking applications of AI.",2022-11-13 15:59:09,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2016,"I. Dan Melamed, Nobal B. Niraula",Towards A Virtual Assistant That Can Be Taught New Tasks In Any Domain By Its End-Users,,,,,http://arxiv.org/abs/1607.00061v1,"The challenge stated in the title can be divided into two main problems. The
first problem is to reliably mimic the way that users interact with user
interfaces. The second problem is to build an instructible agent, i.e. one that
can be taught to execute tasks expressed as previously unseen natural language
commands. This paper proposes a solution to the second problem, a system we
call Helpa. End-users can teach Helpa arbitrary new tasks whose level of
complexity is similar to the tasks available from today's most popular virtual
assistants. Teaching Helpa does not involve any programming. Instead, users
teach Helpa by providing just one example of a command paired with a
demonstration of how to execute that command. Helpa does not rely on any
pre-existing domain-specific knowledge. It is therefore completely
domain-independent. Our usability study showed that end-users can teach Helpa
many new tasks in less than a minute each, often much less.",2022-11-13 15:59:10,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2016,"Gavin Rens, Deshendran Moodley",A Hybrid POMDP-BDI Agent Architecture with Online Stochastic Planning and Plan Caching,,,,,http://arxiv.org/abs/1607.00656v1,"This article presents an agent architecture for controlling an autonomous
agent in stochastic environments. The architecture combines the partially
observable Markov decision process (POMDP) model with the
belief-desire-intention (BDI) framework. The Hybrid POMDP-BDI agent
architecture takes the best features from the two approaches, that is, the
online generation of reward-maximizing courses of action from POMDP theory, and
sophisticated multiple goal management from BDI theory. We introduce the
advances made since the introduction of the basic architecture, including (i)
the ability to pursue multiple goals simultaneously and (ii) a plan library for
storing pre-written plans and for storing recently generated plans for future
reuse. A version of the architecture without the plan library is implemented
and is evaluated using simulations. The results of the simulation experiments
indicate that the approach is feasible.",2022-11-13 15:59:10,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2016,"Michael Ownby, Alexander Kott",Predicting Enemy's Actions Improves Commander Decision-Making,,,,,http://arxiv.org/abs/1607.06759v1,"The Defense Advanced Research Projects Agency (DARPA) Real-time Adversarial
Intelligence and Decision-making (RAID) program is investigating the
feasibility of ""reading the mind of the enemy"" - to estimate and anticipate, in
real-time, the enemy's likely goals, deceptions, actions, movements and
positions. This program focuses specifically on urban battles at echelons of
battalion and below. The RAID program leverages approximate game-theoretic and
deception-sensitive algorithms to provide real-time enemy estimates to a
tactical commander. A key hypothesis of the program is that these predictions
and recommendations will make the commander more effective, i.e. he should be
able to achieve his operational goals safer, faster, and more efficiently.
Realistic experimentation and evaluation drive the development process using
human-in-the-loop wargames to compare humans and the RAID system. Two
experiments were conducted in 2005 as part of Phase I to determine if the RAID
software could make predictions and recommendations as effectively and
accurately as a 4-person experienced staff. This report discusses the
intriguing and encouraging results of these first two experiments conducted by
the RAID program. It also provides details about the experiment environment and
methodology that were used to demonstrate and prove the research goals.",2022-11-13 15:59:11,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2016,"Ulle Endriss, Umberto Grandi",Graph Aggregation,"Artificial Intelligence, Volume 245, pages 86-114, 2017",,,10.1016/j.artint.2017.01.001,http://arxiv.org/abs/1609.03765v1,"Graph aggregation is the process of computing a single output graph that
constitutes a good compromise between several input graphs, each provided by a
different source. One needs to perform graph aggregation in a wide variety of
situations, e.g., when applying a voting rule (graphs as preference orders),
when consolidating conflicting views regarding the relationships between
arguments in a debate (graphs as abstract argumentation frameworks), or when
computing a consensus between several alternative clusterings of a given
dataset (graphs as equivalence relations). In this paper, we introduce a formal
framework for graph aggregation grounded in social choice theory. Our focus is
on understanding which properties shared by the individual input graphs will
transfer to the output graph returned by a given aggregation rule. We consider
both common properties of graphs, such as transitivity and reflexivity, and
arbitrary properties expressible in certain fragments of modal logic. Our
results establish several connections between the types of properties preserved
under aggregation and the choice-theoretic axioms satisfied by the rules used.
The most important of these results is a powerful impossibility theorem that
generalises Arrow's seminal result for the aggregation of preference orders to
a large collection of different types of graphs.",2022-11-13 15:59:12,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2016,"Tathagata Chakraborti, Kartik Talamadupula, Kshitij P. Fadnis, Murray Campbell, Subbarao Kambhampati",UbuntuWorld 1.0 LTS - A Platform for Automated Problem Solving & Troubleshooting in the Ubuntu OS,,,,,http://arxiv.org/abs/1609.08524v2,"In this paper, we present UbuntuWorld 1.0 LTS - a platform for developing
automated technical support agents in the Ubuntu operating system.
Specifically, we propose to use the Bash terminal as a simulator of the Ubuntu
environment for a learning-based agent and demonstrate the usefulness of
adopting reinforcement learning (RL) techniques for basic problem solving and
troubleshooting in this environment. We provide a plug-and-play interface to
the simulator as a python package where different types of agents can be
plugged in and evaluated, and provide pathways for integrating data from online
support forums like AskUbuntu into an automated agent's learning process.
Finally, we show that the use of this data significantly improves the agent's
learning efficiency. We believe that this platform can be adopted as a
real-world test bed for research on automated technical support.",2022-11-13 15:59:12,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,Mark Muraven,Designing a Safe Autonomous Artificial Intelligence Agent based on Human Self-Regulation,,,,,http://arxiv.org/abs/1701.01487v1,"There is a growing focus on how to design safe artificial intelligent (AI)
agents. As systems become more complex, poorly specified goals or control
mechanisms may cause AI agents to engage in unwanted and harmful outcomes. Thus
it is necessary to design AI agents that follow initial programming intentions
as the program grows in complexity. How to specify these initial intentions has
also been an obstacle to designing safe AI agents. Finally, there is a need for
the AI agent to have redundant safety mechanisms to ensure that any programming
errors do not cascade into major problems. Humans are autonomous intelligent
agents that have avoided these problems and the present manuscript argues that
by understanding human self-regulation and goal setting, we may be better able
to design safe AI agents. Some general principles of human self-regulation are
outlined and specific guidance for AI design is given.",2022-11-13 15:59:13,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,"Zohreh Shams, Marina De Vos, Julian Padget, Wamberto W. Vasconcelos",Practical Reasoning with Norms for Autonomous Software Agents (Full Edition),,,,,http://arxiv.org/abs/1701.08306v1,"Autonomous software agents operating in dynamic environments need to
constantly reason about actions in pursuit of their goals, while taking into
consideration norms which might be imposed on those actions. Normative
practical reasoning supports agents making decisions about what is best for
them to (not) do in a given situation. What makes practical reasoning
challenging is the interplay between goals that agents are pursuing and the
norms that the agents are trying to uphold. We offer a formalisation to allow
agents to plan for multiple goals and norms in the presence of durative actions
that can be executed concurrently. We compare plans based on decision-theoretic
notions (i.e. utility) such that the utility gain of goals and utility loss of
norm violations are the basis for this comparison. The set of optimal plans
consists of plans that maximise the overall utility, each of which can be
chosen by the agent to execute. We provide an implementation of our proposal in
Answer Set Programming, thus allowing us to state the original problem in terms
of a logic program that can be queried for solutions with specific properties.
The implementation is proven to be sound and complete.",2022-11-13 15:59:13,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,"Tathagata Chakraborti, Sarath Sreedharan, Yu Zhang, Subbarao Kambhampati",Plan Explanations as Model Reconciliation: Moving Beyond Explanation as Soliloquy,,,,,http://arxiv.org/abs/1701.08317v5,"When AI systems interact with humans in the loop, they are often called on to
provide explanations for their plans and behavior. Past work on plan
explanations primarily involved the AI system explaining the correctness of its
plan and the rationale for its decision in terms of its own model. Such
soliloquy is wholly inadequate in most realistic scenarios where the humans
have domain and task models that differ significantly from that used by the AI
system. We posit that the explanations are best studied in light of these
differing models. In particular, we show how explanation can be seen as a
""model reconciliation problem"" (MRP), where the AI system in effect suggests
changes to the human's model, so as to make its plan be optimal with respect to
that changed human model. We will study the properties of such explanations,
present algorithms for automatically computing them, and evaluate the
performance of the algorithms.",2022-11-13 15:59:14,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,"Ewa Andrejczuk, Juan A. Rodriguez-Aguilar, Carme Roig, Carles Sierra",Synergistic Team Composition,,,,,http://arxiv.org/abs/1702.08222v1,"Effective teams are crucial for organisations, especially in environments
that require teams to be constantly created and dismantled, such as software
development, scientific experiments, crowd-sourcing, or the classroom. Key
factors influencing team performance are competences and personality of team
members. Hence, we present a computational model to compose proficient and
congenial teams based on individuals' personalities and their competences to
perform tasks of different nature. With this purpose, we extend Wilde's
post-Jungian method for team composition, which solely employs individuals'
personalities. The aim of this study is to create a model to partition agents
into teams that are balanced in competences, personality and gender. Finally,
we present some preliminary empirical results that we obtained when analysing
student performance. Results show the benefits of a more informed team
composition that exploits individuals' competences besides information about
their personalities.",2022-11-13 15:59:14,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,"Raul Fervari, Andreas Herzig, Yanjun Li, Yanjing Wang",Strategically knowing how,,,,,http://arxiv.org/abs/1705.05254v1,"In this paper, we propose a single-agent logic of goal-directed knowing how
extending the standard epistemic logic of knowing that with a new knowing how
operator. The semantics of the new operator is based on the idea that knowing
how to achieve $\phi$ means that there exists a (uniform) strategy such that
the agent knows that it can make sure $\phi$. We give an intuitive
axiomatization of our logic and prove the soundness, completeness, and
decidability of the logic. The crucial axioms relating knowing that and knowing
how illustrate our understanding of knowing how in this setting. This logic can
be used in representing both knowledge-that and knowledge-how.",2022-11-13 15:59:15,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,"Pavel Naumov, Jia Tao",Together We Know How to Achieve: An Epistemic Logic of Know-How,,,,,http://arxiv.org/abs/1705.09349v2,"The existence of a coalition strategy to achieve a goal does not necessarily
mean that the coalition has enough information to know how to follow the
strategy. Neither does it mean that the coalition knows that such a strategy
exists. The article studies an interplay between the distributed knowledge,
coalition strategies, and coalition ""know-how"" strategies. The main technical
result is a sound and complete trimodal logical system that describes the
properties of this interplay.",2022-11-13 15:59:16,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,"Smitha Milli, Dylan Hadfield-Menell, Anca Dragan, Stuart Russell",Should Robots be Obedient?,,,,,http://arxiv.org/abs/1705.09990v1,"Intuitively, obedience -- following the order that a human gives -- seems
like a good property for a robot to have. But, we humans are not perfect and we
may give orders that are not best aligned to our preferences. We show that when
a human is not perfectly rational then a robot that tries to infer and act
according to the human's underlying preferences can always perform better than
a robot that simply follows the human's literal order. Thus, there is a
tradeoff between the obedience of a robot and the value it can attain for its
owner. We investigate how this tradeoff is impacted by the way the robot infers
the human's preferences, showing that some methods err more on the side of
obedience than others. We then analyze how performance degrades when the robot
has a misspecified model of the features that the human cares about or the
level of rationality of the human. Finally, we study how robots can start
detecting such model misspecification. Overall, our work suggests that there
might be a middle ground in which robots intelligently decide when to obey
human orders, but err on the side of obedience.",2022-11-13 15:59:16,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,Toby Walsh,Expert and Non-Expert Opinion about Technological Unemployment,,,,,http://arxiv.org/abs/1706.06906v1,"There is significant concern that technological advances, especially in
Robotics and Artificial Intelligence (AI), could lead to high levels of
unemployment in the coming decades. Studies have estimated that around half of
all current jobs are at risk of automation. To look into this issue in more
depth, we surveyed experts in Robotics and AI about the risk, and compared
their views with those of non-experts. Whilst the experts predicted a
significant number of occupations were at risk of automation in the next two
decades, they were more cautious than people outside the field in predicting
occupations at risk. Their predictions were consistent with their estimates for
when computers might be expected to reach human level performance across a wide
range of skills. These estimates were typically decades later than those of the
non-experts. Technological barriers may therefore provide society with more
time to prepare for an automated future than the public fear. In addition,
public expectations may need to be dampened about the speed of progress to be
expected in Robotics and AI.",2022-11-13 15:59:17,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,"Svetlin Penkov, Subramanian Ramamoorthy",Using Program Induction to Interpret Transition System Dynamics,,,,,http://arxiv.org/abs/1708.00376v1,"Explaining and reasoning about processes which underlie observed black-box
phenomena enables the discovery of causal mechanisms, derivation of suitable
abstract representations and the formulation of more robust predictions. We
propose to learn high level functional programs in order to represent abstract
models which capture the invariant structure in the observed data. We introduce
the $\pi$-machine (program-induction machine) -- an architecture able to induce
interpretable LISP-like programs from observed data traces. We propose an
optimisation procedure for program learning based on backpropagation, gradient
descent and A* search. We apply the proposed method to two problems: system
identification of dynamical systems and explaining the behaviour of a DQN
agent. Our results show that the $\pi$-machine can efficiently induce
interpretable programs from individual data traces.",2022-11-13 15:59:17,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,"Philip S. Thomas, Bruno Castro da Silva, Andrew G. Barto, Emma Brunskill",On Ensuring that Intelligent Machines Are Well-Behaved,,,,,http://arxiv.org/abs/1708.05448v1,"Machine learning algorithms are everywhere, ranging from simple data analysis
and pattern recognition tools used across the sciences to complex systems that
achieve super-human performance on various tasks. Ensuring that they are
well-behaved---that they do not, for example, cause harm to humans or act in a
racist or sexist way---is therefore not a hypothetical problem to be dealt with
in the future, but a pressing one that we address here. We propose a new
framework for designing machine learning algorithms that simplifies the problem
of specifying and regulating undesirable behaviors. To show the viability of
this new framework, we use it to create new machine learning algorithms that
preclude the sexist and harmful behaviors exhibited by standard machine
learning algorithms in our experiments. Our framework for designing machine
learning algorithms simplifies the safe and responsible application of machine
learning.",2022-11-13 15:59:18,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,"Ivan Y. Tyukin, Alexander N. Gorban, Konstantin Sofeikov, Ilya Romanenko",Knowledge Transfer Between Artificial Intelligence Systems,Front Neurorobot. 2018; 12: 49,,,10.3389/fnbot.2018.00049,http://arxiv.org/abs/1709.01547v2,"We consider the fundamental question: how a legacy ""student"" Artificial
Intelligent (AI) system could learn from a legacy ""teacher"" AI system or a
human expert without complete re-training and, most importantly, without
requiring significant computational resources. Here ""learning"" is understood as
an ability of one system to mimic responses of the other and vice-versa. We
call such learning an Artificial Intelligence knowledge transfer. We show that
if internal variables of the ""student"" Artificial Intelligent system have the
structure of an $n$-dimensional topological vector space and $n$ is
sufficiently high then, with probability close to one, the required knowledge
transfer can be implemented by simple cascades of linear functionals. In
particular, for $n$ sufficiently large, with probability close to one, the
""student"" system can successfully and non-iteratively learn $k\ll n$ new
examples from the ""teacher"" (or correct the same number of mistakes) at the
cost of two additional inner products. The concept is illustrated with an
example of knowledge transfer from a pre-trained convolutional neural network
to a simple linear classifier with HOG features.",2022-11-13 15:59:18,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,"Kunal Menda, Katherine Driggs-Campbell, Mykel J. Kochenderfer",DropoutDAgger: A Bayesian Approach to Safe Imitation Learning,,,,,http://arxiv.org/abs/1709.06166v1,"While imitation learning is becoming common practice in robotics, this
approach often suffers from data mismatch and compounding errors. DAgger is an
iterative algorithm that addresses these issues by continually aggregating
training data from both the expert and novice policies, but does not consider
the impact of safety. We present a probabilistic extension to DAgger, which
uses the distribution over actions provided by the novice policy, for a given
observation. Our method, which we call DropoutDAgger, uses dropout to train the
novice as a Bayesian neural network that provides insight to its confidence.
Using the distribution over the novice's actions, we estimate a probabilistic
measure of safety with respect to the expert action, tuned to balance
exploration and exploitation. The utility of this approach is evaluated on the
MuJoCo HalfCheetah and in a simple driving experiment, demonstrating improved
performance and safety compared to other DAgger variants and classic imitation
learning.",2022-11-13 15:59:19,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,Ryan Carey,Incorrigibility in the CIRL Framework,,,,,http://arxiv.org/abs/1709.06275v2,"A value learning system has incentives to follow shutdown instructions,
assuming the shutdown instruction provides information (in the technical sense)
about which actions lead to valuable outcomes. However, this assumption is not
robust to model mis-specification (e.g., in the case of programmer errors). We
demonstrate this by presenting some Supervised POMDP scenarios in which errors
in the parameterized reward function remove the incentive to follow shutdown
commands. These difficulties parallel those discussed by Soares et al. (2015)
in their paper on corrigibility. We argue that it is important to consider
systems that follow shutdown commands under some weaker set of assumptions
(e.g., that one small verified module is correctly implemented; as opposed to
an entire prior probability distribution and/or parameterized reward function).
We discuss some difficulties with simple ways to attempt to attain these sorts
of guarantees in a value learning framework.",2022-11-13 15:59:20,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,"Ritesh Noothigattu, Snehalkumar 'Neil' S. Gaikwad, Edmond Awad, Sohan Dsouza, Iyad Rahwan, Pradeep Ravikumar, Ariel D. Procaccia",A Voting-Based System for Ethical Decision Making,,,,,http://arxiv.org/abs/1709.06692v2,"We present a general approach to automating ethical decisions, drawing on
machine learning and computational social choice. In a nutshell, we propose to
learn a model of societal preferences, and, when faced with a specific ethical
dilemma at runtime, efficiently aggregate those preferences to identify a
desirable choice. We provide a concrete algorithm that instantiates our
approach; some of its crucial steps are informed by a new theory of
swap-dominance efficient voting rules. Finally, we implement and evaluate a
system for ethical decision making in the autonomous vehicle domain, using
preference data collected from 1.3 million people through the Moral Machine
website.",2022-11-13 15:59:20,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,"Stefano V. Albrecht, Peter Stone",Autonomous Agents Modelling Other Agents: A Comprehensive Survey and Open Problems,,,,10.1016/j.artint.2018.01.002,http://arxiv.org/abs/1709.08071v2,"Much research in artificial intelligence is concerned with the development of
autonomous agents that can interact effectively with other agents. An important
aspect of such agents is the ability to reason about the behaviours of other
agents, by constructing models which make predictions about various properties
of interest (such as actions, goals, beliefs) of the modelled agents. A variety
of modelling approaches now exist which vary widely in their methodology and
underlying assumptions, catering to the needs of the different sub-communities
within which they were developed and reflecting the different practical uses
for which they are intended. The purpose of the present article is to provide a
comprehensive survey of the salient modelling methods which can be found in the
literature. The article concludes with a discussion of open problems which may
form the basis for fruitful future research.",2022-11-13 15:59:21,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,"Spyros Gkezerlis, Dimitris Kalles",Decision Trees for Helpdesk Advisor Graphs,"Bulletin of the Technical Committee on Learning Technology, Volume
  18, Issue 2-3, April 2016",,,,http://arxiv.org/abs/1710.07075v1,"We use decision trees to build a helpdesk agent reference network to
facilitate the on-the-job advising of junior or less experienced staff on how
to better address telecommunication customer fault reports. Such reports
generate field measurements and remote measurements which, when coupled with
location data and client attributes, and fused with organization-level
statistics, can produce models of how support should be provided. Beyond
decision support, these models can help identify staff who can act as advisors,
based on the quality, consistency and predictability of dealing with complex
troubleshooting reports. Advisor staff models are then used to guide less
experienced staff in their decision making; thus, we advocate the deployment of
a simple mechanism which exploits the availability of staff with a sound track
record at the helpdesk to act as dormant tutors.",2022-11-13 15:59:22,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,Fabio Massimo Zanzotto,Human-in-the-loop Artificial Intelligence,"Journal of Artificial Intelligence Research, 2019",,,10.1613/jair.1.11345,http://arxiv.org/abs/1710.08191v1,"Little by little, newspapers are revealing the bright future that Artificial
Intelligence (AI) is building. Intelligent machines will help everywhere.
However, this bright future has a dark side: a dramatic job market contraction
before its unpredictable transformation. Hence, in a near future, large numbers
of job seekers will need financial support while catching up with these novel
unpredictable jobs. This possible job market crisis has an antidote inside. In
fact, the rise of AI is sustained by the biggest knowledge theft of the recent
years. Learning AI machines are extracting knowledge from unaware skilled or
unskilled workers by analyzing their interactions. By passionately doing their
jobs, these workers are digging their own graves.
  In this paper, we propose Human-in-the-loop Artificial Intelligence (HIT-AI)
as a fairer paradigm for Artificial Intelligence systems. HIT-AI will reward
aware and unaware knowledge producers with a different scheme: decisions of AI
systems generating revenues will repay the legitimate owners of the knowledge
used for taking those decisions. As modern Robin Hoods, HIT-AI researchers
should fight for a fairer Artificial Intelligence that gives back what it
steals.",2022-11-13 15:59:23,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,"Andrew Critch, Stuart Russell",Servant of Many Masters: Shifting priorities in Pareto-optimal sequential decision-making,,,,,http://arxiv.org/abs/1711.00363v1,"It is often argued that an agent making decisions on behalf of two or more
principals who have different utility functions should adopt a {\em
Pareto-optimal} policy, i.e., a policy that cannot be improved upon for one
agent without making sacrifices for another. A famous theorem of Harsanyi shows
that, when the principals have a common prior on the outcome distributions of
all policies, a Pareto-optimal policy for the agent is one that maximizes a
fixed, weighted linear combination of the principals' utilities.
  In this paper, we show that Harsanyi's theorem does not hold for principals
with different priors, and derive a more precise generalization which does
hold, which constitutes our main result. In this more general case, the
relative weight given to each principal's utility should evolve over time
according to how well the agent's observations conform with that principal's
prior. The result has implications for the design of contracts, treaties, joint
ventures, and robots.",2022-11-13 15:59:24,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,"Dylan Hadfield-Menell, Smitha Milli, Pieter Abbeel, Stuart Russell, Anca Dragan",Inverse Reward Design,,,,,http://arxiv.org/abs/1711.02827v2,"Autonomous agents optimize the reward function we give them. What they don't
know is how hard it is for us to design a reward function that actually
captures what we want. When designing the reward, we might think of some
specific training scenarios, and make sure that the reward will lead to the
right behavior in those scenarios. Inevitably, agents encounter new scenarios
(e.g., new types of terrain) where optimizing that same reward may lead to
undesired behavior. Our insight is that reward functions are merely
observations about what the designer actually wants, and that they should be
interpreted in the context in which they were designed. We introduce inverse
reward design (IRD) as the problem of inferring the true objective based on the
designed reward and the training MDP. We introduce approximate methods for
solving IRD problems, and use their solution to plan risk-averse behavior in
test MDPs. Empirical results suggest that this approach can help alleviate
negative side effects of misspecified reward functions and mitigate reward
hacking.",2022-11-13 15:59:24,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,"Stuart Armstrong, Xavier O'Rorke",Good and safe uses of AI Oracles,,,,,http://arxiv.org/abs/1711.05541v5,"It is possible that powerful and potentially dangerous artificial
intelligence (AI) might be developed in the future. An Oracle is a design which
aims to restrain the impact of a potentially dangerous AI by restricting the
agent to no actions besides answering questions. Unfortunately, most Oracles
will be motivated to gain more control over the world by manipulating users
through the content of their answers, and Oracles of potentially high
intelligence might be very successful at this
\citep{DBLP:journals/corr/AlfonsecaCACAR16}. In this paper we present two
designs for Oracles which, even under pessimistic assumptions, will not
manipulate their users into releasing them and yet will still be incentivised
to provide their users with helpful answers. The first design is the
counterfactual Oracle -- which choses its answer as if it expected nobody to
ever read it. The second design is the low-bandwidth Oracle -- which is limited
by the quantity of information it can transmit.",2022-11-13 15:59:25,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,"Housam Khalifa Bashier Babiker, Randy Goebel",Using KL-divergence to focus Deep Visual Explanation,,,,,http://arxiv.org/abs/1711.06431v2,"We present a method for explaining the image classification predictions of
deep convolution neural networks, by highlighting the pixels in the image which
influence the final class prediction. Our method requires the identification of
a heuristic method to select parameters hypothesized to be most relevant in
this prediction, and here we use Kullback-Leibler divergence to provide this
focus. Overall, our approach helps in understanding and interpreting deep
network predictions and we hope contributes to a foundation for such
understanding of deep learning networks. In this brief paper, our experiments
evaluate the performance of two popular networks in this context of
interpretability.",2022-11-13 15:59:25,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,"Daniel Levy, Stefano Ermon",Deterministic Policy Optimization by Combining Pathwise and Score Function Estimators for Discrete Action Spaces,,,,,http://arxiv.org/abs/1711.08068v1,"Policy optimization methods have shown great promise in solving complex
reinforcement and imitation learning tasks. While model-free methods are
broadly applicable, they often require many samples to optimize complex
policies. Model-based methods greatly improve sample-efficiency but at the cost
of poor generalization, requiring a carefully handcrafted model of the system
dynamics for each task. Recently, hybrid methods have been successful in
trading off applicability for improved sample-complexity. However, these have
been limited to continuous action spaces. In this work, we present a new hybrid
method based on an approximation of the dynamics as an expectation over the
next state under the current policy. This relaxation allows us to derive a
novel hybrid policy gradient estimator, combining score function and pathwise
derivative estimators, that is applicable to discrete action spaces. We show
significant gains in sample complexity, ranging between $1.7$ and $25\times$,
when learning parameterized policies on Cart Pole, Acrobot, Mountain Car and
Hand Mass. Our method is applicable to both discrete and continuous action
spaces, when competing pathwise methods are limited to the latter.",2022-11-13 15:59:26,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,"M. Botvinick, D. G. T. Barrett, P. Battaglia, N. de Freitas, D. Kumaran, J. Z Leibo, T. Lillicrap, J. Modayil, S. Mohamed, N. C. Rabinowitz, D. J. Rezende, A. Santoro, T. Schaul, C. Summerfield, G. Wayne, T. Weber, D. Wierstra, S. Legg, D. Hassabis","Building Machines that Learn and Think for Themselves: Commentary on Lake et al., Behavioral and Brain Sciences, 2017",,,,,http://arxiv.org/abs/1711.08378v1,"We agree with Lake and colleagues on their list of key ingredients for
building humanlike intelligence, including the idea that model-based reasoning
is essential. However, we favor an approach that centers on one additional
ingredient: autonomy. In particular, we aim toward agents that can both build
and exploit their own internal models, with minimal human hand-engineering. We
believe an approach centered on autonomous learning has the greatest chance of
success as we scale toward real-world complexity, tackling domains for which
ready-made formal models are not available. Here we survey several important
examples of the progress that has been made toward building autonomous agents
with humanlike abilities, and highlight some outstanding challenges.",2022-11-13 15:59:26,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,"Stuart Armstrong, Sören Mindermann",Occam's razor is insufficient to infer the preferences of irrational agents,,,,,http://arxiv.org/abs/1712.05812v6,"Inverse reinforcement learning (IRL) attempts to infer human rewards or
preferences from observed behavior. Since human planning systematically
deviates from rationality, several approaches have been tried to account for
specific human shortcomings. However, the general problem of inferring the
reward function of an agent of unknown rationality has received little
attention. Unlike the well-known ambiguity problems in IRL, this one is
practically relevant but cannot be resolved by observing the agent's policy in
enough environments. This paper shows (1) that a No Free Lunch result implies
it is impossible to uniquely decompose a policy into a planning algorithm and
reward function, and (2) that even with a reasonable simplicity prior/Occam's
razor on the set of decompositions, we cannot distinguish between the true
decomposition and others that lead to high regret. To address this, we need
simple `normative' assumptions, which cannot be deduced exclusively from
observations.",2022-11-13 15:59:28,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,"Stuart Armstrong, Xavier O'Rourke",Indifference' methods for managing agent rewards,,,,,http://arxiv.org/abs/1712.06365v4,"`Indifference' refers to a class of methods used to control reward based
agents. Indifference techniques aim to achieve one or more of three distinct
goals: rewards dependent on certain events (without the agent being motivated
to manipulate the probability of those events), effective disbelief (where
agents behave as if particular events could never happen), and seamless
transition from one reward function to another (with the agent acting as if
this change is unanticipated). This paper presents several methods for
achieving these goals in the POMDP setting, establishing their uses, strengths,
and requirements. These methods of control work even when the implications of
the agent's reward are otherwise not fully understood.",2022-11-13 15:59:29,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,"Feng Liu, Yong Shi, Ying Liu",Three IQs of AI Systems and their Testing Methods,,,,,http://arxiv.org/abs/1712.06440v1,"The rapid development of artificial intelligence has brought the artificial
intelligence threat theory as well as the problem about how to evaluate the
intelligence level of intelligent products. Both need to find a quantitative
method to evaluate the intelligence level of intelligence systems, including
human intelligence. Based on the standard intelligence system and the extended
Von Neumann architecture, this paper proposes General IQ, Service IQ and Value
IQ evaluation methods for intelligence systems, depending on different
evaluation purposes. Among them, the General IQ of intelligence systems is to
answer the question of whether the artificial intelligence can surpass the
human intelligence, which is reflected in putting the intelligence systems on
an equal status and conducting the unified evaluation. The Service IQ and Value
IQ of intelligence systems are used to answer the question of how the
intelligent products can better serve the human, reflecting the intelligence
and required cost of each intelligence system as a product in the process of
serving human.",2022-11-13 15:59:29,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,"Ziming Li, Julia Kiseleva, Alekh Agarwal, Maarten de Rijke",Learning Data-Driven Objectives to Optimize Interactive Systems,,,,,http://arxiv.org/abs/1802.06306v8,"Effective optimization is essential for interactive systems to provide a
satisfactory user experience. However, it is often challenging to find an
objective to optimize for. Generally, such objectives are manually crafted and
rarely capture complex user needs in an accurate manner. We propose an approach
that infers the objective directly from observed user interactions. These
inferences can be made regardless of prior knowledge and across different types
of user behavior. We introduce interactive system optimization, a novel
algorithm that uses these inferred objectives for optimization. Our main
contribution is a new general principled approach to optimizing interactive
systems using data-driven objectives. We demonstrate the high effectiveness of
interactive system optimization over several simulations.",2022-11-13 15:59:30,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,"Miles Brundage, Shahar Avin, Jack Clark, Helen Toner, Peter Eckersley, Ben Garfinkel, Allan Dafoe, Paul Scharre, Thomas Zeitzoff, Bobby Filar, Hyrum Anderson, Heather Roff, Gregory C. Allen, Jacob Steinhardt, Carrick Flynn, Seán Ó hÉigeartaigh, Simon Beard, Haydn Belfield, Sebastian Farquhar, Clare Lyle, Rebecca Crootof, Owain Evans, Michael Page, Joanna Bryson, Roman Yampolskiy, Dario Amodei","The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation",,,,,http://arxiv.org/abs/1802.07228v1,"This report surveys the landscape of potential security threats from
malicious uses of AI, and proposes ways to better forecast, prevent, and
mitigate these threats. After analyzing the ways in which AI may influence the
threat landscape in the digital, physical, and political domains, we make four
high-level recommendations for AI researchers and other stakeholders. We also
suggest several promising areas for further research that could expand the
portfolio of defenses, or make attacks less effective or harder to execute.
Finally, we discuss, but do not conclusively resolve, the long-term equilibrium
of attackers and defenders.",2022-11-13 15:59:30,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,"Anusha Mujumdar, Swarup Kumar Mohalik, Ramamurthy Badrinath",Antifragility for Intelligent Autonomous Systems,,,,,http://arxiv.org/abs/1802.09159v1,"Antifragile systems grow measurably better in the presence of hazards. This
is in contrast to fragile systems which break down in the presence of hazards,
robust systems that tolerate hazards up to a certain degree, and resilient
systems that -- like self-healing systems -- revert to their earlier expected
behavior after a period of convalescence. The notion of antifragility was
introduced by Taleb for economics systems, but its applicability has been
illustrated in biological and engineering domains as well. In this paper, we
propose an architecture that imparts antifragility to intelligent autonomous
systems, specifically those that are goal-driven and based on AI-planning. We
argue that this architecture allows the system to self-improve by uncovering
new capabilities obtained either through the hazards themselves (opportunistic)
or through deliberation (strategic). An AI planning-based case study of an
autonomous wheeled robot is presented. We show that with the proposed
architecture, the robot develops antifragile behaviour with respect to an oil
spill hazard.",2022-11-13 15:59:31,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,"Alexander Boer, Giovanni Sileno",Institutional Metaphors for Designing Large-Scale Distributed AI versus AI Techniques for Running Institutions,,,,,http://arxiv.org/abs/1803.03407v2,"Artificial Intelligence (AI) started out with an ambition to reproduce the
human mind, but, as the sheer scale of that ambition became manifest, it
quickly retreated into either studying specialized intelligent behaviours, or
proposing over-arching architectural concepts for interfacing specialized
intelligent behaviour components, conceived of as agents in a kind of
organization. This agent-based modeling paradigm, in turn, proves to have
interesting applications in understanding, simulating, and predicting the
behaviour of social and legal structures on an aggregate level. For these
reasons, this chapter examines a number of relevant cross-cutting concerns,
conceptualizations, modeling problems and design challenges in large-scale
distributed Artificial Intelligence, as well as in institutional systems, and
identifies potential grounds for novel advances.",2022-11-13 15:59:31,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,"Daniel S. Weld, Gagan Bansal",The Challenge of Crafting Intelligible Intelligence,,,,,http://arxiv.org/abs/1803.04263v3,"Since Artificial Intelligence (AI) software uses techniques like deep
lookahead search and stochastic optimization of huge neural networks to fit
mammoth datasets, it often results in complex behavior that is difficult for
people to understand. Yet organizations are deploying AI algorithms in many
mission-critical settings. To trust their behavior, we must make AI
intelligible, either by using inherently interpretable models or by developing
new methods for explaining and controlling otherwise overwhelmingly complex
decisions using local approximation, vocabulary alignment, and interactive
explanation. This paper argues that intelligibility is essential, surveys
recent work on building such systems, and highlights key directions for
research.",2022-11-13 15:59:32,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,"Virginia Dignum, Frank Dignum",A Logic of Agent Organizations,"Logic Journal of the IGPL, vol. 20, no. 1, pp. 283-316, Feb. 2012",,,10.1093/jigpal/jzr041,http://arxiv.org/abs/1804.10817v1,"Organization concepts and models are increasingly being adopted for the
design and specification of multi-agent systems. Agent organizations can be
seen as mechanisms of social order, created to achieve global (or
organizational) objectives by more or less autonomous agents. In order to
develop a theory on the relation between organizational structures,
organizational objectives and the actions of agents fulfilling roles in the
organization a theoretical framework is needed to describe organizational
structures and actions of (groups of) agents. Current logical formalisms focus
on specific aspects of organizations (e.g. power, delegation, agent actions, or
normative issues) but a framework that integrates and relates different aspects
is missing. Given the amount of aspects involved and the subsequent complexity
of a formalism encompassing them all, it is difficult to realize. In this
paper, a first step is taken to solve this problem. We present a generic formal
model that enables to specify and relate the main concepts of an organization
(including, activity, structure, environment and others) so that organizations
can be analyzed at a high level of abstraction. However, for some aspects we
use a simplified model in order to avoid the complexity of combining many
different types of (modal) operators.",2022-11-13 15:59:33,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,"Richard Tomsett, Dave Braines, Dan Harborne, Alun Preece, Supriyo Chakraborty",Interpretable to Whom? A Role-based Model for Analyzing Interpretable Machine Learning Systems,,,,,http://arxiv.org/abs/1806.07552v1,"Several researchers have argued that a machine learning system's
interpretability should be defined in relation to a specific agent or task: we
should not ask if the system is interpretable, but to whom is it interpretable.
We describe a model intended to help answer this question, by identifying
different roles that agents can fulfill in relation to the machine learning
system. We illustrate the use of our model in a variety of scenarios, exploring
how an agent's role influences its goals, and the implications for defining
interpretability. Finally, we make suggestions for how our model could be
useful to interpretability researchers, system developers, and regulatory
bodies auditing machine learning systems.",2022-11-13 15:59:33,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,Eray Özkural,The Foundations of Deep Learning with a Path Towards General Intelligence,,,,,http://arxiv.org/abs/1806.08874v1,"Like any field of empirical science, AI may be approached axiomatically. We
formulate requirements for a general-purpose, human-level AI system in terms of
postulates. We review the methodology of deep learning, examining the explicit
and tacit assumptions in deep learning research. Deep Learning methodology
seeks to overcome limitations in traditional machine learning research as it
combines facets of model richness, generality, and practical applicability. The
methodology so far has produced outstanding results due to a productive synergy
of function approximation, under plausible assumptions of irreducibility and
the efficiency of back-propagation family of algorithms. We examine these
winning traits of deep learning, and also observe the various known failure
modes of deep learning. We conclude by giving recommendations on how to extend
deep learning methodology to cover the postulates of general-purpose AI
including modularity, and cognitive architecture. We also relate deep learning
to advances in theoretical neuroscience research.",2022-11-13 15:59:34,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,"James Max Kanter, Benjamin Schreck, Kalyan Veeramachaneni",Machine learning 2.0 : Engineering Data Driven AI Products,,,,,http://arxiv.org/abs/1807.00401v1,"ML 2.0: In this paper, we propose a paradigm shift from the current practice
of creating machine learning models - which requires months-long discovery,
exploration and ""feasibility report"" generation, followed by re-engineering for
deployment - in favor of a rapid, 8-week process of development, understanding,
validation and deployment that can executed by developers or subject matter
experts (non-ML experts) using reusable APIs. This accomplishes what we call a
""minimum viable data-driven model,"" delivering a ready-to-use machine learning
model for problems that haven't been solved before using machine learning. We
provide provisions for the refinement and adaptation of the ""model,"" with
strict enforcement and adherence to both the scaffolding/abstractions and the
process. We imagine that this will bring forth the second phase in machine
learning, in which discovery is subsumed by more targeted goals of delivery and
impact.",2022-11-13 15:59:35,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,"Catarina Moreira, Andreas Wichert",Introducing Quantum-Like Influence Diagrams for Violations of the Sure Thing Principle,"Quantum Interactions, 2018",,,,http://arxiv.org/abs/1807.06142v2,"It is the focus of this work to extend and study the previously proposed
quantum-like Bayesian networks to deal with decision-making scenarios by
incorporating the notion of maximum expected utility in influence diagrams. The
general idea is to take advantage of the quantum interference terms produced in
the quantum-like Bayesian Network to influence the probabilities used to
compute the expected utility of some action. This way, we are not proposing a
new type of expected utility hypothesis. On the contrary, we are keeping it
under its classical definition. We are only incorporating it as an extension of
a probabilistic graphical model in a compact graphical representation called an
influence diagram in which the utility function depends on the probabilistic
influences of the quantum-like Bayesian network.
  Our findings suggest that the proposed quantum-like influence digram can
indeed take advantage of the quantum interference effects of quantum-like
Bayesian Networks to maximise the utility of a cooperative behaviour in
detriment of a fully rational defect behaviour under the prisoner's dilemma
game.",2022-11-13 15:59:35,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,Jean-Marie Chauvet,The 30-Year Cycle In The AI Debate,,,,,http://arxiv.org/abs/1810.04053v1,"In the last couple of years, the rise of Artificial Intelligence and the
successes of academic breakthroughs in the field have been inescapable. Vast
sums of money have been thrown at AI start-ups. Many existing tech companies --
including the giants like Google, Amazon, Facebook, and Microsoft -- have
opened new research labs. The rapid changes in these everyday work and
entertainment tools have fueled a rising interest in the underlying technology
itself; journalists write about AI tirelessly, and companies -- of tech nature
or not -- brand themselves with AI, Machine Learning or Deep Learning whenever
they get a chance. Confronting squarely this media coverage, several analysts
are starting to voice concerns about over-interpretation of AI's blazing
successes and the sometimes poor public reporting on the topic. This paper
reviews briefly the track-record in AI and Machine Learning and finds this
pattern of early dramatic successes, followed by philosophical critique and
unexpected difficulties, if not downright stagnation, returning almost to the
clock in 30-year cycles since 1958.",2022-11-13 15:59:36,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,"Yuu Jinnai, David Abel, D Ellis Hershkowitz, Michael Littman, George Konidaris",Finding Options that Minimize Planning Time,,,,,http://arxiv.org/abs/1810.07311v3,"We formalize the problem of selecting the optimal set of options for planning
as that of computing the smallest set of options so that planning converges in
less than a given maximum of value-iteration passes. We first show that the
problem is NP-hard, even if the task is constrained to be deterministic---the
first such complexity result for option discovery. We then present the first
polynomial-time boundedly suboptimal approximation algorithm for this setting,
and empirically evaluate it against both the optimal options and a
representative collection of heuristic approaches in simple grid-based domains
including the classic four-rooms problem.",2022-11-13 15:59:37,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,"Tae Wan Kim, Thomas Donaldson, John Hooker",Mimetic vs Anchored Value Alignment in Artificial Intelligence,,,,,http://arxiv.org/abs/1810.11116v1,"""Value alignment"" (VA) is considered as one of the top priorities in AI
research. Much of the existing research focuses on the ""A"" part and not the ""V""
part of ""value alignment."" This paper corrects that neglect by emphasizing the
""value"" side of VA and analyzes VA from the vantage point of requirements in
value theory, in particular, of avoiding the ""naturalistic fallacy""--a major
epistemic caveat. The paper begins by isolating two distinct forms of VA:
""mimetic"" and ""anchored."" Then it discusses which VA approach better avoids the
naturalistic fallacy. The discussion reveals stumbling blocks for VA approaches
that neglect implications of the naturalistic fallacy. Such problems are more
serious in mimetic VA since the mimetic process imitates human behavior that
may or may not rise to the level of correct ethical behavior. Anchored VA,
including hybrid VA, in contrast, holds more promise for future VA since it
anchors alignment by normative concepts of intrinsic value.",2022-11-13 15:59:38,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,Dmitry Maximov,An Optimal Itinerary Generation in a Configuration Space of Large Intellectual Agent Groups with Linear Logic,"Advances in Systems Science and Applications. 2019. Vol. 19, No 4.
  P. 79-86 https://ijassa.ipu.ru/index.php/ijassa/article/view/829/513",,,,http://arxiv.org/abs/1811.02216v1,"A group of intelligent agents which fulfill a set of tasks in parallel is
represented first by the tensor multiplication of corresponding processes in a
linear logic game category. An optimal itinerary in the configuration space of
the group states is defined as a play with maximal total reward in the
category. New moments also are: the reward is represented as a degree of
certainty (visibility) of an agent goal, and the system goals are chosen by the
greatest value corresponding to these processes in the system goal lattice.",2022-11-13 15:59:39,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,Daniel Muller,Economics of Human-AI Ecosystem: Value Bias and Lost Utility in Multi-Dimensional Gaps,,,,,http://arxiv.org/abs/1811.06606v2,"In recent years, artificial intelligence (AI) decision-making and autonomous
systems became an integrated part of the economy, industry, and society. The
evolving economy of the human-AI ecosystem raising concerns regarding the risks
and values inherited in AI systems. This paper investigates the dynamics of
creation and exchange of values and points out gaps in perception of
cost-value, knowledge, space and time dimensions. It shows aspects of value
bias in human perception of achievements and costs that encoded in AI systems.
It also proposes rethinking hard goals definitions and cost-optimal
problem-solving principles in the lens of effectiveness and efficiency in the
development of trusted machines. The paper suggests a value-driven with cost
awareness strategy and principles for problem-solving and planning of effective
research progress to address real-world problems that involve diverse forms of
achievements, investments, and survival scenarios.",2022-11-13 15:59:39,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,"Naveen Sundar Govindarajulu, Selmer Bringsjord, Rikhiya Ghosh",Toward the Engineering of Virtuous Machines,,,,,http://arxiv.org/abs/1812.03868v2,"While various traditions under the 'virtue ethics' umbrella have been studied
extensively and advocated by ethicists, it has not been clear that there exists
a version of virtue ethics rigorous enough to be a target for machine ethics
(which we take to include the engineering of an ethical sensibility in a
machine or robot itself, not only the study of ethics in the humans who might
create artificial agents). We begin to address this by presenting an embryonic
formalization of a key part of any virtue-ethics theory: namely, the learning
of virtue by a focus on exemplars of moral virtue. Our work is based in part on
a computational formal logic previously used to formally model other ethical
theories and principles therein, and to implement these models in artificial
agents.",2022-11-13 15:59:40,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,"Yi Zeng, Enmeng Lu, Cunqing Huangfu",Linking Artificial Intelligence Principles,,,,,http://arxiv.org/abs/1812.04814v1,"Artificial Intelligence principles define social and ethical considerations
to develop future AI. They come from research institutes, government
organizations and industries. All versions of AI principles are with different
considerations covering different perspectives and making different emphasis.
None of them can be considered as complete and can cover the rest AI principle
proposals. Here we introduce LAIP, an effort and platform for linking and
analyzing different Artificial Intelligence Principles. We want to explicitly
establish the common topics and links among AI Principles proposed by different
organizations and investigate on their uniqueness. Based on these efforts, for
the long-term future of AI, instead of directly adopting any of the AI
principles, we argue for the necessity of incorporating various AI Principles
into a comprehensive framework and focusing on how they can interact and
complete each other.",2022-11-13 15:59:40,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,Tshilidzi Marwala,Can rationality be measured?,,,,,http://arxiv.org/abs/1812.10144v1,"This paper studies whether rationality can be computed. Rationality is
defined as the use of complete information, which is processed with a perfect
biological or physical brain, in an optimized fashion. To compute rationality
one needs to quantify how complete is the information, how perfect is the
physical or biological brain and how optimized is the entire decision making
system. The rationality of a model (i.e. physical or biological brain) is
measured by the expected accuracy of the model. The rationality of the
optimization procedure is measured as the ratio of the achieved objective (i.e.
utility) to the global objective. The overall rationality of a decision is
measured as the product of the rationality of the model and the rationality of
the optimization procedure. The conclusion reached is that rationality can be
computed for convex optimization problems.",2022-11-13 15:59:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Alexander Matt Turner, Dylan Hadfield-Menell, Prasad Tadepalli",Conservative Agency via Attainable Utility Preservation,,,,10.1145/3375627.3375851,http://arxiv.org/abs/1902.09725v3,"Reward functions are easy to misspecify; although designers can make
corrections after observing mistakes, an agent pursuing a misspecified reward
function can irreversibly change the state of its environment. If that change
precludes optimization of the correctly specified reward function, then
correction is futile. For example, a robotic factory assistant could break
expensive equipment due to a reward misspecification; even if the designers
immediately correct the reward function, the damage is done. To mitigate this
risk, we introduce an approach that balances optimization of the primary reward
function with preservation of the ability to optimize auxiliary reward
functions. Surprisingly, even when the auxiliary reward functions are randomly
generated and therefore uninformative about the correctly specified reward
function, this approach induces conservative, effective behavior.",2022-11-13 15:59:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Tom Everitt, Pedro A. Ortega, Elizabeth Barnes, Shane Legg",Understanding Agent Incentives using Causal Influence Diagrams. Part I: Single Action Settings,,,,,http://arxiv.org/abs/1902.09980v7,"Agents are systems that optimize an objective function in an environment.
Together, the goal and the environment induce secondary objectives, incentives.
Modeling the agent-environment interaction using causal influence diagrams, we
can answer two fundamental questions about an agent's incentives directly from
the graph: (1) which nodes can the agent have an incentivize to observe, and
(2) which nodes can the agent have an incentivize to control? The answers tell
us which information and influence points need extra protection. For example,
we may want a classifier for job applications to not use the ethnicity of the
candidate, and a reinforcement learning agent not to take direct control of its
reward mechanism. Different algorithms and training paradigms can lead to
different causal influence diagrams, so our method can be used to identify
algorithms with problematic incentives and help in designing algorithms with
better incentives.",2022-11-13 15:59:42,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,Thilo Hagendorff,The Ethics of AI Ethics -- An Evaluation of Guidelines,"Minds & Machines, 2020",,,10.1007/s11023-020-09517-8,http://arxiv.org/abs/1903.03425v2,"Current advances in research, development and application of artificial
intelligence (AI) systems have yielded a far-reaching discourse on AI ethics.
In consequence, a number of ethics guidelines have been released in recent
years. These guidelines comprise normative principles and recommendations aimed
to harness the ""disruptive"" potentials of new AI technologies. Designed as a
comprehensive evaluation, this paper analyzes and compares these guidelines
highlighting overlaps but also omissions. As a result, I give a detailed
overview of the field of AI ethics. Finally, I also examine to what extent the
respective ethical principles and values are implemented in the practice of
research, development and application of AI systems - and how the effectiveness
in the demands of AI ethics can be improved.",2022-11-13 15:59:42,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Smitha Milli, Anca D. Dragan",Literal or Pedagogic Human? Analyzing Human Model Misspecification in Objective Learning,,,,,http://arxiv.org/abs/1903.03877v2,"It is incredibly easy for a system designer to misspecify the objective for
an autonomous system (""robot''), thus motivating the desire to have the robot
learn the objective from human behavior instead. Recent work has suggested that
people have an interest in the robot performing well, and will thus behave
pedagogically, choosing actions that are informative to the robot. In turn,
robots benefit from interpreting the behavior by accounting for this pedagogy.
In this work, we focus on misspecification: we argue that robots might not know
whether people are being pedagogic or literal and that it is important to ask
which assumption is safer to make. We cast objective learning into the more
general form of a common-payoff game between the robot and human, and prove
that in any such game literal interpretation is more robust to
misspecification. Experiments with human data support our theoretical results
and point to the sensitivity of the pedagogic assumption.",2022-11-13 15:59:43,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Bharat Prakash, Mohit Khatwani, Nicholas Waytowich, Tinoosh Mohsenin",Improving Safety in Reinforcement Learning Using Model-Based Architectures and Human Intervention,,,,,http://arxiv.org/abs/1903.09328v1,"Recent progress in AI and Reinforcement learning has shown great success in
solving complex problems with high dimensional state spaces. However, most of
these successes have been primarily in simulated environments where failure is
of little or no consequence. Most real-world applications, however, require
training solutions that are safe to operate as catastrophic failures are
inadmissible especially when there is human interaction involved. Currently,
Safe RL systems use human oversight during training and exploration in order to
make sure the RL agent does not go into a catastrophic state. These methods
require a large amount of human labor and it is very difficult to scale up. We
present a hybrid method for reducing the human intervention time by combining
model-based approaches and training a supervised learner to improve sample
efficiency while also ensuring safety. We evaluate these methods on various
grid-world environments using both standard and visual representations and show
that our approach achieves better performance in terms of sample efficiency,
number of catastrophic states reached as well as overall task performance
compared to traditional model-free approaches",2022-11-13 15:59:43,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Mohannad Babli, Eva Onaindia, Eliseo Marzal",Extending planning knowledge using ontologies for goal opportunities,"31st IBIMA Conference (2018), INNOVATION MANAGEMENT AND EDUCATION
  EXCELLENCE THROUGH VISION 2020, VOLS IV-VI (3199-3208)",,,,http://arxiv.org/abs/1904.03606v1,"Approaches to goal-directed behaviour including online planning and
opportunistic planning tackle a change in the environment by generating
alternative goals to avoid failures or seize opportunities. However, current
approaches only address unanticipated changes related to objects or object
types already defined in the planning task that is being solved. This article
describes a domain-independent approach that advances the state of the art by
extending the knowledge of a planning task with relevant objects of new types.
The approach draws upon the use of ontologies, semantic measures, and ontology
alignment to accommodate newly acquired data that trigger the formulation of
goal opportunities inducing a better-valued plan.",2022-11-13 15:59:44,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,George Cevora,The relationship between Biological and Artificial Intelligence,,,,,http://arxiv.org/abs/1905.00547v1,"Intelligence can be defined as a predominantly human ability to accomplish
tasks that are generally hard for computers and animals. Artificial
Intelligence [AI] is a field attempting to accomplish such tasks with
computers. AI is becoming increasingly widespread, as are claims of its
relationship with Biological Intelligence. Often these claims are made to imply
higher chances of a given technology succeeding, working on the assumption that
AI systems which mimic the mechanisms of Biological Intelligence should be more
successful.
  In this article I will discuss the similarities and differences between AI
and the extent of our knowledge about the mechanisms of intelligence in
biology, especially within humans. I will also explore the validity of the
assumption that biomimicry in AI systems aids their advancement, and I will
argue that existing similarity to biological systems in the way Artificial
Neural Networks [ANNs] tackle tasks is due to design decisions, rather than
inherent similarity of underlying mechanisms. This article is aimed at people
who understand the basics of AI (especially ANNs), and would like to be better
able to evaluate the often wild claims about the value of biomimicry in AI.",2022-11-13 15:59:44,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Philip Feldman, Aaron Dant, Aaron Massey",Integrating Artificial Intelligence into Weapon Systems,,,,,http://arxiv.org/abs/1905.03899v1,"The integration of Artificial Intelligence (AI) into weapon systems is one of
the most consequential tactical and strategic decisions in the history of
warfare. Current AI development is a remarkable combination of accelerating
capability, hidden decision mechanisms, and decreasing costs. Implementation of
these systems is in its infancy and exists on a spectrum from resilient and
flexible to simplistic and brittle. Resilient systems should be able to
effectively handle the complexities of a high-dimensional battlespace.
Simplistic AI implementations could be manipulated by an adversarial AI that
identifies and exploits their weaknesses.
  In this paper, we present a framework for understanding the development of
dynamic AI/ML systems that interactively and continuously adapt to their user's
needs. We explore the implications of increasingly capable AI in the kill chain
and how this will lead inevitably to a fully automated, always on system,
barring regulation by treaty. We examine the potential of total integration of
cyber and physical security and how this likelihood must inform the development
of AI-enabled systems with respect to the ""fog of war"", human morals, and
ethics.",2022-11-13 15:59:45,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Stefano Giovanni Rizzo, Ji Lucas, Zoi Kaoudi, Jorge-Arnulfo Quiane-Ruiz, Sanjay Chawla",AI-CARGO: A Data-Driven Air-Cargo Revenue Management System,,,,,http://arxiv.org/abs/1905.09130v1,"We propose AI-CARGO, a revenue management system for air-cargo that combines
machine learning prediction with decision-making using mathematical
optimization methods. AI-CARGO addresses a problem that is unique to the
air-cargo business, namely the wide discrepancy between the quantity (weight or
volume) that a shipper will book and the actual received amount at departure
time by the airline. The discrepancy results in sub-optimal and inefficient
behavior by both the shipper and the airline resulting in the overall loss of
potential revenue for the airline. AI-CARGO also includes a data cleaning
component to deal with the heterogeneous forms in which booking data is
transmitted to the airline cargo system. AI-CARGO is deployed in the production
environment of a large commercial airline company. We have validated the
benefits of AI-CARGO using real and synthetic datasets. Especially, we have
carried out simulations using dynamic programming techniques to elicit the
impact on offloading costs and revenue generation of our proposed system. Our
results suggest that combining prediction within a decision-making framework
can help dramatically to reduce offloading costs and optimize revenue
generation.",2022-11-13 15:59:45,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Cristian Ivan, Bipin Indurkhya",On modelling the emergence of logical thinking,,,,,http://arxiv.org/abs/1905.09730v1,"Recent progress in machine learning techniques have revived interest in
building artificial general intelligence using these particular tools. There
has been a tremendous success in applying them for narrow intellectual tasks
such as pattern recognition, natural language processing and playing Go. The
latter application vastly outperforms the strongest human player in recent
years. However, these tasks are formalized by people in such ways that it has
become ""easy"" for automated recipes to find better solutions than humans do. In
the sense of John Searle's Chinese Room Argument, the computer playing Go does
not actually understand anything from the game. Thinking like a human mind
requires to go beyond the curve fitting paradigm of current systems. There is a
fundamental limit to what they can achieve currently as only very specific
problem formalization can increase their performances in particular tasks. In
this paper, we argue than one of the most important aspects of the human mind
is its capacity for logical thinking, which gives rise to many intellectual
expressions that differentiate us from animal brains. We propose to model the
emergence of logical thinking based on Piaget's theory of cognitive
development.",2022-11-13 15:59:46,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Tae Wan Kim, Thomas Donaldson, John Hooker",Grounding Value Alignment with Ethical Principles,,,,,http://arxiv.org/abs/1907.05447v1,"An important step in the development of value alignment (VA) systems in AI is
understanding how values can interrelate with facts. Designers of future VA
systems will need to utilize a hybrid approach in which ethical reasoning and
empirical observation interrelate successfully in machine behavior. In this
article we identify two problems about this interrelation that have been
overlooked by AI discussants and designers. The first problem is that many AI
designers commit inadvertently a version of what has been called by moral
philosophers the ""naturalistic fallacy,"" that is, they attempt to derive an
""ought"" from an ""is."" We illustrate when and why this occurs. The second
problem is that AI designers adopt training routines that fail fully to
simulate human ethical reasoning in the integration of ethical principles and
facts. Using concepts of quantified modal logic, we proceed to offer an
approach that promises to simulate ethical reasoning in humans by connecting
ethical principles on the one hand and propositions about states of affairs on
the other.",2022-11-13 15:59:47,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Charu Aggarwal, Djallel Bouneffouf, Horst Samulowitz, Beat Buesser, Thanh Hoang, Udayan Khurana, Sijia Liu, Tejaswini Pedapati, Parikshit Ram, Ambrish Rawat, Martin Wistuba, Alexander Gray",How can AI Automate End-to-End Data Science?,,,,,http://arxiv.org/abs/1910.14436v1,"Data science is labor-intensive and human experts are scarce but heavily
involved in every aspect of it. This makes data science time consuming and
restricted to experts with the resulting quality heavily dependent on their
experience and skills. To make data science more accessible and scalable, we
need its democratization. Automated Data Science (AutoDS) is aimed towards that
goal and is emerging as an important research and business topic. We introduce
and define the AutoDS challenge, followed by a proposal of a general AutoDS
framework that covers existing approaches but also provides guidance for the
development of new methods. We categorize and review the existing literature
from multiple aspects of the problem setup and employed techniques. Then we
provide several views on how AI could succeed in automating end-to-end AutoDS.
We hope this survey can serve as insightful guideline for the AutoDS field and
provide inspiration for future research.",2022-11-13 15:59:47,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Elsa Rizk, Roula Nassif, Ali H. Sayed",Network Classifiers With Output Smoothing,,,,,http://arxiv.org/abs/1911.04870v1,"This work introduces two strategies for training network classifiers with
heterogeneous agents. One strategy promotes global smoothing over the graph and
a second strategy promotes local smoothing over neighbourhoods. It is assumed
that the feature sizes can vary from one agent to another, with some agents
observing insufficient attributes to be able to make reliable decisions on
their own. As a result, cooperation with neighbours is necessary. However, due
to the fact that the feature dimensions are different across the agents, their
classifier dimensions will also be different. This means that cooperation
cannot rely on combining the classifier parameters. We instead propose
smoothing the outputs of the classifiers, which are the predicted labels. By
doing so, the dynamics that describes the evolution of the network classifier
becomes more challenging than usual because the classifier parameters end up
appearing as part of the regularization term as well. We illustrate performance
by means of computer simulations.",2022-11-13 15:59:48,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Roel Dobbe, Thomas Krendl Gilbert, Yonatan Mintz",Hard Choices in Artificial Intelligence: Addressing Normative Uncertainty through Sociotechnical Commitments,,,,,http://arxiv.org/abs/1911.09005v1,"As AI systems become prevalent in high stakes domains such as surveillance
and healthcare, researchers now examine how to design and implement them in a
safe manner. However, the potential harms caused by systems to stakeholders in
complex social contexts and how to address these remains unclear. In this
paper, we explain the inherent normative uncertainty in debates about the
safety of AI systems. We then address this as a problem of vagueness by
examining its place in the design, training, and deployment stages of AI system
development. We adopt Ruth Chang's theory of intuitive comparability to
illustrate the dilemmas that manifest at each stage. We then discuss how
stakeholders can navigate these dilemmas by incorporating distinct forms of
dissent into the development pipeline, drawing on Elizabeth Anderson's work on
the epistemic powers of democratic institutions. We outline a framework of
sociotechnical commitments to formal, substantive and discursive challenges
that address normative uncertainty across stakeholders, and propose the
cultivation of related virtues by those responsible for development.",2022-11-13 15:59:48,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,Samuel Allen Alexander,Measuring the intelligence of an idealized mechanical knowing agent,,,,,http://arxiv.org/abs/1912.09571v1,"We define a notion of the intelligence level of an idealized mechanical
knowing agent. This is motivated by efforts within artificial intelligence
research to define real-number intelligence levels of complicated intelligent
systems. Our agents are more idealized, which allows us to define a much
simpler measure of intelligence level for them. In short, we define the
intelligence level of a mechanical knowing agent to be the supremum of the
computable ordinals that have codes the agent knows to be codes of computable
ordinals. We prove that if one agent knows certain things about another agent,
then the former necessarily has a higher intelligence level than the latter.
This allows our intelligence notion to serve as a stepping stone to obtain
results which, by themselves, are not stated in terms of our intelligence
notion (results of potential interest even to readers totally skeptical that
our notion correctly captures intelligence). As an application, we argue that
these results comprise evidence against the possibility of intelligence
explosion (that is, the notion that sufficiently intelligent machines will
eventually be capable of designing even more intelligent machines, which can
then design even more intelligent machines, and so on).",2022-11-13 15:59:49,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,Jordan Ott,Questions to Guide the Future of Artificial Intelligence Research,,,,,http://arxiv.org/abs/1912.10305v2,"The field of machine learning has focused, primarily, on discretized
sub-problems (i.e. vision, speech, natural language) of intelligence. While
neuroscience tends to be observation heavy, providing few guiding theories. It
is unlikely that artificial intelligence will emerge through only one of these
disciplines. Instead, it is likely to be some amalgamation of their algorithmic
and observational findings. As a result, there are a number of problems that
should be addressed in order to select the beneficial aspects of both fields.
In this article, we propose leading questions to guide the future of artificial
intelligence research. There are clear computational principles on which the
brain operates. The problem is finding these computational needles in a
haystack of biological complexity. Biology has clear constraints but by not
using it as a guide we are constraining ourselves.",2022-11-13 15:59:49,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Alex Kearney, Anna Koop, Patrick M. Pilarski",What's a Good Prediction? Challenges in evaluating an agent's knowledge,,,,,http://arxiv.org/abs/2001.08823v2,"Constructing general knowledge by learning task-independent models of the
world can help agents solve challenging problems. However, both constructing
and evaluating such models remains an open challenge. The most common
approaches to evaluating models is to assess their accuracy with respect to
observable values. However, the prevailing reliance on estimator accuracy as a
proxy for the usefulness of the knowledge has the potential to lead us astray.
We demonstrate the conflict between accuracy and usefulness through a series of
illustrative examples including both a thought experiment and empirical example
in MineCraft, using the General Value Function framework (GVF). Having
identified challenges in assessing an agent's knowledge, we propose an
alternate evaluation approach that arises continually in the online continual
learning setting we recommend evaluation by examining internal learning
processes, specifically the relevance of a GVF's features to the prediction
task at hand. This paper contributes a first look into evaluation of
predictions through their use, an integral component of predictive knowledge
which is as of yet unexplored.",2022-11-13 15:59:50,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Sarath Sreedharan, Utkarsh Soni, Mudit Verma, Siddharth Srivastava, Subbarao Kambhampati",Bridging the Gap: Providing Post-Hoc Symbolic Explanations for Sequential Decision-Making Problems with Inscrutable Representations,,,,,http://arxiv.org/abs/2002.01080v4,"As increasingly complex AI systems are introduced into our daily lives, it
becomes important for such systems to be capable of explaining the rationale
for their decisions and allowing users to contest these decisions. A
significant hurdle to allowing for such explanatory dialogue could be the
vocabulary mismatch between the user and the AI system. This paper introduces
methods for providing contrastive explanations in terms of user-specified
concepts for sequential decision-making settings where the system's model of
the task may be best represented as an inscrutable model. We do this by
building partial symbolic models of a local approximation of the task that can
be leveraged to answer the user queries. We test these methods on a popular
Atari game (Montezuma's Revenge) and variants of Sokoban (a well-known planning
benchmark) and report the results of user studies to evaluate whether people
find explanations generated in this form useful.",2022-11-13 15:59:50,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,Cameron Reid,Student/Teacher Advising through Reward Augmentation,,,,,http://arxiv.org/abs/2002.02938v1,"Transfer learning is an important new subfield of multiagent reinforcement
learning that aims to help an agent learn about a problem by using knowledge
that it has gained solving another problem, or by using knowledge that is
communicated to it by an agent who already knows the problem. This is useful
when one wishes to change the architecture or learning algorithm of an agent
(so that the new knowledge need not be built ""from scratch""), when new agents
are frequently introduced to the environment with no knowledge, or when an
agent must adapt to similar but different problems. Great progress has been
made in the agent-to-agent case using the Teacher/Student framework proposed by
(Torrey and Taylor 2013). However, that approach requires that learning from a
teacher be treated differently from learning in every other reinforcement
learning context. In this paper, I propose a method which allows the
teacher/student framework to be applied in a way that fits directly and
naturally into the more general reinforcement learning framework by integrating
the teacher feedback into the reward signal received by the learning agent. I
show that this approach can significantly improve the rate of learning for an
agent playing a one-player stochastic game; I give examples of potential
pitfalls of the approach; and I propose further areas of research building on
this framework.",2022-11-13 15:59:51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Jens Braband, Hendrik Schäbe",On Safety Assessment of Artificial Intelligence,"Dependability, vol. 20 no. 4, 2020",,,10.21683/1729-2646-2020-20-4-25-34,http://arxiv.org/abs/2003.00260v1,"In this paper we discuss how systems with Artificial Intelligence (AI) can
undergo safety assessment. This is relevant, if AI is used in safety related
applications. Taking a deeper look into AI models, we show, that many models of
artificial intelligence, in particular machine learning, are statistical
models. Safety assessment would then have t o concentrate on the model that is
used in AI, besides the normal assessment procedure. Part of the budget of
dangerous random failures for the relevant safety integrity level needs to be
used for the probabilistic faulty behavior of the AI system. We demonstrate our
thoughts with a simple example and propose a research challenge that may be
decisive for the use of AI in safety related systems.",2022-11-13 15:59:51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Ernesto Jiménez-Ruiz, Asan Agibetov, Jiaoyan Chen, Matthias Samwald, Valerie Cross",Dividing the Ontology Alignment Task with Semantic Embeddings and Logic-based Modules,,,,,http://arxiv.org/abs/2003.05370v1,"Large ontologies still pose serious challenges to state-of-the-art ontology
alignment systems. In this paper we present an approach that combines a neural
embedding model and logic-based modules to accurately divide an input ontology
matching task into smaller and more tractable matching (sub)tasks. We have
conducted a comprehensive evaluation using the datasets of the Ontology
Alignment Evaluation Initiative. The results are encouraging and suggest that
the proposed method is adequate in practice and can be integrated within the
workflow of systems unable to cope with very large ontologies.",2022-11-13 15:59:52,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"S. Atakishiyev, H. Babiker, N. Farruque, R. Goebel1, M-Y. Kima, M. H. Motallebi, J. Rabelo, T. Syed, O. R. Zaïane",A multi-component framework for the analysis and design of explainable artificial intelligence,,,,,http://arxiv.org/abs/2005.01908v1,"The rapid growth of research in explainable artificial intelligence (XAI)
follows on two substantial developments. First, the enormous application
success of modern machine learning methods, especially deep and reinforcement
learning, which have created high expectations for industrial, commercial and
social value. Second, the emergence of concern for creating trusted AI systems,
including the creation of regulatory principles to ensure transparency and
trust of AI systems.These two threads have created a kind of ""perfect storm"" of
research activity, all eager to create and deliver it any set of tools and
techniques to address the XAI demand. As some surveys of current XAI suggest,
there is yet to appear a principled framework that respects the literature of
explainability in the history of science, and which provides a basis for the
development of a framework for transparent XAI. Here we intend to provide a
strategic inventory of XAI requirements, demonstrate their connection to a
history of XAI ideas, and synthesize those ideas into a simple framework to
calibrate five successive levels of XAI.",2022-11-13 15:59:53,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Johannes Schneider, Frank Breitinger",AI Forensics: Did the Artificial Intelligence System Do It? Why?,,,,,http://arxiv.org/abs/2005.13635v2,"In an increasingly autonomous manner AI systems make decisions impacting our
daily life. Their actions might cause accidents, harm or, more generally,
violate regulations -- either intentionally or not. Thus, AI systems might be
considered suspects for various events. Therefore, it is essential to relate
particular events to an AI, its owner and its creator. Given a multitude of AI
systems from multiple manufactures, potentially, altered by their owner or
changing through self-learning, this seems non-trivial. This paper discusses
how to identify AI systems responsible for incidents as well as their motives
that might be ""malicious by design"". In addition to a conceptualization, we
conduct two case studies based on reinforcement learning and convolutional
neural networks to illustrate our proposed methods and challenges. Our cases
illustrate that ""catching AI systems"" seems often far from trivial and requires
extensive expertise in machine learning. Legislative measures that enforce
mandatory information to be collected during operation of AI systems as well as
means to uniquely identify systems might facilitate the problem.",2022-11-13 15:59:53,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Andrea Aler Tubella, Andreas Theodorou, Virginia Dignum, Loizos Michael",Contestable Black Boxes,,,,10.1007/978-3-030-57977-7_12,http://arxiv.org/abs/2006.05133v2,"The right to contest a decision with consequences on individuals or the
society is a well-established democratic right. Despite this right also being
explicitly included in GDPR in reference to automated decision-making, its
study seems to have received much less attention in the AI literature compared,
for example, to the right for explanation. This paper investigates the type of
assurances that are needed in the contesting process when algorithmic
black-boxes are involved, opening new questions about the interplay of
contestability and explainability. We argue that specialised complementary
methodologies to evaluate automated decision-making in the case of a particular
decision being contested need to be developed. Further, we propose a
combination of well-established software engineering and rule-based approaches
as a possible socio-technical solution to the issue of contestability, one of
the new democratic challenges posed by the automation of decision making.",2022-11-13 15:59:54,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Alexander Matt Turner, Neale Ratzlaff, Prasad Tadepalli",Avoiding Side Effects in Complex Environments,,,,,http://arxiv.org/abs/2006.06547v2,"Reward function specification can be difficult. Rewarding the agent for
making a widget may be easy, but penalizing the multitude of possible negative
side effects is hard. In toy environments, Attainable Utility Preservation
(AUP) avoided side effects by penalizing shifts in the ability to achieve
randomly generated goals. We scale this approach to large, randomly generated
environments based on Conway's Game of Life. By preserving optimal value for a
single randomly generated reward function, AUP incurs modest overhead while
leading the agent to complete the specified task and avoid many side effects.
Videos and code are available at https://avoiding-side-effects.github.io/.",2022-11-13 15:59:54,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Gagan Bansal, Tongshuang Wu, Joyce Zhou, Raymond Fok, Besmira Nushi, Ece Kamar, Marco Tulio Ribeiro, Daniel S. Weld",Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance,,,,,http://arxiv.org/abs/2006.14779v3,"Many researchers motivate explainable AI with studies showing that human-AI
team performance on decision-making tasks improves when the AI explains its
recommendations. However, prior studies observed improvements from explanations
only when the AI, alone, outperformed both the human and the best team. Can
explanations help lead to complementary performance, where team accuracy is
higher than either the human or the AI working solo? We conduct mixed-method
user studies on three datasets, where an AI with accuracy comparable to humans
helps participants solve a task (explaining itself in some conditions). While
we observed complementary improvements from AI augmentation, they were not
increased by explanations. Rather, explanations increased the chance that
humans will accept the AI's recommendation, regardless of its correctness. Our
result poses new challenges for human-centered AI: Can we develop explanatory
approaches that encourage appropriate trust in AI, and therefore help generate
(or improve) complementary performance?",2022-11-13 15:59:55,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Amir Hosein Afshar Sedigh, Martin K. Purvis, Bastin Tony Roy Savarimuthu, Maryam A. Purvis, Christopher K. Frantz",Impact of meta-roles on the evolution of organisational institutions,,,,,http://arxiv.org/abs/2008.04096v1,"This paper investigates the impact of changes in agents' beliefs coupled with
dynamics in agents' meta-roles on the evolution of institutions. The study
embeds agents' meta-roles in the BDI architecture. In this context, the study
scrutinises the impact of cognitive dissonance in agents due to unfairness of
institutions. To showcase our model, two historical long-distance trading
societies, namely Armenian merchants of New-Julfa and the English East India
Company are simulated. Results show how change in roles of agents coupled with
specific institutional characteristics leads to changes of the rules in the
system.",2022-11-13 15:59:55,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Weichao Zhou, Ruihan Gao, BaekGyu Kim, Eunsuk Kang, Wenchao Li",Runtime-Safety-Guided Policy Repair,,,,,http://arxiv.org/abs/2008.07667v1,"We study the problem of policy repair for learning-based control policies in
safety-critical settings. We consider an architecture where a high-performance
learning-based control policy (e.g. one trained as a neural network) is paired
with a model-based safety controller. The safety controller is endowed with the
abilities to predict whether the trained policy will lead the system to an
unsafe state, and take over control when necessary. While this architecture can
provide added safety assurances, intermittent and frequent switching between
the trained policy and the safety controller can result in undesirable
behaviors and reduced performance. We propose to reduce or even eliminate
control switching by `repairing' the trained policy based on runtime data
produced by the safety controller in a way that deviates minimally from the
original policy. The key idea behind our approach is the formulation of a
trajectory optimization problem that allows the joint reasoning of policy
update and safety constraints. Experimental results demonstrate that our
approach is effective even when the system model in the safety controller is
unknown and only approximated.",2022-11-13 15:59:56,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Zihan Ding, Tianyang Yu, Yanhua Huang, Hongming Zhang, Guo Li, Quancheng Guo, Luo Mai, Hao Dong",Efficient Reinforcement Learning Development with RLzoo,,,,,http://arxiv.org/abs/2009.08644v2,"Many researchers and developers are exploring for adopting Deep Reinforcement
Learning (DRL) techniques in their applications. They however often find such
an adoption challenging. Existing DRL libraries provide poor support for
prototyping DRL agents (i.e., models), customising the agents, and comparing
the performance of DRL agents. As a result, the developers often report low
efficiency in developing DRL agents. In this paper, we introduce RLzoo, a new
DRL library that aims to make the development of DRL agents efficient. RLzoo
provides developers with (i) high-level yet flexible APIs for prototyping DRL
agents, and further customising the agents for best performance, (ii) a model
zoo where users can import a wide range of DRL agents and easily compare their
performance, and (iii) an algorithm that can automatically construct DRL agents
with custom components (which are critical to improve agent's performance in
custom applications). Evaluation results show that RLzoo can effectively reduce
the development cost of DRL agents, while achieving comparable performance with
existing DRL libraries.",2022-11-13 15:59:57,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Herman Yau, Chris Russell, Simon Hadfield",What Did You Think Would Happen? Explaining Agent Behaviour Through Intended Outcomes,,,,,http://arxiv.org/abs/2011.05064v1,"We present a novel form of explanation for Reinforcement Learning, based
around the notion of intended outcome. These explanations describe the outcome
an agent is trying to achieve by its actions. We provide a simple proof that
general methods for post-hoc explanations of this nature are impossible in
traditional reinforcement learning. Rather, the information needed for the
explanations must be collected in conjunction with training the agent. We
derive approaches designed to extract local explanations based on intention for
several variants of Q-function approximation and prove consistency between the
explanations and the Q-values learned. We demonstrate our method on multiple
reinforcement learning problems, and provide code to help researchers
introspecting their RL environments and algorithms.",2022-11-13 15:59:57,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Spyros Angelopoulos, Shahin Kamali",Contract Scheduling With Predictions,,,,,http://arxiv.org/abs/2011.12439v1,"Contract scheduling is a general technique that allows to design a system
with interruptible capabilities, given an algorithm that is not necessarily
interruptible. Previous work on this topic has largely assumed that the
interruption is a worst-case deadline that is unknown to the scheduler. In this
work, we study the setting in which there is a potentially erroneous prediction
concerning the interruption. Specifically, we consider the setting in which the
prediction describes the time that the interruption occurs, as well as the
setting in which the prediction is obtained as a response to a single or
multiple binary queries. For both settings, we investigate tradeoffs between
the robustness (i.e., the worst-case performance assuming adversarial
prediction) and the consistency (i.e, the performance assuming that the
prediction is error-free), both from the side of positive and negative results.",2022-11-13 15:59:58,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Francesca Foffano, Teresa Scantamburlo, Atia Cortés, Chiara Bissolo",European Strategy on AI: Are we truly fostering social good?,,,,,http://arxiv.org/abs/2011.12863v1,"Artificial intelligence (AI) is already part of our daily lives and is
playing a key role in defining the economic and social shape of the future. In
2018, the European Commission introduced its AI strategy able to compete in the
next years with world powers such as China and US, but relying on the respect
of European values and fundamental rights. As a result, most of the Member
States have published their own National Strategy with the aim to work on a
coordinated plan for Europe. In this paper, we present an ongoing study on how
European countries are approaching the field of Artificial Intelligence, with
its promises and risks, through the lens of their national AI strategies. In
particular, we aim to investigate how European countries are investing in AI
and to what extent the stated plans can contribute to the benefit of the whole
society. This paper reports the main findings of a qualitative analysis of the
investment plans reported in 15 European National Strategies",2022-11-13 15:59:59,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Taoan Huang, Bistra Dilkina, Sven Koenig",Learning to Resolve Conflicts for Multi-Agent Path Finding with Conflict-Based Search,,,,,http://arxiv.org/abs/2012.06005v1,"Conflict-Based Search (CBS) is a state-of-the-art algorithm for multi-agent
path finding. At the high level, CBS repeatedly detects conflicts and resolves
one of them by splitting the current problem into two subproblems. Previous
work chooses the conflict to resolve by categorizing the conflict into three
classes and always picking a conflict from the highest-priority class. In this
work, we propose an oracle for conflict selection that results in smaller
search tree sizes than the one used in previous work. However, the computation
of the oracle is slow. Thus, we propose a machine-learning framework for
conflict selection that observes the decisions made by the oracle and learns a
conflict-selection strategy represented by a linear ranking function that
imitates the oracle's decisions accurately and quickly. Experiments on
benchmark maps indicate that our method significantly improves the success
rates, the search tree sizes and runtimes over the current state-of-the-art CBS
solver.",2022-11-13 15:59:59,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Qi Zhang, Edmund H. Durfee, Satinder Singh",Efficient Querying for Cooperative Probabilistic Commitments,,,,,http://arxiv.org/abs/2012.07195v1,"Multiagent systems can use commitments as the core of a general coordination
infrastructure, supporting both cooperative and non-cooperative interactions.
Agents whose objectives are aligned, and where one agent can help another
achieve greater reward by sacrificing some of its own reward, should choose a
cooperative commitment to maximize their joint reward. We present a solution to
the problem of how cooperative agents can efficiently find an (approximately)
optimal commitment by querying about carefully-selected commitment choices. We
prove structural properties of the agents' values as functions of the
parameters of the commitment specification, and develop a greedy method for
composing a query with provable approximation bounds, which we empirically show
can find nearly optimal commitments in a fraction of the time methods that lack
our insights require.",2022-11-13 16:00:00,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Tae Wan Kim, John Hooker, Thomas Donaldson",Taking Principles Seriously: A Hybrid Approach to Value Alignment,,,,,http://arxiv.org/abs/2012.11705v1,"An important step in the development of value alignment (VA) systems in AI is
understanding how VA can reflect valid ethical principles. We propose that
designers of VA systems incorporate ethics by utilizing a hybrid approach in
which both ethical reasoning and empirical observation play a role. This, we
argue, avoids committing the ""naturalistic fallacy,"" which is an attempt to
derive ""ought"" from ""is,"" and it provides a more adequate form of ethical
reasoning when the fallacy is not committed. Using quantified model logic, we
precisely formulate principles derived from deontological ethics and show how
they imply particular ""test propositions"" for any given action plan in an AI
rule base. The action plan is ethical only if the test proposition is
empirically true, a judgment that is made on the basis of empirical VA. This
permits empirical VA to integrate seamlessly with independently justified
ethical principles.",2022-11-13 16:00:00,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Nodens Koren, Qiuhong Ke, Yisen Wang, James Bailey, Xingjun Ma",Adversarial Interaction Attack: Fooling AI to Misinterpret Human Intentions,,,,,http://arxiv.org/abs/2101.06704v1,"Understanding the actions of both humans and artificial intelligence (AI)
agents is important before modern AI systems can be fully integrated into our
daily life. In this paper, we show that, despite their current huge success,
deep learning based AI systems can be easily fooled by subtle adversarial noise
to misinterpret the intention of an action in interaction scenarios. Based on a
case study of skeleton-based human interactions, we propose a novel adversarial
attack on interactions, and demonstrate how DNN-based interaction models can be
tricked to predict the participants' reactions in unexpected ways. From a
broader perspective, the scope of our proposed attack method is not confined to
problems related to skeleton data but can also be extended to any type of
problems involving sequential regressions. Our study highlights potential risks
in the interaction loop with AI and humans, which need to be carefully
addressed when deploying AI systems in safety-critical applications.",2022-11-13 16:00:01,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Samuel Alexander, Bill Hibbard",Measuring Intelligence and Growth Rate: Variations on Hibbard's Intelligence Measure,"Journal of Artificial General Intelligence 12(1), 2021",,,10.2478/jagi-2021-0001,http://arxiv.org/abs/2101.12047v1,"In 2011, Hibbard suggested an intelligence measure for agents who compete in
an adversarial sequence prediction game. We argue that Hibbard's idea should
actually be considered as two separate ideas: first, that the intelligence of
such agents can be measured based on the growth rates of the runtimes of the
competitors that they defeat; and second, one specific (somewhat arbitrary)
method for measuring said growth rates. Whereas Hibbard's intelligence measure
is based on the latter growth-rate-measuring method, we survey other methods
for measuring function growth rates, and exhibit the resulting Hibbard-like
intelligence measures and taxonomies. Of particular interest, we obtain
intelligence taxonomies based on Big-O and Big-Theta notation systems, which
taxonomies are novel in that they challenge conventional notions of what an
intelligence measure should look like. We discuss how intelligence measurement
of sequence predictors can indirectly serve as intelligence measurement for
agents with Artificial General Intelligence (AGIs).",2022-11-13 16:00:01,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Violet Xinying Chen, J. N. Hooker",Fairness through Social Welfare Optimization,,,,,http://arxiv.org/abs/2102.00311v4,"We propose social welfare optimization as a general paradigm for formalizing
fairness in AI systems. We argue that optimization models allow formulation of
a wide range of fairness criteria as social welfare functions, while enabling
AI to take advantage of highly advanced solution technology. Rather than
attempting to reduce bias between selected groups, one can achieve equity
across all groups by incorporating fairness into the social welfare function.
This also allows a fuller accounting of the welfare of the individuals
involved. We show how to integrate social welfare optimization with both
rule-based AI and machine learning, using either an in-processing or a
post-processing approach. We present empirical results from a case study as a
preliminary examination of the validity and potential of these integration
strategies.",2022-11-13 16:00:02,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,Koen Holtman,Counterfactual Planning in AGI Systems,,,,,http://arxiv.org/abs/2102.00834v1,"We present counterfactual planning as a design approach for creating a range
of safety mechanisms that can be applied in hypothetical future AI systems
which have Artificial General Intelligence.
  The key step in counterfactual planning is to use an AGI machine learning
system to construct a counterfactual world model, designed to be different from
the real world the system is in. A counterfactual planning agent determines the
action that best maximizes expected utility in this counterfactual planning
world, and then performs the same action in the real world.
  We use counterfactual planning to construct an AGI agent emergency stop
button, and a safety interlock that will automatically stop the agent before it
undergoes an intelligence explosion. We also construct an agent with an input
terminal that can be used by humans to iteratively improve the agent's reward
function, where the incentive for the agent to manipulate this improvement
process is suppressed. As an example of counterfactual planning in a non-agent
AGI system, we construct a counterfactual oracle.
  As a design approach, counterfactual planning is built around the use of a
graphical notation for defining mathematical counterfactuals. This two-diagram
notation also provides a compact and readable language for reasoning about the
complex types of self-referencing and indirect representation which are
typically present inside machine learning agents.",2022-11-13 16:00:02,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Tom Everitt, Ryan Carey, Eric Langlois, Pedro A Ortega, Shane Legg",Agent Incentives: A Causal Perspective,,,,,http://arxiv.org/abs/2102.01685v2,"We present a framework for analysing agent incentives using causal influence
diagrams. We establish that a well-known criterion for value of information is
complete. We propose a new graphical criterion for value of control,
establishing its soundness and completeness. We also introduce two new concepts
for incentive analysis: response incentives indicate which changes in the
environment affect an optimal decision, while instrumental control incentives
establish whether an agent can influence its utility via a variable X. For both
new concepts, we provide sound and complete graphical criteria. We show by
example how these results can help with evaluating the safety and fairness of
an AI system.",2022-11-13 16:00:03,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Sandhya Saisubramanian, Shlomo Zilberstein",Mitigating Negative Side Effects via Environment Shaping,,,,,http://arxiv.org/abs/2102.07017v1,"Agents operating in unstructured environments often produce negative side
effects (NSE), which are difficult to identify at design time. While the agent
can learn to mitigate the side effects from human feedback, such feedback is
often expensive and the rate of learning is sensitive to the agent's state
representation. We examine how humans can assist an agent, beyond providing
feedback, and exploit their broader scope of knowledge to mitigate the impacts
of NSE. We formulate this problem as a human-agent team with decoupled
objectives. The agent optimizes its assigned task, during which its actions may
produce NSE. The human shapes the environment through minor reconfiguration
actions so as to mitigate the impacts of the agent's side effects, without
affecting the agent's ability to complete its assigned task. We present an
algorithm to solve this problem and analyze its theoretical properties. Through
experiments with human subjects, we assess the willingness of users to perform
minor environment modifications to mitigate the impacts of NSE. Empirical
evaluation of our approach shows that the proposed framework can successfully
mitigate NSE, without affecting the agent's ability to complete its assigned
task.",2022-11-13 16:00:03,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Eric D. Langlois, Tom Everitt",How RL Agents Behave When Their Actions Are Modified,"Proceedings of the AAAI Conference on Artificial Intelligence,
  35(13), 11586-11594 (2021)",,,,http://arxiv.org/abs/2102.07716v2,"Reinforcement learning in complex environments may require supervision to
prevent the agent from attempting dangerous actions. As a result of supervisor
intervention, the executed action may differ from the action specified by the
policy. How does this affect learning? We present the Modified-Action Markov
Decision Process, an extension of the MDP model that allows actions to differ
from the policy. We analyze the asymptotic behaviours of common reinforcement
learning algorithms in this setting and show that they adapt in different ways:
some completely ignore modifications while others go to various lengths in
trying to avoid action modifications that decrease reward. By choosing the
right algorithm, developers can prevent their agents from learning to
circumvent interruptions or constraints, and better control agent responses to
other kinds of action modification, like self-damage.",2022-11-13 16:00:04,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Rukshan Wijesinghe, Kasun Vithanage, Dumindu Tissera, Alex Xavier, Subha Fernando, Jayathu Samarawickrama",Transferring Domain Knowledge with an Adviser in Continuous Tasks,,,,,http://arxiv.org/abs/2102.08029v1,"Recent advances in Reinforcement Learning (RL) have surpassed human-level
performance in many simulated environments. However, existing reinforcement
learning techniques are incapable of explicitly incorporating already known
domain-specific knowledge into the learning process. Therefore, the agents have
to explore and learn the domain knowledge independently through a trial and
error approach, which consumes both time and resources to make valid responses.
Hence, we adapt the Deep Deterministic Policy Gradient (DDPG) algorithm to
incorporate an adviser, which allows integrating domain knowledge in the form
of pre-learned policies or pre-defined relationships to enhance the agent's
learning process. Our experiments on OpenAi Gym benchmark tasks show that
integrating domain knowledge through advisers expedites the learning and
improves the policy towards better optima.",2022-11-13 16:00:04,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Sebastian Graef, Ilche Georgievski",Software Architecture for Next-Generation AI Planning Systems,,,,,http://arxiv.org/abs/2102.10985v1,"Artificial Intelligence (AI) planning is a flourishing research and
development discipline that provides powerful tools for searching a course of
action that achieves some user goal. While these planning tools show excellent
performance on benchmark planning problems, they represent challenging software
systems when it comes to their use and integration in real-world applications.
In fact, even in-depth understanding of their internal mechanisms does not
guarantee that one can successfully set up, use and manipulate existing
planning tools. We contribute toward alleviating this situation by proposing a
service-oriented planning architecture to be at the core of the ability to
design, develop and use next-generation AI planning systems. We collect and
classify common planning capabilities to form the building blocks of the
planning architecture. We incorporate software design principles and patterns
into the architecture to allow for usability, interoperability and reusability
of the planning capabilities. Our prototype planning system demonstrates the
potential of our approach for rapid prototyping and flexibility of system
composition. Finally, we provide insight into the qualitative advantages of our
approach when compared to a typical planning tool.",2022-11-13 16:00:05,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Grégoire Déletang, Jordi Grau-Moya, Miljan Martic, Tim Genewein, Tom McGrath, Vladimir Mikulik, Markus Kunesch, Shane Legg, Pedro A. Ortega",Causal Analysis of Agent Behavior for AI Safety,,,,,http://arxiv.org/abs/2103.03938v1,"As machine learning systems become more powerful they also become
increasingly unpredictable and opaque. Yet, finding human-understandable
explanations of how they work is essential for their safe deployment. This
technical report illustrates a methodology for investigating the causal
mechanisms that drive the behaviour of artificial agents. Six use cases are
covered, each addressing a typical question an analyst might ask about an
agent. In particular, we show that each question cannot be addressed by pure
observation alone, but instead requires conducting experiments with
systematically chosen manipulations so as to generate the correct causal
evidence.",2022-11-13 16:00:05,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Daniel Zhang, Saurabh Mishra, Erik Brynjolfsson, John Etchemendy, Deep Ganguli, Barbara Grosz, Terah Lyons, James Manyika, Juan Carlos Niebles, Michael Sellitto, Yoav Shoham, Jack Clark, Raymond Perrault",The AI Index 2021 Annual Report,,,,,http://arxiv.org/abs/2103.06312v1,"Welcome to the fourth edition of the AI Index Report. This year we
significantly expanded the amount of data available in the report, worked with
a broader set of external organizations to calibrate our data, and deepened our
connections with the Stanford Institute for Human-Centered Artificial
Intelligence (HAI). The AI Index Report tracks, collates, distills, and
visualizes data related to artificial intelligence. Its mission is to provide
unbiased, rigorously vetted, and globally sourced data for policymakers,
researchers, executives, journalists, and the general public to develop
intuitions about the complex field of AI. The report aims to be the most
credible and authoritative source for data and insights about AI in the world.",2022-11-13 16:00:06,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Alexandros Nikou, Anusha Mujumdar, Marin Orlic, Aneta Vulgarakis Feljan",Symbolic Reinforcement Learning for Safe RAN Control,,,,,http://arxiv.org/abs/2103.06602v1,"In this paper, we demonstrate a Symbolic Reinforcement Learning (SRL)
architecture for safe control in Radio Access Network (RAN) applications. In
our automated tool, a user can select a high-level safety specifications
expressed in Linear Temporal Logic (LTL) to shield an RL agent running in a
given cellular network with aim of optimizing network performance, as measured
through certain Key Performance Indicators (KPIs). In the proposed
architecture, network safety shielding is ensured through model-checking
techniques over combined discrete system models (automata) that are abstracted
through reinforcement learning. We demonstrate the user interface (UI) helping
the user set intent specifications to the architecture and inspect the
difference in allowed and blocked actions.",2022-11-13 16:00:07,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Ramya Ramakrishnan, Vaibhav Unhelkar, Ece Kamar, Julie Shah",A Bayesian Approach to Identifying Representational Errors,,,,,http://arxiv.org/abs/2103.15171v1,"Trained AI systems and expert decision makers can make errors that are often
difficult to identify and understand. Determining the root cause for these
errors can improve future decisions. This work presents Generative Error Model
(GEM), a generative model for inferring representational errors based on
observations of an actor's behavior (either simulated agent, robot, or human).
The model considers two sources of error: those that occur due to
representational limitations -- ""blind spots"" -- and non-representational
errors, such as those caused by noise in execution or systematic errors present
in the actor's policy. Disambiguating these two error types allows for targeted
refinement of the actor's policy (i.e., representational errors require
perceptual augmentation, while other errors can be reduced through methods such
as improved training or attention support). We present a Bayesian inference
algorithm for GEM and evaluate its utility in recovering representational
errors on multiple domains. Results show that our approach can recover blind
spots of both reinforcement learning agents as well as human users.",2022-11-13 16:00:07,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Siani Pearson, Martin Lloyd, Vivek Nallur",Towards An Ethics-Audit Bot,,,,,http://arxiv.org/abs/2103.15746v1,"In this paper we focus on artificial intelligence (AI) for governance, not
governance for AI, and on just one aspect of governance, namely ethics audit.
Different kinds of ethical audit bots are possible, but who makes the choices
and what are the implications? In this paper, we do not provide
ethical/philosophical solutions, but rather focus on the technical aspects of
what an AI-based solution for validating the ethical soundness of a target
system would be like. We propose a system that is able to conduct an ethical
audit of a target system, given certain socio-technical conditions. To be more
specific, we propose the creation of a bot that is able to support
organisations in ensuring that their software development lifecycles contain
processes that meet certain ethical standards.",2022-11-13 16:00:08,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,Melanie Mitchell,Why AI is Harder Than We Think,,,,,http://arxiv.org/abs/2104.12871v2,"Since its beginning in the 1950s, the field of artificial intelligence has
cycled several times between periods of optimistic predictions and massive
investment (""AI spring"") and periods of disappointment, loss of confidence, and
reduced funding (""AI winter""). Even with today's seemingly fast pace of AI
breakthroughs, the development of long-promised technologies such as
self-driving cars, housekeeping robots, and conversational companions has
turned out to be much harder than many people expected. One reason for these
repeating cycles is our limited understanding of the nature and complexity of
intelligence itself. In this paper I describe four fallacies in common
assumptions made by AI researchers, which can lead to overconfident predictions
about the field. I conclude by discussing the open questions spurred by these
fallacies, including the age-old challenge of imbuing machines with humanlike
common sense.",2022-11-13 16:00:09,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Anagha Kulkarni, Siddharth Srivastava, Subbarao Kambhampati",Planning for Proactive Assistance in Environments with Partial Observability,,,,,http://arxiv.org/abs/2105.00525v2,"This paper addresses the problem of synthesizing the behavior of an AI agent
that provides proactive task assistance to a human in settings like factory
floors where they may coexist in a common environment. Unlike in the case of
requested assistance, the human may not be expecting proactive assistance and
hence it is crucial for the agent to ensure that the human is aware of how the
assistance affects her task. This becomes harder when there is a possibility
that the human may neither have full knowledge of the AI agent's capabilities
nor have full observability of its activities. Therefore, our \textit{proactive
assistant} is guided by the following three principles: \textbf{(1)} its
activity decreases the human's cost towards her goal; \textbf{(2)} the human is
able to recognize the potential reduction in her cost; \textbf{(3)} its
activity optimizes the human's overall cost (time/resources) of achieving her
goal. Through empirical evaluation and user studies, we demonstrate the
usefulness of our approach.",2022-11-13 16:00:09,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Dominik Dellermann, Nikolaus Lipusch, Philipp Ebel, Karl Michael Popp, Jan Marco Leimeister",Finding the unicorn: Predicting early stage startup success through a hybrid intelligence method,,,,,http://arxiv.org/abs/2105.03360v1,"Artificial intelligence is an emerging topic and will soon be able to perform
decisions better than humans. In more complex and creative contexts such as
innovation, however, the question remains whether machines are superior to
humans. Machines fail in two kinds of situations: processing and interpreting
soft information (information that cannot be quantified) and making predictions
in unknowable risk situations of extreme uncertainty. In such situations, the
machine does not have representative information for a certain outcome.
Thereby, humans are still the gold standard for assessing soft signals and make
use of intuition. To predict the success of startups, we, thus, combine the
complementary capabilities of humans and machines in a Hybrid Intelligence
method. To reach our aim, we follow a design science research approach to
develop a Hybrid Intelligence method that combines the strength of both machine
and collective intelligence to demonstrate its utility for predictions under
extreme uncertainty.",2022-11-13 16:00:10,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,Hal Ashton,Definitions of intent suitable for algorithms,,,,,http://arxiv.org/abs/2106.04235v1,"Intent modifies an actor's culpability of many types wrongdoing. Autonomous
Algorithmic Agents have the capability of causing harm, and whilst their
current lack of legal personhood precludes them from committing crimes, it is
useful for a number of parties to understand under what type of intentional
mode an algorithm might transgress. From the perspective of the creator or
owner they would like ensure that their algorithms never intend to cause harm
by doing things that would otherwise be labelled criminal if committed by a
legal person. Prosecutors might have an interest in understanding whether the
actions of an algorithm were internally intended according to a transparent
definition of the concept. The presence or absence of intention in the
algorithmic agent might inform the court as to the complicity of its owner.
This article introduces definitions for direct, oblique (or indirect) and
ulterior intent which can be used to test for intent in an algorithmic actor.",2022-11-13 16:00:10,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Utkarsh Soni, Sarath Sreedharan, Subbarao Kambhampati",Not all users are the same: Providing personalized explanations for sequential decision making problems,,,,,http://arxiv.org/abs/2106.12207v1,"There is a growing interest in designing autonomous agents that can work
alongside humans. Such agents will undoubtedly be expected to explain their
behavior and decisions. While generating explanations is an actively researched
topic, most works tend to focus on methods that generate explanations that are
one size fits all. As in the specifics of the user-model are completely
ignored. The handful of works that look at tailoring their explanation to the
user's background rely on having specific models of the users (either analytic
models or learned labeling models). The goal of this work is thus to propose an
end-to-end adaptive explanation generation system that begins by learning the
different types of users that the agent could interact with. Then during the
interaction with the target user, it is tasked with identifying the type on the
fly and adjust its explanations accordingly. The former is achieved by a
data-driven clustering approach while for the latter, we compile our
explanation generation problem into a POMDP. We demonstrate the usefulness of
our system on two domains using state-of-the-art POMDP solvers. We also report
the results of a user study that investigates the benefits of providing
personalized explanations in a human-robot interaction setting.",2022-11-13 16:00:11,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Sriram Gopalakrishnan, Utkarsh Soni, Tung Thai, Panagiotis Lymperopoulos, Matthias Scheutz, Subbarao Kambhampati","Integrating Planning, Execution and Monitoring in the presence of Open World Novelties: Case Study of an Open World Monopoly Solver",,,,,http://arxiv.org/abs/2107.04303v2,"The game of monopoly is an adversarial multi-agent domain where there is no
fixed goal other than to be the last player solvent, There are useful subgoals
like monopolizing sets of properties, and developing them. There is also a lot
of randomness from dice rolls, card-draws, and adversaries' strategies. This
unpredictability is made worse when unknown novelties are added during
gameplay. Given these challenges, Monopoly was one of the test beds chosen for
the DARPA-SAILON program which aims to create agents that can detect and
accommodate novelties. To handle the game complexities, we developed an agent
that eschews complete plans, and adapts it's policy online as the game evolves.
In the most recent independent evaluation in the SAILON program, our agent was
the best performing agent on most measures. We herein present our approach and
results.",2022-11-13 16:00:11,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Pulkit Verma, Shashank Rao Marpally, Siddharth Srivastava",Discovering User-Interpretable Capabilities of Black-Box Planning Agents,,,,,http://arxiv.org/abs/2107.13668v3,"Several approaches have been developed for answering users' specific
questions about AI behavior and for assessing their core functionality in terms
of primitive executable actions. However, the problem of summarizing an AI
agent's broad capabilities for a user is comparatively new. This paper presents
an algorithm for discovering from scratch the suite of high-level
""capabilities"" that an AI system with arbitrary internal planning
algorithms/policies can perform. It computes conditions describing the
applicability and effects of these capabilities in user-interpretable terms.
Starting from a set of user-interpretable state properties, an AI agent, and a
simulator that the agent can interact with, our algorithm returns a set of
high-level capabilities with their parameterized descriptions. Empirical
evaluation on several game-based scenarios shows that this approach efficiently
learns descriptions of various types of AI agents in deterministic, fully
observable settings. User studies show that such descriptions are easier to
understand and reason with than the agent's primitive actions.",2022-11-13 16:00:12,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Mingyi Liu, Zhiying Tu, Xiaofei Xu, Zhongjie Wang",DySR: A Dynamic Representation Learning and Aligning based Model for Service Bundle Recommendation,,,,,http://arxiv.org/abs/2108.03360v1,"An increasing number and diversity of services are available, which result in
significant challenges to effective reuse service during requirement
satisfaction. There have been many service bundle recommendation studies and
achieved remarkable results. However, there is still plenty of room for
improvement in the performance of these methods. The fundamental problem with
these studies is that they ignore the evolution of services over time and the
representation gap between services and requirements. In this paper, we propose
a dynamic representation learning and aligning based model called DySR to
tackle these issues. DySR eliminates the representation gap between services
and requirements by learning a transformation function and obtains service
representations in an evolving social environment through dynamic graph
representation learning. Extensive experiments conducted on a real-world
dataset from ProgrammableWeb show that DySR outperforms existing
state-of-the-art methods in commonly used evaluation metrics, improving $F1@5$
from $36.1\%$ to $69.3\%$.",2022-11-13 16:00:12,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Pulkit Verma, Siddharth Srivastava",Learning Causal Models of Autonomous Agents using Interventions,,,,,http://arxiv.org/abs/2108.09586v1,"One of the several obstacles in the widespread use of AI systems is the lack
of requirements of interpretability that can enable a layperson to ensure the
safe and reliable behavior of such systems. We extend the analysis of an agent
assessment module that lets an AI system execute high-level instruction
sequences in simulators and answer the user queries about its execution of
sequences of actions. We show that such a primitive query-response capability
is sufficient to efficiently derive a user-interpretable causal model of the
system in stationary, fully observable, and deterministic settings. We also
introduce dynamic causal decision networks (DCDNs) that capture the causal
structure of STRIPS-like domains. A comparative analysis of different classes
of queries is also presented in terms of the computational requirements needed
to answer them and the efforts required to evaluate their responses to learn
the correct model.",2022-11-13 16:00:13,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Ido Shapira, Amos Azaria",A Socially Aware Reinforcement Learning Agent for The Single Track Road Problem,,,,,http://arxiv.org/abs/2109.05486v3,"We present the single track road problem. In this problem two agents face
each-other at opposite positions of a road that can only have one agent pass at
a time. We focus on the scenario in which one agent is human, while the other
is an autonomous agent. We run experiments with human subjects in a simple grid
domain, which simulates the single track road problem. We show that when data
is limited, building an accurate human model is very challenging, and that a
reinforcement learning agent, which is based on this data, does not perform
well in practice. However, we show that an agent that tries to maximize a
linear combination of the human's utility and its own utility, achieves a high
score, and significantly outperforms other baselines, including an agent that
tries to maximize only its own utility.",2022-11-13 16:00:14,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Helen Bubinger, Jesse David Dinneen",Actionable Approaches to Promote Ethical AI in Libraries,,,,,http://arxiv.org/abs/2109.09672v1,"The widespread use of artificial intelligence (AI) in many domains has
revealed numerous ethical issues from data and design to deployment. In
response, countless broad principles and guidelines for ethical AI have been
published, and following those, specific approaches have been proposed for how
to encourage ethical outcomes of AI. Meanwhile, library and information
services too are seeing an increase in the use of AI-powered and machine
learning-powered information systems, but no practical guidance currently
exists for libraries to plan for, evaluate, or audit the ethics of intended or
deployed AI. We therefore report on several promising approaches for promoting
ethical AI that can be adapted from other contexts to AI-powered information
services and in different stages of the software lifecycle.",2022-11-13 16:00:14,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Antti Keurulainen, Isak Westerlund, Samuel Kaski, Alexander Ilin",Learning to Assist Agents by Observing Them,,,,,http://arxiv.org/abs/2110.01311v1,"The ability of an AI agent to assist other agents, such as humans, is an
important and challenging goal, which requires the assisting agent to reason
about the behavior and infer the goals of the assisted agent. Training such an
ability by using reinforcement learning usually requires large amounts of
online training, which is difficult and costly. On the other hand, offline data
about the behavior of the assisted agent might be available, but is non-trivial
to take advantage of by methods such as offline reinforcement learning. We
introduce methods where the capability to create a representation of the
behavior is first pre-trained with offline data, after which only a small
amount of interaction data is needed to learn an assisting policy. We test the
setting in a gridworld where the helper agent has the capability to manipulate
the environment of the assisted artificial agents, and introduce three
different scenarios where the assistance considerably improves the performance
of the assisted agents.",2022-11-13 16:00:15,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Carles Sierra, Nardine Osman, Pablo Noriega, Jordi Sabater-Mir, Antoni Perelló",Value alignment: a formal approach,,,,,http://arxiv.org/abs/2110.09240v1,"principles that should govern autonomous AI systems. It essentially states
that a system's goals and behaviour should be aligned with human values. But
how to ensure value alignment? In this paper we first provide a formal model to
represent values through preferences and ways to compute value aggregations;
i.e. preferences with respect to a group of agents and/or preferences with
respect to sets of values. Value alignment is then defined, and computed, for a
given norm with respect to a given value through the increase/decrease that it
results in the preferences of future states of the world. We focus on norms as
it is norms that govern behaviour, and as such, the alignment of a given system
with a given value will be dictated by the norms the system follows.",2022-11-13 16:00:16,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Montaser Mohammedalamen, Dustin Morrill, Alexander Sieusahai, Yash Satsangi, Michael Bowling",Learning to Be Cautious,,,,,http://arxiv.org/abs/2110.15907v1,"A key challenge in the field of reinforcement learning is to develop agents
that behave cautiously in novel situations. It is generally impossible to
anticipate all situations that an autonomous system may face or what behavior
would best avoid bad outcomes. An agent that could learn to be cautious would
overcome this challenge by discovering for itself when and how to behave
cautiously. In contrast, current approaches typically embed task-specific
safety information or explicit cautious behaviors into the system, which is
error-prone and imposes extra burdens on practitioners. In this paper, we
present both a sequence of tasks where cautious behavior becomes increasingly
non-obvious, as well as an algorithm to demonstrate that it is possible for a
system to \emph{learn} to be cautious. The essential features of our algorithm
are that it characterizes reward function uncertainty without task-specific
safety information and uses this uncertainty to construct a robust policy.
Specifically, we construct robust policies with a $k$-of-$N$ counterfactual
regret minimization (CFR) subroutine given a learned reward function
uncertainty represented by a neural network ensemble belief. These policies
exhibit caution in each of our tasks without any task-specific safety tuning.",2022-11-13 16:00:16,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Haofeng Liu, Yiwen Chen, Jiayi Tan, Marcelo H Ang Jr",Improving Learning from Demonstrations by Learning from Experience,,,,,http://arxiv.org/abs/2111.08156v1,"How to make imitation learning more general when demonstrations are
relatively limited has been a persistent problem in reinforcement learning
(RL). Poor demonstrations lead to narrow and biased date distribution,
non-Markovian human expert demonstration makes it difficult for the agent to
learn, and over-reliance on sub-optimal trajectories can make it hard for the
agent to improve its performance. To solve these problems we propose a new
algorithm named TD3fG that can smoothly transition from learning from experts
to learning from experience. Our algorithm achieves good performance in the
MUJOCO environment with limited and sub-optimal demonstrations. We use behavior
cloning to train the network as a reference action generator and utilize it in
terms of both loss function and exploration noise. This innovation can help
agents extract a priori knowledge from demonstrations while reducing the
detrimental effects of the poor Markovian properties of the demonstrations. It
has a better performance compared to the BC+ fine-tuning and DDPGfD approach,
especially when the demonstrations are relatively limited. We call our method
TD3fG meaning TD3 from a generator.",2022-11-13 16:00:17,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Qinghua Lu, Liming Zhu, Xiwei Xu, Jon Whittle, David Douglas, Conrad Sanderson",Software Engineering for Responsible AI: An Empirical Study and Operationalised Patterns,,,,10.1109/ICSE-SEIP55303.2022.9793864,http://arxiv.org/abs/2111.09478v1,"Although artificial intelligence (AI) is solving real-world challenges and
transforming industries, there are serious concerns about its ability to behave
and make decisions in a responsible way. Many AI ethics principles and
guidelines for responsible AI have been recently issued by governments,
organisations, and enterprises. However, these AI ethics principles and
guidelines are typically high-level and do not provide concrete guidance on how
to design and develop responsible AI systems. To address this shortcoming, we
first present an empirical study where we interviewed 21 scientists and
engineers to understand the practitioners' perceptions on AI ethics principles
and their implementation. We then propose a template that enables AI ethics
principles to be operationalised in the form of concrete patterns and suggest a
list of patterns using the newly created template. These patterns provide
concrete, operationalised guidance that facilitate the development of
responsible AI systems.",2022-11-13 16:00:17,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,Koen Holtman,Demanding and Designing Aligned Cognitive Architectures,,,,,http://arxiv.org/abs/2112.10190v1,"With AI systems becoming more powerful and pervasive, there is increasing
debate about keeping their actions aligned with the broader goals and needs of
humanity. This multi-disciplinary and multi-stakeholder debate must resolve
many issues, here we examine three of them. The first issue is to clarify what
demands stakeholders might usefully make on the designers of AI systems, useful
because the technology exists to implement them. We make this technical topic
more accessible by using the framing of cognitive architectures. The second
issue is to move beyond an analytical framing that treats useful intelligence
as being reward maximization only. To support this move, we define several AI
cognitive architectures that combine reward maximization with other technical
elements designed to improve alignment. The third issue is how stakeholders
should calibrate their interactions with modern machine learning researchers.
We consider how current fashions in machine learning create a narrative pull
that participants in technical and policy discussions should be aware of, so
that they can compensate for it. We identify several technically tractable but
currently unfashionable options for improving AI alignment.",2022-11-13 16:00:18,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Ruiqi He, Yash Raj Jain, Falk Lieder",Have I done enough planning or should I plan more?,,,,,http://arxiv.org/abs/2201.00764v1,"People's decisions about how to allocate their limited computational
resources are essential to human intelligence. An important component of this
metacognitive ability is deciding whether to continue thinking about what to do
and move on to the next decision. Here, we show that people acquire this
ability through learning and reverse-engineer the underlying learning
mechanisms. Using a process-tracing paradigm that externalises human planning,
we find that people quickly adapt how much planning they perform to the cost
and benefit of planning. To discover the underlying metacognitive learning
mechanisms we augmented a set of reinforcement learning models with
metacognitive features and performed Bayesian model selection. Our results
suggest that the metacognitive ability to adjust the amount of planning might
be learned through a policy-gradient mechanism that is guided by metacognitive
pseudo-rewards that communicate the value of planning.",2022-11-13 16:00:18,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Federico Malato, Joona Jehkonen, Ville Hautamäki",Improving Behavioural Cloning with Human-Driven Dynamic Dataset Augmentation,,,,,http://arxiv.org/abs/2201.07719v1,"Behavioural cloning has been extensively used to train agents and is
recognized as a fast and solid approach to teach general behaviours based on
expert trajectories. Such method follows the supervised learning paradigm and
it strongly depends on the distribution of the data. In our paper, we show how
combining behavioural cloning with human-in-the-loop training solves some of
its flaws and provides an agent task-specific corrections to overcome tricky
situations while speeding up the training time and lowering the required
resources. To do this, we introduce a novel approach that allows an expert to
take control of the agent at any moment during a simulation and provide optimal
solutions to its problematic situations. Our experiments show that this
approach leads to better policies both in terms of quantitative evaluation and
in human-likeliness.",2022-11-13 16:00:19,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,Junchen Zhao,Safety-Aware Multi-Agent Apprenticeship Learning,,,,,http://arxiv.org/abs/2201.08111v2,"Our objective of this project is to make the extension based on the technique
mentioned in the paper ""Safety-Aware Apprenticeship Learning"" to improve the
utility and the efficiency of the existing Reinforcement Learning model from a
Single-Agent Learning framework to a Multi-Agent Learning framework. Our
contributions to the project are presented in the following bullet points: 1.
Regarding the fact that we will add an extension to the Inverse Reinforcement
Learning model from a Single-Agent scenario to a Multi-Agentscenario. Our first
contribution to this project is considering the case of extracting safe reward
functions from expert behaviors in a Multi-Agent scenario instead of being from
the Single-Agent scenario. 2. Our second contribution is extending the
Single-Agent Learning Framework to a Multi-Agent Learning framework and
designing a novel Learning Framework based on the extension in the end. 3. Our
final contribution to this project is evaluating empirically the performance of
my extension to the Single-Agent Inverse Reinforcement Learning framework.",2022-11-13 16:00:19,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Sasha Salter, Kristian Hartikainen, Walter Goodwin, Ingmar Posner","Priors, Hierarchy, and Information Asymmetry for Skill Transfer in Reinforcement Learning",,,,,http://arxiv.org/abs/2201.08115v1,"The ability to discover behaviours from past experience and transfer them to
new tasks is a hallmark of intelligent agents acting sample-efficiently in the
real world. Equipping embodied reinforcement learners with the same ability may
be crucial for their successful deployment in robotics. While hierarchical and
KL-regularized RL individually hold promise here, arguably a hybrid approach
could combine their respective benefits. Key to these fields is the use of
information asymmetry to bias which skills are learnt. While asymmetric choice
has a large influence on transferability, prior works have explored a narrow
range of asymmetries, primarily motivated by intuition. In this paper, we
theoretically and empirically show the crucial trade-off, controlled by
information asymmetry, between the expressivity and transferability of skills
across sequential tasks. Given this insight, we provide a principled approach
towards choosing asymmetry and apply our approach to a complex, robotic block
stacking domain, unsolvable by baselines, demonstrating the effectiveness of
hierarchical KL-regularized RL, coupled with correct asymmetric choice, for
sample-efficient transfer learning.",2022-11-13 16:00:20,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Stephanie Galaitsi, Benjamin D. Trump, Jeffrey M. Keisler, Igor Linkov, Alexander Kott",Cybertrust: From Explainable to Actionable and Interpretable AI (AI2),,,,,http://arxiv.org/abs/2201.11117v1,"To benefit from AI advances, users and operators of AI systems must have
reason to trust it. Trust arises from multiple interactions, where predictable
and desirable behavior is reinforced over time. Providing the system's users
with some understanding of AI operations can support predictability, but
forcing AI to explain itself risks constraining AI capabilities to only those
reconcilable with human cognition. We argue that AI systems should be designed
with features that build trust by bringing decision-analytic perspectives and
formal tools into AI. Instead of trying to achieve explainable AI, we should
develop interpretable and actionable AI. Actionable and Interpretable AI (AI2)
will incorporate explicit quantifications and visualizations of user confidence
in AI recommendations. In doing so, it will allow examining and testing of AI
system predictions to establish a basis for trust in the systems' decision
making and ensure broad benefits from deploying and advancing its computational
capabilities.",2022-11-13 16:00:21,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Raphael Koster, Jan Balaguer, Andrea Tacchetti, Ari Weinstein, Tina Zhu, Oliver Hauser, Duncan Williams, Lucy Campbell-Gillingham, Phoebe Thacker, Matthew Botvinick, Christopher Summerfield",Human-centered mechanism design with Democratic AI,,,,,http://arxiv.org/abs/2201.11441v1,"Building artificial intelligence (AI) that aligns with human values is an
unsolved problem. Here, we developed a human-in-the-loop research pipeline
called Democratic AI, in which reinforcement learning is used to design a
social mechanism that humans prefer by majority. A large group of humans played
an online investment game that involved deciding whether to keep a monetary
endowment or to share it with others for collective benefit. Shared revenue was
returned to players under two different redistribution mechanisms, one designed
by the AI and the other by humans. The AI discovered a mechanism that redressed
initial wealth imbalance, sanctioned free riders, and successfully won the
majority vote. By optimizing for human preferences, Democratic AI may be a
promising method for value-aligned policy innovation.",2022-11-13 16:00:21,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,Vacslav Glukhov,Reward is not enough: can we liberate AI from the reinforcement learning paradigm?,,,,,http://arxiv.org/abs/2202.03192v2,"I present arguments against the hypothesis put forward by Silver, Singh,
Precup, and Sutton (
https://www.sciencedirect.com/science/article/pii/S0004370221000862 ) : reward
maximization is not enough to explain many activities associated with natural
and artificial intelligence including knowledge, learning, perception, social
intelligence, evolution, language, generalisation and imitation. I show such
reductio ad lucrum has its intellectual origins in the political economy of
Homo economicus and substantially overlaps with the radical version of
behaviourism. I show why the reinforcement learning paradigm, despite its
demonstrable usefulness in some practical application, is an incomplete
framework for intelligence -- natural and artificial. Complexities of
intelligent behaviour are not simply second-order complications on top of
reward maximisation. This fact has profound implications for the development of
practically usable, smart, safe and robust artificially intelligent agents.",2022-11-13 16:00:22,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Cameron Haigh, Zichen Zhang, Negar Hassanpour, Khurram Javed, Yingying Fu, Shayan Shahramian, Shawn Zhang, Jun Luo",Drawing Inductor Layout with a Reinforcement Learning Agent: Method and Application for VCO Inductors,,,,,http://arxiv.org/abs/2202.11798v2,"Design of Voltage-Controlled Oscillator (VCO) inductors is a laborious and
time-consuming task that is conventionally done manually by human experts. In
this paper, we propose a framework for automating the design of VCO inductors,
using Reinforcement Learning (RL). We formulate the problem as a sequential
procedure, where wire segments are drawn one after another, until a complete
inductor is created. We then employ an RL agent to learn to draw inductors that
meet certain target specifications. In light of the need to tweak the target
specifications throughout the circuit design cycle, we also develop a variant
in which the agent can learn to quickly adapt to draw new inductors for
moderately different target specifications. Our empirical results show that the
proposed framework is successful at automatically generating VCO inductors that
meet or exceed the target specification.",2022-11-13 16:00:22,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Peter Schüller, João Paolo Costeira, James Crowley, Jasmin Grosinger, Félix Ingrand, Uwe Köckemann, Alessandro Saffiotti, Martin Welss",Composing Complex and Hybrid AI Solutions,,,,,http://arxiv.org/abs/2202.12566v1,"Progress in several areas of computer science has been enabled by comfortable
and efficient means of experimentation, clear interfaces, and interchangable
components, for example using OpenCV for computer vision or ROS for robotics.
We describe an extension of the Acumos system towards enabling the above
features for general AI applications. Originally, Acumos was created for
telecommunication purposes, mainly for creating linear pipelines of machine
learning components. Our extensions include support for more generic components
with gRPC/Protobuf interfaces, automatic orchestration of graphically assembled
solutions including control loops, sub-component topologies, and event-based
communication,and provisions for assembling solutions which contain user
interfaces and shared storage areas. We provide examples of deployable
solutions and their interfaces. The framework is deployed at
http://aiexp.ai4europe.eu/ and its source code is managed as an open source
Eclipse project.",2022-11-13 16:00:23,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Rebecca Gorman, Stuart Armstrong",The dangers in algorithms learning humans' values and irrationalities,,,,,http://arxiv.org/abs/2202.13985v2,"For an artificial intelligence (AI) to be aligned with human values (or human
preferences), it must first learn those values. AI systems that are trained on
human behavior, risk miscategorising human irrationalities as human values --
and then optimising for these irrationalities. Simply learning human values
still carries risks: AI learning them will inevitably also gain information on
human irrationalities and human behaviour/policy. Both of these can be
dangerous: knowing human policy allows an AI to become generically more
powerful (whether it is partially aligned or not aligned at all), while
learning human irrationalities allows it to exploit humans without needing to
provide value in return. This paper analyses the danger in developing
artificial intelligence that learns about human irrationalities and human
policy, and constructs a model recommendation system with various levels of
information about human biases, human policy, and human values. It concludes
that, whatever the power and knowledge of the AI, it is more dangerous for it
to know human irrationalities than human values. Thus it is better for the AI
to learn human values directly, rather than learning human biases and then
deducing values from behaviour.",2022-11-13 16:00:23,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Qinghua Lu, Liming Zhu, Xiwei Xu, Jon Whittle",Responsible-AI-by-Design: a Pattern Collection for Designing Responsible AI Systems,,,,,http://arxiv.org/abs/2203.00905v2,"Although AI has significant potential to transform society, there are serious
concerns about its ability to behave and make decisions responsibly. Many
ethical regulations, principles, and guidelines for responsible AI have been
issued recently. However, these principles are high-level and difficult to put
into practice. In the meantime much effort has been put into responsible AI
from the algorithm perspective, but they are limited to a small subset of
ethical principles amenable to mathematical analysis. Responsible AI issues go
beyond data and algorithms and are often at the system-level crosscutting many
system components and the entire software engineering lifecycle. Based on the
result of a systematic literature review, this paper identifies one missing
element as the system-level guidance - how to design the architecture of
responsible AI systems. We present a summary of design patterns that can be
embedded into the AI systems as product features to contribute to
responsible-AI-by-design.",2022-11-13 16:00:24,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Jinghui Lu, Linyi Yang, Brian Mac Namee, Yue Zhang",A Rationale-Centric Framework for Human-in-the-loop Machine Learning,,,,,http://arxiv.org/abs/2203.12918v1,"We present a novel rationale-centric framework with human-in-the-loop --
Rationales-centric Double-robustness Learning (RDL) -- to boost model
out-of-distribution performance in few-shot learning scenarios. By using static
semi-factual generation and dynamic human-intervened correction, RDL exploits
rationales (i.e. phrases that cause the prediction), human interventions and
semi-factual augmentations to decouple spurious associations and bias models
towards generally applicable underlying distributions, which enables fast and
accurate generalisation. Experimental results show that RDL leads to
significant prediction benefits on both in-distribution and out-of-distribution
tests compared to many state-of-the-art benchmarks -- especially for few-shot
learning scenarios. We also perform extensive ablation studies to support
in-depth analyses of each component in our framework.",2022-11-13 16:00:25,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Daniel Zhang, Nestor Maslej, Erik Brynjolfsson, John Etchemendy, Terah Lyons, James Manyika, Helen Ngo, Juan Carlos Niebles, Michael Sellitto, Ellie Sakhaee, Yoav Shoham, Jack Clark, Raymond Perrault",The AI Index 2022 Annual Report,,,,,http://arxiv.org/abs/2205.03468v1,"Welcome to the fifth edition of the AI Index Report! The latest edition
includes data from a broad set of academic, private, and nonprofit
organizations as well as more self-collected data and original analysis than
any previous editions, including an expanded technical performance chapter, a
new survey of robotics researchers around the world, data on global AI
legislation records in 25 countries, and a new chapter with an in-depth
analysis of technical AI ethics metrics.
  The AI Index Report tracks, collates, distills, and visualizes data related
to artificial intelligence. Its mission is to provide unbiased, rigorously
vetted, and globally sourced data for policymakers, researchers, executives,
journalists, and the general public to develop a more thorough and nuanced
understanding of the complex field of AI. The report aims to be the world's
most credible and authoritative source for data and insights about AI.",2022-11-13 16:00:25,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Giuseppe De Giacomo, Dror Fried, Fabio Patrizi, Shufang Zhu",Mimicking Behaviors in Separated Domains,,,,,http://arxiv.org/abs/2205.09201v1,"Devising a strategy to make a system mimicking behaviors from another system
is a problem that naturally arises in many areas of Computer Science. In this
work, we interpret this problem in the context of intelligent agents, from the
perspective of LTLf, a formalism commonly used in AI for expressing
finite-trace properties. Our model consists of two separated dynamic domains,
D_A and D_B, and an LTLf specification that formalizes the notion of mimicking
by mapping properties on behaviors (traces) of D_A into properties on behaviors
of D_B. The goal is to synthesize a strategy that step-by-step maps every
behavior of D_A into a behavior of D_B so that the specification is met. We
consider several forms of mapping specifications, ranging from simple ones to
full LTLf, and for each we study synthesis algorithms and computational
properties.",2022-11-13 16:00:26,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Julien Girard-Satabin, Michele Alberti, François Bobot, Zakaria Chihani, Augustin Lemesle",CAISAR: A platform for Characterizing Artificial Intelligence Safety and Robustness,"AISafety, Jul 2022, Vienne, Austria",,,,http://arxiv.org/abs/2206.03044v2,"We present CAISAR, an open-source platform under active development for the
characterization of AI systems' robustness and safety. CAISAR provides a
unified entry point for defining verification problems by using WhyML, the
mature and expressive language of the Why3 verification platform. Moreover,
CAISAR orchestrates and composes state-of-the-art machine learning verification
tools which, individually, are not able to efficiently handle all problems but,
collectively, can cover a growing number of properties. Our aim is to assist,
on the one hand, the V\&V process by reducing the burden of choosing the
methodology tailored to a given verification problem, and on the other hand the
tools developers by factorizing useful features-visualization, report
generation, property description-in one platform. CAISAR will soon be available
at https://git.frama-c.com/pub/caisar.",2022-11-13 16:00:27,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Alexander Matt Turner, Aseem Saxena, Prasad Tadepalli",Formalizing the Problem of Side Effect Regularization,,,,,http://arxiv.org/abs/2206.11812v3,"AI objectives are often hard to specify properly. Some approaches tackle this
problem by regularizing the AI's side effects: Agents must weigh off ""how much
of a mess they make"" with an imperfectly specified proxy objective. We propose
a formal criterion for side effect regularization via the assistance game
framework. In these games, the agent solves a partially observable Markov
decision process (POMDP) representing its uncertainty about the objective
function it should optimize. We consider the setting where the true objective
is revealed to the agent at a later time step. We show that this POMDP is
solved by trading off the proxy reward with the agent's ability to achieve a
range of future tasks. We empirically demonstrate the reasonableness of our
problem formalization via ground-truth evaluation in two gridworld
environments.",2022-11-13 16:00:27,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,Alexander Matt Turner,On Avoiding Power-Seeking by Artificial Intelligence,,,,,http://arxiv.org/abs/2206.11831v1,"We do not know how to align a very intelligent AI agent's behavior with human
interests. I investigate whether -- absent a full solution to this AI alignment
problem -- we can build smart AI agents which have limited impact on the world,
and which do not autonomously seek power. In this thesis, I introduce the
attainable utility preservation (AUP) method. I demonstrate that AUP produces
conservative, option-preserving behavior within toy gridworlds and within
complex environments based off of Conway's Game of Life. I formalize the
problem of side effect avoidance, which provides a way to quantify the side
effects an agent had on the world. I also give a formal definition of
power-seeking in the context of AI agents and show that optimal policies tend
to seek power. In particular, most reward functions have optimal policies which
avoid deactivation. This is a problem if we want to deactivate or correct an
intelligent agent after we have deployed it. My theorems suggest that since
most agent goals conflict with ours, the agent would very probably resist
correction. I extend these theorems to show that power-seeking incentives occur
not just for optimal decision-makers, but under a wide range of decision-making
procedures.",2022-11-13 16:00:28,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Alexander Matt Turner, Prasad Tadepalli",Parametrically Retargetable Decision-Makers Tend To Seek Power,,,,,http://arxiv.org/abs/2206.13477v2,"If capable AI agents are generally incentivized to seek power in service of
the objectives we specify for them, then these systems will pose enormous
risks, in addition to enormous benefits. In fully observable environments, most
reward functions have an optimal policy which seeks power by keeping options
open and staying alive. However, the real world is neither fully observable,
nor must trained agents be even approximately reward-optimal. We consider a
range of models of AI decision-making, from optimal, to random, to choices
informed by learning and interacting with an environment. We discover that many
decision-making functions are retargetable, and that retargetability is
sufficient to cause power-seeking tendencies. Our functional criterion is
simple and broad. We show that a range of qualitatively dissimilar
decision-making procedures incentivize agents to seek power. We demonstrate the
flexibility of our results by reasoning about learned policy incentives in
Montezuma's Revenge. These results suggest a safety risk: Eventually,
retargetable training procedures may train real-world agents which seek power
over humans.",2022-11-13 16:00:28,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Zachary Kenton, Ramana Kumar, Sebastian Farquhar, Jonathan Richens, Matt MacDermott, Tom Everitt",Discovering Agents,,,,,http://arxiv.org/abs/2208.08345v2,"Causal models of agents have been used to analyse the safety aspects of
machine learning systems. But identifying agents is non-trivial -- often the
causal model is just assumed by the modeler without much justification -- and
modelling failures can lead to mistakes in the safety analysis. This paper
proposes the first formal causal definition of agents -- roughly that agents
are systems that would adapt their policy if their actions influenced the world
in a different way. From this we derive the first causal discovery algorithm
for discovering agents from empirical data, and give algorithms for translating
between causal models and game-theoretic influence diagrams. We demonstrate our
approach by resolving some previous confusions caused by incorrect causal
modelling of agents.",2022-11-13 16:00:29,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Bettina Könighofer, Roderick Bloem, Rüdiger Ehlers, Christian Pek",Correct-by-Construction Runtime Enforcement in AI -- A Survey,,,,,http://arxiv.org/abs/2208.14426v1,"Runtime enforcement refers to the theories, techniques, and tools for
enforcing correct behavior with respect to a formal specification of systems at
runtime. In this paper, we are interested in techniques for constructing
runtime enforcers for the concrete application domain of enforcing safety in
AI. We discuss how safety is traditionally handled in the field of AI and how
more formal guarantees on the safety of a self-learning agent can be given by
integrating a runtime enforcer. We survey a selection of work on such
enforcers, where we distinguish between approaches for discrete and continuous
action spaces. The purpose of this paper is to foster a better understanding of
advantages and limitations of different enforcement techniques, focusing on the
specific challenges that arise due to their application in AI. Finally, we
present some open challenges and avenues for future work.",2022-11-13 16:00:29,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Peter Jamieson, Indrima Upadhyay",A Technique to Create Weaker Abstract Board Game Agents via Reinforcement Learning,,,,,http://arxiv.org/abs/2209.00711v1,"Board games, with the exception of solo games, need at least one other player
to play. Because of this, we created Artificial Intelligent (AI) agents to play
against us when an opponent is missing. These AI agents are created in a number
of ways, but one challenge with these agents is that an agent can have superior
ability compared to us. In this work, we describe how to create weaker AI
agents that play board games. We use Tic-Tac-Toe, Nine-Men's Morris, and
Mancala, and our technique uses a Reinforcement Learning model where an agent
uses the Q-learning algorithm to learn these games. We show how these agents
can learn to play the board game perfectly, and we then describe our approach
to making weaker versions of these agents. Finally, we provide a methodology to
compare AI agents.",2022-11-13 16:00:30,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Yohai Trabelsi, Abhijin Adiga, Sarit Kraus, S. S. Ravi",Resource Allocation to Agents with Restrictions: Maximizing Likelihood with Minimum Compromise,,,,,http://arxiv.org/abs/2209.05170v1,"Many scenarios where agents with restrictions compete for resources can be
cast as maximum matching problems on bipartite graphs. Our focus is on resource
allocation problems where agents may have restrictions that make them
incompatible with some resources. We assume that a Principle chooses a maximum
matching randomly so that each agent is matched to a resource with some
probability. Agents would like to improve their chances of being matched by
modifying their restrictions within certain limits. The Principle's goal is to
advise an unsatisfied agent to relax its restrictions so that the total cost of
relaxation is within a budget (chosen by the agent) and the increase in the
probability of being assigned a resource is maximized. We establish hardness
results for some variants of this budget-constrained maximization problem and
present algorithmic results for other variants. We experimentally evaluate our
methods on synthetic datasets as well as on two novel real-world datasets: a
vacation activities dataset and a classrooms dataset.",2022-11-13 16:00:30,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Dylan M. Asmar, Mykel J. Kochenderfer",Collaborative Decision Making Using Action Suggestions,,,,,http://arxiv.org/abs/2209.13160v1,"The level of autonomy is increasing in systems spanning multiple domains, but
these systems still experience failures. One way to mitigate the risk of
failures is to integrate human oversight of the autonomous systems and rely on
the human to take control when the autonomy fails. In this work, we formulate a
method of collaborative decision making through action suggestions that
improves action selection without taking control of the system. Our approach
uses each suggestion efficiently by incorporating the implicit information
shared through suggestions to modify the agent's belief and achieves better
performance with fewer suggestions than naively following the suggested
actions. We assume collaborative agents share the same objective and
communicate through valid actions. By assuming the suggested action is
dependent only on the state, we can incorporate the suggested action as an
independent observation of the environment. The assumption of a collaborative
environment enables us to use the agent's policy to estimate the distribution
over action suggestions. We propose two methods that use suggested actions and
demonstrate the approach through simulated experiments. The proposed
methodology results in increased performance while also being robust to
suboptimal suggestions.",2022-11-13 16:00:31,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Sander Beckers, Hana Chockler, Joseph Y. Halpern",Quantifying Harm,,,,,http://arxiv.org/abs/2209.15111v2,"In a companion paper (Beckers et al. 2022), we defined a qualitative notion
of harm: either harm is caused, or it is not. For practical applications, we
often need to quantify harm; for example, we may want to choose the lest
harmful of a set of possible interventions. We first present a quantitative
definition of harm in a deterministic context involving a single individual,
then we consider the issues involved in dealing with uncertainty regarding the
context and going from a notion of harm for a single individual to a notion of
""societal harm"", which involves aggregating the harm to individuals. We show
that the ""obvious"" way of doing this (just taking the expected harm for an
individual and then summing the expected harm over all individuals can lead to
counterintuitive or inappropriate answers, and discuss alternatives, drawing on
work from the decision-theory literature.",2022-11-13 16:00:31,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Cosmin Badea, Leilani Gilpin","Establishing Meta-Decision-Making for AI: An Ontology of Relevance, Representation and Reasoning",,,,,http://arxiv.org/abs/2210.00608v1,"We propose an ontology of building decision-making systems, with the aim of
establishing Meta-Decision-Making for Artificial Intelligence (AI), improving
autonomy, and creating a framework to build metrics and benchmarks upon. To
this end, we propose the three parts of Relevance, Representation, and
Reasoning, and discuss their value in ensuring safety and mitigating risk in
the context of third wave cognitive systems. Our nomenclature reflects the
literature on decision-making, and our ontology allows researchers that adopt
it to frame their work in relation to one or more of these parts.",2022-11-13 16:00:32,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Hengyuan Hu, David J Wu, Adam Lerer, Jakob Foerster, Noam Brown",Human-AI Coordination via Human-Regularized Search and Learning,,,,,http://arxiv.org/abs/2210.05125v1,"We consider the problem of making AI agents that collaborate well with humans
in partially observable fully cooperative environments given datasets of human
behavior. Inspired by piKL, a human-data-regularized search method that
improves upon a behavioral cloning policy without diverging far away from it,
we develop a three-step algorithm that achieve strong performance in
coordinating with real humans in the Hanabi benchmark. We first use a
regularized search algorithm and behavioral cloning to produce a better human
model that captures diverse skill levels. Then, we integrate the policy
regularization idea into reinforcement learning to train a human-like best
response to the human model. Finally, we apply regularized search on top of the
best response policy at test time to handle out-of-distribution challenges when
playing with humans. We evaluate our method in two large scale experiments with
humans. First, we show that our method outperforms experts when playing with a
group of diverse human players in ad-hoc teams. Second, we show that our method
beats a vanilla best response to behavioral cloning baseline by having experts
play repeatedly with the two agents.",2022-11-13 16:00:32,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Anthony Zador, Blake Richards, Bence Ölveczky, Sean Escola, Yoshua Bengio, Kwabena Boahen, Matthew Botvinick, Dmitri Chklovskii, Anne Churchland, Claudia Clopath, James DiCarlo, Surya Ganguli, Jeff Hawkins, Konrad Koerding, Alexei Koulakov, Yann LeCun, Timothy Lillicrap, Adam Marblestone, Bruno Olshausen, Alexandre Pouget, Cristina Savin, Terrence Sejnowski, Eero Simoncelli, Sara Solla, David Sussillo, Andreas S. Tolias, Doris Tsao",Toward Next-Generation Artificial Intelligence: Catalyzing the NeuroAI Revolution,,,,,http://arxiv.org/abs/2210.08340v2,"Neuroscience has long been an important driver of progress in artificial
intelligence (AI). We propose that to accelerate progress in AI, we must invest
in fundamental research in NeuroAI.",2022-11-13 16:00:33,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Andrea Tocchetti, Lorenzo Corti, Agathe Balayn, Mireia Yurrita, Philip Lippmann, Marco Brambilla, Jie Yang",A.I. Robustness: a Human-Centered Perspective on Technological Challenges and Opportunities,,,,,http://arxiv.org/abs/2210.08906v2,"Despite the impressive performance of Artificial Intelligence (AI) systems,
their robustness remains elusive and constitutes a key issue that impedes
large-scale adoption. Robustness has been studied in many domains of AI, yet
with different interpretations across domains and contexts. In this work, we
systematically survey the recent progress to provide a reconciled terminology
of concepts around AI robustness. We introduce three taxonomies to organize and
describe the literature both from a fundamental and applied point of view: 1)
robustness by methods and approaches in different phases of the machine
learning pipeline; 2) robustness for specific model architectures, tasks, and
systems; and in addition, 3) robustness assessment methodologies and insights,
particularly the trade-offs with other trustworthiness properties. Finally, we
identify and discuss research gaps and opportunities and give an outlook on the
field. We highlight the central role of humans in evaluating and enhancing AI
robustness, considering the necessary knowledge humans can provide, and discuss
the need for better understanding practices and developing supportive tools in
the future.",2022-11-13 16:00:34,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2006,"V. V. Kryssanov, V. A. Abramov, Y. Fukuda, K. Konishi",A Decision-Making Support System Based on Know-How,"CIRP Journal of Manufacturing Systems. 1998, Vol. 27, No.4,
  427-432",,,,http://arxiv.org/abs/cs/0606010v1,"The research results described are concerned with: - developing a domain
modeling method and tools to provide the design and implementation of
decision-making support systems for computer integrated manufacturing; -
building a decision-making support system based on know-how and its software
environment. The research is funded by NEDO, Japan.",2022-11-13 16:00:34,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2012,"C. Calderón, L. Delaye, V. Mireles, P. Miramontes",Detecting lateral genetic material transfer,,,,,http://arxiv.org/abs/1204.2601v1,"The bioinformatical methods to detect lateral gene transfer events are mainly
based on functional coding DNA characteristics. In this paper, we propose the
use of DNA traits not depending on protein coding requirements. We introduce
several semilocal variables that depend on DNA primary sequence and that
reflect thermodynamic as well as physico-chemical magnitudes that are able to
tell apart the genome of different organisms. After combining these variables
in a neural classificator, we obtain results whose power of resolution go as
far as to detect the exchange of genomic material between bacteria that are
phylogenetically close.",2022-11-13 16:00:35,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2013,Piyush Ahuja,"Man and Machine: Questions of Risk, Trust and Accountability in Today's AI Technology",,,,,http://arxiv.org/abs/1307.7127v1,"Artificial Intelligence began as a field probing some of the most fundamental
questions of science - the nature of intelligence and the design of intelligent
artifacts. But it has grown into a discipline that is deeply entwined with
commerce and society. Today's AI technology, such as expert systems and
intelligent assistants, pose some difficult questions of risk, trust and
accountability. In this paper, we present these concerns, examining them in the
context of historical developments that have shaped the nature and direction of
AI research. We also suggest the exploration and further development of two
paradigms, human intelligence-machine cooperation, and a sociological view of
intelligence, which might help address some of these concerns.",2022-11-13 16:00:36,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2015,"Guido Governatori, Francesco Olivieri, Simone Scannapieco, Antonino Rotolo, Matteo Cristani",The Rationale behind the Concept of Goal,Theory and Practice of Logic Programming 16 (2016) 296-324,,,10.1017/S1471068416000053,http://arxiv.org/abs/1512.04021v1,"The paper proposes a fresh look at the concept of goal and advances that
motivational attitudes like desire, goal and intention are just facets of the
broader notion of (acceptable) outcome. We propose to encode the preferences of
an agent as sequences of ""alternative acceptable outcomes"". We then study how
the agent's beliefs and norms can be used to filter the mental attitudes out of
the sequences of alternative acceptable outcomes. Finally, we formalise such
intuitions in a novel Modal Defeasible Logic and we prove that the resulting
formalisation is computationally feasible.",2022-11-13 16:00:36,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,Richard Pettigrew,Aggregating incoherent agents who disagree,,,,,http://arxiv.org/abs/1709.03981v1,"In this paper, we explore how we should aggregate the degrees of belief of of
a group of agents to give a single coherent set of degrees of belief, when at
least some of those agents might be probabilistically incoherent. There are a
number of way of aggregating degrees of belief, and there are a number of ways
of fixing incoherent degrees of belief. When we have picked one of each, should
we aggregate first and then fix, or fix first and then aggregate? Or should we
try to do both at once? And when do these different procedures agree with one
another? In this paper, we focus particularly on the final question.",2022-11-13 16:00:37,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Saurabh Srivastava, Vinay P. Namboodiri, T. V. Prabhakar",PUTWorkbench: Analysing Privacy in AI-intensive Systems,,,,,http://arxiv.org/abs/1902.01580v1,"AI intensive systems that operate upon user data face the challenge of
balancing data utility with privacy concerns. We propose the idea and present
the prototype of an open-source tool called Privacy Utility Trade-off (PUT)
Workbench which seeks to aid software practitioners to take such crucial
decisions. We pick a simple privacy model that doesn't require any background
knowledge in Data Science and show how even that can achieve significant
results over standard and real-life datasets. The tool and the source code is
made freely available for extensions and usage.",2022-11-13 16:00:38,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Kristian Kersting, Jan Peters, Constantin Rothkopf",Was ist eine Professur fuer Kuenstliche Intelligenz?,,,,,http://arxiv.org/abs/1903.09516v1,"The Federal Government of Germany aims to boost the research in the field of
Artificial Intelligence (AI). For instance, 100 new professorships are said to
be established. However, the white paper of the government does not answer what
an AI professorship is at all. In order to give colleagues, politicians, and
citizens an idea, we present a view that is often followed when appointing
professors for AI at German and international universities. We hope that it
will help to establish a guideline with internationally accepted measures and
thus make the public debate more informed.",2022-11-13 16:00:38,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Amanda Askell, Miles Brundage, Gillian Hadfield",The Role of Cooperation in Responsible AI Development,,,,,http://arxiv.org/abs/1907.04534v1,"In this paper, we argue that competitive pressures could incentivize AI
companies to underinvest in ensuring their systems are safe, secure, and have a
positive social impact. Ensuring that AI systems are developed responsibly may
therefore require preventing and solving collective action problems between
companies. We note that there are several key factors that improve the
prospects for cooperation in collective action problems. We use this to
identify strategies to improve the prospects for industry cooperation on the
responsible development of AI.",2022-11-13 16:00:39,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Nikos Arechiga, Jonathan DeCastro, Soonho Kong, Karen Leung",Better AI through Logical Scaffolding,,,,,http://arxiv.org/abs/1909.06965v1,"We describe the concept of logical scaffolds, which can be used to improve
the quality of software that relies on AI components. We explain how some of
the existing ideas on runtime monitors for perception systems can be seen as a
specific instance of logical scaffolds. Furthermore, we describe how logical
scaffolds may be useful for improving AI programs beyond perception systems, to
include general prediction systems and agent behavior models.",2022-11-13 16:00:39,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,Samuel Allen Alexander,The Archimedean trap: Why traditional reinforcement learning will probably not yield AGI,Journal of Artificial General Intelligence 11(1): 70--85 (2020),,,10.2478/jagi-2020-0004,http://arxiv.org/abs/2002.10221v2,"After generalizing the Archimedean property of real numbers in such a way as
to make it adaptable to non-numeric structures, we demonstrate that the real
numbers cannot be used to accurately measure non-Archimedean structures. We
argue that, since an agent with Artificial General Intelligence (AGI) should
have no problem engaging in tasks that inherently involve non-Archimedean
rewards, and since traditional reinforcement learning rewards are real numbers,
therefore traditional reinforcement learning probably will not lead to AGI. We
indicate two possible ways traditional reinforcement learning could be altered
to remove this roadblock.",2022-11-13 16:00:40,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Gabriel Lima, Meeyoung Cha",Responsible AI and Its Stakeholders,,,,,http://arxiv.org/abs/2004.11434v1,"Responsible Artificial Intelligence (AI) proposes a framework that holds all
stakeholders involved in the development of AI to be responsible for their
systems. It, however, fails to accommodate the possibility of holding AI
responsible per se, which could close some legal and moral gaps concerning the
deployment of autonomous and self-learning systems. We discuss three notions of
responsibility (i.e., blameworthiness, accountability, and liability) for all
stakeholders, including AI, and suggest the roles of jurisdiction and the
general public in this matter.",2022-11-13 16:00:40,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,Markus Borg,The AIQ Meta-Testbed: Pragmatically Bridging Academic AI Testing and Industrial Q Needs,,,,,http://arxiv.org/abs/2009.05260v1,"AI solutions seem to appear in any and all application domains. As AI becomes
more pervasive, the importance of quality assurance increases. Unfortunately,
there is no consensus on what artificial intelligence means and interpretations
range from simple statistical analysis to sentient humanoid robots. On top of
that, quality is a notoriously hard concept to pinpoint. What does this mean
for AI quality? In this paper, we share our working definition and a pragmatic
approach to address the corresponding quality assurance with a focus on
testing. Finally, we present our ongoing work on establishing the AIQ
Meta-Testbed.",2022-11-13 16:00:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,Evan Hubinger,An overview of 11 proposals for building safe advanced AI,,,,,http://arxiv.org/abs/2012.07532v1,"This paper analyzes and compares 11 different proposals for building safe
advanced AI under the current machine learning paradigm, including major
contenders such as iterated amplification, AI safety via debate, and recursive
reward modeling. Each proposal is evaluated on the four components of outer
alignment, inner alignment, training competitiveness, and performance
competitiveness, of which the distinction between the latter two is introduced
in this paper. While prior literature has primarily focused on analyzing
individual proposals, or primarily focused on outer alignment at the expense of
inner alignment, this analysis seeks to take a comparative look at a wide range
of proposals including a comparative analysis across all four previously
mentioned components.",2022-11-13 16:00:42,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Jurriaan van Diggelen, Wiard Jorritsma, Bob van der Vecht",Teaming up with information agents,,,,,http://arxiv.org/abs/2101.06133v1,"Despite the intricacies involved in designing a computer as a teampartner, we
can observe patterns in team behavior which allow us to describe at a general
level how AI systems are to collaborate with humans. Whereas most work on
human-machine teaming has focused on physical agents (e.g. robotic systems),
our aim is to study how humans can collaborate with information agents. We
propose some appropriate team design patterns, and test them using our
Collaborative Intelligence Analysis (CIA) tool.",2022-11-13 16:00:42,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,Bryce Goodman,Hard Choices and Hard Limits for Artificial Intelligence,,,,10.1145/3461702.3462539,http://arxiv.org/abs/2105.07852v1,"Artificial intelligence (AI) is supposed to help us make better choices. Some
of these choices are small, like what route to take to work, or what music to
listen to. Others are big, like what treatment to administer for a disease or
how long to sentence someone for a crime. If AI can assist with these big
decisions, we might think it can also help with hard choices, cases where
alternatives are neither better, worse nor equal but on a par. The aim of this
paper, however, is to show that this view is mistaken: the fact of parity shows
that there are hard limits on AI in decision making and choices that AI cannot,
and should not, resolve.",2022-11-13 16:00:43,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Abdullah Khan, Alexei Vernitski, Alexei Lisitsa",Untangling Braids with Multi-agent Q-Learning,,,,,http://arxiv.org/abs/2109.14502v1,"We use reinforcement learning to tackle the problem of untangling braids. We
experiment with braids with 2 and 3 strands. Two competing players learn to
tangle and untangle a braid. We interface the braid untangling problem with the
OpenAI Gym environment, a widely used way of connecting agents to reinforcement
learning problems. The results provide evidence that the more we train the
system, the better the untangling player gets at untangling braids. At the same
time, our tangling player produces good examples of tangled braids.",2022-11-13 16:00:43,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,John Piorkowski,The 6-Ds of Creating AI-Enabled Systems,,,,,http://arxiv.org/abs/2202.03172v1,"We are entering our tenth year of the current Artificial Intelligence (AI)
spring, and, as with previous AI hype cycles, the threat of an AI winter looms.
AI winters occurred because of ineffective approaches towards navigating the
technology valley of death. The 6-D framework provides an end-to-end framework
to successfully navigate this challenge. The 6-D framework starts with problem
decomposition to identify potential AI solutions, and ends with considerations
for deployment of AI-enabled systems. Each component of the 6-D framework and a
precision medicine use case is described in this paper.",2022-11-13 16:00:44,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Thao Le, Tim Miller, Ronal Singh, Liz Sonenberg",Improving Model Understanding and Trust with Counterfactual Explanations of Model Confidence,,,,,http://arxiv.org/abs/2206.02790v1,"In this paper, we show that counterfactual explanations of confidence scores
help users better understand and better trust an AI model's prediction in
human-subject studies. Showing confidence scores in human-agent interaction
systems can help build trust between humans and AI systems. However, most
existing research only used the confidence score as a form of communication,
and we still lack ways to explain why the algorithm is confident. This paper
also presents two methods for understanding model confidence using
counterfactual explanation: (1) based on counterfactual examples; and (2) based
on visualisation of the counterfactual space.",2022-11-13 16:00:44,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Aastha Acharya, Rebecca Russell, Nisar R. Ahmed",Uncertainty Quantification for Competency Assessment of Autonomous Agents,,,,,http://arxiv.org/abs/2206.10553v1,"For safe and reliable deployment in the real world, autonomous agents must
elicit appropriate levels of trust from human users. One method to build trust
is to have agents assess and communicate their own competencies for performing
given tasks. Competency depends on the uncertainties affecting the agent,
making accurate uncertainty quantification vital for competency assessment. In
this work, we show how ensembles of deep generative models can be used to
quantify the agent's aleatoric and epistemic uncertainties when forecasting
task outcomes as part of competency assessment.",2022-11-13 16:00:45,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,Shuchan Wang,Information-Theoretic Equivalence of Entropic Multi-Marginal Optimal Transport: A Theory for Multi-Agent Communication,,,,,http://arxiv.org/abs/2208.10256v2,"In this paper, we propose our information-theoretic equivalence of entropic
multi-marginal optimal transport (MOT). This equivalence can be easily reduced
to the case of entropic optimal transport (OT). Because OT is widely used to
compare differences between knowledge or beliefs, we apply this result to the
communication between agents with different beliefs. Our results formally prove
the statement that entropic OT is information-theoretically optimal given by
Wang et al. [2020] and generalize it to the multi-agent case. We believe that
our work can shed light on OT theory in future multi-agent teaming systems.",2022-11-13 16:00:46,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Vasudev Gohil, Satwik Patnaik, Hao Guo, Dileep Kalathil, Jeyavijayan, Rajendran",DETERRENT: Detecting Trojans using Reinforcement Learning,,,,10.1145/3489517.3530518,http://arxiv.org/abs/2208.12878v1,"Insertion of hardware Trojans (HTs) in integrated circuits is a pernicious
threat. Since HTs are activated under rare trigger conditions, detecting them
using random logic simulations is infeasible. In this work, we design a
reinforcement learning (RL) agent that circumvents the exponential search space
and returns a minimal set of patterns that is most likely to detect HTs.
Experimental results on a variety of benchmarks demonstrate the efficacy and
scalability of our RL agent, which obtains a significant reduction
($169\times$) in the number of test patterns required while maintaining or
improving coverage ($95.75\%$) compared to the state-of-the-art techniques.",2022-11-13 16:00:46,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Andrés García-Silva, Cristian Berrío, José Manuel Gómez-Pérez",Generating Quizzes to Support Training on Quality Management and Assurance in Space Science and Engineering,,,,,http://arxiv.org/abs/2210.03427v2,"Quality management and assurance is key for space agencies to guarantee the
success of space missions, which are high-risk and extremely costly. In this
paper, we present a system to generate quizzes, a common resource to evaluate
the effectiveness of training sessions, from documents about quality assurance
procedures in the Space domain. Our system leverages state of the art
auto-regressive models like T5 and BART to generate questions, and a RoBERTa
model to extract answers for such questions, thus verifying their suitability.",2022-11-13 16:00:47,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,1995,"S. J. Russell, D. Subramanian",Provably Bounded-Optimal Agents,"Journal of Artificial Intelligence Research, Vol 2, (1995),
  575-609",,,,http://arxiv.org/abs/cs/9505103v1,"Since its inception, artificial intelligence has relied upon a theoretical
foundation centered around perfect rationality as the desired property of
intelligent systems. We argue, as others have done, that this foundation is
inadequate because it imposes fundamentally unsatisfiable requirements. As a
result, there has arisen a wide gap between theory and practice in AI,
hindering progress in the field. We propose instead a property called bounded
optimality. Roughly speaking, an agent is bounded-optimal if its program is a
solution to the constrained optimization problem presented by its architecture
and the task environment. We show how to construct agents with this property
for a simple class of machine architectures in a broad class of real-time
environments. We illustrate these results using a simple model of an automated
mail sorting facility. We also define a weaker property, asymptotic bounded
optimality (ABO), that generalizes the notion of optimality in classical
complexity theory. We then construct universal ABO programs, i.e., programs
that are ABO no matter what real-time constraints are applied. Universal ABO
programs can be used as building blocks for more complex systems. We conclude
with a discussion of the prospects for bounded optimality as a theoretical
basis for AI, and relate it to similar trends in philosophy, economics, and
game theory.",2022-11-13 16:00:47,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,1997,M. Tambe,Towards Flexible Teamwork,"Journal of Artificial Intelligence Research, Vol 7, (1997), 83-124",,,,http://arxiv.org/abs/cs/9709101v1,"Many AI researchers are today striving to build agent teams for complex,
dynamic multi-agent domains, with intended applications in arenas such as
education, training, entertainment, information integration, and collective
robotics. Unfortunately, uncertainties in these complex, dynamic domains
obstruct coherent teamwork. In particular, team members often encounter
differing, incomplete, and possibly inconsistent views of their environment.
Furthermore, team members can unexpectedly fail in fulfilling responsibilities
or discover unexpected opportunities. Highly flexible coordination and
communication is key in addressing such uncertainties. Simply fitting
individual agents with precomputed coordination plans will not do, for their
inflexibility can cause severe failures in teamwork, and their
domain-specificity hinders reusability. Our central hypothesis is that the key
to such flexibility and reusability is providing agents with general models of
teamwork. Agents exploit such models to autonomously reason about coordination
and communication, providing requisite flexibility. Furthermore, the models
enable reuse across domains, both saving implementation effort and enforcing
consistency. This article presents one general, implemented model of teamwork,
called STEAM. The basic building block of teamwork in STEAM is joint intentions
(Cohen & Levesque, 1991b); teamwork in STEAM is based on agents' building up a
(partial) hierarchy of joint intentions (this hierarchy is seen to parallel
Grosz & Kraus's partial SharedPlans, 1996). Furthermore, in STEAM, team members
monitor the team's and individual members' performance, reorganizing the team
as necessary. Finally, decision-theoretic communication selectivity in STEAM
ensures reduction in communication overheads of teamwork, with appropriate
sensitivity to the environmental conditions. This article describes STEAM's
application in three different complex domains, and presents detailed empirical
results.",2022-11-13 16:00:48,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2012,"Tobias Jung, Daniel Polani, Peter Stone",Empowerment for Continuous Agent-Environment Systems,"Adaptive Behavior 19(1),2011",,,,http://arxiv.org/abs/1201.6583v1,"This paper develops generalizations of empowerment to continuous states.
Empowerment is a recently introduced information-theoretic quantity motivated
by hypotheses about the efficiency of the sensorimotor loop in biological
organisms, but also from considerations stemming from curiosity-driven
learning. Empowemerment measures, for agent-environment systems with stochastic
transitions, how much influence an agent has on its environment, but only that
influence that can be sensed by the agent sensors. It is an
information-theoretic generalization of joint controllability (influence on
environment) and observability (measurement by sensors) of the environment by
the agent, both controllability and observability being usually defined in
control theory as the dimensionality of the control/observation spaces. Earlier
work has shown that empowerment has various interesting and relevant
properties, e.g., it allows us to identify salient states using only the
dynamics, and it can act as intrinsic reward without requiring an external
reward. However, in this previous work empowerment was limited to the case of
small-scale and discrete domains and furthermore state transition probabilities
were assumed to be known. The goal of this paper is to extend empowerment to
the significantly more important and relevant case of continuous vector-valued
state spaces and initially unknown state transition probabilities. The
continuous state space is addressed by Monte-Carlo approximation; the unknown
transitions are addressed by model learning and prediction for which we apply
Gaussian processes regression with iterated forecasting. In a number of
well-known continuous control tasks we examine the dynamics induced by
empowerment and include an application to exploration and online model
learning.",2022-11-13 16:00:49,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2012,"Patrick Rodler, Kostyantyn Shchekotykhin, Philipp Fleiss, Gerhard Friedrich",RIO: Minimizing User Interaction in Ontology Debugging,,,,,http://arxiv.org/abs/1209.3734v1,"Efficient ontology debugging is a cornerstone for many activities in the
context of the Semantic Web, especially when automatic tools produce (parts of)
ontologies such as in the field of ontology matching. The best currently known
interactive debugging systems rely upon some meta information in terms of fault
probabilities, which can speed up the debugging procedure in the good case, but
can also have negative impact on the performance in the bad case. The problem
is that assessment of the meta information is only possible a-posteriori.
Consequently, as long as the actual fault is unknown, there is always some risk
of suboptimal interactive diagnoses discrimination. As an alternative, one
might prefer to rely on a tool which pursues a no-risk strategy. In this case,
however, possibly well-chosen meta information cannot be exploited, resulting
again in inefficient debugging actions. In this work we present a reinforcement
learning strategy that continuously adapts its behavior depending on the
performance achieved and minimizes the risk of using low-quality meta
information. Therefore, this method is suitable for application scenarios where
reliable a-priori fault estimates are difficult to obtain. Using problematic
ontologies in the field of ontology matching, we show that the proposed
risk-aware query strategy outperforms both active learning approaches and
no-risk strategies on average in terms of required amount of user interaction.",2022-11-13 16:00:49,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2014,"Yifeng Zeng, Prashant Doshi",Exploiting Model Equivalences for Solving Interactive Dynamic Influence Diagrams,"Journal Of Artificial Intelligence Research, Volume 43, pages
  211-255, 2012",,,10.1613/jair.3461,http://arxiv.org/abs/1401.4600v1,"We focus on the problem of sequential decision making in partially observable
environments shared with other agents of uncertain types having similar or
conflicting objectives. This problem has been previously formalized by multiple
frameworks one of which is the interactive dynamic influence diagram (I-DID),
which generalizes the well-known influence diagram to the multiagent setting.
I-DIDs are graphical models and may be used to compute the policy of an agent
given its belief over the physical state and others models, which changes as
the agent acts and observes in the multiagent setting.
  As we may expect, solving I-DIDs is computationally hard. This is
predominantly due to the large space of candidate models ascribed to the other
agents and its exponential growth over time. We present two methods for
reducing the size of the model space and stemming its exponential growth. Both
these methods involve aggregating individual models into equivalence classes.
Our first method groups together behaviorally equivalent models and selects
only those models for updating which will result in predictive behaviors that
are distinct from others in the updated model space. The second method further
compacts the model space by focusing on portions of the behavioral predictions.
Specifically, we cluster actionally equivalent models that prescribe identical
actions at a single time step. Exactly identifying the equivalences would
require us to solve all models in the initial set. We avoid this by selectively
solving some of the models, thereby introducing an approximation. We discuss
the error introduced by the approximation, and empirically demonstrate the
improved efficiency in solving I-DIDs due to the equivalences.",2022-11-13 16:00:50,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2015,Hao Wu,What is Learning? A primary discussion about information and Representation,,,,,http://arxiv.org/abs/1505.04813v1,"Nowadays, represented by Deep Learning techniques, the field of machine
learning is experiencing unprecedented prosperity and its influence is
demonstrated in academia, industry and civil society. ""Intelligent"" has become
a label which could not be neglected for most applications; celebrities and
scientists also warned that the development of full artificial intelligence may
spell the end of the human race. It seems that the answer to building a
computer system that could automatically improve with experience is right on
the next corner. While for AI and machine learning researchers, it is a
consensus that we are not anywhere near the core technique which could bring
the Terminator, Number 5 or R2D2 into real life, and there is not even a formal
definition about what is intelligence, or one of its basic properties:
Learning. Therefore, even though researchers know these concerns are not
necessary currently, there is no generalized explanation about why these
concerns are not necessary, and what properties people should take into account
that would make these concerns to be necessary. In this paper, starts from
analysing the relation between information and its representation, a necessary
condition for a model to be a learning model is proposed. This condition and
related future works could be used to verify whether a system is able to learn
or not, and enrich our understanding of learning: one important property of
Intelligence.",2022-11-13 16:00:51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2015,"Song-Ju Kim, Tohru Tsuruoka, Tsuyoshi Hasegawa, Masakazu Aono",Decision Maker based on Atomic Switches,,,,,http://arxiv.org/abs/1507.05895v1,"We propose a simple model for an atomic switch-based decision maker (ASDM),
and show that, as long as its total volume of precipitated Ag atoms is
conserved when coupled with suitable operations, an atomic switch system
provides a sophisticated ""decision-making"" capability that is known to be one
of the most important intellectual abilities in human beings. We considered the
multi-armed bandit problem (MAB); the problem of finding, as accurately and
quickly as possible, the most profitable option from a set of options that
gives stochastic rewards. These decisions are made as dictated by each volume
of precipitated Ag atoms, which is moved in a manner similar to the
fluctuations of a rigid body in a tug-of-war game. The ""tug-of-war (TOW)
dynamics"" of the ASDM exhibits higher efficiency than conventional MAB solvers.
We show analytical calculations that validate the statistical reasons for the
ASDM dynamics to produce such high performance, despite its simplicity. These
results imply that various physical systems, in which some conservation law
holds, can be used to implement efficient ""decision-making objects."" Efficient
MAB solvers are useful for many practical applications, because MAB abstracts a
variety of decision-making problems in real- world situations where an
efficient trial-and-error is required. The proposed scheme will introduce a new
physics-based analog computing paradigm, which will include such things as
""intelligent nano devices"" and ""intelligent information networks"" based on
self-detection and self-judgment.",2022-11-13 16:00:51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2015,"Owain Evans, Andreas Stuhlmueller, Noah D. Goodman","Learning the Preferences of Ignorant, Inconsistent Agents",,,,,http://arxiv.org/abs/1512.05832v1,"An important use of machine learning is to learn what people value. What
posts or photos should a user be shown? Which jobs or activities would a person
find rewarding? In each case, observations of people's past choices can inform
our inferences about their likes and preferences. If we assume that choices are
approximately optimal according to some utility function, we can treat
preference inference as Bayesian inverse planning. That is, given a prior on
utility functions and some observed choices, we invert an optimal
decision-making process to infer a posterior distribution on utility functions.
However, people often deviate from approximate optimality. They have false
beliefs, their planning is sub-optimal, and their choices may be temporally
inconsistent due to hyperbolic discounting and other biases. We demonstrate how
to incorporate these deviations into algorithms for preference inference by
constructing generative models of planning for agents who are subject to false
beliefs and time inconsistency. We explore the inferences these models make
about preferences, beliefs, and biases. We present a behavioral experiment in
which human subjects perform preference inference given the same observations
of choices as our model. Results show that human subjects (like our model)
explain choices in terms of systematic deviations from optimal behavior and
suggest that they take such deviations into account when inferring preferences.",2022-11-13 16:00:52,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2016,Andrew MacFie,Analysis of Algorithms and Partial Algorithms,"Artificial General Intelligence 2016, New York, USA, July 16-19,
  2016, Proceedings, 284-293",,,10.1007/978-3-319-41649-6_29,http://arxiv.org/abs/1601.03411v5,"We present an alternative methodology for the analysis of algorithms, based
on the concept of expected discounted reward. This methodology naturally
handles algorithms that do not always terminate, so it can (theoretically) be
used with partial algorithms for undecidable problems, such as those found in
artificial general intelligence (AGI) and automated theorem proving. We mention
an approach to self-improving AGI enabled by this methodology.
  Aug 2017 addendum: This article was originally written with multiple
audiences in mind. It is really best put in the following terms. Goertzel,
Hutter, Legg, and others have developed a definition of an intelligence score
for a general abstract agent: expected lifetime reward in a random environment.
AIXI is generally the optimal agent according to this score, but there may be
reasons to analyze other agents and compare score values. If we want to use
this definition of intelligence in practice, perhaps we can start by analyzing
some simple agents. Common algorithms can be thought of as simple agents
(environment is input, reward is based on running time) so we take the goal of
applying the agent intelligence score to algorithms. That is, we want to find,
what are the IQ scores of algorithms? We can do some very simple analysis, but
the real answer is that even for simple algorithms, the intelligence score is
too difficult to work with in practice.",2022-11-13 16:00:52,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2016,"Peter M. Krafft, Chris L. Baker, Alex Pentland, Joshua B. Tenenbaum",Modeling Human Ad Hoc Coordination,,,,,http://arxiv.org/abs/1602.03924v1,"Whether in groups of humans or groups of computer agents, collaboration is
most effective between individuals who have the ability to coordinate on a
joint strategy for collective action. However, in general a rational actor will
only intend to coordinate if that actor believes the other group members have
the same intention. This circular dependence makes rational coordination
difficult in uncertain environments if communication between actors is
unreliable and no prior agreements have been made. An important normative
question with regard to coordination in these ad hoc settings is therefore how
one can come to believe that other actors will coordinate, and with regard to
systems involving humans, an important empirical question is how humans arrive
at these expectations. We introduce an exact algorithm for computing the
infinitely recursive hierarchy of graded beliefs required for rational
coordination in uncertain environments, and we introduce a novel mechanism for
multiagent coordination that uses it. Our algorithm is valid in any environment
with a finite state space, and extensions to certain countably infinite state
spaces are likely possible. We test our mechanism for multiagent coordination
as a model for human decisions in a simple coordination game using existing
experimental data. We then explore via simulations whether modeling humans in
this way may improve human-agent collaboration.",2022-11-13 16:00:52,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2016,"Kevin H. Knuth, Philip M. Erner, Scott Frasso",Designing Intelligent Instruments,"AIP Conference Proceedings 954, American Institute of Physics,
  Melville NY, 203-211, 2007",,,10.1063/1.2821263,http://arxiv.org/abs/1602.04290v1,"Remote science operations require automated systems that can both act and
react with minimal human intervention. One such vision is that of an
intelligent instrument that collects data in an automated fashion, and based on
what it learns, decides which new measurements to take. This innovation
implements experimental design and unites it with data analysis in such a way
that it completes the cycle of learning. This cycle is the basis of the
Scientific Method.
  The three basic steps of this cycle are hypothesis generation, inquiry, and
inference. Hypothesis generation is implemented by artificially supplying the
instrument with a parameterized set of possible hypotheses that might be used
to describe the physical system. The act of inquiry is handled by an inquiry
engine that relies on Bayesian adaptive exploration where the optimal
experiment is chosen as the one which maximizes the expected information gain.
The inference engine is implemented using the nested sampling algorithm, which
provides the inquiry engine with a set of posterior samples from which the
expected information gain can be estimated. With these computational structures
in place, the instrument will refine its hypotheses, and repeat the learning
cycle by taking measurements until the system under study is described within a
pre-specified tolerance. We will demonstrate our first attempts toward
achieving this goal with an intelligent instrument constructed using the LEGO
MINDSTORMS NXT robotics platform.",2022-11-13 16:00:53,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2016,"Juan M. Alberola, Elena Del Val, Victor Sanchez-Anguix, Alberto Palomares, Maria Dolores Teruel",An artificial intelligence tool for heterogeneous team formation in the classroom,"Knowledge-Based Systems, 2016",,,10.1016/j.knosys.2016.02.010,http://arxiv.org/abs/1604.04721v1,"Nowadays, there is increasing interest in the development of teamwork skills
in the educational context. This growing interest is motivated by its
pedagogical effectiveness and the fact that, in labour contexts, enterprises
organize their employees in teams to carry out complex projects. Despite its
crucial importance in the classroom and industry, there is a lack of support
for the team formation process. Not only do many factors influence team
performance, but the problem becomes exponentially costly if teams are to be
optimized. In this article, we propose a tool whose aim it is to cover such a
gap. It combines artificial intelligence techniques such as coalition structure
generation, Bayesian learning, and Belbin's role theory to facilitate the
generation of working groups in an educational context. This tool improves
current state of the art proposals in three ways: i) it takes into account the
feedback of other teammates in order to establish the most predominant role of
a student instead of self-perception questionnaires; ii) it handles uncertainty
with regard to each student's predominant team role; iii) it is iterative since
it considers information from several interactions in order to improve the
estimation of role assignments. We tested the performance of the proposed tool
in an experiment involving students that took part in three different team
activities. The experiments suggest that the proposed tool is able to improve
different teamwork aspects such as team dynamics and student satisfaction.",2022-11-13 16:00:53,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2016,"Dylan Hadfield-Menell, Anca Dragan, Pieter Abbeel, Stuart Russell",The Off-Switch Game,,,,,http://arxiv.org/abs/1611.08219v3,"It is clear that one of the primary tools we can use to mitigate the
potential risk from a misbehaving AI system is the ability to turn the system
off. As the capabilities of AI systems improve, it is important to ensure that
such systems do not adopt subgoals that prevent a human from switching them
off. This is a challenge because many formulations of rational agents create
strong incentives for self-preservation. This is not caused by a built-in
instinct, but because a rational agent will maximize expected utility and
cannot achieve whatever objective it has been given if it is dead. Our goal is
to study the incentives an agent has to allow itself to be switched off. We
analyze a simple game between a human H and a robot R, where H can press R's
off switch but R can disable the off switch. A traditional agent takes its
reward function for granted: we show that such agents have an incentive to
disable the off switch, except in the special case where H is perfectly
rational. Our key insight is that for R to want to preserve its off switch, it
needs to be uncertain about the utility associated with the outcome, and to
treat H's actions as important observations about that utility. (R also has no
incentive to switch itself off in this setting.) We conclude that giving
machines an appropriate level of uncertainty about their objectives leads to
safer designs, and we argue that this setting is a useful generalization of the
classical AI paradigm of rational agents.",2022-11-13 16:00:54,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,"William Saunders, Girish Sastry, Andreas Stuhlmueller, Owain Evans",Trial without Error: Towards Safe Reinforcement Learning via Human Intervention,,,,,http://arxiv.org/abs/1707.05173v1,"AI systems are increasingly applied to complex tasks that involve interaction
with humans. During training, such systems are potentially dangerous, as they
haven't yet learned to avoid actions that could cause serious harm. How can an
AI system explore and learn without making a single mistake that harms humans
or otherwise causes serious damage? For model-free reinforcement learning,
having a human ""in the loop"" and ready to intervene is currently the only way
to prevent all catastrophes. We formalize human intervention for RL and show
how to reduce the human labor required by training a supervised learner to
imitate the human's intervention decisions. We evaluate this scheme on Atari
games, with a Deep RL agent being overseen by a human for four hours. When the
class of catastrophes is simple, we are able to prevent all catastrophes
without affecting the agent's learning (whereas an RL baseline fails due to
catastrophic forgetting). However, this scheme is less successful when
catastrophes are more complex: it reduces but does not eliminate catastrophes
and the supervised learner fails on adversarial examples found by the agent.
Extrapolating to more challenging environments, we show that our implementation
would not scale (due to the infeasible amount of human labor required). We
outline extensions of the scheme that are necessary if we are to train
model-free agents without a single catastrophe.",2022-11-13 16:00:55,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,"Marco Gavanelli, Maddalena Nonato, Andrea Peano, Davide Bertozzi",Logic Programming approaches for routing fault-free and maximally-parallel Wavelength Routed Optical Networks on Chip (Application paper),,,,,http://arxiv.org/abs/1707.05858v1,"One promising trend in digital system integration consists of boosting
on-chip communication performance by means of silicon photonics, thus
materializing the so-called Optical Networks-on-Chip (ONoCs). Among them,
wavelength routing can be used to route a signal to destination by univocally
associating a routing path to the wavelength of the optical carrier. Such
wavelengths should be chosen so to minimize interferences among optical
channels and to avoid routing faults. As a result, physical parameter selection
of such networks requires the solution of complex constrained optimization
problems. In previous work, published in the proceedings of the International
Conference on Computer-Aided Design, we proposed and solved the problem of
computing the maximum parallelism obtainable in the communication between any
two endpoints while avoiding misrouting of optical signals. The underlying
technology, only quickly mentioned in that paper, is Answer Set Programming
(ASP). In this work, we detail the ASP approach we used to solve such problem.
  Another important design issue is to select the wavelengths of optical
carriers such that they are spread across the available spectrum, in order to
reduce the likelihood that, due to imperfections in the manufacturing process,
unintended routing faults arise. We show how to address such problem in
Constraint Logic Programming on Finite Domains (CLP(FD)).
  This paper is under consideration for possible publication on Theory and
Practice of Logic Programming.",2022-11-13 16:00:55,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,"Jaime F. Fisac, Monica A. Gates, Jessica B. Hamrick, Chang Liu, Dylan Hadfield-Menell, Malayandi Palaniappan, Dhruv Malik, S. Shankar Sastry, Thomas L. Griffiths, Anca D. Dragan",Pragmatic-Pedagogic Value Alignment,"International Symposium on Robotics Research, 2017",,,,http://arxiv.org/abs/1707.06354v2,"As intelligent systems gain autonomy and capability, it becomes vital to
ensure that their objectives match those of their human users; this is known as
the value-alignment problem. In robotics, value alignment is key to the design
of collaborative robots that can integrate into human workflows, successfully
inferring and adapting to their users' objectives as they go. We argue that a
meaningful solution to value alignment must combine multi-agent decision theory
with rich mathematical models of human cognition, enabling robots to tap into
people's natural collaborative capabilities. We present a solution to the
cooperative inverse reinforcement learning (CIRL) dynamic game based on
well-established cognitive models of decision making and theory of mind. The
solution captures a key reciprocity relation: the human will not plan her
actions in isolation, but rather reason pedagogically about how the robot might
learn from them; the robot, in turn, can anticipate this and interpret the
human's actions pragmatically. To our knowledge, this work constitutes the
first formal analysis of value alignment grounded in empirically validated
cognitive models.",2022-11-13 16:00:56,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,"Ion Stoica, Dawn Song, Raluca Ada Popa, David Patterson, Michael W. Mahoney, Randy Katz, Anthony D. Joseph, Michael Jordan, Joseph M. Hellerstein, Joseph E. Gonzalez, Ken Goldberg, Ali Ghodsi, David Culler, Pieter Abbeel",A Berkeley View of Systems Challenges for AI,,,,,http://arxiv.org/abs/1712.05855v1,"With the increasing commoditization of computer vision, speech recognition
and machine translation systems and the widespread deployment of learning-based
back-end technologies such as digital advertising and intelligent
infrastructures, AI (Artificial Intelligence) has moved from research labs to
production. These changes have been made possible by unprecedented levels of
data and computation, by methodological advances in machine learning, by
innovations in systems software and architectures, and by the broad
accessibility of these technologies.
  The next generation of AI systems promises to accelerate these developments
and increasingly impact our lives via frequent interactions and making (often
mission-critical) decisions on our behalf, often in highly personalized
contexts. Realizing this promise, however, raises daunting challenges. In
particular, we need AI systems that make timely and safe decisions in
unpredictable environments, that are robust against sophisticated adversaries,
and that can process ever increasing amounts of data across organizations and
individuals without compromising confidentiality. These challenges will be
exacerbated by the end of the Moore's Law, which will constrain the amount of
data these technologies can store and process. In this paper, we propose
several open research directions in systems, architectures, and security that
can address these challenges and help unlock AI's potential to improve lives
and society.",2022-11-13 16:00:56,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,Rajesh Chidambaram,Towards an unanimous international regulatory body for responsible use of Artificial Intelligence [UIRB-AI],,,,,http://arxiv.org/abs/1712.07752v3,"Artificial Intelligence (AI), is once again in the phase of drastic
advancements. Unarguably, the technology itself can revolutionize the way we
live our everyday life. But the exponential growth of technology poses a
daunting task for policy researchers and law makers in making amendments to the
existing norms. In addition, not everyone in the society is studying the
potential socio-economic intricacies and cultural drifts that AI can bring
about. It is prudence to reflect from our historical past to propel the
development of technology in the right direction. To benefit the society of the
present and future, I scientifically explore the societal impact of AI. While
there are many public and private partnerships working on similar aspects, here
I describe the necessity for an Unanimous International Regulatory Body for all
applications of AI (UIRB-AI). I also discuss the benefits and drawbacks of such
an organization. To combat any drawbacks in the formation of an UIRB-AI, both
idealistic and pragmatic perspectives are discussed alternatively. The paper
further advances the discussion by proposing novel policies on how such
organization should be structured and how it can bring about a win-win
situation for everyone in the society.",2022-11-13 16:00:57,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,"Jahanzaib Shabbir, Tarique Anwer",Artificial Intelligence and its Role in Near Future,,,,,http://arxiv.org/abs/1804.01396v1,"AI technology has a long history which is actively and constantly changing
and growing. It focuses on intelligent agents, which contain devices that
perceive the environment and based on which takes actions in order to maximize
goal success chances. In this paper, we will explain the modern AI basics and
various representative applications of AI. In the context of the modern
digitalized world, AI is the property of machines, computer programs, and
systems to perform the intellectual and creative functions of a person,
independently find ways to solve problems, be able to draw conclusions and make
decisions. Most artificial intelligence systems have the ability to learn,
which allows people to improve their performance over time. The recent research
on AI tools, including machine learning, deep learning and predictive analysis
intended toward increasing the planning, learning, reasoning, thinking and
action taking ability. Based on which, the proposed research intends towards
exploring on how the human intelligence differs from the artificial
intelligence. Moreover, we critically analyze what AI of today is capable of
doing, why it still cannot reach human intelligence and what are the open
challenges existing in front of AI to reach and outperform human level of
intelligence. Furthermore, it will explore the future predictions for
artificial intelligence and based on which potential solution will be
recommended to solve it within next decades.",2022-11-13 16:00:58,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,"Weiran Shen, Pingzhong Tang, Song Zuo",Automated Mechanism Design via Neural Networks,,,,,http://arxiv.org/abs/1805.03382v2,"Using AI approaches to automatically design mechanisms has been a central
research mission at the interface of AI and economics [Conitzer and Sandholm,
2002]. Previous approaches that attempt to design revenue optimal auctions for
the multi-dimensional settings fall short in at least one of the three aspects:
1) representation -- search in a space that probably does not even contain the
optimal mechanism; 2) exactness -- finding a mechanism that is either not
truthful or far from optimal; 3) domain dependence -- need a different design
for different environment settings.
  To resolve the three difficulties, in this paper, we put forward -- MenuNet
-- a unified neural network based framework that automatically learns to design
revenue optimal mechanisms. Our framework consists of a mechanism network that
takes an input distribution for training and outputs a mechanism, as well as a
buyer network that takes a mechanism as input and output an action. Such a
separation in design mitigates the difficulty to impose incentive compatibility
constraints on the mechanism, by making it a rational choice of the buyer. As a
result, our framework easily overcomes the previously mentioned difficulty in
incorporating IC constraints and always returns exactly incentive compatible
mechanisms.
  We then apply our framework to a number of multi-item revenue optimal design
settings, for a few of which the theoretically optimal mechanisms are unknown.
We then go on to theoretically prove that the mechanisms found by our framework
are indeed optimal.
  To the best of our knowledge, we are the first to apply neural networks to
discover optimal auction mechanisms with provable optimality.",2022-11-13 16:00:58,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,"Fernando Martínez-Plumed, Shahar Avin, Miles Brundage, Allan Dafoe, Sean Ó hÉigeartaigh, José Hernández-Orallo",Between Progress and Potential Impact of AI: the Neglected Dimensions,,,,,http://arxiv.org/abs/1806.00610v2,"We reframe the analysis of progress in AI by incorporating into an overall
framework both the task performance of a system, and the time and resource
costs incurred in the development and deployment of the system. These costs
include: data, expert knowledge, human oversight, software resources, computing
cycles, hardware and network facilities, and (what kind of) time. These costs
are distributed over the life cycle of the system, and may place differing
demands on different developers and users. The multidimensional performance and
cost space we present can be collapsed to a single utility metric that measures
the value of the system for different stakeholders. Even without a single
utility function, AI advances can be generically assessed by whether they
expand the Pareto surface. We label these types of costs as neglected
dimensions of AI progress, and explore them using four case studies: Alpha*
(Go, Chess, and other board games), ALE (Atari games), ImageNet (Image
classification) and Virtual Personal Assistants (Siri, Alexa, Cortana, and
Google Assistant). This broader model of progress in AI will lead to novel ways
of estimating the potential societal use and impact of an AI system, and the
establishment of milestones for future progress.",2022-11-13 16:00:59,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,"Oshani Seneviratne, Sabbir M. Rashid, Shruthi Chari, James P. McCusker, Kristin P. Bennett, James A. Hendler, Deborah L. McGuinness",Knowledge Integration for Disease Characterization: A Breast Cancer Example,,,,,http://arxiv.org/abs/1807.07991v1,"With the rapid advancements in cancer research, the information that is
useful for characterizing disease, staging tumors, and creating treatment and
survivorship plans has been changing at a pace that creates challenges when
physicians try to remain current. One example involves increasing usage of
biomarkers when characterizing the pathologic prognostic stage of a breast
tumor. We present our semantic technology approach to support cancer
characterization and demonstrate it in our end-to-end prototype system that
collects the newest breast cancer staging criteria from authoritative oncology
manuals to construct an ontology for breast cancer. Using a tool we developed
that utilizes this ontology, physician-facing applications can be used to
quickly stage a new patient to support identifying risks, treatment options,
and monitoring plans based on authoritative and best practice guidelines.
Physicians can also re-stage existing patients or patient populations, allowing
them to find patients whose stage has changed in a given patient cohort. As new
guidelines emerge, using our proposed mechanism, which is grounded by semantic
technologies for ingesting new data from staging manuals, we have created an
enriched cancer staging ontology that integrates relevant data from several
sources with very little human intervention.",2022-11-13 16:01:00,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,"Chia-Chun Hung, Timothy Lillicrap, Josh Abramson, Yan Wu, Mehdi Mirza, Federico Carnevale, Arun Ahuja, Greg Wayne",Optimizing Agent Behavior over Long Time Scales by Transporting Value,,,,,http://arxiv.org/abs/1810.06721v2,"Humans spend a remarkable fraction of waking life engaged in acts of ""mental
time travel"". We dwell on our actions in the past and experience satisfaction
or regret. More than merely autobiographical storytelling, we use these event
recollections to change how we will act in similar scenarios in the future.
This process endows us with a computationally important ability to link actions
and consequences across long spans of time, which figures prominently in
addressing the problem of long-term temporal credit assignment; in artificial
intelligence (AI) this is the question of how to evaluate the utility of the
actions within a long-duration behavioral sequence leading to success or
failure in a task. Existing approaches to shorter-term credit assignment in AI
cannot solve tasks with long delays between actions and consequences. Here, we
introduce a new paradigm for reinforcement learning where agents use recall of
specific memories to credit actions from the past, allowing them to solve
problems that are intractable for existing algorithms. This paradigm broadens
the scope of problems that can be investigated in AI and offers a mechanistic
account of behaviors that may inspire computational models in neuroscience,
psychology, and behavioral economics.",2022-11-13 16:01:00,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,"Francesca Rossi, Nicholas Mattei",Building Ethically Bounded AI,,,,,http://arxiv.org/abs/1812.03980v1,"The more AI agents are deployed in scenarios with possibly unexpected
situations, the more they need to be flexible, adaptive, and creative in
achieving the goal we have given them. Thus, a certain level of freedom to
choose the best path to the goal is inherent in making AI robust and flexible
enough. At the same time, however, the pervasive deployment of AI in our life,
whether AI is autonomous or collaborating with humans, raises several ethical
challenges. AI agents should be aware and follow appropriate ethical principles
and should thus exhibit properties such as fairness or other virtues. These
ethical principles should define the boundaries of AI's freedom and creativity.
However, it is still a challenge to understand how to specify and reason with
ethical boundaries in AI agents and how to combine them appropriately with
subjective preferences and goal specifications. Some initial attempts employ
either a data-driven example-based approach for both, or a symbolic rule-based
approach for both. We envision a modular approach where any AI technique can be
used for any of these essential ingredients in decision making or decision
support systems, paired with a contextual approach to define their combination
and relative weight. In a world where neither humans nor AI systems work in
isolation, but are tightly interconnected, e.g., the Internet of Things, we
also envision a compositional approach to building ethically bounded AI, where
the ethical properties of each component can be fruitfully exploited to derive
those of the overall system. In this paper we define and motivate the notion of
ethically-bounded AI, we describe two concrete examples, and we outline some
outstanding challenges.",2022-11-13 16:01:01,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,Peter Eckersley,Impossibility and Uncertainty Theorems in AI Value Alignment (or why your AGI should not have a utility function),,,,,http://arxiv.org/abs/1901.00064v3,"Utility functions or their equivalents (value functions, objective functions,
loss functions, reward functions, preference orderings) are a central tool in
most current machine learning systems. These mechanisms for defining goals and
guiding optimization run into practical and conceptual difficulty when there
are independent, multi-dimensional objectives that need to be pursued
simultaneously and cannot be reduced to each other. Ethicists have proved
several impossibility theorems that stem from this origin; those results appear
to show that there is no way of formally specifying what it means for an
outcome to be good for a population without violating strong human ethical
intuitions (in such cases, the objective function is a social welfare
function). We argue that this is a practical problem for any machine learning
system (such as medical decision support systems or autonomous weapons) or
rigidly rule-based bureaucracy that will make high stakes decisions about human
lives: such systems should not use objective functions in the strict
mathematical sense.
  We explore the alternative of using uncertain objectives, represented for
instance as partially ordered preferences, or as probability distributions over
total orders. We show that previously known impossibility theorems can be
transformed into uncertainty theorems in both of those settings, and prove
lower bounds on how much uncertainty is implied by the impossibility results.
We close by proposing two conjectures about the relationship between
uncertainty in objectives and severe unintended consequences from AI systems.",2022-11-13 16:01:01,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Brian Lubars, Chenhao Tan","Ask Not What AI Can Do, But What AI Should Do: Towards a Framework of Task Delegability",,,,,http://arxiv.org/abs/1902.03245v2,"While artificial intelligence (AI) holds promise for addressing societal
challenges, issues of exactly which tasks to automate and to what extent to do
so remain understudied. We approach this problem of task delegability from a
human-centered perspective by developing a framework on human perception of
task delegation to AI. We consider four high-level factors that can contribute
to a delegation decision: motivation, difficulty, risk, and trust. To obtain an
empirical understanding of human preferences in different tasks, we build a
dataset of 100 tasks from academic papers, popular media portrayal of AI, and
everyday life, and administer a survey based on our proposed framework. We find
little preference for full AI control and a strong preference for
machine-in-the-loop designs, in which humans play the leading role. Among the
four factors, trust is the most correlated with human preferences of optimal
human-machine delegation. This framework represents a first step towards
characterizing human preferences of AI automation across tasks. We hope this
work encourages future efforts towards understanding such individual attitudes;
our goal is to inform the public and the AI research community rather than
dictating any direction in technology development.",2022-11-13 16:01:02,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Christoph Benzmüller, Xavier Parent, Leendert van der Torre","Designing Normative Theories for Ethical and Legal Reasoning: LogiKEy Framework, Methodology, and Tool Support",,,,,http://arxiv.org/abs/1903.10187v6,"A framework and methodology---termed LogiKEy---for the design and engineering
of ethical reasoners, normative theories and deontic logics is presented. The
overall motivation is the development of suitable means for the control and
governance of intelligent autonomous systems. LogiKEy's unifying formal
framework is based on semantical embeddings of deontic logics, logic
combinations and ethico-legal domain theories in expressive classic
higher-order logic (HOL). This meta-logical approach enables the provision of
powerful tool support in LogiKEy: off-the-shelf theorem provers and model
finders for HOL are assisting the LogiKEy designer of ethical intelligent
agents to flexibly experiment with underlying logics and their combinations,
with ethico-legal domain theories, and with concrete examples---all at the same
time. Continuous improvements of these off-the-shelf provers, without further
ado, leverage the reasoning performance in LogiKEy. Case studies, in which the
LogiKEy framework and methodology has been applied and tested, give evidence
that HOL's undecidability often does not hinder efficient experimentation.",2022-11-13 16:01:03,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Patrick Rodler, Dietmar Jannach, Konstantin Schekotihin, Philipp Fleiss",Are Query-Based Ontology Debuggers Really Helping Knowledge Engineers?,,,,,http://arxiv.org/abs/1904.01484v2,"Real-world semantic or knowledge-based systems, e.g., in the biomedical
domain, can become large and complex. Tool support for the localization and
repair of faults within knowledge bases of such systems can therefore be
essential for their practical success. Correspondingly, a number of knowledge
base debugging approaches, in particular for ontology-based systems, were
proposed throughout recent years. Query-based debugging is a comparably recent
interactive approach that localizes the true cause of an observed problem by
asking knowledge engineers a series of questions. Concrete implementations of
this approach exist, such as the OntoDebug plug-in for the ontology editor
Prot\'eg\'e.
  To validate that a newly proposed method is favorable over an existing one,
researchers often rely on simulation-based comparisons. Such an evaluation
approach however has certain limitations and often cannot fully inform us about
a method's true usefulness. We therefore conducted different user studies to
assess the practical value of query-based ontology debugging. One main insight
from the studies is that the considered interactive approach is indeed more
efficient than an alternative algorithmic debugging based on test cases. We
also observed that users frequently made errors in the process, which
highlights the importance of a careful design of the queries that users need to
answer.",2022-11-13 16:01:03,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Julian Stier, Gabriele Gianini, Michael Granitzer, Konstantin Ziegler",Analysing Neural Network Topologies: a Game Theoretic Approach,Procedia Computer Science 126 (2018): 234-243,,,10.1016/j.procs.2018.07.257,http://arxiv.org/abs/1904.08166v1,"Artificial Neural Networks have shown impressive success in very different
application cases. Choosing a proper network architecture is a critical
decision for a network's success, usually done in a manual manner. As a
straightforward strategy, large, mostly fully connected architectures are
selected, thereby relying on a good optimization strategy to find proper
weights while at the same time avoiding overfitting. However, large parts of
the final network are redundant. In the best case, large parts of the network
become simply irrelevant for later inferencing. In the worst case, highly
parameterized architectures hinder proper optimization and allow the easy
creation of adverserial examples fooling the network. A first step in removing
irrelevant architectural parts lies in identifying those parts, which requires
measuring the contribution of individual components such as neurons. In
previous work, heuristics based on using the weight distribution of a neuron as
contribution measure have shown some success, but do not provide a proper
theoretical understanding. Therefore, in our work we investigate game theoretic
measures, namely the Shapley value (SV), in order to separate relevant from
irrelevant parts of an artificial neural network. We begin by designing a
coalitional game for an artificial neural network, where neurons form
coalitions and the average contributions of neurons to coalitions yield to the
Shapley value. In order to measure how well the Shapley value measures the
contribution of individual neurons, we remove low-contributing neurons and
measure its impact on the network performance. In our experiments we show that
the Shapley value outperforms other heuristics for measuring the contribution
of neurons.",2022-11-13 16:01:04,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Ramon Fraga Pereira, Nir Oren, Felipe Meneguzzi",Using Sub-Optimal Plan Detection to Identify Commitment Abandonment in Discrete Environments,,,,,http://arxiv.org/abs/1904.11737v2,"Assessing whether an agent has abandoned a goal or is actively pursuing it is
important when multiple agents are trying to achieve joint goals, or when
agents commit to achieving goals for each other. Making such a determination
for a single goal by observing only plan traces is not trivial as agents often
deviate from optimal plans for various reasons, including the pursuit of
multiple goals or the inability to act optimally. In this article, we develop
an approach based on domain independent heuristics from automated planning,
landmarks, and fact partitions to identify sub-optimal action steps - with
respect to a plan - within a plan execution trace. Such capability is very
important in domains where multiple agents cooperate and delegate tasks among
themselves, e.g. through social commitments, and need to ensure that a
delegating agent can infer whether or not another agent is actually progressing
towards a delegated task. We demonstrate how an agent can use our technique to
determine - by observing a trace - whether an agent is honouring a commitment.
We empirically show, for a number of representative domains, that our approach
infers sub-optimal action steps with very high accuracy and detects commitment
abandonment in nearly all cases.",2022-11-13 16:01:05,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Rocio Gomez, Mohan Sridharan, Heather Riley",Towards a Theory of Intentions for Human-Robot Collaboration,,,,,http://arxiv.org/abs/1907.13275v1,"The architecture described in this paper encodes a theory of intentions based
on the the key principles of non-procrastination, persistence, and
automatically limiting reasoning to relevant knowledge and observations. The
architecture reasons with transition diagrams of any given domain at two
different resolutions, with the fine-resolution description defined as a
refinement of, and hence tightly-coupled to, a coarse-resolution description.
Non-monotonic logical reasoning with the coarse-resolution description computes
an activity (i.e., plan) comprising abstract actions for any given goal. Each
abstract action is implemented as a sequence of concrete actions by
automatically zooming to and reasoning with the part of the fine-resolution
transition diagram relevant to the current coarse-resolution transition and the
goal. Each concrete action in this sequence is executed using probabilistic
models of the uncertainty in sensing and actuation, and the corresponding
fine-resolution outcomes are used to infer coarse-resolution observations that
are added to the coarse-resolution history. The architecture's capabilities are
evaluated in the context of a simulated robot assisting humans in an office
domain, on a physical robot (Baxter) manipulating tabletop objects, and on a
wheeled robot (Turtlebot) moving objects to particular places or people. The
experimental results indicate improvements in reliability and computational
efficiency compared with an architecture that does not include the theory of
intentions, and an architecture that does not include zooming for
fine-resolution reasoning.",2022-11-13 16:01:05,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Dung T. Phan, Radu Grosu, Nils Jansen, Nicola Paoletti, Scott A. Smolka, Scott D. Stoller",Neural Simplex Architecture,,,,,http://arxiv.org/abs/1908.00528v2,"We present the Neural Simplex Architecture (NSA), a new approach to runtime
assurance that provides safety guarantees for neural controllers (obtained e.g.
using reinforcement learning) of autonomous and other complex systems without
unduly sacrificing performance. NSA is inspired by the Simplex control
architecture of Sha et al., but with some significant differences. In the
traditional approach, the advanced controller (AC) is treated as a black box;
when the decision module switches control to the baseline controller (BC), the
BC remains in control forever. There is relatively little work on switching
control back to the AC, and there are no techniques for correcting the AC's
behavior after it generates a potentially unsafe control input that causes a
failover to the BC. Our NSA addresses both of these limitations. NSA not only
provides safety assurances in the presence of a possibly unsafe neural
controller, but can also improve the safety of such a controller in an online
setting via retraining, without overly degrading its performance. To
demonstrate NSA's benefits, we have conducted several significant case studies
in the continuous control domain. These include a target-seeking ground rover
navigating an obstacle field, and a neural controller for an artificial
pancreas system.",2022-11-13 16:01:06,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Alexander Matt Turner, Logan Smith, Rohin Shah, Andrew Critch, Prasad Tadepalli",Optimal Policies Tend to Seek Power,,,,,http://arxiv.org/abs/1912.01683v9,"Some researchers speculate that intelligent reinforcement learning (RL)
agents would be incentivized to seek resources and power in pursuit of their
objectives. Other researchers point out that RL agents need not have human-like
power-seeking instincts. To clarify this discussion, we develop the first
formal theory of the statistical tendencies of optimal policies. In the context
of Markov decision processes, we prove that certain environmental symmetries
are sufficient for optimal policies to tend to seek power over the environment.
These symmetries exist in many environments in which the agent can be shut down
or destroyed. We prove that in these environments, most reward functions make
it optimal to seek power by keeping a range of options available and, when
maximizing average reward, by navigating towards larger sets of potential
terminal states.",2022-11-13 16:01:07,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Thomas Chatain, Mathilde Boltenhagen, Josep Carmona",Anti-Alignments -- Measuring The Precision of Process Models and Event Logs,,,,,http://arxiv.org/abs/1912.05907v1,"Processes are a crucial artefact in organizations, since they coordinate the
execution of activities so that products and services are provided. The use of
models to analyse the underlying processes is a well-known practice. However,
due to the complexity and continuous evolution of their processes,
organizations need an effective way of analysing the relation between processes
and models. Conformance checking techniques asses the suitability of a process
model in representing an underlying process, observed through a collection of
real executions. One important metric in conformance checking is to asses the
precision of the model with respect to the observed executions, i.e.,
characterize the ability of the model to produce behavior unrelated to the one
observed. In this paper we present the notion of anti-alignment as a concept to
help unveiling runs in the model that may deviate significantly from the
observed behavior. Using anti-alignments, a new metric for precision is
proposed. In contrast to existing metrics, anti-alignment based precision
metrics satisfy most of the required axioms highlighted in a recent
publication. Moreover, a complexity analysis of the problem of computing
anti-alignments is provided, which sheds light into the practicability of using
anti-alignment to estimate precision. Experiments are provided that witness the
validity of the concepts introduced in this paper.",2022-11-13 16:01:08,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Min Chen, Mateu Sbert, Alfie Abdul-Rahman, Deborah Silver",A Bounded Measure for Estimating the Benefit of Visualization,"Entropy, 24(2), 228, 2022",,,10.3390/e24020228,http://arxiv.org/abs/2002.05282v2,"Information theory can be used to analyze the cost-benefit of visualization
processes. However, the current measure of benefit contains an unbounded term
that is neither easy to estimate nor intuitive to interpret. In this work, we
propose to revise the existing cost-benefit measure by replacing the unbounded
term with a bounded one. We examine a number of bounded measures that include
the Jenson-Shannon divergence and a new divergence measure formulated as part
of this work. We use visual analysis to support the multi-criteria comparison,
narrowing the search down to those options with better mathematical properties.
We apply those remaining options to two visualization case studies to
instantiate their uses in practical scenarios, while the collected real world
data further informs the selection of a bounded measure, which can be used to
estimate the benefit of visualization.",2022-11-13 16:01:08,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Emile van Krieken, Erman Acar, Frank van Harmelen",Analyzing Differentiable Fuzzy Logic Operators,,,,10.1016/j.artint.2021.103602,http://arxiv.org/abs/2002.06100v2,"The AI community is increasingly putting its attention towards combining
symbolic and neural approaches, as it is often argued that the strengths and
weaknesses of these approaches are complementary. One recent trend in the
literature are weakly supervised learning techniques that employ operators from
fuzzy logics. In particular, these use prior background knowledge described in
such logics to help the training of a neural network from unlabeled and noisy
data. By interpreting logical symbols using neural networks, this background
knowledge can be added to regular loss functions, hence making reasoning a part
of learning. We study, both formally and empirically, how a large collection of
logical operators from the fuzzy logic literature behave in a differentiable
learning setting. We find that many of these operators, including some of the
most well-known, are highly unsuitable in this setting. A further finding
concerns the treatment of implication in these fuzzy logics, and shows a strong
imbalance between gradients driven by the antecedent and the consequent of the
implication. Furthermore, we introduce a new family of fuzzy implications
(called sigmoidal implications) to tackle this phenomenon. Finally, we
empirically show that it is possible to use Differentiable Fuzzy Logics for
semi-supervised learning, and compare how different operators behave in
practice. We find that, to achieve the largest performance improvement over a
supervised baseline, we have to resort to non-standard combinations of logical
operators which perform well in learning, but no longer satisfy the usual
logical laws.",2022-11-13 16:01:09,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,Philip Paquette,A Road Map to Strong Intelligence,,,,,http://arxiv.org/abs/2002.09044v1,"I wrote this paper because technology can really improve people's lives. With
it, we can live longer in a healthy body, save time through increased
efficiency and automation, and make better decisions. To get to the next level,
we need to start looking at intelligence from a much broader perspective, and
promote international interdisciplinary collaborations. Section 1 of this paper
delves into sociology and social psychology to explain that the mechanisms
underlying intelligence are inherently social. Section 2 proposes a method to
classify intelligence, and describes the differences between weak and strong
intelligence. Section 3 examines the Chinese Room argument from a different
perspective. It demonstrates that a Turing-complete machine cannot have strong
intelligence, and considers the modifications necessary for a computer to be
intelligent and have understanding. Section 4 argues that the existential risk
caused by the technological explosion of a single agent should not be of
serious concern. Section 5 looks at the AI control problem and argues that it
is impossible to build a super-intelligent machine that will do what it
creators want. By using insights from biology, it also proposes a solution to
the control problem. Section 6 discusses some of the implications of strong
intelligence. Section 7 lists the main challenges with deep learning, and
asserts that radical changes will be required to reach strong intelligence.
Section 8 examines a neuroscience framework that could help explain how a
cortical column works. Section 9 lays out the broad strokes of a road map
towards strong intelligence. Finally, section 10 analyzes the impacts and the
challenges of greater intelligence.",2022-11-13 16:01:10,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,Andrés Páez,The Pragmatic Turn in Explainable Artificial Intelligence (XAI),"Minds and Machines, 29(3), 441-459, 2019",,,10.1007/s11023-019-09502-w,http://arxiv.org/abs/2002.09595v1,"In this paper I argue that the search for explainable models and
interpretable decisions in AI must be reformulated in terms of the broader
project of offering a pragmatic and naturalistic account of understanding in
AI. Intuitively, the purpose of providing an explanation of a model or a
decision is to make it understandable to its stakeholders. But without a
previous grasp of what it means to say that an agent understands a model or a
decision, the explanatory strategies will lack a well-defined goal. Aside from
providing a clearer objective for XAI, focusing on understanding also allows us
to relax the factivity condition on explanation, which is impossible to fulfill
in many machine learning models, and to focus instead on the pragmatic
conditions that determine the best fit between a model and the methods and
devices deployed to understand it. After an examination of the different types
of understanding discussed in the philosophical and psychological literature, I
conclude that interpretative or approximation models not only provide the best
way to achieve the objectual understanding of a machine learning model, but are
also a necessary condition to achieve post-hoc interpretability. This
conclusion is partly based on the shortcomings of the purely functionalist
approach to post-hoc interpretability that seems to be predominant in most
recent literature.",2022-11-13 16:01:10,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Jianyu Su, Stephen Adams, Peter A. Beling",Counterfactual Multi-Agent Reinforcement Learning with Graph Convolution Communication,,,,,http://arxiv.org/abs/2004.00470v2,"We consider a fully cooperative multi-agent system where agents cooperate to
maximize a system's utility in a partial-observable environment. We propose
that multi-agent systems must have the ability to (1) communicate and
understand the inter-plays between agents and (2) correctly distribute rewards
based on an individual agent's contribution. In contrast, most work in this
setting considers only one of the above abilities. In this study, we develop an
architecture that allows for communication among agents and tailors the
system's reward for each individual agent. Our architecture represents agent
communication through graph convolution and applies an existing credit
assignment structure, counterfactual multi-agent policy gradient (COMA), to
assist agents to learn communication by back-propagation. The flexibility of
the graph structure enables our method to be applicable to a variety of
multi-agent systems, e.g. dynamic systems that consist of varying numbers of
agents and static systems with a fixed number of agents. We evaluate our method
on a range of tasks, demonstrating the advantage of marrying communication with
credit assignment. In the experiments, our proposed method yields better
performance than the state-of-art methods, including COMA. Moreover, we show
that the communication strategies offers us insights and interpretability of
the system's cooperative policies.",2022-11-13 16:01:11,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Gagan Bansal, Besmira Nushi, Ece Kamar, Eric Horvitz, Daniel S. Weld",Is the Most Accurate AI the Best Teammate? Optimizing AI for Teamwork,,,,,http://arxiv.org/abs/2004.13102v3,"AI practitioners typically strive to develop the most accurate systems,
making an implicit assumption that the AI system will function autonomously.
However, in practice, AI systems often are used to provide advice to people in
domains ranging from criminal justice and finance to healthcare. In such
AI-advised decision making, humans and machines form a team, where the human is
responsible for making final decisions. But is the most accurate AI the best
teammate? We argue ""No"" -- predictable performance may be worth a slight
sacrifice in AI accuracy. Instead, we argue that AI systems should be trained
in a human-centered manner, directly optimized for team performance. We study
this proposal for a specific type of human-AI teaming, where the human overseer
chooses to either accept the AI recommendation or solve the task themselves. To
optimize the team performance for this setting we maximize the team's expected
utility, expressed in terms of the quality of the final decision, cost of
verifying, and individual accuracies of people and machines. Our experiments
with linear and non-linear models on real-world, high-stakes datasets show that
the most accuracy AI may not lead to highest team performance and show the
benefit of modeling teamwork during training through improvements in expected
team utility across datasets, considering parameters such as human skill and
the cost of mistakes. We discuss the shortcoming of current optimization
approaches beyond well-studied loss functions such as log-loss, and encourage
future work on AI optimization problems motivated by human-AI collaboration.",2022-11-13 16:01:11,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Eric MSP Veith, Nils Wenninghoff, Emilie Frost","The Adversarial Resilience Learning Architecture for AI-based Modelling, Exploration, and Operation of Complex Cyber-Physical Systems",,,,,http://arxiv.org/abs/2005.13601v1,"Modern algorithms in the domain of Deep Reinforcement Learning (DRL)
demonstrated remarkable successes; most widely known are those in game-based
scenarios, from ATARI video games to Go and the StarCraft~\textsc{II} real-time
strategy game. However, applications in the domain of modern Cyber-Physical
Systems (CPS) that take advantage a vast variety of DRL algorithms are few. We
assume that the benefits would be considerable: Modern CPS have become
increasingly complex and evolved beyond traditional methods of modelling and
analysis. At the same time, these CPS are confronted with an increasing amount
of stochastic inputs, from volatile energy sources in power grids to broad user
participation stemming from markets. Approaches of system modelling that use
techniques from the domain of Artificial Intelligence (AI) do not focus on
analysis and operation. In this paper, we describe the concept of Adversarial
Resilience Learning (ARL) that formulates a new approach to complex environment
checking and resilient operation: It defines two agent classes, attacker and
defender agents. The quintessence of ARL lies in both agents exploring the
system and training each other without any domain knowledge. Here, we introduce
the ARL software architecture that allows to use a wide range of model-free as
well as model-based DRL-based algorithms, and document results of concrete
experiment runs on a complex power grid.",2022-11-13 16:01:12,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Michael K. Cohen, Marcus Hutter",Pessimism About Unknown Unknowns Inspires Conservatism,,,,,http://arxiv.org/abs/2006.08753v1,"If we could define the set of all bad outcomes, we could hard-code an agent
which avoids them; however, in sufficiently complex environments, this is
infeasible. We do not know of any general-purpose approaches in the literature
to avoiding novel failure modes. Motivated by this, we define an idealized
Bayesian reinforcement learner which follows a policy that maximizes the
worst-case expected reward over a set of world-models. We call this agent
pessimistic, since it optimizes assuming the worst case. A scalar parameter
tunes the agent's pessimism by changing the size of the set of world-models
taken into account. Our first main contribution is: given an assumption about
the agent's model class, a sufficiently pessimistic agent does not cause
""unprecedented events"" with probability $1-\delta$, whether or not designers
know how to precisely specify those precedents they are concerned with. Since
pessimism discourages exploration, at each timestep, the agent may defer to a
mentor, who may be a human or some known-safe policy we would like to improve.
Our other main contribution is that the agent's policy's value approaches at
least that of the mentor, while the probability of deferring to the mentor goes
to 0. In high-stakes environments, we might like advanced artificial agents to
pursue goals cautiously, which is a non-trivial problem even if the agent were
allowed arbitrary computing power; we present a formal solution.",2022-11-13 16:01:12,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Yuqing Du, Stas Tiomkin, Emre Kiciman, Daniel Polani, Pieter Abbeel, Anca Dragan",AvE: Assistance via Empowerment,,,,,http://arxiv.org/abs/2006.14796v5,"One difficulty in using artificial agents for human-assistive applications
lies in the challenge of accurately assisting with a person's goal(s). Existing
methods tend to rely on inferring the human's goal, which is challenging when
there are many potential goals or when the set of candidate goals is difficult
to identify. We propose a new paradigm for assistance by instead increasing the
human's ability to control their environment, and formalize this approach by
augmenting reinforcement learning with human empowerment. This task-agnostic
objective preserves the person's autonomy and ability to achieve any eventual
state. We test our approach against assistance based on goal inference,
highlighting scenarios where our method overcomes failure modes stemming from
goal ambiguity or misspecification. As existing methods for estimating
empowerment in continuous domains are computationally hard, precluding its use
in real time learned assistance, we also propose an efficient
empowerment-inspired proxy metric. Using this, we are able to successfully
demonstrate our method in a shared autonomy user study for a challenging
simulated teleoperation task with human-in-the-loop training.",2022-11-13 16:01:13,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Vedant Nanda, Till Speicher, John P. Dickerson, Krishna P. Gummadi, Muhammad Bilal Zafar",Unifying Model Explainability and Robustness via Machine-Checkable Concepts,,,,,http://arxiv.org/abs/2007.00251v2,"As deep neural networks (DNNs) get adopted in an ever-increasing number of
applications, explainability has emerged as a crucial desideratum for these
models. In many real-world tasks, one of the principal reasons for requiring
explainability is to in turn assess prediction robustness, where predictions
(i.e., class labels) that do not conform to their respective explanations
(e.g., presence or absence of a concept in the input) are deemed to be
unreliable. However, most, if not all, prior methods for checking
explanation-conformity (e.g., LIME, TCAV, saliency maps) require significant
manual intervention, which hinders their large-scale deployability. In this
paper, we propose a robustness-assessment framework, at the core of which is
the idea of using machine-checkable concepts. Our framework defines a large
number of concepts that the DNN explanations could be based on and performs the
explanation-conformity check at test time to assess prediction robustness. Both
steps are executed in an automated manner without requiring any human
intervention and are easily scaled to datasets with a very large number of
classes. Experiments on real-world datasets and human surveys show that our
framework is able to enhance prediction robustness significantly: the
predictions marked to be robust by our framework have significantly higher
accuracy and are more robust to adversarial perturbations.",2022-11-13 16:01:13,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Connor Basich, Justin Svegliato, Kyle Hollins Wray, Stefan J. Witwicki, Shlomo Zilberstein",Improving Competence for Reliable Autonomy,"EPTCS 319, 2020, pp. 37-53",,,10.4204/EPTCS.319.4,http://arxiv.org/abs/2007.11740v1,"Given the complexity of real-world, unstructured domains, it is often
impossible or impractical to design models that include every feature needed to
handle all possible scenarios that an autonomous system may encounter. For an
autonomous system to be reliable in such domains, it should have the ability to
improve its competence online. In this paper, we propose a method for improving
the competence of a system over the course of its deployment. We specifically
focus on a class of semi-autonomous systems known as competence-aware systems
that model their own competence -- the optimal extent of autonomy to use in any
given situation -- and learn this competence over time from feedback received
through interactions with a human authority. Our method exploits such feedback
to identify important state features missing from the system's initial model,
and incorporates them into its state representation. The result is an agent
that better predicts human involvement, leading to improvements in its
competence and reliability, and as a result, its overall performance.",2022-11-13 16:01:14,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Mariela Morveli-Espinoza, Juan Carlos Nieves, Ayslan Possebom, Josep Puyol-Gruart, Cesar Augusto Tacla",An Argumentation-based Approach for Identifying and Dealing with Incompatibilities among Procedural Goals,"International Journal of Approximate Reasoning, year 2019, vol.
  105, pp. 1-26",,,10.4114/intartif.vol22iss64pp47-62,http://arxiv.org/abs/2009.05186v1,"During the first step of practical reasoning, i.e. deliberation, an
intelligent agent generates a set of pursuable goals and then selects which of
them he commits to achieve. An intelligent agent may in general generate
multiple pursuable goals, which may be incompatible among them. In this paper,
we focus on the definition, identification and resolution of these
incompatibilities. The suggested approach considers the three forms of
incompatibility introduced by Castelfranchi and Paglieri, namely the terminal
incompatibility, the instrumental or resources incompatibility and the
superfluity. We characterise computationally these forms of incompatibility by
means of arguments that represent the plans that allow an agent to achieve his
goals. Thus, the incompatibility among goals is defined based on the conflicts
among their plans, which are represented by means of attacks in an
argumentation framework. We also work on the problem of goals selection; we
propose to use abstract argumentation theory to deal with this problem, i.e. by
applying argumentation semantics. We use a modified version of the ""cleaner
world"" scenario in order to illustrate the performance of our proposal.",2022-11-13 16:01:14,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Lun Ai, Stephen H. Muggleton, Céline Hocquette, Mark Gromowski, Ute Schmid",Beneficial and Harmful Explanatory Machine Learning,,,,,http://arxiv.org/abs/2009.06410v2,"Given the recent successes of Deep Learning in AI there has been increased
interest in the role and need for explanations in machine learned theories. A
distinct notion in this context is that of Michie's definition of Ultra-Strong
Machine Learning (USML). USML is demonstrated by a measurable increase in human
performance of a task following provision to the human of a symbolic machine
learned theory for task performance. A recent paper demonstrates the beneficial
effect of a machine learned logic theory for a classification task, yet no
existing work to our knowledge has examined the potential harmfulness of
machine's involvement for human comprehension during learning. This paper
investigates the explanatory effects of a machine learned theory in the context
of simple two person games and proposes a framework for identifying the
harmfulness of machine explanations based on the Cognitive Science literature.
The approach involves a cognitive window consisting of two quantifiable bounds
and it is supported by empirical evidence collected from human trials. Our
quantitative and qualitative results indicate that human learning aided by a
symbolic machine learned theory which satisfies a cognitive window has achieved
significantly higher performance than human self learning. Results also
demonstrate that human learning aided by a symbolic machine learned theory that
fails to satisfy this window leads to significantly worse performance than
unaided human learning.",2022-11-13 16:01:15,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Theodore R. Sumers, Mark K. Ho, Robert D. Hawkins, Karthik Narasimhan, Thomas L. Griffiths",Learning Rewards from Linguistic Feedback,,,,,http://arxiv.org/abs/2009.14715v3,"We explore unconstrained natural language feedback as a learning signal for
artificial agents. Humans use rich and varied language to teach, yet most prior
work on interactive learning from language assumes a particular form of input
(e.g., commands). We propose a general framework which does not make this
assumption, using aspect-based sentiment analysis to decompose feedback into
sentiment about the features of a Markov decision process. We then perform an
analogue of inverse reinforcement learning, regressing the sentiment on the
features to infer the teacher's latent reward function. To evaluate our
approach, we first collect a corpus of teaching behavior in a cooperative task
where both teacher and learner are human. We implement three artificial
learners: sentiment-based ""literal"" and ""pragmatic"" models, and an inference
network trained end-to-end to predict latent rewards. We then repeat our
initial experiment and pair them with human teachers. All three successfully
learn from interactive human feedback. The sentiment models outperform the
inference network, with the ""pragmatic"" model approaching human performance.
Our work thus provides insight into the information structure of naturalistic
linguistic feedback as well as methods to leverage it for reinforcement
learning.",2022-11-13 16:01:16,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"The Anh Han, Luis Moniz Pereira, Tom Lenaerts, Francisco C. Santos",Mediating Artificial Intelligence Developments through Negative and Positive Incentives,,,,10.1371/journal.pone.0244592,http://arxiv.org/abs/2010.00403v1,"The field of Artificial Intelligence (AI) is going through a period of great
expectations, introducing a certain level of anxiety in research, business and
also policy. This anxiety is further energised by an AI race narrative that
makes people believe they might be missing out. Whether real or not, a belief
in this narrative may be detrimental as some stake-holders will feel obliged to
cut corners on safety precautions, or ignore societal consequences just to
""win"". Starting from a baseline model that describes a broad class of
technology races where winners draw a significant benefit compared to others
(such as AI advances, patent race, pharmaceutical technologies), we investigate
here how positive (rewards) and negative (punishments) incentives may
beneficially influence the outcomes. We uncover conditions in which punishment
is either capable of reducing the development speed of unsafe participants or
has the capacity to reduce innovation through over-regulation. Alternatively,
we show that, in several scenarios, rewarding those that follow safety measures
may increase the development speed while ensuring safe choices. Moreover, in
{the latter} regimes, rewards do not suffer from the issue of over-regulation
as is the case for punishment. Overall, our findings provide valuable insights
into the nature and kinds of regulatory actions most suitable to improve safety
compliance in the contexts of both smooth and sudden technological shifts.",2022-11-13 16:01:16,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Jakub Tětek, Marek Sklenka, Tomáš Gavenčiak",Performance of Bounded-Rational Agents With the Ability to Self-Modify,,,,,http://arxiv.org/abs/2011.06275v2,"Self-modification of agents embedded in complex environments is hard to
avoid, whether it happens via direct means (e.g. own code modification) or
indirectly (e.g. influencing the operator, exploiting bugs or the environment).
It has been argued that intelligent agents have an incentive to avoid modifying
their utility function so that their future instances work towards the same
goals.
  Everitt et al. (2016) formally show that providing an option to self-modify
is harmless for perfectly rational agents. We show that this result is no
longer true for agents with bounded rationality. In such agents,
self-modification may cause exponential deterioration in performance and
gradual misalignment of a previously aligned agent. We investigate how the size
of this effect depends on the type and magnitude of imperfections in the
agent's rationality (1-4 below). We also discuss model assumptions and the
wider problem and framing space.
  We examine four ways in which an agent can be bounded-rational: it either (1)
doesn't always choose the optimal action, (2) is not perfectly aligned with
human values, (3) has an inaccurate model of the environment, or (4) uses the
wrong temporal discounting factor. We show that while in the cases (2)-(4) the
misalignment caused by the agent's imperfection does not increase over time,
with (1) the misalignment may grow exponentially.",2022-11-13 16:01:17,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Tianchen Zhao, Xuefei Ning, Xiangsheng Shi, Songyi Yang, Shuang Liang, Peng Lei, Jianfei Chen, Huazhong Yang, Yu Wang",BARS: Joint Search of Cell Topology and Layout for Accurate and Efficient Binary ARchitectures,,,,,http://arxiv.org/abs/2011.10804v3,"Binary Neural Networks (BNNs) have received significant attention due to
their promising efficiency. Currently, most BNN studies directly adopt
widely-used CNN architectures, which can be suboptimal for BNNs. This paper
proposes a novel Binary ARchitecture Search (BARS) flow to discover superior
binary architecture in a large design space. Specifically, we analyze the
information bottlenecks that are related to both the topology and layout
architecture design choices. And we propose to automatically search for the
optimal information flow. To achieve that, we design a two-level (Macro &
Micro) search space tailored for BNNs and apply a differentiable neural
architecture search (NAS) to explore this search space efficiently. The
macro-level search space includes width and depth decisions, which is required
for better balancing the model performance and complexity. We also design the
micro-level search space to strengthen the information flow for BNN. %A notable
challenge of BNN architecture search lies in that binary operations exacerbate
the ""collapse"" problem of differentiable NAS, for which we incorporate various
search and derive strategies to stabilize the search process. On CIFAR-10, BARS
achieves 1.5% higher accuracy with 2/3 binary operations and 1/10
floating-point operations comparing with existing BNN NAS studies. On ImageNet,
with similar resource consumption, BARS-discovered architecture achieves a 6%
accuracy gain than hand-crafted binary ResNet-18 architectures and outperforms
other binary architectures while fully binarizing the architecture backbone.",2022-11-13 16:01:17,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Simon Zhuang, Dylan Hadfield-Menell",Consequences of Misaligned AI,NeurIPS 2020,,,,http://arxiv.org/abs/2102.03896v1,"AI systems often rely on two key components: a specified goal or reward
function and an optimization algorithm to compute the optimal behavior for that
goal. This approach is intended to provide value for a principal: the user on
whose behalf the agent acts. The objectives given to these agents often refer
to a partial specification of the principal's goals. We consider the cost of
this incompleteness by analyzing a model of a principal and an agent in a
resource constrained world where the $L$ attributes of the state correspond to
different sources of utility for the principal. We assume that the reward
function given to the agent only has support on $J < L$ attributes. The
contributions of our paper are as follows: 1) we propose a novel model of an
incomplete principal-agent problem from artificial intelligence; 2) we provide
necessary and sufficient conditions under which indefinitely optimizing for any
incomplete proxy objective leads to arbitrarily low overall utility; and 3) we
show how modifying the setup to allow reward functions that reference the full
state or allowing the principal to update the proxy objective over time can
lead to higher utility solutions. The results in this paper argue that we
should view the design of reward functions as an interactive and dynamic
process and identifies a theoretical scenario where some degree of
interactivity is desirable.",2022-11-13 16:01:17,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Aquib Mustafa, Majid Mazouchi, Subramanya Nageshrao, Hamidreza Modares",Assured Learning-enabled Autonomy: A Metacognitive Reinforcement Learning Framework,,,,,http://arxiv.org/abs/2103.12558v2,"Reinforcement learning (RL) agents with pre-specified reward functions cannot
provide guaranteed safety across variety of circumstances that an uncertain
system might encounter. To guarantee performance while assuring satisfaction of
safety constraints across variety of circumstances, an assured autonomous
control framework is presented in this paper by empowering RL algorithms with
metacognitive learning capabilities. More specifically, adapting the reward
function parameters of the RL agent is performed in a metacognitive
decision-making layer to assure the feasibility of RL agent. That is, to assure
that the learned policy by the RL agent satisfies safety constraints specified
by signal temporal logic while achieving as much performance as possible. The
metacognitive layer monitors any possible future safety violation under the
actions of the RL agent and employs a higher-layer Bayesian RL algorithm to
proactively adapt the reward function for the lower-layer RL agent. To minimize
the higher-layer Bayesian RL intervention, a fitness function is leveraged by
the metacognitive layer as a metric to evaluate success of the lower-layer RL
agent in satisfaction of safety and liveness specifications, and the
higher-layer Bayesian RL intervenes only if there is a risk of lower-layer RL
failure. Finally, a simulation example is provided to validate the
effectiveness of the proposed approach.",2022-11-13 16:01:18,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Tri Minh Nguyen, Thomas P Quinn, Thin Nguyen, Truyen Tran",Counterfactual Explanation with Multi-Agent Reinforcement Learning for Drug Target Prediction,,,,,http://arxiv.org/abs/2103.12983v2,"Motivation: Many high-performance DTA models have been proposed, but they are
mostly black-box and thus lack human interpretability. Explainable AI (XAI) can
make DTA models more trustworthy, and can also enable scientists to distill
biological knowledge from the models. Counterfactual explanation is one popular
approach to explaining the behaviour of a deep neural network, which works by
systematically answering the question ""How would the model output change if the
inputs were changed in this way?"". Most counterfactual explanation methods only
operate on single input data. It remains an open problem how to extend
counterfactual-based XAI methods to DTA models, which have two inputs, one for
drug and one for target, that also happen to be discrete in nature.
  Methods: We propose a multi-agent reinforcement learning framework,
Multi-Agent Counterfactual Drug target binding Affinity (MACDA), to generate
counterfactual explanations for the drug-protein complex. Our proposed
framework provides human-interpretable counterfactual instances while
optimizing both the input drug and target for counterfactual generation at the
same time.
  Results: We benchmark the proposed MACDA framework using the Davis dataset
and find that our framework produces more parsimonious explanations with no
loss in explanation validity, as measured by encoding similarity and QED. We
then present a case study involving ABL1 and Nilotinib to demonstrate how MACDA
can explain the behaviour of a DTA model in the underlying substructure
interaction between inputs in its prediction, revealing mechanisms that align
with prior domain knowledge.",2022-11-13 16:01:18,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Abdullah Khalili, Abdelhamid Bouchachia",Toward Building Science Discovery Machines,,,,,http://arxiv.org/abs/2103.15551v7,"The dream of building machines that can do science has inspired scientists
for decades. Remarkable advances have been made recently; however, we are still
far from achieving this goal. In this paper, we focus on the scientific
discovery process where a high level of reasoning and remarkable
problem-solving ability are required. We review different machine learning
techniques used in scientific discovery with their limitations. We survey and
discuss the main principles driving the scientific discovery process. These
principles are used in different fields and by different scientists to solve
problems and discover new knowledge. We provide many examples of the use of
these principles in different fields such as physics, mathematics, and biology.
We also review AI systems that attempt to implement some of these principles.
We argue that building science discovery machines should be guided by these
principles as an alternative to the dominant approach of current AI systems
that focuses on narrow objectives. Building machines that fully incorporate
these principles in an automated way might open the doors for many
advancements.",2022-11-13 16:01:19,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"The Anh Han, Tom Lenaerts, Francisco C. Santos, Luis Moniz Pereira",Voluntary safety commitments provide an escape from over-regulation in AI development,,,,,http://arxiv.org/abs/2104.03741v1,"With the introduction of Artificial Intelligence (AI) and related
technologies in our daily lives, fear and anxiety about their misuse as well as
the hidden biases in their creation have led to a demand for regulation to
address such issues. Yet blindly regulating an innovation process that is not
well understood, may stifle this process and reduce benefits that society may
gain from the generated technology, even under the best intentions. In this
paper, starting from a baseline model that captures the fundamental dynamics of
a race for domain supremacy using AI technology, we demonstrate how socially
unwanted outcomes may be produced when sanctioning is applied unconditionally
to risk-taking, i.e. potentially unsafe, behaviours. As an alternative to
resolve the detrimental effect of over-regulation, we propose a voluntary
commitment approach wherein technologists have the freedom of choice between
independently pursuing their course of actions or establishing binding
agreements to act safely, with sanctioning of those that do not abide to what
they pledged. Overall, this work reveals for the first time how voluntary
commitments, with sanctions either by peers or an institution, leads to
socially beneficial outcomes in all scenarios envisageable in a short-term race
towards domain supremacy through AI technology. These results are directly
relevant for the design of governance and regulatory policies that aim to
ensure an ethical and responsible AI technology development process.",2022-11-13 16:01:19,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Dominik Dellermann, Philipp Ebel, Matthias Soellner, Jan Marco Leimeister",Hybrid Intelligence,,,,10.1007/s12599-019-00595-2,http://arxiv.org/abs/2105.00691v1,"Research has a long history of discussing what is superior in predicting
certain outcomes: statistical methods or the human brain. This debate has
repeatedly been sparked off by the remarkable technological advances in the
field of artificial intelligence (AI), such as solving tasks like object and
speech recognition, achieving significant improvements in accuracy through
deep-learning algorithms (Goodfellow et al. 2016), or combining various methods
of computational intelligence, such as fuzzy logic, genetic algorithms, and
case-based reasoning (Medsker 2012). One of the implicit promises that underlie
these advancements is that machines will 1 day be capable of performing complex
tasks or may even supersede humans in performing these tasks. This triggers new
heated debates of when machines will ultimately replace humans (McAfee and
Brynjolfsson 2017). While previous research has proved that AI performs well in
some clearly defined tasks such as playing chess, playing Go or identifying
objects on images, it is doubted that the development of an artificial general
intelligence (AGI) which is able to solve multiple tasks at the same time can
be achieved in the near future (e.g., Russell and Norvig 2016). Moreover, the
use of AI to solve complex business problems in organizational contexts occurs
scarcely, and applications for AI that solve complex problems remain mainly in
laboratory settings instead of being implemented in practice. Since the road to
AGI is still a long one, we argue that the most likely paradigm for the
division of labor between humans and machines in the next decades is Hybrid
Intelligence. This concept aims at using the complementary strengths of human
intelligence and AI, so that they can perform better than each of the two could
separately (e.g., Kamar 2016).",2022-11-13 16:01:20,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,Beren Millidge,Towards a Mathematical Theory of Abstraction,,,,,http://arxiv.org/abs/2106.01826v2,"While the utility of well-chosen abstractions for understanding and
predicting the behaviour of complex systems is well appreciated, precisely what
an abstraction $\textit{is}$ has so far has largely eluded mathematical
formalization. In this paper, we aim to set out a mathematical theory of
abstraction. We provide a precise characterisation of what an abstraction is
and, perhaps more importantly, suggest how abstractions can be learnt directly
from data both for static datasets and for dynamical systems. We define an
abstraction to be a small set of `summaries' of a system which can be used to
answer a set of queries about the system or its behaviour. The difference
between the ground truth behaviour of the system on the queries and the
behaviour of the system predicted only by the abstraction provides a measure of
the `leakiness' of the abstraction which can be used as a loss function to
directly learn abstractions from data. Our approach can be considered a
generalization of classical statistics where we are not interested in
reconstructing `the data' in full, but are instead only concerned with
answering a set of arbitrary queries about the data. While highly theoretical,
our results have deep implications for statistical inference and machine
learning and could be used to develop explicit methods for learning precise
kinds of abstractions directly from data.",2022-11-13 16:01:20,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Youri Coppens, Denis Steckelmacher, Catholijn M. Jonker, Ann Nowé",Synthesising Reinforcement Learning Policies through Set-Valued Inductive Rule Learning,"Trustworthy AI - Integrating Learning, Optimization and Reasoning
  (2021), Lecture Notes in Computer Science, vol. 12641, pp. 163-179",,,10.1007/978-3-030-73959-1_15,http://arxiv.org/abs/2106.06009v1,"Today's advanced Reinforcement Learning algorithms produce black-box
policies, that are often difficult to interpret and trust for a person. We
introduce a policy distilling algorithm, building on the CN2 rule mining
algorithm, that distills the policy into a rule-based decision system. At the
core of our approach is the fact that an RL process does not just learn a
policy, a mapping from states to actions, but also produces extra
meta-information, such as action values indicating the quality of alternative
actions. This meta-information can indicate whether more than one action is
near-optimal for a certain state. We extend CN2 to make it able to leverage
knowledge about equally-good actions to distill the policy into fewer rules,
increasing its interpretability by a person. Then, to ensure that the rules
explain a valid, non-degenerate policy, we introduce a refinement algorithm
that fine-tunes the rules to obtain good performance when executed in the
environment. We demonstrate the applicability of our algorithm on the Mario AI
benchmark, a complex task that requires modern reinforcement learning
algorithms including neural networks. The explanations we produce capture the
learned policy in only a few rules, that allow a person to understand what the
black-box agent learned. Source code:
https://gitlab.ai.vub.ac.be/yocoppen/svcn2",2022-11-13 16:01:21,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Arwa Alanqary, Gloria Z. Lin, Joie Le, Tan Zhi-Xuan, Vikash K. Mansinghka, Joshua B. Tenenbaum",Modeling the Mistakes of Boundedly Rational Agents Within a Bayesian Theory of Mind,,,,,http://arxiv.org/abs/2106.13249v1,"When inferring the goals that others are trying to achieve, people
intuitively understand that others might make mistakes along the way. This is
crucial for activities such as teaching, offering assistance, and deciding
between blame or forgiveness. However, Bayesian models of theory of mind have
generally not accounted for these mistakes, instead modeling agents as mostly
optimal in achieving their goals. As a result, they are unable to explain
phenomena like locking oneself out of one's house, or losing a game of chess.
Here, we extend the Bayesian Theory of Mind framework to model boundedly
rational agents who may have mistaken goals, plans, and actions. We formalize
this by modeling agents as probabilistic programs, where goals may be confused
with semantically similar states, plans may be misguided due to
resource-bounded planning, and actions may be unintended due to execution
errors. We present experiments eliciting human goal inferences in two domains:
(i) a gridworld puzzle with gems locked behind doors, and (ii) a block-stacking
domain. Our model better explains human inferences than alternatives, while
generalizing across domains. These findings indicate the importance of modeling
others as bounded agents, in order to account for the full richness of human
intuitive psychology.",2022-11-13 16:01:21,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Yisroel Mirsky, Ambra Demontis, Jaidip Kotak, Ram Shankar, Deng Gelei, Liu Yang, Xiangyu Zhang, Wenke Lee, Yuval Elovici, Battista Biggio",The Threat of Offensive AI to Organizations,,,,,http://arxiv.org/abs/2106.15764v1,"AI has provided us with the ability to automate tasks, extract information
from vast amounts of data, and synthesize media that is nearly
indistinguishable from the real thing. However, positive tools can also be used
for negative purposes. In particular, cyber adversaries can use AI (such as
machine learning) to enhance their attacks and expand their campaigns.
  Although offensive AI has been discussed in the past, there is a need to
analyze and understand the threat in the context of organizations. For example,
how does an AI-capable adversary impact the cyber kill chain? Does AI benefit
the attacker more than the defender? What are the most significant AI threats
facing organizations today and what will be their impact on the future?
  In this survey, we explore the threat of offensive AI on organizations.
First, we present the background and discuss how AI changes the adversary's
methods, strategies, goals, and overall attack model. Then, through a
literature review, we identify 33 offensive AI capabilities which adversaries
can use to enhance their attacks. Finally, through a user study spanning
industry and academia, we rank the AI threats and provide insights on the
adversaries.",2022-11-13 16:01:21,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,Dorien Herremans,aiSTROM -- A roadmap for developing a successful AI strategy,"IEEE Access, 2021",,,10.1109/ACCESS.2021.3127548,http://arxiv.org/abs/2107.06071v2,"A total of 34% of AI research and development projects fails or are
abandoned, according to a recent survey by Rackspace Technology of 1,870
companies. We propose a new strategic framework, aiSTROM, that empowers
managers to create a successful AI strategy based on a thorough literature
review. This provides a unique and integrated approach that guides managers and
lead developers through the various challenges in the implementation process.
In the aiSTROM framework, we start by identifying the top n potential projects
(typically 3-5). For each of those, seven areas of focus are thoroughly
analysed. These areas include creating a data strategy that takes into account
unique cross-departmental machine learning data requirements, security, and
legal requirements. aiSTROM then guides managers to think about how to put
together an interdisciplinary artificial intelligence (AI) implementation team
given the scarcity of AI talent. Once an AI team strategy has been established,
it needs to be positioned within the organization, either cross-departmental or
as a separate division. Other considerations include AI as a service (AIaas),
or outsourcing development. Looking at new technologies, we have to consider
challenges such as bias, legality of black-box-models, and keeping humans in
the loop. Next, like any project, we need value-based key performance
indicators (KPIs) to track and validate the progress. Depending on the
company's risk-strategy, a SWOT analysis (strengths, weaknesses, opportunities,
and threats) can help further classify the shortlisted projects. Finally, we
should make sure that our strategy includes continuous education of employees
to enable a culture of adoption. This unique and comprehensive framework offers
a valuable, literature supported, tool for managers and lead developers.",2022-11-13 16:01:22,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,Desmond C. Ong,An Ethical Framework for Guiding the Development of Affectively-Aware Artificial Intelligence,,,,,http://arxiv.org/abs/2107.13734v1,"The recent rapid advancements in artificial intelligence research and
deployment have sparked more discussion about the potential ramifications of
socially- and emotionally-intelligent AI. The question is not if research can
produce such affectively-aware AI, but when it will. What will it mean for
society when machines -- and the corporations and governments they serve -- can
""read"" people's minds and emotions? What should developers and operators of
such AI do, and what should they not do? The goal of this article is to
pre-empt some of the potential implications of these developments, and propose
a set of guidelines for evaluating the (moral and) ethical consequences of
affectively-aware AI, in order to guide researchers, industry professionals,
and policy-makers. We propose a multi-stakeholder analysis framework that
separates the ethical responsibilities of AI Developers vis-\`a-vis the
entities that deploy such AI -- which we term Operators. Our analysis produces
two pillars that clarify the responsibilities of each of these stakeholders:
Provable Beneficence, which rests on proving the effectiveness of the AI, and
Responsible Stewardship, which governs responsible collection, use, and storage
of data and the decisions made from such data. We end with recommendations for
researchers, developers, operators, as well as regulators and law-makers.",2022-11-13 16:01:23,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Marianna Bergamaschi Ganapini, Murray Campbell, Francesco Fabiano, Lior Horesh, Jon Lenchner, Andrea Loreggia, Nicholas Mattei, Francesca Rossi, Biplav Srivastava, Kristen Brent Venable",Thinking Fast and Slow in AI: the Role of Metacognition,,,,,http://arxiv.org/abs/2110.01834v1,"AI systems have seen dramatic advancement in recent years, bringing many
applications that pervade our everyday life. However, we are still mostly
seeing instances of narrow AI: many of these recent developments are typically
focused on a very limited set of competencies and goals, e.g., image
interpretation, natural language processing, classification, prediction, and
many others. Moreover, while these successes can be accredited to improved
algorithms and techniques, they are also tightly linked to the availability of
huge datasets and computational power. State-of-the-art AI still lacks many
capabilities that would naturally be included in a notion of (human)
intelligence.
  We argue that a better study of the mechanisms that allow humans to have
these capabilities can help us understand how to imbue AI systems with these
competencies. We focus especially on D. Kahneman's theory of thinking fast and
slow, and we propose a multi-agent AI architecture where incoming problems are
solved by either system 1 (or ""fast"") agents, that react by exploiting only
past experience, or by system 2 (or ""slow"") agents, that are deliberately
activated when there is the need to reason and search for optimal solutions
beyond what is expected from the system 1 agent. Both kinds of agents are
supported by a model of the world, containing domain knowledge about the
environment, and a model of ""self"", containing information about past actions
of the system and solvers' skills.",2022-11-13 16:01:23,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Shahar Avin, Haydn Belfield, Miles Brundage, Gretchen Krueger, Jasmine Wang, Adrian Weller, Markus Anderljung, Igor Krawczuk, David Krueger, Jonathan Lebensold, Tegan Maharaj, Noa Zilberman",Filling gaps in trustworthy development of AI,"Science (2021) Vol 374, Issue 6573, pp. 1327-1329",,,10.1126/science.abi7176,http://arxiv.org/abs/2112.07773v1,"The range of application of artificial intelligence (AI) is vast, as is the
potential for harm. Growing awareness of potential risks from AI systems has
spurred action to address those risks, while eroding confidence in AI systems
and the organizations that develop them. A 2019 study found over 80
organizations that published and adopted ""AI ethics principles'', and more have
joined since. But the principles often leave a gap between the ""what"" and the
""how"" of trustworthy AI development. Such gaps have enabled questionable or
ethically dubious behavior, which casts doubts on the trustworthiness of
specific organizations, and the field more broadly. There is thus an urgent
need for concrete methods that both enable AI developers to prevent harm and
allow them to demonstrate their trustworthiness through verifiable behavior.
Below, we explore mechanisms (drawn from arXiv:2004.07213) for creating an
ecosystem where AI developers can earn trust - if they are trustworthy. Better
assessment of developer trustworthiness could inform user choice, employee
actions, investment decisions, legal recourse, and emerging governance regimes.",2022-11-13 16:01:23,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Enrique Iglesias, Samaneh Jozashoori, Maria-Esther Vidal",Scaling Up Knowledge Graph Creation to Large and Heterogeneous Data Sources,,,,,http://arxiv.org/abs/2201.09694v3,"RDF knowledge graphs (KG) are powerful data structures to represent factual
statements created from heterogeneous data sources. KG creation is laborious
and demands data management techniques to be executed efficiently. This paper
tackles the problem of the automatic generation of KG creation processes
declaratively specified; it proposes techniques for planning and transforming
heterogeneous data into RDF triples following mapping assertions specified in
the RDF Mapping Language (RML). Given a set of mapping assertions, the planner
provides an optimized execution plan by partitioning and scheduling the
execution of the assertions. First, the planner assesses an optimized number of
partitions considering the number of data sources, type of mapping assertions,
and the associations between different assertions. After providing a list of
partitions and assertions that belong to each partition, the planner determines
their execution order. A greedy algorithm is implemented to generate the
partitions' bushy tree execution plan. Bushy tree plans are translated into
operating system commands that guide the execution of the partitions of the
mapping assertions in the order indicated by the bushy tree. The proposed
optimization approach is evaluated over state-of-the-art RML-compliant engines,
and existing benchmarks of data sources and RML triples maps. Our experimental
results suggest that the performance of the studied engines can be considerably
improved, particularly in a complex setting with numerous triples maps and
large data sources. As a result, engines that time out in complex cases are
enabled to produce at least a portion of the KG applying the planner.",2022-11-13 16:01:24,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Chacha Chen, Shi Feng, Amit Sharma, Chenhao Tan",Machine Explanations and Human Understanding,,,,,http://arxiv.org/abs/2202.04092v1,"Explanations are hypothesized to improve human understanding of machine
learning models and achieve a variety of desirable outcomes, ranging from model
debugging to enhancing human decision making. However, empirical studies have
found mixed and even negative results. An open question, therefore, is under
what conditions explanations can improve human understanding and in what way.
Using adapted causal diagrams, we provide a formal characterization of the
interplay between machine explanations and human understanding, and show how
human intuitions play a central role in enabling human understanding.
Specifically, we identify three core concepts of interest that cover all
existing quantitative measures of understanding in the context of human-AI
decision making: task decision boundary, model decision boundary, and model
error. Our key result is that without assumptions about task-specific
intuitions, explanations may potentially improve human understanding of model
decision boundary, but they cannot improve human understanding of task decision
boundary or model error. To achieve complementary human-AI performance, we
articulate possible ways on how explanations need to work with human
intuitions. For instance, human intuitions about the relevance of features
(e.g., education is more important than age in predicting a person's income)
can be critical in detecting model error. We validate the importance of human
intuitions in shaping the outcome of machine explanations with empirical
human-subject studies. Overall, our work provides a general framework along
with actionable implications for future algorithmic development and empirical
experiments of machine explanations.",2022-11-13 16:01:25,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Kailas Vodrahalli, Tobias Gerstenberg, James Zou",Uncalibrated Models Can Improve Human-AI Collaboration,,,,,http://arxiv.org/abs/2202.05983v3,"In many practical applications of AI, an AI model is used as a decision aid
for human users. The AI provides advice that a human (sometimes) incorporates
into their decision-making process. The AI advice is often presented with some
measure of ""confidence"" that the human can use to calibrate how much they
depend on or trust the advice. In this paper, we present an initial exploration
that suggests showing AI models as more confident than they actually are, even
when the original AI is well-calibrated, can improve human-AI performance
(measured as the accuracy and confidence of the human's final prediction after
seeing the AI advice). We first train a model to predict human incorporation of
AI advice using data from thousands of human-AI interactions. This enables us
to explicitly estimate how to transform the AI's prediction confidence, making
the AI uncalibrated, in order to improve the final human prediction. We
empirically validate our results across four different tasks--dealing with
images, text and tabular data--involving hundreds of human participants. We
further support our findings with simulation analysis. Our findings suggest the
importance of jointly optimizing the human-AI system as opposed to the standard
paradigm of optimizing the AI model alone.",2022-11-13 16:01:25,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,Richard S. Sutton,The Quest for a Common Model of the Intelligent Decision Maker,,,,,http://arxiv.org/abs/2202.13252v3,"The premise of the Multi-disciplinary Conference on Reinforcement Learning
and Decision Making is that multiple disciplines share an interest in
goal-directed decision making over time. The idea of this paper is to sharpen
and deepen this premise by proposing a perspective on the decision maker that
is substantive and widely held across psychology, artificial intelligence,
economics, control theory, and neuroscience, which I call the ""common model of
the intelligent agent"". The common model does not include anything specific to
any organism, world, or application domain. The common model does include
aspects of the decision maker's interaction with its world (there must be input
and output, and a goal) and internal components of the decision maker (for
perception, decision-making, internal evaluation, and a world model). I
identify these aspects and components, note that they are given different names
in different disciplines but refer essentially to the same ideas, and discuss
the challenges and benefits of devising a neutral terminology that can be used
across disciplines. It is time to recognize and build on the convergence of
multiple diverse disciplines on a substantive common model of the intelligent
agent.",2022-11-13 16:01:26,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Sreejan Kumar, Ishita Dasgupta, Raja Marjieh, Nathaniel D. Daw, Jonathan D. Cohen, Thomas L. Griffiths",Disentangling Abstraction from Statistical Pattern Matching in Human and Machine Learning,,,,,http://arxiv.org/abs/2204.01437v2,"The ability to acquire abstract knowledge is a hallmark of human intelligence
and is believed by many to be one of the core differences between humans and
neural network models. Agents can be endowed with an inductive bias towards
abstraction through meta-learning, where they are trained on a distribution of
tasks that share some abstract structure that can be learned and applied.
However, because neural networks are hard to interpret, it can be difficult to
tell whether agents have learned the underlying abstraction, or alternatively
statistical patterns that are characteristic of that abstraction. In this work,
we compare the performance of humans and agents in a meta-reinforcement
learning paradigm in which tasks are generated from abstract rules. We define a
novel methodology for building ""task metamers"" that closely match the
statistics of the abstract tasks but use a different underlying generative
process, and evaluate performance on both abstract and metamer tasks. In our
first set of experiments, we found that humans perform better at abstract tasks
than metamer tasks whereas a widely-used meta-reinforcement learning agent
performs worse on the abstract tasks than the matched metamers. In a second set
of experiments, we base the tasks on abstractions derived directly from
empirically identified human priors. We utilize the same procedure to generate
corresponding metamer tasks, and see the same double dissociation between
humans and agents. This work provides a foundation for characterizing
differences between humans and machine learning that can be used in future work
towards developing machines with human-like behavior.",2022-11-13 16:01:26,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Theodore R. Sumers, Robert D. Hawkins, Mark K. Ho, Thomas L. Griffiths, Dylan Hadfield-Menell",Linguistic communication as (inverse) reward design,,,,,http://arxiv.org/abs/2204.05091v1,"Natural language is an intuitive and expressive way to communicate reward
information to autonomous agents. It encompasses everything from concrete
instructions to abstract descriptions of the world. Despite this, natural
language is often challenging to learn from: it is difficult for machine
learning methods to make appropriate inferences from such a wide range of
input. This paper proposes a generalization of reward design as a unifying
principle to ground linguistic communication: speakers choose utterances to
maximize expected rewards from the listener's future behaviors. We first extend
reward design to incorporate reasoning about unknown future states in a linear
bandit setting. We then define a speaker model which chooses utterances
according to this objective. Simulations show that short-horizon speakers
(reasoning primarily about a single, known state) tend to use instructions,
while long-horizon speakers (reasoning primarily about unknown, future states)
tend to describe the reward function. We then define a pragmatic listener which
performs inverse reward design by jointly inferring the speaker's latent
horizon and rewards. Our findings suggest that this extension of reward design
to linguistic communication, including the notion of a latent speaker horizon,
is a promising direction for achieving more robust alignment outcomes from
natural language supervision.",2022-11-13 16:01:27,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Huili Chen, Xinqiao Zhang, Ke Huang, Farinaz Koushanfar",AdaTest:Reinforcement Learning and Adaptive Sampling for On-chip Hardware Trojan Detection,,,,,http://arxiv.org/abs/2204.06117v1,"This paper proposes AdaTest, a novel adaptive test pattern generation
framework for efficient and reliable Hardware Trojan (HT) detection. HT is a
backdoor attack that tampers with the design of victim integrated circuits
(ICs). AdaTest improves the existing HT detection techniques in terms of
scalability and accuracy of detecting smaller Trojans in the presence of noise
and variations. To achieve high trigger coverage, AdaTest leverages
Reinforcement Learning (RL) to produce a diverse set of test inputs.
Particularly, we progressively generate test vectors with high reward values in
an iterative manner. In each iteration, the test set is evaluated and
adaptively expanded as needed. Furthermore, AdaTest integrates adaptive
sampling to prioritize test samples that provide more information for HT
detection, thus reducing the number of samples while improving the sample
quality for faster exploration. We develop AdaTest with a Software/Hardware
co-design principle and provide an optimized on-chip architecture solution.
AdaTest's architecture minimizes the hardware overhead in two ways:(i)
Deploying circuit emulation on programmable hardware to accelerate reward
evaluation of the test input; (ii) Pipelining each computation stage in AdaTest
by automatically constructing auxiliary circuit for test input generation,
reward evaluation, and adaptive sampling. We evaluate AdaTest's performance on
various HT benchmarks and compare it with two prior works that use logic
testing for HT detection. Experimental results show that AdaTest engenders up
to two orders of test generation speedup and two orders of test set size
reduction compared to the prior works while achieving the same level or higher
Trojan detection rate.",2022-11-13 16:01:27,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Jennafer S. Roberts, Laura N. Montoya","Contextualizing Artificially Intelligent Morality: A Meta-Ethnography of Top-Down, Bottom-Up, and Hybrid Models for Theoretical and Applied Ethics in Artificial Intelligence",,,,,http://arxiv.org/abs/2204.07612v2,"In this meta-ethnography, we explore three different angles of ethical
artificial intelligence (AI) design implementation including the philosophical
ethical viewpoint, the technical perspective, and framing through a political
lens. Our qualitative research includes a literature review that highlights the
cross-referencing of these angles by discussing the value and drawbacks of
contrastive top-down, bottom-up, and hybrid approaches previously published.
The novel contribution to this framework is the political angle, which
constitutes ethics in AI either being determined by corporations and
governments and imposed through policies or law (coming from the top), or
ethics being called for by the people (coming from the bottom), as well as
top-down, bottom-up, and hybrid technicalities of how AI is developed within a
moral construct and in consideration of its users, with expected and unexpected
consequences and long-term impact in the world. There is a focus on
reinforcement learning as an example of a bottom-up applied technical approach
and AI ethics principles as a practical top-down approach. This investigation
includes real-world case studies to impart a global perspective, as well as
philosophical debate on the ethics of AI and theoretical future thought
experimentation based on historical facts, current world circumstances, and
possible ensuing realities.",2022-11-13 16:01:28,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Zhenghua Chen, Min Wu, Alvin Chan, Xiaoli Li, Yew-Soon Ong",A Survey on AI Sustainability: Emerging Trends on Learning Algorithms and Research Challenges,,,,,http://arxiv.org/abs/2205.03824v1,"Artificial Intelligence (AI) is a fast-growing research and development (R&D)
discipline which is attracting increasing attention because of its promises to
bring vast benefits for consumers and businesses, with considerable benefits
promised in productivity growth and innovation. To date it has reported
significant accomplishments in many areas that have been deemed as challenging
for machines, ranging from computer vision, natural language processing, audio
analysis to smart sensing and many others. The technical trend in realizing the
successes has been towards increasing complex and large size AI models so as to
solve more complex problems at superior performance and robustness. This rapid
progress, however, has taken place at the expense of substantial environmental
costs and resources. Besides, debates on the societal impacts of AI, such as
fairness, safety and privacy, have continued to grow in intensity. These issues
have presented major concerns pertaining to the sustainable development of AI.
In this work, we review major trends in machine learning approaches that can
address the sustainability problem of AI. Specifically, we examine emerging AI
methodologies and algorithms for addressing the sustainability issue of AI in
two major aspects, i.e., environmental sustainability and social sustainability
of AI. We will also highlight the major limitations of existing studies and
propose potential research challenges and directions for the development of
next generation of sustainable AI techniques. We believe that this technical
review can help to promote a sustainable development of AI R&D activities for
the research community.",2022-11-13 16:01:29,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Sascha Saralajew, Ammar Shaker, Zhao Xu, Kiril Gashteovski, Bhushan Kotnis, Wiem Ben Rim, Jürgen Quittek, Carolin Lawrence",A Human-Centric Assessment Framework for AI,,,,,http://arxiv.org/abs/2205.12749v2,"With the rise of AI systems in real-world applications comes the need for
reliable and trustworthy AI. An essential aspect of this are explainable AI
systems. However, there is no agreed standard on how explainable AI systems
should be assessed. Inspired by the Turing test, we introduce a human-centric
assessment framework where a leading domain expert accepts or rejects the
solutions of an AI system and another domain expert. By comparing the
acceptance rates of provided solutions, we can assess how the AI system
performs compared to the domain expert, and whether the AI system's
explanations (if provided) are human-understandable. This setup -- comparable
to the Turing test -- can serve as a framework for a wide range of
human-centric AI system assessments. We demonstrate this by presenting two
instantiations: (1) an assessment that measures the classification accuracy of
a system with the option to incorporate label uncertainties; (2) an assessment
where the usefulness of provided explanations is determined in a human-centric
manner.",2022-11-13 16:01:29,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Yushi Cao, Zhiming Li, Tianpei Yang, Hao Zhang, Yan Zheng, Yi Li, Jianye Hao, Yang Liu",GALOIS: Boosting Deep Reinforcement Learning via Generalizable Logic Synthesis,,,,,http://arxiv.org/abs/2205.13728v1,"Despite achieving superior performance in human-level control problems,
unlike humans, deep reinforcement learning (DRL) lacks high-order intelligence
(e.g., logic deduction and reuse), thus it behaves ineffectively than humans
regarding learning and generalization in complex problems. Previous works
attempt to directly synthesize a white-box logic program as the DRL policy,
manifesting logic-driven behaviors. However, most synthesis methods are built
on imperative or declarative programming, and each has a distinct limitation,
respectively. The former ignores the cause-effect logic during synthesis,
resulting in low generalizability across tasks. The latter is strictly
proof-based, thus failing to synthesize programs with complex hierarchical
logic. In this paper, we combine the above two paradigms together and propose a
novel Generalizable Logic Synthesis (GALOIS) framework to synthesize
hierarchical and strict cause-effect logic programs. GALOIS leverages the
program sketch and defines a new sketch-based hybrid program language for
guiding the synthesis. Based on that, GALOIS proposes a sketch-based program
synthesis method to automatically generate white-box programs with
generalizable and interpretable cause-effect logic. Extensive evaluations on
various decision-making tasks with complex logic demonstrate the superiority of
GALOIS over mainstream baselines regarding the asymptotic performance,
generalizability, and great knowledge reusability across different
environments.",2022-11-13 16:01:30,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Kuang-Huei Lee, Ofir Nachum, Mengjiao Yang, Lisa Lee, Daniel Freeman, Winnie Xu, Sergio Guadarrama, Ian Fischer, Eric Jang, Henryk Michalewski, Igor Mordatch",Multi-Game Decision Transformers,,,,,http://arxiv.org/abs/2205.15241v2,"A longstanding goal of the field of AI is a method for learning a highly
capable, generalist agent from diverse experience. In the subfields of vision
and language, this was largely achieved by scaling up transformer-based models
and training them on large, diverse datasets. Motivated by this progress, we
investigate whether the same strategy can be used to produce generalist
reinforcement learning agents. Specifically, we show that a single
transformer-based model - with a single set of weights - trained purely offline
can play a suite of up to 46 Atari games simultaneously at close-to-human
performance. When trained and evaluated appropriately, we find that the same
trends observed in language and vision hold, including scaling of performance
with model size and rapid adaptation to new games via fine-tuning. We compare
several approaches in this multi-game setting, such as online and offline RL
methods and behavioral cloning, and find that our Multi-Game Decision
Transformer models offer the best scalability and performance. We release the
pre-trained models and code to encourage further research in this direction.",2022-11-13 16:01:30,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Claire Stevenson, Iris Smal, Matthijs Baas, Raoul Grasman, Han van der Maas",Putting GPT-3's Creativity to the (Alternative Uses) Test,,,,,http://arxiv.org/abs/2206.08932v1,"AI large language models have (co-)produced amazing written works from
newspaper articles to novels and poetry. These works meet the standards of the
standard definition of creativity: being original and useful, and sometimes
even the additional element of surprise. But can a large language model
designed to predict the next text fragment provide creative, out-of-the-box,
responses that still solve the problem at hand? We put Open AI's generative
natural language model, GPT-3, to the test. Can it provide creative solutions
to one of the most commonly used tests in creativity research? We assessed
GPT-3's creativity on Guilford's Alternative Uses Test and compared its
performance to previously collected human responses on expert ratings of
originality, usefulness and surprise of responses, flexibility of each set of
ideas as well as an automated method to measure creativity based on the
semantic distance between a response and the AUT object in question. Our
results show that -- on the whole -- humans currently outperform GPT-3 when it
comes to creative output. But, we believe it is only a matter of time before
GPT-3 catches up on this particular task. We discuss what this work reveals
about human and AI creativity, creativity testing and our definition of
creativity.",2022-11-13 16:01:31,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Théophile Champion, Marek Grześ, Howard Bowman",Multi-Modal and Multi-Factor Branching Time Active Inference,,,,,http://arxiv.org/abs/2206.12503v1,"Active inference is a state-of-the-art framework for modelling the brain that
explains a wide range of mechanisms such as habit formation, dopaminergic
discharge and curiosity. Recently, two versions of branching time active
inference (BTAI) based on Monte-Carlo tree search have been developed to handle
the exponential (space and time) complexity class that occurs when computing
the prior over all possible policies up to the time horizon. However, those two
versions of BTAI still suffer from an exponential complexity class w.r.t the
number of observed and latent variables being modelled. In the present paper,
we resolve this limitation by first allowing the modelling of several
observations, each of them having its own likelihood mapping. Similarly, we
allow each latent state to have its own transition mapping. The inference
algorithm then exploits the factorisation of the likelihood and transition
mappings to accelerate the computation of the posterior. Those two
optimisations were tested on the dSprites environment in which the metadata of
the dSprites dataset was used as input to the model instead of the dSprites
images. On this task, $BTAI_{VMP}$ (Champion et al., 2022b,a) was able to solve
96.9\% of the task in 5.1 seconds, and $BTAI_{BF}$ (Champion et al., 2021a) was
able to solve 98.6\% of the task in 17.5 seconds. Our new approach
($BTAI_{3MF}$) outperformed both of its predecessors by solving the task
completly (100\%) in only 2.559 seconds. Finally, $BTAI_{3MF}$ has been
implemented in a flexible and easy to use (python) package, and we developed a
graphical user interface to enable the inspection of the model's beliefs,
planning process and behaviour.",2022-11-13 16:01:31,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Hiroshi Yamakawa, Yutaka Matsuo",Recognition of All Categories of Entities by AI,,,,,http://arxiv.org/abs/2208.06590v2,"Human-level AI will have significant impacts on human society. However,
estimates for the realization time are debatable. To arrive at human-level AI,
artificial general intelligence (AGI), as opposed to AI systems that are
specialized for a specific task, was set as a technically meaningful long-term
goal. But now, propelled by advances in deep learning, that achievement is
getting much closer. Considering the recent technological developments, it
would be meaningful to discuss the completion date of human-level AI through
the ""comprehensive technology map approach,"" wherein we map human-level
capabilities at a reasonable granularity, identify the current range of
technology, and discuss the technical challenges in traversing unexplored areas
and predict when all of them will be overcome. This paper presents a new
argumentative option to view the ontological sextet, which encompasses entities
in a way that is consistent with our everyday intuition and scientific
practice, as a comprehensive technological map. Because most of the modeling of
the world, in terms of how to interpret it, by an intelligent subject is the
recognition of distal entities and the prediction of their temporal evolution,
being able to handle all distal entities is a reasonable goal. Based on the
findings of philosophy and engineering cognitive technology, we predict that in
the relatively near future, AI will be able to recognize various entities to
the same degree as humans.",2022-11-13 16:01:32,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Gali Noti, Yiling Chen",Learning When to Advise Human Decision Makers,,,,,http://arxiv.org/abs/2209.13578v1,"Artificial intelligence (AI) systems are increasingly used for providing
advice to facilitate human decision making. While a large body of work has
explored how AI systems can be optimized to produce accurate and fair advice
and how algorithmic advice should be presented to human decision makers, in
this work we ask a different basic question: When should algorithms provide
advice? Motivated by limitations of the current practice of constantly
providing algorithmic advice, we propose the design of AI systems that interact
with the human user in a two-sided manner and provide advice only when it is
likely to be beneficial to the human in making their decision. Our AI systems
learn advising policies using past human decisions. Then, for new cases, the
learned policies utilize input from the human to identify cases where
algorithmic advice would be useful, as well as those where the human is better
off deciding alone. We conduct a large-scale experiment to evaluate our
approach by using data from the US criminal justice system on pretrial-release
decisions. In our experiment, participants were asked to assess the risk of
defendants to violate their release terms if released and were advised by
different advising approaches. The results show that our interactive-advising
approach manages to provide advice at times of need and to significantly
improve human decision making compared to fixed, non-interactive advising
approaches. Our approach has additional advantages in facilitating human
learning, preserving complementary strengths of human decision makers, and
leading to more positive responsiveness to the advice.",2022-11-13 16:01:32,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Mu Yuan, Lan Zhang, Fengxiang He, Xueting Tong, Miao-Hui Song, Xiang-Yang Li",InFi: End-to-End Learning to Filter Input for Resource-Efficiency in Mobile-Centric Inference,,,,,http://arxiv.org/abs/2209.13873v1,"Mobile-centric AI applications have high requirements for resource-efficiency
of model inference. Input filtering is a promising approach to eliminate the
redundancy so as to reduce the cost of inference. Previous efforts have
tailored effective solutions for many applications, but left two essential
questions unanswered: (1) theoretical filterability of an inference workload to
guide the application of input filtering techniques, thereby avoiding the
trial-and-error cost for resource-constrained mobile applications; (2) robust
discriminability of feature embedding to allow input filtering to be widely
effective for diverse inference tasks and input content. To answer them, we
first formalize the input filtering problem and theoretically compare the
hypothesis complexity of inference models and input filters to understand the
optimization potential. Then we propose the first end-to-end learnable input
filtering framework that covers most state-of-the-art methods and surpasses
them in feature embedding with robust discriminability. We design and implement
InFi that supports six input modalities and multiple mobile-centric
deployments. Comprehensive evaluations confirm our theoretical results and show
that InFi outperforms strong baselines in applicability, accuracy, and
efficiency. InFi achieve 8.5x throughput and save 95% bandwidth, while keeping
over 90% accuracy, for a video analytics application on mobile platforms.",2022-11-13 16:01:33,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Michael L. Littman, Ifeoma Ajunwa, Guy Berger, Craig Boutilier, Morgan Currie, Finale Doshi-Velez, Gillian Hadfield, Michael C. Horowitz, Charles Isbell, Hiroaki Kitano, Karen Levy, Terah Lyons, Melanie Mitchell, Julie Shah, Steven Sloman, Shannon Vallor, Toby Walsh","Gathering Strength, Gathering Storms: The One Hundred Year Study on Artificial Intelligence (AI100) 2021 Study Panel Report",,,,,http://arxiv.org/abs/2210.15767v1,"In September 2021, the ""One Hundred Year Study on Artificial Intelligence""
project (AI100) issued the second report of its planned long-term periodic
assessment of artificial intelligence (AI) and its impact on society. It was
written by a panel of 17 study authors, each of whom is deeply rooted in AI
research, chaired by Michael Littman of Brown University. The report, entitled
""Gathering Strength, Gathering Storms,"" answers a set of 14 questions probing
critical areas of AI development addressing the major risks and dangers of AI,
its effects on society, its public perception and the future of the field. The
report concludes that AI has made a major leap from the lab to people's lives
in recent years, which increases the urgency to understand its potential
negative effects. The questions were developed by the AI100 Standing Committee,
chaired by Peter Stone of the University of Texas at Austin, consisting of a
group of AI leaders with expertise in computer science, sociology, ethics,
economics, and other disciplines.",2022-11-13 16:01:33,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"David Fernández Llorca, Vicky Charisi, Ronan Hamon, Ignacio Sánchez, Emilia Gómez",Liability regimes in the age of AI: a use-case driven analysis of the burden of proof,,,,,http://arxiv.org/abs/2211.01817v1,"New emerging technologies powered by Artificial Intelligence (AI) have the
potential to disruptively transform our societies for the better. In
particular, data-driven learning approaches (i.e., Machine Learning (ML)) have
been a true revolution in the advancement of multiple technologies in various
application domains. But at the same time there is growing concerns about
certain intrinsic characteristics of these methodologies that carry potential
risks to both safety and fundamental rights. Although there are mechanisms in
the adoption process to minimize these risks (e.g., safety regulations), these
do not exclude the possibility of harm occurring, and if this happens, victims
should be able to seek compensation. Liability regimes will therefore play a
key role in ensuring basic protection for victims using or interacting with
these systems. However, the same characteristics that make AI systems
inherently risky, such as lack of causality, opacity, unpredictability or their
self and continuous learning capabilities, lead to considerable difficulties
when it comes to proving causation. This paper presents three case studies, as
well as the methodology to reach them, that illustrate these difficulties.
Specifically, we address the cases of cleaning robots, delivery drones and
robots in education. The outcome of the proposed analysis suggests the need to
revise liability regimes to alleviate the burden of proof on victims in cases
involving AI technologies.",2022-11-13 16:01:34,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Kyle A. Kilian, Christopher J. Ventura, Mark M. Bailey",Examining the Differential Risk from High-level Artificial Intelligence and the Question of Control,,,,,http://arxiv.org/abs/2211.03157v2,"Artificial Intelligence (AI) is one of the most transformative technologies
of the 21st century. The extent and scope of future AI capabilities remain a
key uncertainty, with widespread disagreement on timelines and potential
impacts. As nations and technology companies race toward greater complexity and
autonomy in AI systems, there are concerns over the extent of integration and
oversight of opaque AI decision processes. This is especially true in the
subfield of machine learning (ML), where systems learn to optimize objectives
without human assistance. Objectives can be imperfectly specified or executed
in an unexpected or potentially harmful way. This becomes more concerning as
systems increase in power and autonomy, where an abrupt capability jump could
result in unexpected shifts in power dynamics or even catastrophic failures.
This study presents a hierarchical complex systems framework to model AI risk
and provide a template for alternative futures analysis. Survey data were
collected from domain experts in the public and private sectors to classify AI
impact and likelihood. The results show increased uncertainty over the powerful
AI agent scenario, confidence in multiagent environments, and increased concern
over AI alignment failures and influence-seeking behavior.",2022-11-13 16:01:35,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2014,"Wiebe van der Hoek, Dirk Walther, Michael Wooldridge",Reasoning About the Transfer of Control,"Journal Of Artificial Intelligence Research, Volume 37, pages
  437-477, 2010",,,10.1613/jair.2901,http://arxiv.org/abs/1401.3825v1,"We present DCL-PC: a logic for reasoning about how the abilities of agents
and coalitions of agents are altered by transferring control from one agent to
another. The logical foundation of DCL-PC is CL-PC, a logic for reasoning about
cooperation in which the abilities of agents and coalitions of agents stem from
a distribution of atomic Boolean variables to individual agents -- the choices
available to a coalition correspond to assignments to the variables the
coalition controls. The basic modal constructs of DCL-PC are of the form
coalition C can cooperate to bring about phi. DCL-PC extends CL-PC with dynamic
logic modalities in which atomic programs are of the form agent i gives control
of variable p to agent j; as usual in dynamic logic, these atomic programs may
be combined using sequence, iteration, choice, and test operators to form
complex programs. By combining such dynamic transfer programs with cooperation
modalities, it becomes possible to reason about how the power of agents and
coalitions is affected by the transfer of control. We give two alternative
semantics for the logic: a direct semantics, in which we capture the
distributions of Boolean variables to agents; and a more conventional Kripke
semantics. We prove that these semantics are equivalent, and then present an
axiomatization for the logic. We investigate the computational complexity of
model checking and satisfiability for DCL-PC, and show that both problems are
PSPACE-complete (and hence no worse than the underlying logic CL-PC). Finally,
we investigate the characterisation of control in DCL-PC. We distinguish
between first-order control -- the ability of an agent or coalition to control
some state of affairs through the assignment of values to the variables under
the control of the agent or coalition -- and second-order control -- the
ability of an agent to exert control over the control that other agents have by
transferring variables to other agents. We give a logical characterisation of
second-order control.",2022-11-13 16:01:35,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2015,"Stefano V. Albrecht, Jacob W. Crandall, Subramanian Ramamoorthy",Belief and Truth in Hypothesised Behaviours,,,,10.1016/j.artint.2016.02.004,http://arxiv.org/abs/1507.07688v3,"There is a long history in game theory on the topic of Bayesian or ""rational""
learning, in which each player maintains beliefs over a set of alternative
behaviours, or types, for the other players. This idea has gained increasing
interest in the artificial intelligence (AI) community, where it is used as a
method to control a single agent in a system composed of multiple agents with
unknown behaviours. The idea is to hypothesise a set of types, each specifying
a possible behaviour for the other agents, and to plan our own actions with
respect to those types which we believe are most likely, given the observed
actions of the agents. The game theory literature studies this idea primarily
in the context of equilibrium attainment. In contrast, many AI applications
have a focus on task completion and payoff maximisation. With this perspective
in mind, we identify and address a spectrum of questions pertaining to belief
and truth in hypothesised types. We formulate three basic ways to incorporate
evidence into posterior beliefs and show when the resulting beliefs are
correct, and when they may fail to be correct. Moreover, we demonstrate that
prior beliefs can have a significant impact on our ability to maximise payoffs
in the long-term, and that they can be computed automatically with consistent
performance effects. Furthermore, we analyse the conditions under which we are
able complete our task optimally, despite inaccuracies in the hypothesised
types. Finally, we show how the correctness of hypothesised types can be
ascertained during the interaction via an automated statistical analysis.",2022-11-13 16:01:36,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2016,"Gopal P. Sarma, Nick J. Hay",Mammalian Value Systems,Informatica Vol. 41 No. 3 (2017),,,,http://arxiv.org/abs/1607.08289v4,"Characterizing human values is a topic deeply interwoven with the sciences,
humanities, art, and many other human endeavors. In recent years, a number of
thinkers have argued that accelerating trends in computer science, cognitive
science, and related disciplines foreshadow the creation of intelligent
machines which meet and ultimately surpass the cognitive abilities of human
beings, thereby entangling an understanding of human values with future
technological development. Contemporary research accomplishments suggest
sophisticated AI systems becoming widespread and responsible for managing many
aspects of the modern world, from preemptively planning users' travel schedules
and logistics, to fully autonomous vehicles, to domestic robots assisting in
daily living. The extrapolation of these trends has been most forcefully
described in the context of a hypothetical ""intelligence explosion,"" in which
the capabilities of an intelligent software agent would rapidly increase due to
the presence of feedback loops unavailable to biological organisms. The
possibility of superintelligent agents, or simply the widespread deployment of
sophisticated, autonomous AI systems, highlights an important theoretical
problem: the need to separate the cognitive and rational capacities of an agent
from the fundamental goal structure, or value system, which constrains and
guides the agent's actions. The ""value alignment problem"" is to specify a goal
structure for autonomous agents compatible with human values. In this brief
article, we suggest that recent ideas from affective neuroscience and related
disciplines aimed at characterizing neurological and behavioral universals in
the mammalian class provide important conceptual foundations relevant to
describing human values. We argue that the notion of ""mammalian value systems""
points to a potential avenue for fundamental research in AI safety and AI
ethics.",2022-11-13 16:01:36,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2016,"Sarmimala Saikia, Lovekesh Vig, Ashwin Srinivasan, Gautam Shroff, Puneet Agarwal, Richa Rawat",Neuro-symbolic EDA-based Optimisation using ILP-enhanced DBNs,,,,,http://arxiv.org/abs/1612.06528v1,"We investigate solving discrete optimisation problems using the estimation of
distribution (EDA) approach via a novel combination of deep belief
networks(DBN) and inductive logic programming (ILP).While DBNs are used to
learn the structure of successively better feasible solutions,ILP enables the
incorporation of domain-based background knowledge related to the goodness of
solutions.Recent work showed that ILP could be an effective way to use domain
knowledge in an EDA scenario.However,in a purely ILP-based EDA,sampling
successive populations is either inefficient or not straightforward.In our
Neuro-symbolic EDA,an ILP engine is used to construct a model for good
solutions using domain-based background knowledge.These rules are introduced as
Boolean features in the last hidden layer of DBNs used for EDA-based
optimization.This incorporation of logical ILP features requires some changes
while training and sampling from DBNs: (a)our DBNs need to be trained with data
for units at the input layer as well as some units in an otherwise hidden
layer, and (b)we would like the samples generated to be drawn from instances
entailed by the logical model.We demonstrate the viability of our approach on
instances of two optimisation problems: predicting optimal depth-of-win for the
KRK endgame,and jobshop scheduling.Our results are promising: (i)On each
iteration of distribution estimation,samples obtained with an ILP-assisted DBN
have a substantially greater proportion of good solutions than samples
generated using a DBN without ILP features, and (ii)On termination of
distribution estimation,samples obtained using an ILP-assisted DBN contain more
near-optimal samples than samples from a DBN without ILP features.These results
suggest that the use of ILP-constructed theories could be useful for
incorporating complex domain-knowledge into deep models for estimation of
distribution based procedures.",2022-11-13 16:01:37,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2016,"C. J. C. Burges, T. Hart, Z. Yang, S. Cucerzan, R. W. White, A. Pastusiak, J. Lewis",A Base Camp for Scaling AI,,,,,http://arxiv.org/abs/1612.07896v1,"Modern statistical machine learning (SML) methods share a major limitation
with the early approaches to AI: there is no scalable way to adapt them to new
domains. Human learning solves this in part by leveraging a rich, shared,
updateable world model. Such scalability requires modularity: updating part of
the world model should not impact unrelated parts. We have argued that such
modularity will require both ""correctability"" (so that errors can be corrected
without introducing new errors) and ""interpretability"" (so that we can
understand what components need correcting).
  To achieve this, one could attempt to adapt state of the art SML systems to
be interpretable and correctable; or one could see how far the simplest
possible interpretable, correctable learning methods can take us, and try to
control the limitations of SML methods by applying them only where needed. Here
we focus on the latter approach and we investigate two main ideas: ""Teacher
Assisted Learning"", which leverages crowd sourcing to learn language; and
""Factored Dialog Learning"", which factors the process of application
development into roles where the language competencies needed are isolated,
enabling non-experts to quickly create new applications.
  We test these ideas in an ""Automated Personal Assistant"" (APA) setting, with
two scenarios: that of detecting user intent from a user-APA dialog; and that
of creating a class of event reminder applications, where a non-expert
""teacher"" can then create specific apps. For the intent detection task, we use
a dataset of a thousand labeled utterances from user dialogs with Cortana, and
we show that our approach matches state of the art SML methods, but in addition
provides full transparency: the whole (editable) model can be summarized on one
human-readable page. For the reminder app task, we ran small user studies to
verify the efficacy of the approach.",2022-11-13 16:01:37,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,"Naveen Sundar Govindarajulu, Selmer Bringsjord",On Automating the Doctrine of Double Effect,,,,,http://arxiv.org/abs/1703.08922v5,"The doctrine of double effect ($\mathcal{DDE}$) is a long-studied ethical
principle that governs when actions that have both positive and negative
effects are to be allowed. The goal in this paper is to automate
$\mathcal{DDE}$. We briefly present $\mathcal{DDE}$, and use a first-order
modal logic, the deontic cognitive event calculus, as our framework to
formalize the doctrine. We present formalizations of increasingly stronger
versions of the principle, including what is known as the doctrine of triple
effect. We then use our framework to simulate successfully scenarios that have
been used to test for the presence of the principle in human subjects. Our
framework can be used in two different modes: One can use it to build
$\mathcal{DDE}$-compliant autonomous systems from scratch, or one can use it to
verify that a given AI system is $\mathcal{DDE}$-compliant, by applying a
$\mathcal{DDE}$ layer on an existing system or model. For the latter mode, the
underlying AI system can be built using any architecture (planners, deep neural
networks, bayesian networks, knowledge-representation systems, or a hybrid); as
long as the system exposes a few parameters in its model, such verification is
possible. The role of the $\mathcal{DDE}$ layer here is akin to a (dynamic or
static) software verifier that examines existing software modules. Finally, we
end by presenting initial work on how one can apply our $\mathcal{DDE}$ layer
to the STRIPS-style planning model, and to a modified POMDP model.This is
preliminary work to illustrate the feasibility of the second mode, and we hope
that our initial sketches can be useful for other researchers in incorporating
DDE in their own frameworks.",2022-11-13 16:01:38,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Stuart M. Marshall, Douglas Moore, Alastair R. G. Murray, Sara I. Walker, Leroy Cronin",Quantifying the pathways to life using assembly spaces,,,,,http://arxiv.org/abs/1907.04649v2,"We have developed the concept of pathway assembly to explore the amount of
extrinsic information required to build an object. To quantify this information
in an agnostic way, we present a method to determine the amount of pathway
assembly information contained within such an object by deconstructing the
object into its irreducible parts, and then evaluating the minimum number of
steps to reconstruct the object along any pathway. The mathematical
formalisation of this approach uses an assembly space. By finding the minimal
number of steps contained in the route by which the objects can be assembled
within that space, we can compare how much information (I) is gained from
knowing this pathway assembly index (PA) according to I_PA=log (|N|)/(|N_PA |)
where, for an end product with PA=x, N is the set of objects possible that can
be created from the same irreducible parts within x steps regardless of PA, and
NPA is the subset of those objects with the precise pathway assembly index
PA=x. Applying this formalism to objects formed in 1D, 2D and 3D space allows
us to identify objects in the world or wider Universe that have high assembly
numbers. We propose that objects with PA greater than a threshold are important
because these are uniquely identifiable as those that must have been produced
by biological or technological processes, rather than the assembly occurring
via unbiased random processes alone. We think this approach is needed to help
identify the new physical and chemical laws needed to understand what life is,
by quantifying what life does.",2022-11-13 16:01:38,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,Pedro Casas,Two Decades of AI4NETS-AI/ML for Data Networks: Challenges & Research Directions,"5th IEEE/IFIP International Workshop on Analytics for Network and
  Service Management (AnNet 2020)",,,,http://arxiv.org/abs/2003.04080v1,"The popularity of Artificial Intelligence (AI) -- and of Machine Learning
(ML) as an approach to AI, has dramatically increased in the last few years,
due to its outstanding performance in various domains, notably in image, audio,
and natural language processing. In these domains, AI success-stories are
boosting the applied field. When it comes to AI/ML for data communication
Networks (AI4NETS), and despite the many attempts to turn networks into
learning agents, the successful application of AI/ML in networking is limited.
There is a strong resistance against AI/ML-based solutions, and a striking gap
between the extensive academic research and the actual deployments of such
AI/ML-based systems in operational environments. The truth is, there are still
many unsolved complex challenges associated to the analysis of networking data
through AI/ML, which hinders its acceptability and adoption in the practice. In
this positioning paper I elaborate on the most important show-stoppers in
AI4NETS, and present a research agenda to tackle some of these challenges,
enabling a natural adoption of AI/ML for networking. In particular, I focus the
future research in AI4NETS around three major pillars: (i) to make AI/ML
immediately applicable in networking problems through the concepts of effective
learning, turning it into a useful and reliable way to deal with complex
data-driven networking problems; (ii) to boost the adoption of AI/ML at the
large scale by learning from the Internet-paradigm itself, conceiving novel
distributed and hierarchical learning approaches mimicking the distributed
topological principles and operation of the Internet itself; and (iii) to
exploit the softwarization and distribution of networks to conceive
AI/ML-defined Networks (AIDN), relying on the distributed generation and
re-usage of knowledge through novel Knowledge Delivery Networks (KDNs).",2022-11-13 16:01:39,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Dongbo Xi, Zhen Chen, Peng Yan, Yinger Zhang, Yongchun Zhu, Fuzhen Zhuang, Yu Chen",Modeling the Sequential Dependence among Audience Multi-step Conversions with Multi-task Learning in Targeted Display Advertising,,,,,http://arxiv.org/abs/2105.08489v2,"In most real-world large-scale online applications (e.g., e-commerce or
finance), customer acquisition is usually a multi-step conversion process of
audiences. For example, an impression->click->purchase process is usually
performed of audiences for e-commerce platforms. However, it is more difficult
to acquire customers in financial advertising (e.g., credit card advertising)
than in traditional advertising. On the one hand, the audience multi-step
conversion path is longer. On the other hand, the positive feedback is sparser
(class imbalance) step by step, and it is difficult to obtain the final
positive feedback due to the delayed feedback of activation. Multi-task
learning is a typical solution in this direction. While considerable multi-task
efforts have been made in this direction, a long-standing challenge is how to
explicitly model the long-path sequential dependence among audience multi-step
conversions for improving the end-to-end conversion. In this paper, we propose
an Adaptive Information Transfer Multi-task (AITM) framework, which models the
sequential dependence among audience multi-step conversions via the Adaptive
Information Transfer (AIT) module. The AIT module can adaptively learn what and
how much information to transfer for different conversion stages. Besides, by
combining the Behavioral Expectation Calibrator in the loss function, the AITM
framework can yield more accurate end-to-end conversion identification. The
proposed framework is deployed in Meituan app, which utilizes it to real-timely
show a banner to the audience with a high end-to-end conversion rate for
Meituan Co-Branded Credit Cards. Offline experimental results on both
industrial and public real-world datasets clearly demonstrate that the proposed
framework achieves significantly better performance compared with
state-of-the-art baselines.",2022-11-13 16:01:40,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,Yongfeng Zhang,Problem Learning: Towards the Free Will of Machines,,,,,http://arxiv.org/abs/2109.00177v1,"A machine intelligence pipeline usually consists of six components: problem,
representation, model, loss, optimizer and metric. Researchers have worked hard
trying to automate many components of the pipeline. However, one key component
of the pipeline--problem definition--is still left mostly unexplored in terms
of automation. Usually, it requires extensive efforts from domain experts to
identify, define and formulate important problems in an area. However,
automatically discovering research or application problems for an area is
beneficial since it helps to identify valid and potentially important problems
hidden in data that are unknown to domain experts, expand the scope of tasks
that we can do in an area, and even inspire completely new findings.
  This paper describes Problem Learning, which aims at learning to discover and
define valid and ethical problems from data or from the machine's interaction
with the environment. We formalize problem learning as the identification of
valid and ethical problems in a problem space and introduce several possible
approaches to problem learning. In a broader sense, problem learning is an
approach towards the free will of intelligent machines. Currently, machines are
still limited to solving the problems defined by humans, without the ability or
flexibility to freely explore various possible problems that are even unknown
to humans. Though many machine learning techniques have been developed and
integrated into intelligent systems, they still focus on the means rather than
the purpose in that machines are still solving human defined problems. However,
proposing good problems is sometimes even more important than solving problems,
because a good problem can help to inspire new ideas and gain deeper
understandings. The paper also discusses the ethical implications of problem
learning under the background of Responsible AI.",2022-11-13 16:01:40,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Théophile Champion, Howard Bowman, Marek Grześ",Branching Time Active Inference: empirical study and complexity class analysis,,,,,http://arxiv.org/abs/2111.11276v2,"Active inference is a state-of-the-art framework for modelling the brain that
explains a wide range of mechanisms such as habit formation, dopaminergic
discharge and curiosity. However, recent implementations suffer from an
exponential complexity class when computing the prior over all the possible
policies up to the time horizon. Fountas et al (2020) used Monte Carlo tree
search to address this problem, leading to very good results in two different
tasks. Additionally, Champion et al (2021a) proposed a tree search approach
based on (temporal) structure learning. This was enabled by the development of
a variational message passing approach to active inference, which enables
compositional construction of Bayesian networks for active inference. However,
this message passing tree search approach, which we call branching-time active
inference (BTAI), has never been tested empirically. In this paper, we present
an experimental study of BTAI in the context of a maze solving agent. In this
context, we show that both improved prior preferences and deeper search help
mitigate the vulnerability to local minima. Then, we compare BTAI to standard
active inference (AcI) on a graph navigation task. We show that for small
graphs, both BTAI and AcI successfully solve the task. For larger graphs, AcI
exhibits an exponential (space) complexity class, making the approach
intractable. However, BTAI explores the space of policies more efficiently,
successfully scaling to larger graphs. Then, BTAI was compared to the POMCP
algorithm on the frozen lake environment. The experiments suggest that BTAI and
the POMCP algorithm accumulate a similar amount of reward. Also, we describe
when BTAI receives more rewards than the POMCP agent, and when the opposite is
true. Finally, we compared BTAI to the approach of Fountas et al (2020) on the
dSprites dataset, and we discussed the pros and cons of each approach.",2022-11-13 16:01:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"David Leslie, Christopher Burr, Mhairi Aitken, Michael Katell, Morgan Briggs, Cami Rincon","Human rights, democracy, and the rule of law assurance framework for AI systems: A proposal",,,,10.5281/zenodo.5981676,http://arxiv.org/abs/2202.02776v1,"Following on from the publication of its Feasibility Study in December 2020,
the Council of Europe's Ad Hoc Committee on Artificial Intelligence (CAHAI) and
its subgroups initiated efforts to formulate and draft its Possible Elements of
a Legal Framework on Artificial Intelligence, based on the Council of Europe's
standards on human rights, democracy, and the rule of law. This document was
ultimately adopted by the CAHAI plenary in December 2021. To support this
effort, The Alan Turing Institute undertook a programme of research that
explored the governance processes and practical tools needed to operationalise
the integration of human right due diligence with the assurance of trustworthy
AI innovation practices.
  The resulting framework was completed and submitted to the Council of Europe
in September 2021. It presents an end-to-end approach to the assurance of AI
project lifecycles that integrates context-based risk analysis and appropriate
stakeholder engagement with comprehensive impact assessment, and transparent
risk management, impact mitigation, and innovation assurance practices. Taken
together, these interlocking processes constitute a Human Rights, Democracy and
the Rule of Law Assurance Framework (HUDERAF). The HUDERAF combines the
procedural requirements for principles-based human rights due diligence with
the governance mechanisms needed to set up technical and socio-technical
guardrails for responsible and trustworthy AI innovation practices. Its purpose
is to provide an accessible and user-friendly set of mechanisms for
facilitating compliance with a binding legal framework on artificial
intelligence, based on the Council of Europe's standards on human rights,
democracy, and the rule of law, and to ensure that AI innovation projects are
carried out with appropriate levels of public accountability, transparency, and
democratic governance.",2022-11-13 16:01:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Anu K. Myne, Kevin J. Leahy, Ryan J. Soklaski",Knowledge-Integrated Informed AI for National Security,,,,,http://arxiv.org/abs/2202.03188v1,"The state of artificial intelligence technology has a rich history that dates
back decades and includes two fall-outs before the explosive resurgence of
today, which is credited largely to data-driven techniques. While AI technology
has and continues to become increasingly mainstream with impact across domains
and industries, it's not without several drawbacks, weaknesses, and potential
to cause undesired effects. AI techniques are numerous with many approaches and
variants, but they can be classified simply based on the degree of knowledge
they capture and how much data they require; two broad categories emerge as
prominent across AI to date: (1) techniques that are primarily, and often
solely, data-driven while leveraging little to no knowledge and (2) techniques
that primarily leverage knowledge and depend less on data. Now, a third
category is starting to emerge that leverages both data and knowledge, that
some refer to as ""informed AI."" This third category can be a game changer
within the national security domain where there is ample scientific and
domain-specific knowledge that stands ready to be leveraged, and where purely
data-driven AI can lead to serious unwanted consequences.
  This report shares findings from a thorough exploration of AI approaches that
exploit data as well as principled and/or practical knowledge, which we refer
to as ""knowledge-integrated informed AI."" Specifically, we review illuminating
examples of knowledge integrated in deep learning and reinforcement learning
pipelines, taking note of the performance gains they provide. We also discuss
an apparent trade space across variants of knowledge-integrated informed AI,
along with observed and prominent issues that suggest worthwhile future
research directions. Most importantly, this report suggests how the advantages
of knowledge-integrated informed AI stand to benefit the national security
domain.",2022-11-13 16:01:42,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Sam Clarke, Ben Cottier, Aryeh Englander, Daniel Eth, David Manheim, Samuel Dylan Martin, Issa Rice",Modeling Transformative AI Risks (MTAIR) Project -- Summary Report,,,,,http://arxiv.org/abs/2206.09360v1,"This report outlines work by the Modeling Transformative AI Risk (MTAIR)
project, an attempt to map out the key hypotheses, uncertainties, and
disagreements in debates about catastrophic risks from advanced AI, and the
relationships between them. This builds on an earlier diagram by Ben Cottier
and Rohin Shah which laid out some of the crucial disagreements (""cruxes"")
visually, with some explanation. Based on an extensive literature review and
engagement with experts, the report explains a model of the issues involved,
and the initial software-based implementation that can incorporate probability
estimates or other quantitative factors to enable exploration, planning, and/or
decision support. By gathering information from various debates and discussions
into a single more coherent presentation, we hope to enable better discussions
and debates about the issues involved.
  The model starts with a discussion of reasoning via analogies and general
prior beliefs about artificial intelligence. Following this, it lays out a
model of different paths and enabling technologies for high-level machine
intelligence, and a model of how advances in the capabilities of these systems
might proceed, including debates about self-improvement, discontinuous
improvements, and the possibility of distributed, non-agentic high-level
intelligence or slower improvements. The model also looks specifically at the
question of learned optimization, and whether machine learning systems will
create mesa-optimizers. The impact of different safety research on the previous
sets of questions is then examined, to understand whether and how research
could be useful in enabling safer systems. Finally, we discuss a model of
different failure modes and loss of control or takeover scenarios.",2022-11-13 16:01:42,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Paulo Pirozelli, Ais B. R. Castro, Ana Luiza C. de Oliveira, André S. Oliveira, Flávio N. Cação, Igor C. Silveira, João G. M. Campos, Laura C. Motheo, Leticia F. Figueiredo, Lucas F. A. O. Pellicer, Marcelo A. José, Marcos M. José, Pedro de M. Ligabue, Ricardo S. Grava, Rodrigo M. Tavares, Vinícius B. Matos, Yan V. Sym, Anna H. R. Costa, Anarosa A. F. Brandão, Denis D. Mauá, Fabio G. Cozman, Sarajane M. Peres",The BLue Amazon Brain (BLAB): A Modular Architecture of Services about the Brazilian Maritime Territory,"AI: Modeling Oceans and Climate Change (IJCAI-ECAI), 2022",,,,http://arxiv.org/abs/2209.07928v1,"We describe the first steps in the development of an artificial agent focused
on the Brazilian maritime territory, a large region within the South Atlantic
also known as the Blue Amazon. The ""BLue Amazon Brain"" (BLAB) integrates a
number of services aimed at disseminating information about this region and its
importance, functioning as a tool for environmental awareness. The main service
provided by BLAB is a conversational facility that deals with complex questions
about the Blue Amazon, called BLAB-Chat; its central component is a controller
that manages several task-oriented natural language processing modules (e.g.,
question answering and summarizer systems). These modules have access to an
internal data lake as well as to third-party databases. A news reporter
(BLAB-Reporter) and a purposely-developed wiki (BLAB-Wiki) are also part of the
BLAB service architecture. In this paper, we describe our current version of
BLAB's architecture (interface, backend, web services, NLP modules, and
resources) and comment on the challenges we have faced so far, such as the lack
of training data and the scattered state of domain information. Solving these
issues presents a considerable challenge in the development of artificial
intelligence for technical domains.",2022-11-13 16:01:43,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Lin Guan, Karthik Valmeekam, Subbarao Kambhampati",Relative Behavioral Attributes: Filling the Gap between Symbolic Goal Specification and Reward Learning from Human Preferences,,,,,http://arxiv.org/abs/2210.15906v1,"Generating complex behaviors from goals specified by non-expert users is a
crucial aspect of intelligent agents. Interactive reward learning from
trajectory comparisons is one way to allow non-expert users to convey complex
objectives by expressing preferences over short clips of agent behaviors. Even
though this method can encode complex tacit knowledge present in the underlying
tasks, it implicitly assumes that the human is unable to provide rich-form
feedback other than binary preference labels, leading to extremely high
feedback complexity and poor user experience. While providing a detailed
symbolic specification of the objectives might be tempting, it is not always
feasible even for an expert user. However, in most cases, humans are aware of
how the agent should change its behavior along meaningful axes to fulfill the
underlying purpose, even if they are not able to fully specify task objectives
symbolically. Using this as motivation, we introduce the notion of Relative
Behavioral Attributes, which acts as a middle ground, between exact goal
specification and reward learning purely from preference labels, by enabling
the users to tweak the agent's behavior through nameable concepts (e.g.,
increasing the softness of the movement of a two-legged ""sneaky"" agent). We
propose two different parametric methods that can potentially encode any kind
of behavioral attributes from ordered behavior clips. We demonstrate the
effectiveness of our methods on 4 tasks with 9 different behavioral attributes
and show that once the attributes are learned, end users can effortlessly
produce desirable agent behaviors, by providing feedback just around 10 times.
The feedback complexity of our approach is over 10 times less than the
learning-from-human-preferences baseline and this demonstrates that our
approach is readily applicable in real-world applications.",2022-11-13 16:01:43,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2008,"Amanda Whitbrook, Uwe Aickelin, Jonathan Garibaldi",Idiotypic Immune Networks in Mobile Robot Control,"IEEE Transactions on Systems, Man and Cybernetics, Part B, 37(6),
  1581- 1598, 2007",,,10.1109/TSMCB.2007.907334,http://arxiv.org/abs/0803.2981v1,"Jerne's idiotypic network theory postulates that the immune response involves
inter-antibody stimulation and suppression as well as matching to antigens. The
theory has proved the most popular Artificial Immune System (ais) model for
incorporation into behavior-based robotics but guidelines for implementing
idiotypic selection are scarce. Furthermore, the direct effects of employing
the technique have not been demonstrated in the form of a comparison with
non-idiotypic systems. This paper aims to address these issues. A method for
integrating an idiotypic ais network with a Reinforcement Learning based
control system (rl) is described and the mechanisms underlying antibody
stimulation and suppression are explained in detail. Some hypotheses that
account for the network advantage are put forward and tested using three
systems with increasing idiotypic complexity. The basic rl, a simplified hybrid
ais-rl that implements idiotypic selection independently of derived
concentration levels and a full hybrid ais-rl scheme are examined. The test bed
takes the form of a simulated Pioneer robot that is required to navigate
through maze worlds detecting and tracking door markers.",2022-11-13 16:01:44,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2009,Elena S. Vishnevskaya,Building the information kernel and the problem of recognition,,,,,http://arxiv.org/abs/0903.4513v7,"At this point in time there is a need for a new representation of different
information, to identify and organize descending its characteristics. Today,
science is a powerful tool for the description of reality - the numbers. Why
the most important property of numbers. Suppose we have a number 0.2351734, it
is clear that the figures are there in order of importance. If necessary, we
can round the number up to some value, eg 0.235. Arguably, the 0,235 - the most
important information of 0.2351734. Thus, we can reduce the size of numbers is
not losing much with the accuracy. Clearly, if learning to provide a graphical
or audio information kernel, we can provide the most relevant information,
discarding the rest. Introduction of various kinds of information in an
information kernel, is an important task, to solve many problems in artificial
intelligence and information theory.",2022-11-13 16:01:44,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2010,"Louise A. Dennis, Michael Fisher, Nicholas Lincoln, Alexei Lisitsa, Sandor M. Veres",Agent Based Approaches to Engineering Autonomous Space Software,"EPTCS 20, 2010, pp. 63-67",,,10.4204/EPTCS.20.6,http://arxiv.org/abs/1003.0617v1,"Current approaches to the engineering of space software such as satellite
control systems are based around the development of feedback controllers using
packages such as MatLab's Simulink toolbox. These provide powerful tools for
engineering real time systems that adapt to changes in the environment but are
limited when the controller itself needs to be adapted.
  We are investigating ways in which ideas from temporal logics and agent
programming can be integrated with the use of such control systems to provide a
more powerful layer of autonomous decision making. This paper will discuss our
initial approaches to the engineering of such systems.",2022-11-13 16:01:45,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2010,"David H. Wolpert, Gregory Benford",What does Newcomb's paradox teach us?,,,,,http://arxiv.org/abs/1003.1343v1,"In Newcomb's paradox you choose to receive either the contents of a
particular closed box, or the contents of both that closed box and another one.
Before you choose, a prediction algorithm deduces your choice, and fills the
two boxes based on that deduction. Newcomb's paradox is that game theory
appears to provide two conflicting recommendations for what choice you should
make in this scenario. We analyze Newcomb's paradox using a recent extension of
game theory in which the players set conditional probability distributions in a
Bayes net. We show that the two game theory recommendations in Newcomb's
scenario have different presumptions for what Bayes net relates your choice and
the algorithm's prediction. We resolve the paradox by proving that these two
Bayes nets are incompatible. We also show that the accuracy of the algorithm's
prediction, the focus of much previous work, is irrelevant. In addition we show
that Newcomb's scenario only provides a contradiction between game theory's
expected utility and dominance principles if one is sloppy in specifying the
underlying Bayes net. We also show that Newcomb's paradox is time-reversal
invariant; both the paradox and its resolution are unchanged if the algorithm
makes its `prediction' after you make your choice rather than before.",2022-11-13 16:01:45,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2011,Stuart Armstrong,Anthropic decision theory,,,,,http://arxiv.org/abs/1110.6437v3,"This paper sets out to resolve how agents ought to act in the Sleeping Beauty
problem and various related anthropic (self-locating belief) problems, not
through the calculation of anthropic probabilities, but through finding the
correct decision to make. It creates an anthropic decision theory (ADT) that
decides these problems from a small set of principles. By doing so, it
demonstrates that the attitude of agents with regards to each other (selfish or
altruistic) changes the decisions they reach, and that it is very important to
take this into account. To illustrate ADT, it is then applied to two major
anthropic problems and paradoxes, the Presumptuous Philosopher and Doomsday
problems, thus resolving some issues about the probability of human extinction.",2022-11-13 16:01:46,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2012,"Léon Bottou, Jonas Peters, Joaquin Quiñonero-Candela, Denis X. Charles, D. Max Chickering, Elon Portugaly, Dipankar Ray, Patrice Simard, Ed Snelson",Counterfactual Reasoning and Learning Systems,,,,,http://arxiv.org/abs/1209.2355v5,"This work shows how to leverage causal inference to understand the behavior
of complex learning systems interacting with their environment and predict the
consequences of changes to the system. Such predictions allow both humans and
algorithms to select changes that improve both the short-term and long-term
performance of such systems. This work is illustrated by experiments carried
out on the ad placement system associated with the Bing search engine.",2022-11-13 16:01:46,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2012,"Abhimanu Kumar, Jason Baldridge, Matthew Lease, Joydeep Ghosh",Dating Texts without Explicit Temporal Cues,,,,,http://arxiv.org/abs/1211.2290v1,"This paper tackles temporal resolution of documents, such as determining when
a document is about or when it was written, based only on its text. We apply
techniques from information retrieval that predict dates via language models
over a discretized timeline. Unlike most previous works, we rely {\it solely}
on temporal cues implicit in the text. We consider both document-likelihood and
divergence based techniques and several smoothing methods for both of them. Our
best model predicts the mid-point of individuals' lives with a median of 22 and
mean error of 36 years for Wikipedia biographies from 3800 B.C. to the present
day. We also show that this approach works well when training on such
biographies and predicting dates both for non-biographical Wikipedia pages
about specific years (500 B.C. to 2010 A.D.) and for publication dates of short
stories (1798 to 2008). Together, our work shows that, even in absence of
temporal extraction resources, it is possible to achieve remarkable temporal
locality across a diverse set of texts.",2022-11-13 16:01:47,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2012,"Daniel Faria, Catia Pesquita, Emanuel Santos, Francisco M. Couto, Cosmin Stroe, Isabel F. Cruz",Testing the AgreementMaker System in the Anatomy Task of OAEI 2012,,,,,http://arxiv.org/abs/1212.1625v1,"The AgreementMaker system was the leading system in the anatomy task of the
Ontology Alignment Evaluation Initiative (OAEI) competition in 2011. While
AgreementMaker did not compete in OAEI 2012, here we report on its performance
in the 2012 anatomy task, using the same configurations of AgreementMaker
submitted to OAEI 2011. Additionally, we also test AgreementMaker using an
updated version of the UBERON ontology as a mediating ontology, and otherwise
identical configurations. AgreementMaker achieved an F-measure of 91.8% with
the 2011 configurations, and an F-measure of 92.2% with the updated UBERON
ontology. Thus, AgreementMaker would have been the second best system had it
competed in the anatomy task of OAEI 2012, and only 0.1% below the F-measure of
the best system.",2022-11-13 16:01:47,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2013,"Yoram Moses, Marcia K. Shamo",A Knowledge-based Treatment of Human-Automation Systems,,,,,http://arxiv.org/abs/1307.2191v1,"In a supervisory control system the human agent knowledge of past, current,
and future system behavior is critical for system performance. Being able to
reason about that knowledge in a precise and structured manner is central to
effective system design. In this paper we introduce the application of a
well-established formal approach to reasoning about knowledge to the modeling
and analysis of complex human-automation systems. An intuitive notion of
knowledge in human-automation systems is sketched and then cast as a formal
model. We present a case study in which the approach is used to model and
reason about a classic problem from the human-automation systems literature;
the results of our analysis provide evidence for the validity and value of
reasoning about complex systems in terms of the knowledge of the system agents.
To conclude, we discuss research directions that will extend this approach, and
note several systems in the aviation and human-robot team domains that are of
particular interest.",2022-11-13 16:01:48,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2013,"Christoph Benzmüller, Bruno Woltzenlogel Paleo","Formalization, Mechanization and Automation of Gödel's Proof of God's Existence","Frontiers in Artificial Intelligence and Applications, Volume 263:
  ECAI 2014",,,10.3233/978-1-61499-419-0-93,http://arxiv.org/abs/1308.4526v5,"G\""odel's ontological proof has been analysed for the first-time with an
unprecedent degree of detail and formality with the help of higher-order
theorem provers. The following has been done (and in this order): A detailed
natural deduction proof. A formalization of the axioms, definitions and
theorems in the TPTP THF syntax. Automatic verification of the consistency of
the axioms and definitions with Nitpick. Automatic demonstration of the
theorems with the provers LEO-II and Satallax. A step-by-step formalization
using the Coq proof assistant. A formalization using the Isabelle proof
assistant, where the theorems (and some additional lemmata) have been automated
with Sledgehammer and Metis.",2022-11-13 16:01:48,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2014,"Yaakov Gal, Avi Pfeffer",Networks of Influence Diagrams: A Formalism for Representing Agents' Beliefs and Decision-Making Processes,"Journal Of Artificial Intelligence Research, Volume 33, pages
  109-147, 2008",,,10.1613/jair.2503,http://arxiv.org/abs/1401.3426v1,"This paper presents Networks of Influence Diagrams (NID), a compact, natural
and highly expressive language for reasoning about agents beliefs and
decision-making processes. NIDs are graphical structures in which agents mental
models are represented as nodes in a network; a mental model for an agent may
itself use descriptions of the mental models of other agents. NIDs are
demonstrated by examples, showing how they can be used to describe conflicting
and cyclic belief structures, and certain forms of bounded rationality. In an
opponent modeling domain, NIDs were able to outperform other computational
agents whose strategies were not known in advance. NIDs are equivalent in
representation to Bayesian games but they are more compact and structured than
this formalism. In particular, the equilibrium definition for NIDs makes an
explicit distinction between agents optimal strategies, and how they actually
behave in reality.",2022-11-13 16:01:49,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2014,"Wei Bai, Emmanuel M. Tadjouddine, Yu Guo",Enabling Automatic Certification of Online Auctions,"EPTCS 147, 2014, pp. 123-132",,,10.4204/EPTCS.147.9,http://arxiv.org/abs/1404.0854v1,"We consider the problem of building up trust in a network of online auctions
by software agents. This requires agents to have a deeper understanding of
auction mechanisms and be able to verify desirable properties of a given
mechanism. We have shown how these mechanisms can be formalised as semantic web
services in OWL-S, a good enough expressive machine-readable formalism enabling
software agents, to discover, invoke, and execute a web service. We have also
used abstract interpretation to translate the auction's specifications from
OWL-S, based on description logic, to COQ, based on typed lambda calculus, in
order to enable automatic verification of desirable properties of the auction
by the software agents. For this language translation, we have discussed the
syntactic transformation as well as the semantics connections between both
concrete and abstract domains. This work contributes to the implementation of
the vision of agent-mediated e-commerce systems.",2022-11-13 16:01:50,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2014,"Matthias Englert, Sandra Siebert, Martin Ziegler",Logical Limitations to Machine Ethics with Consequences to Lethal Autonomous Weapons,,,,,http://arxiv.org/abs/1411.2842v1,"Lethal Autonomous Weapons promise to revolutionize warfare -- and raise a
multitude of ethical and legal questions. It has thus been suggested to program
values and principles of conduct (such as the Geneva Conventions) into the
machines' control, thereby rendering them both physically and morally superior
to human combatants.
  We employ mathematical logic and theoretical computer science to explore
fundamental limitations to the moral behaviour of intelligent machines in a
series of ""Gedankenexperiments"": Refining and sharpening variants of the
Trolley Problem leads us to construct an (admittedly artificial but) fully
deterministic situation where a robot is presented with two choices: one
morally clearly preferable over the other -- yet, based on the undecidability
of the Halting problem, it provably cannot decide algorithmically which one.
Our considerations have surprising implications to the question of
responsibility and liability for an autonomous system's actions and lead to
specific technical recommendations.",2022-11-13 16:01:50,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2015,"Krzysztof Chalupka, Pietro Perona, Frederick Eberhardt",Multi-Level Cause-Effect Systems,,,,,http://arxiv.org/abs/1512.07942v1,"We present a domain-general account of causation that applies to settings in
which macro-level causal relations between two systems are of interest, but the
relevant causal features are poorly understood and have to be aggregated from
vast arrays of micro-measurements. Our approach generalizes that of Chalupka et
al. (2015) to the setting in which the macro-level effect is not specified. We
formalize the connection between micro- and macro-variables in such situations
and provide a coherent framework describing causal relations at multiple levels
of analysis. We present an algorithm that discovers macro-variable causes and
effects from micro-level measurements obtained from an experiment. We further
show how to design experiments to discover macro-variables from observational
micro-variable data. Finally, we show that under specific conditions, one can
identify multiple levels of causal structure. Throughout the article, we use a
simulated neuroscience multi-unit recording experiment to illustrate the ideas
and the algorithms.",2022-11-13 16:01:51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2016,"Siddharth Reddy, Igor Labutov, Thorsten Joachims",Latent Skill Embedding for Personalized Lesson Sequence Recommendation,,,,,http://arxiv.org/abs/1602.07029v1,"Students in online courses generate large amounts of data that can be used to
personalize the learning process and improve quality of education. In this
paper, we present the Latent Skill Embedding (LSE), a probabilistic model of
students and educational content that can be used to recommend personalized
sequences of lessons with the goal of helping students prepare for specific
assessments. Akin to collaborative filtering for recommender systems, the
algorithm does not require students or content to be described by features, but
it learns a representation using access traces. We formulate this problem as a
regularized maximum-likelihood embedding of students, lessons, and assessments
from historical student-content interactions. An empirical evaluation on
large-scale data from Knewton, an adaptive learning technology company, shows
that this approach predicts assessment results competitively with benchmark
models and is able to discriminate between lesson sequences that lead to
mastery and failure.",2022-11-13 16:01:51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2016,"Kai Arulkumaran, Nat Dilokthanakul, Murray Shanahan, Anil Anthony Bharath",Classifying Options for Deep Reinforcement Learning,,,,,http://arxiv.org/abs/1604.08153v3,"In this paper we combine one method for hierarchical reinforcement learning -
the options framework - with deep Q-networks (DQNs) through the use of
different ""option heads"" on the policy network, and a supervisory network for
choosing between the different options. We utilise our setup to investigate the
effects of architectural constraints in subtasks with positive and negative
transfer, across a range of network capacities. We empirically show that our
augmented DQN has lower sample complexity when simultaneously learning subtasks
with negative transfer, without degrading performance when learning subtasks
with positive transfer.",2022-11-13 16:01:52,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2016,"Michael Crosscombe, Jonathan Lawry",Exploiting Vagueness for Multi-Agent Consensus,,,,10.1007/978-981-10-2564-8_5,http://arxiv.org/abs/1607.05540v2,"A framework for consensus modelling is introduced using Kleene's three valued
logic as a means to express vagueness in agents' beliefs. Explicitly borderline
cases are inherent to propositions involving vague concepts where sentences of
a propositional language may be absolutely true, absolutely false or
borderline. By exploiting these intermediate truth values, we can allow agents
to adopt a more vague interpretation of underlying concepts in order to weaken
their beliefs and reduce the levels of inconsistency, so as to achieve
consensus. We consider a consensus combination operation which results in
agents adopting the borderline truth value as a shared viewpoint if they are in
direct conflict. Simulation experiments are presented which show that applying
this operator to agents chosen at random (subject to a consistency threshold)
from a population, with initially diverse opinions, results in convergence to a
smaller set of more precise shared beliefs. Furthermore, if the choice of
agents for combination is dependent on the payoff of their beliefs, this acting
as a proxy for performance or usefulness, then the system converges to beliefs
which, on average, have higher payoff.",2022-11-13 16:01:52,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2016,"Ethan Fast, Eric Horvitz",Long-Term Trends in the Public Perception of Artificial Intelligence,,,,,http://arxiv.org/abs/1609.04904v2,"Analyses of text corpora over time can reveal trends in beliefs, interest,
and sentiment about a topic. We focus on views expressed about artificial
intelligence (AI) in the New York Times over a 30-year period. General
interest, awareness, and discussion about AI has waxed and waned since the
field was founded in 1956. We present a set of measures that captures levels of
engagement, measures of pessimism and optimism, the prevalence of specific
hopes and concerns, and topics that are linked to discussions about AI over
decades. We find that discussion of AI has increased sharply since 2009, and
that these discussions have been consistently more optimistic than pessimistic.
However, when we examine specific concerns, we find that worries of loss of
control of AI, ethical concerns for AI, and the negative impact of AI on work
have grown in recent years. We also find that hopes for AI in healthcare and
education have increased over time.",2022-11-13 16:01:53,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2016,"Paolo Izzo, Hongyang Qu, Sandor M. Veres",A stochastically verifiable autonomous control architecture with reasoning,,,,,http://arxiv.org/abs/1611.03372v1,"A new agent architecture called Limited Instruction Set Agent (LISA) is
introduced for autonomous control. The new architecture is based on previous
implementations of AgentSpeak and it is structurally simpler than its
predecessors with the aim of facilitating design-time and run-time verification
methods. The process of abstracting the LISA system to two different types of
discrete probabilistic models (DTMC and MDP) is investigated and illustrated.
The LISA system provides a tool for complete modelling of the agent and the
environment for probabilistic verification. The agent program can be
automatically compiled into a DTMC or a MDP model for verification with Prism.
The automatically generated Prism model can be used for both design-time and
run-time verification. The run-time verification is investigated and
illustrated in the LISA system as an internal modelling mechanism for
prediction of future outcomes.",2022-11-13 16:01:53,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2016,"Ofir Nachum, Mohammad Norouzi, Dale Schuurmans",Improving Policy Gradient by Exploring Under-appreciated Rewards,,,,,http://arxiv.org/abs/1611.09321v3,"This paper presents a novel form of policy gradient for model-free
reinforcement learning (RL) with improved exploration properties. Current
policy-based methods use entropy regularization to encourage undirected
exploration of the reward landscape, which is ineffective in high dimensional
spaces with sparse rewards. We propose a more directed exploration strategy
that promotes exploration of under-appreciated reward regions. An action
sequence is considered under-appreciated if its log-probability under the
current policy under-estimates its resulting reward. The proposed exploration
strategy is easy to implement, requiring small modifications to an
implementation of the REINFORCE algorithm. We evaluate the approach on a set of
algorithmic tasks that have long challenged RL methods. Our approach reduces
hyper-parameter sensitivity and demonstrates significant improvements over
baseline methods. Our algorithm successfully solves a benchmark multi-digit
addition task and generalizes to long sequences. This is, to our knowledge, the
first time that a pure RL method has solved addition using only reward
feedback.",2022-11-13 16:01:53,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,"Andrew Slavin Ross, Michael C. Hughes, Finale Doshi-Velez",Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations,,,,,http://arxiv.org/abs/1703.03717v2,"Neural networks are among the most accurate supervised learning methods in
use today, but their opacity makes them difficult to trust in critical
applications, especially when conditions in training differ from those in test.
Recent work on explanations for black-box models has produced tools (e.g. LIME)
to show the implicit rules behind predictions, which can help us identify when
models are right for the wrong reasons. However, these methods do not scale to
explaining entire datasets and cannot correct the problems they reveal. We
introduce a method for efficiently explaining and regularizing differentiable
models by examining and selectively penalizing their input gradients, which
provide a normal to the decision boundary. We apply these penalties both based
on expert annotation and in an unsupervised fashion that encourages diverse
models with qualitatively different decision boundaries for the same
classification problem. On multiple datasets, we show our approach generates
faithful explanations and models that generalize much better when conditions
differ between training and test.",2022-11-13 16:01:54,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,"Tambet Matiisen, Avital Oliver, Taco Cohen, John Schulman",Teacher-Student Curriculum Learning,,,,,http://arxiv.org/abs/1707.00183v2,"We propose Teacher-Student Curriculum Learning (TSCL), a framework for
automatic curriculum learning, where the Student tries to learn a complex task
and the Teacher automatically chooses subtasks from a given set for the Student
to train on. We describe a family of Teacher algorithms that rely on the
intuition that the Student should practice more those tasks on which it makes
the fastest progress, i.e. where the slope of the learning curve is highest. In
addition, the Teacher algorithms address the problem of forgetting by also
choosing tasks where the Student's performance is getting worse. We demonstrate
that TSCL matches or surpasses the results of carefully hand-crafted curricula
in two tasks: addition of decimal numbers with LSTM and navigation in
Minecraft. Using our automatically generated curriculum enabled to solve a
Minecraft maze that could not be solved at all when training directly on
solving the maze, and the learning was an order of magnitude faster than
uniform sampling of subtasks.",2022-11-13 16:01:55,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,"Mohammed Alshiekh, Roderick Bloem, Ruediger Ehlers, Bettina Könighofer, Scott Niekum, Ufuk Topcu",Safe Reinforcement Learning via Shielding,,,,,http://arxiv.org/abs/1708.08611v2,"Reinforcement learning algorithms discover policies that maximize reward, but
do not necessarily guarantee safety during learning or execution phases. We
introduce a new approach to learn optimal policies while enforcing properties
expressed in temporal logic. To this end, given the temporal logic
specification that is to be obeyed by the learning system, we propose to
synthesize a reactive system called a shield. The shield is introduced in the
traditional learning process in two alternative ways, depending on the location
at which the shield is implemented. In the first one, the shield acts each time
the learning agent is about to make a decision and provides a list of safe
actions. In the second way, the shield is introduced after the learning agent.
The shield monitors the actions from the learner and corrects them only if the
chosen action causes a violation of the specification. We discuss which
requirements a shield must meet to preserve the convergence guarantees of the
learner. Finally, we demonstrate the versatility of our approach on several
challenging reinforcement learning scenarios.",2022-11-13 16:01:55,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,Daniel Estrada,"Value Alignment, Fair Play, and the Rights of Service Robots",ACM/AIES 2018,,,10.1145/3278721.3278730,http://arxiv.org/abs/1803.02852v1,"Ethics and safety research in artificial intelligence is increasingly framed
in terms of ""alignment"" with human values and interests. I argue that Turing's
call for ""fair play for machines"" is an early and often overlooked contribution
to the alignment literature. Turing's appeal to fair play suggests a need to
correct human behavior to accommodate our machines, a surprising inversion of
how value alignment is treated today. Reflections on ""fair play"" motivate a
novel interpretation of Turing's notorious ""imitation game"" as a condition not
of intelligence but instead of value alignment: a machine demonstrates a
minimal degree of alignment (with the norms of conversation, for instance) when
it can go undetected when interrogated by a human. I carefully distinguish this
interpretation from the Moral Turing Test, which is not motivated by a
principle of fair play, but instead depends on imitation of human moral
behavior. Finally, I consider how the framework of fair play can be used to
situate the debate over robot rights within the alignment literature. I argue
that extending rights to service robots operating in public spaces is ""fair"" in
precisely the sense that it encourages an alignment of interests between humans
and machines.",2022-11-13 16:01:56,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,"Jade Shi, Rhiju Das, Vijay S. Pande",SentRNA: Improving computational RNA design by incorporating a prior of human design strategies,,,,,http://arxiv.org/abs/1803.03146v2,"Solving the RNA inverse folding problem is a critical prerequisite to RNA
design, an emerging field in bioengineering with a broad range of applications
from reaction catalysis to cancer therapy. Although significant progress has
been made in developing machine-based inverse RNA folding algorithms, current
approaches still have difficulty designing sequences for large or complex
targets. On the other hand, human players of the online RNA design game EteRNA
have consistently shown superior performance in this regard, being able to
readily design sequences for targets that are challenging for machine
algorithms. Here we present a novel approach to the RNA design problem,
SentRNA, a design agent consisting of a fully-connected neural network trained
end-to-end using human-designed RNA sequences. We show that through this
approach, SentRNA can solve complex targets previously unsolvable by any
machine-based approach and achieve state-of-the-art performance on two separate
challenging test sets. Our results demonstrate that incorporating human design
strategies into a design algorithm can significantly boost machine performance
and suggests a new paradigm for machine-based RNA design.",2022-11-13 16:01:56,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,"Henrik Aslund, El Mahdi El Mhamdi, Rachid Guerraoui, Alexandre Maurer",Virtuously Safe Reinforcement Learning,,,,,http://arxiv.org/abs/1805.11447v1,"We show that when a third party, the adversary, steps into the two-party
setting (agent and operator) of safely interruptible reinforcement learning, a
trade-off has to be made between the probability of following the optimal
policy in the limit, and the probability of escaping a dangerous situation
created by the adversary. So far, the work on safely interruptible agents has
assumed a perfect perception of the agent about its environment (no adversary),
and therefore implicitly set the second probability to zero, by explicitly
seeking a value of one for the first probability. We show that (1) agents can
be made both interruptible and adversary-resilient, and (2) the
interruptibility can be made safe in the sense that the agent itself will not
seek to avoid it. We also solve the problem that arises when the agent does not
go completely greedy, i.e. issues with safe exploration in the limit.
Resilience to perturbed perception, safe exploration in the limit, and safe
interruptibility are the three pillars of what we call \emph{virtuously safe
reinforcement learning}.",2022-11-13 16:01:57,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,"Victoria Krakovna, Laurent Orseau, Ramana Kumar, Miljan Martic, Shane Legg",Penalizing side effects using stepwise relative reachability,,,,,http://arxiv.org/abs/1806.01186v2,"How can we design safe reinforcement learning agents that avoid unnecessary
disruptions to their environment? We show that current approaches to penalizing
side effects can introduce bad incentives, e.g. to prevent any irreversible
changes in the environment, including the actions of other agents. To isolate
the source of such undesirable incentives, we break down side effects penalties
into two components: a baseline state and a measure of deviation from this
baseline state. We argue that some of these incentives arise from the choice of
baseline, and others arise from the choice of deviation measure. We introduce a
new variant of the stepwise inaction baseline and a new deviation measure based
on relative reachability of states. The combination of these design choices
avoids the given undesirable incentives, while simpler baselines and the
unreachability measure fail. We demonstrate this empirically by comparing
different combinations of baseline and deviation measure choices on a set of
gridworld experiments designed to illustrate possible bad incentives.",2022-11-13 16:01:58,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,"Cinjon Resnick, Roberta Raileanu, Sanyam Kapoor, Alexander Peysakhovich, Kyunghyun Cho, Joan Bruna","Backplay: ""Man muss immer umkehren""",,,,,http://arxiv.org/abs/1807.06919v5,"Model-free reinforcement learning (RL) requires a large number of trials to
learn a good policy, especially in environments with sparse rewards. We explore
a method to improve the sample efficiency when we have access to
demonstrations. Our approach, Backplay, uses a single demonstration to
construct a curriculum for a given task. Rather than starting each training
episode in the environment's fixed initial state, we start the agent near the
end of the demonstration and move the starting point backwards during the
course of training until we reach the initial state. Our contributions are that
we analytically characterize the types of environments where Backplay can
improve training speed, demonstrate the effectiveness of Backplay both in large
grid worlds and a complex four player zero-sum game (Pommerman), and show that
Backplay compares favorably to other competitive methods known to improve
sample efficiency. This includes reward shaping, behavioral cloning, and
reverse curriculum generation.",2022-11-13 16:01:58,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,"Jonathan Lacotte, Mohammad Ghavamzadeh, Yinlam Chow, Marco Pavone",Risk-Sensitive Generative Adversarial Imitation Learning,,,,,http://arxiv.org/abs/1808.04468v2,"We study risk-sensitive imitation learning where the agent's goal is to
perform at least as well as the expert in terms of a risk profile. We first
formulate our risk-sensitive imitation learning setting. We consider the
generative adversarial approach to imitation learning (GAIL) and derive an
optimization problem for our formulation, which we call it risk-sensitive GAIL
(RS-GAIL). We then derive two different versions of our RS-GAIL optimization
problem that aim at matching the risk profiles of the agent and the expert
w.r.t. Jensen-Shannon (JS) divergence and Wasserstein distance, and develop
risk-sensitive generative adversarial imitation learning algorithms based on
these optimization problems. We evaluate the performance of our algorithms and
compare them with GAIL and the risk-averse imitation learning (RAIL) algorithms
in two MuJoCo and two OpenAI classical control tasks.",2022-11-13 16:01:59,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,"Christopher Iliffe Sprague, Petter Ögren",Adding Neural Network Controllers to Behavior Trees without Destroying Performance Guarantees,,,,,http://arxiv.org/abs/1809.10283v3,"In this paper, we show how Behavior Trees that have performance guarantees,
in terms of safety and goal convergence, can be extended with components that
were designed using machine learning, without destroying those performance
guarantees.
  Machine learning approaches such as reinforcement learning or learning from
demonstration can be very appealing to AI designers that want efficient and
realistic behaviors in their agents. However, those algorithms seldom provide
guarantees for solving the given task in all different situations while keeping
the agent safe. Instead, such guarantees are often easier to find for manually
designed model-based approaches. In this paper we exploit the modularity of
behavior trees to extend a given design with an efficient, but possibly
unreliable, machine learning component in a way that preserves the guarantees.
The approach is illustrated with an inverted pendulum example.",2022-11-13 16:02:00,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,Andrew Slavin Ross,Training Machine Learning Models by Regularizing their Explanations,,,,,http://arxiv.org/abs/1810.00869v1,"Neural networks are among the most accurate supervised learning methods in
use today. However, their opacity makes them difficult to trust in critical
applications, especially when conditions in training may differ from those in
practice. Recent efforts to develop explanations for neural networks and
machine learning models more generally have produced tools to shed light on the
implicit rules behind predictions. These tools can help us identify when models
are right for the wrong reasons. However, they do not always scale to
explaining predictions for entire datasets, are not always at the right level
of abstraction, and most importantly cannot correct the problems they reveal.
In this thesis, we explore the possibility of training machine learning models
(with a particular focus on neural networks) using explanations themselves. We
consider approaches where models are penalized not only for making incorrect
predictions but also for providing explanations that are either inconsistent
with domain knowledge or overly complex. These methods let us train models
which can not only provide more interpretable rationales for their predictions
but also generalize better when training data is confounded or meaningfully
different from test data (even adversarially so).",2022-11-13 16:02:00,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,"Brett W Israelsen, Nisar R Ahmed, Eric Frew, Dale Lawrence, Brian Argrow",Factorized Machine Self-Confidence for Decision-Making Agents,,,,,http://arxiv.org/abs/1810.06519v2,"Algorithmic assurances from advanced autonomous systems assist human users in
understanding, trusting, and using such systems appropriately. Designing these
systems with the capacity of assessing their own capabilities is one approach
to creating an algorithmic assurance. The idea of `machine self-confidence' is
introduced for autonomous systems. Using a factorization based framework for
self-confidence assessment, one component of self-confidence, called
`solver-quality', is discussed in the context of Markov decision processes for
autonomous systems. Markov decision processes underlie much of the theory of
reinforcement learning, and are commonly used for planning and decision making
under uncertainty in robotics and autonomous systems. A `solver quality' metric
is formally defined in the context of decision making algorithms based on
Markov decision processes. A method for assessing solver quality is then
derived, drawing inspiration from empirical hardness models. Finally, numerical
experiments for an unmanned autonomous vehicle navigation problem under
different solver, parameter, and environment conditions indicate that the
self-confidence metric exhibits the desired properties. Discussion of results,
and avenues for future investigation are included.",2022-11-13 16:02:00,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,"Guansong Lu, Zhiming Zhou, Yuxuan Song, Kan Ren, Yong Yu",Guiding the One-to-one Mapping in CycleGAN via Optimal Transport,,,,,http://arxiv.org/abs/1811.06284v1,"CycleGAN is capable of learning a one-to-one mapping between two data
distributions without paired examples, achieving the task of unsupervised data
translation. However, there is no theoretical guarantee on the property of the
learned one-to-one mapping in CycleGAN. In this paper, we experimentally find
that, under some circumstances, the one-to-one mapping learned by CycleGAN is
just a random one within the large feasible solution space. Based on this
observation, we explore to add extra constraints such that the one-to-one
mapping is controllable and satisfies more properties related to specific
tasks. We propose to solve an optimal transport mapping restrained by a
task-specific cost function that reflects the desired properties, and use the
barycenters of optimal transport mapping to serve as references for CycleGAN.
Our experiments indicate that the proposed algorithm is capable of learning a
one-to-one mapping with the desired properties.",2022-11-13 16:02:01,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,"Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, Shane Legg",Scalable agent alignment via reward modeling: a research direction,,,,,http://arxiv.org/abs/1811.07871v1,"One obstacle to applying reinforcement learning algorithms to real-world
problems is the lack of suitable reward functions. Designing such reward
functions is difficult in part because the user only has an implicit
understanding of the task objective. This gives rise to the agent alignment
problem: how do we create agents that behave in accordance with the user's
intentions? We outline a high-level research direction to solve the agent
alignment problem centered around reward modeling: learning a reward function
from interaction with the user and optimizing the learned reward function with
reinforcement learning. We discuss the key challenges we expect to face when
scaling reward modeling to complex and general domains, concrete approaches to
mitigate these challenges, and ways to establish trust in the resulting agents.",2022-11-13 16:02:01,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Thanh Thi Nguyen, Ngoc Duy Nguyen, Fernando Bello, Saeid Nahavandi",A New Tensioning Method using Deep Reinforcement Learning for Surgical Pattern Cutting,2019 IEEE International Conference on Industrial Technology (ICIT),,,10.1109/ICIT.2019.8755235,http://arxiv.org/abs/1901.03327v1,"Surgeons normally need surgical scissors and tissue grippers to cut through a
deformable surgical tissue. The cutting accuracy depends on the skills to
manipulate these two tools. Such skills are part of basic surgical skills
training as in the Fundamentals of Laparoscopic Surgery. The gripper is used to
pinch a point on the surgical sheet and pull the tissue to a certain direction
to maintain the tension while the scissors cut through a trajectory. As the
surgical materials are deformable, it requires a comprehensive tensioning
policy to yield appropriate tensioning direction at each step of the cutting
process. Automating a tensioning policy for a given cutting trajectory will
support not only the human surgeons but also the surgical robots to improve the
cutting accuracy and reliability. This paper presents a multiple pinch point
approach to modelling an autonomous tensioning planner based on a deep
reinforcement learning algorithm. Experiments on a simulator show that the
proposed method is superior to existing methods in terms of both performance
and robustness.",2022-11-13 16:02:02,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Ismail Akrout, Amal Feriani, Mohamed Akrout",Hacking Google reCAPTCHA v3 using Reinforcement Learning,,,,,http://arxiv.org/abs/1903.01003v3,"We present a Reinforcement Learning (RL) methodology to bypass Google
reCAPTCHA v3. We formulate the problem as a grid world where the agent learns
how to move the mouse and click on the reCAPTCHA button to receive a high
score. We study the performance of the agent when we vary the cell size of the
grid world and show that the performance drops when the agent takes big steps
toward the goal. Finally, we used a divide and conquer strategy to defeat the
reCAPTCHA system for any grid resolution. Our proposed method achieves a
success rate of 97.4% on a 100x100 grid and 96.7% on a 1000x1000 screen
resolution.",2022-11-13 16:02:02,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,Scott H. Hawley,Challenges for an Ontology of Artificial Intelligence,,,,,http://arxiv.org/abs/1903.03171v1,"Of primary importance in formulating a response to the increasing prevalence
and power of artificial intelligence (AI) applications in society are questions
of ontology. Questions such as: What ""are"" these systems? How are they to be
regarded? How does an algorithm come to be regarded as an agent? We discuss
three factors which hinder discussion and obscure attempts to form a clear
ontology of AI: (1) the various and evolving definitions of AI, (2) the
tendency for pre-existing technologies to be assimilated and regarded as
""normal,"" and (3) the tendency of human beings to anthropomorphize. This list
is not intended as exhaustive, nor is it seen to preclude entirely a clear
ontology, however, these challenges are a necessary set of topics for
consideration. Each of these factors is seen to present a 'moving target' for
discussion, which poses a challenge for both technical specialists and
non-practitioners of AI systems development (e.g., philosophers and
theologians) to speak meaningfully given that the corpus of AI structures and
capabilities evolves at a rapid pace. Finally, we present avenues for moving
forward, including opportunities for collaborative synthesis for scholars in
philosophy and science.",2022-11-13 16:02:03,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Yash Goyal, Ziyan Wu, Jan Ernst, Dhruv Batra, Devi Parikh, Stefan Lee",Counterfactual Visual Explanations,,,,,http://arxiv.org/abs/1904.07451v2,"In this work, we develop a technique to produce counterfactual visual
explanations. Given a 'query' image $I$ for which a vision system predicts
class $c$, a counterfactual visual explanation identifies how $I$ could change
such that the system would output a different specified class $c'$. To do this,
we select a 'distractor' image $I'$ that the system predicts as class $c'$ and
identify spatial regions in $I$ and $I'$ such that replacing the identified
region in $I$ with the identified region in $I'$ would push the system towards
classifying $I$ as $c'$. We apply our approach to multiple image classification
datasets generating qualitative results showcasing the interpretability and
discriminativeness of our counterfactual explanations. To explore the
effectiveness of our explanations in teaching humans, we present machine
teaching experiments for the task of fine-grained bird classification. We find
that users trained to distinguish bird species fare better when given access to
counterfactual explanations in addition to training examples.",2022-11-13 16:02:03,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,Mario Gleirscher,Risk Structures: Towards Engineering Risk-aware Autonomous Systems,,,,10.1007/s00165-021-00545-4,http://arxiv.org/abs/1904.10386v1,"Inspired by widely-used techniques of causal modelling in risk, failure, and
accident analysis, this work discusses a compositional framework for risk
modelling. Risk models capture fragments of the space of risky events likely to
occur when operating a machine in a given environment. Moreover, one can build
such models into machines such as autonomous robots, to equip them with the
ability of risk-aware perception, monitoring, decision making, and control.
With the notion of a risk factor as the modelling primitive, the framework
provides several means to construct and shape risk models. Relational and
algebraic properties are investigated and proofs support the validity and
consistency of these properties over the corresponding models. Several examples
throughout the discussion illustrate the applicability of the concepts.
Overall, this work focuses on the qualitative treatment of risk with the
outlook of transferring these results to probabilistic refinements of the
discussed framework.",2022-11-13 16:02:04,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Radoslaw Martin Cichy, Gemma Roig, Alex Andonian, Kshitij Dwivedi, Benjamin Lahner, Alex Lascelles, Yalda Mohsenzadeh, Kandan Ramakrishnan, Aude Oliva",The Algonauts Project: A Platform for Communication between the Sciences of Biological and Artificial Intelligence,,,,,http://arxiv.org/abs/1905.05675v1,"In the last decade, artificial intelligence (AI) models inspired by the brain
have made unprecedented progress in performing real-world perceptual tasks like
object classification and speech recognition. Recently, researchers of natural
intelligence have begun using those AI models to explore how the brain performs
such tasks. These developments suggest that future progress will benefit from
increased interaction between disciplines. Here we introduce the Algonauts
Project as a structured and quantitative communication channel for
interdisciplinary interaction between natural and artificial intelligence
researchers. The project's core is an open challenge with a quantitative
benchmark whose goal is to account for brain data through computational models.
This project has the potential to provide better models of natural intelligence
and to gather findings that advance AI. The 2019 Algonauts Project focuses on
benchmarking computational models predicting human brain activity when people
look at pictures of objects. The 2019 edition of the Algonauts Project is
available online: http://algonauts.csail.mit.edu/.",2022-11-13 16:02:04,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Sebastian Tschiatschek, Ahana Ghosh, Luis Haug, Rati Devidze, Adish Singla",Learner-aware Teaching: Inverse Reinforcement Learning with Preferences and Constraints,,,,,http://arxiv.org/abs/1906.00429v2,"Inverse reinforcement learning (IRL) enables an agent to learn complex
behavior by observing demonstrations from a (near-)optimal policy. The typical
assumption is that the learner's goal is to match the teacher's demonstrated
behavior. In this paper, we consider the setting where the learner has its own
preferences that it additionally takes into consideration. These preferences
can for example capture behavioral biases, mismatched worldviews, or physical
constraints. We study two teaching approaches: learner-agnostic teaching, where
the teacher provides demonstrations from an optimal policy ignoring the
learner's preferences, and learner-aware teaching, where the teacher accounts
for the learner's preferences. We design learner-aware teaching algorithms and
show that significant performance improvements can be achieved over
learner-agnostic teaching.",2022-11-13 16:02:05,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Noel C. F. Codella, Michael Hind, Karthikeyan Natesan Ramamurthy, Murray Campbell, Amit Dhurandhar, Kush R. Varshney, Dennis Wei, Aleksandra Mojsilović",Teaching AI to Explain its Decisions Using Embeddings and Multi-Task Learning,,,,,http://arxiv.org/abs/1906.02299v1,"Using machine learning in high-stakes applications often requires predictions
to be accompanied by explanations comprehensible to the domain user, who has
ultimate responsibility for decisions and outcomes. Recently, a new framework
for providing explanations, called TED, has been proposed to provide meaningful
explanations for predictions. This framework augments training data to include
explanations elicited from domain users, in addition to features and labels.
This approach ensures that explanations for predictions are tailored to the
complexity expectations and domain knowledge of the consumer. In this paper, we
build on this foundational work, by exploring more sophisticated instantiations
of the TED framework and empirically evaluate their effectiveness in two
diverse domains, chemical odor and skin cancer prediction. Results demonstrate
that meaningful explanations can be reliably taught to machine learning
algorithms, and in some cases, improving modeling accuracy.",2022-11-13 16:02:05,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Angela Daly, Thilo Hagendorff, Li Hui, Monique Mann, Vidushi Marda, Ben Wagner, Wei Wang, Saskia Witteborn",Artificial Intelligence Governance and Ethics: Global Perspectives,,,,,http://arxiv.org/abs/1907.03848v1,"Artificial intelligence (AI) is a technology which is increasingly being
utilised in society and the economy worldwide, and its implementation is
planned to become more prevalent in coming years. AI is increasingly being
embedded in our lives, supplementing our pervasive use of digital technologies.
But this is being accompanied by disquiet over problematic and dangerous
implementations of AI, or indeed, even AI itself deciding to do dangerous and
problematic actions, especially in fields such as the military, medicine and
criminal justice. These developments have led to concerns about whether and how
AI systems adhere, and will adhere to ethical standards. These concerns have
stimulated a global conversation on AI ethics, and have resulted in various
actors from different countries and sectors issuing ethics and governance
initiatives and guidelines for AI. Such developments form the basis for our
research in this report, combining our international and interdisciplinary
expertise to give an insight into what is happening in Australia, China,
Europe, India and the US.",2022-11-13 16:02:06,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Joseph Y. Halpern, Rafael Pass","A Conceptually Well-Founded Characterization of Iterated Admissibility Using an ""All I Know"" Operator","EPTCS 297, 2019, pp. 221-232",,,10.4204/EPTCS.297.15,http://arxiv.org/abs/1907.09106v1,"Brandenburger, Friedenberg, and Keisler provide an epistemic characterization
of iterated admissibility (IA), also known as iterated deletion of weakly
dominated strategies, where uncertainty is represented using LPSs
(lexicographic probability sequences). Their characterization holds in a rich
structure called a complete structure, where all types are possible. In earlier
work, we gave a characterization of iterated admissibility using an ""all I
know"" operator, that captures the intuition that ""all the agent knows"" is that
agents satisfy the appropriate rationality assumptions. That characterization
did not need complete structures and used probability structures, not LPSs.
However, that characterization did not deal with Samuelson's conceptual concern
regarding IA, namely, that at higher levels, players do not consider possible
strategies that were used to justify their choice of strategy at lower levels.
In this paper, we give a characterization of IA using the all I know operator
that does deal with Samuelson's concern. However, it uses LPSs. We then show
how to modify the characterization using notions of ""approximate belief"" and
""approximately all I know"" so as to deal with Samuelson's concern while still
working with probability structures.",2022-11-13 16:02:06,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Alexander Serb, Themistoklis Prodromakis",A system of different layers of abstraction for artificial intelligence,,,,,http://arxiv.org/abs/1907.10508v1,"The field of artificial intelligence (AI) represents an enormous endeavour of
humankind that is currently transforming our societies down to their very
foundations. Its task, building truly intelligent systems, is underpinned by a
vast array of subfields ranging from the development of new electronic
components to mathematical formulations of highly abstract and complex
reasoning. This breadth of subfields renders it often difficult to understand
how they all fit together into a bigger picture and hides the multi-faceted,
multi-layered conceptual structure that in a sense can be said to be what AI
truly is. In this perspective we propose a system of five levels/layers of
abstraction that underpin many AI implementations. We further posit that each
layer is subject to a complexity-performance trade-off whilst different layers
are interlocked with one another in a control-complexity trade-off. This
overview provides a conceptual map that can help to identify how and where
innovation should be targeted in order to achieve different levels of
functionality, assure them for safety, optimise performance under various
operating constraints and map the opportunity space for social and economic
exploitation.",2022-11-13 16:02:07,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, Igor Mordatch",Emergent Tool Use From Multi-Agent Autocurricula,,,,,http://arxiv.org/abs/1909.07528v2,"Through multi-agent competition, the simple objective of hide-and-seek, and
standard reinforcement learning algorithms at scale, we find that agents create
a self-supervised autocurriculum inducing multiple distinct rounds of emergent
strategy, many of which require sophisticated tool use and coordination. We
find clear evidence of six emergent phases in agent strategy in our
environment, each of which creates a new pressure for the opposing team to
adapt; for instance, agents learn to build multi-object shelters using moveable
boxes which in turn leads to agents discovering that they can overcome
obstacles using ramps. We further provide evidence that multi-agent competition
may scale better with increasing environment complexity and leads to behavior
that centers around far more human-relevant skills than other self-supervised
reinforcement learning methods such as intrinsic motivation. Finally, we
propose transfer and fine-tuning as a way to quantitatively evaluate targeted
capabilities, and we compare hide-and-seek agents to both intrinsic motivation
and random initialization baselines in a suite of domain-specific intelligence
tests.",2022-11-13 16:02:08,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,F. Richard Yu,From the Internet of Information to the Internet of Intelligence,,,,,http://arxiv.org/abs/1909.08068v1,"In the era of the Internet of information, we have gone through layering,
cross-layer, and cross-system design paradigms. Recently, the ``curse of
modeling"" and ``curse of dimensionality"" of the cross-system design paradigm
have resulted in the popularity of using artificial intelligence (AI) to
optimize the Internet of information. However, many significant research
challenges remain to be addressed for the AI approach, including the lack of
high-quality training data due to privacy and resources constraints in this
data-driven approach. To address these challenges, we need to take a look at
humans' cooperation in a larger time scale. To facilitate cooperation in modern
history, we have built three major technologies: ``grid of transportation"",
``grid of energy"", and ``the Internet of information"". In this paper, we argue
that the next cooperation paradigm could be the ``Internet of intelligence
(Intelligence-Net)"", where intelligence can be easily obtained like energy and
information, enabled by the recent advances in blockchain technology. We
present some recent advances in these areas, and discuss some open issues and
challenges that need to be addressed in the future.",2022-11-13 16:02:08,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Ahana Ghosh, Sebastian Tschiatschek, Hamed Mahdavi, Adish Singla",Towards Deployment of Robust AI Agents for Human-Machine Partnerships,,,,,http://arxiv.org/abs/1910.02330v2,"We study the problem of designing AI agents that can robustly cooperate with
people in human-machine partnerships. Our work is inspired by real-life
scenarios in which an AI agent, e.g., a virtual assistant, has to cooperate
with new users after its deployment. We model this problem via a parametric MDP
framework where the parameters correspond to a user's type and characterize her
behavior. In the test phase, the AI agent has to interact with a user of
unknown type. Our approach to designing a robust AI agent relies on observing
the user's actions to make inferences about the user's type and adapting its
policy to facilitate efficient cooperation. We show that without being
adaptive, an AI agent can end up performing arbitrarily bad in the test phase.
We develop two algorithms for computing policies that automatically adapt to
the user in the test phase. We demonstrate the effectiveness of our approach in
solving a two-agent collaborative task.",2022-11-13 16:02:08,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Louis Kirsch, Sjoerd van Steenkiste, Jürgen Schmidhuber",Improving Generalization in Meta Reinforcement Learning using Learned Objectives,,,,,http://arxiv.org/abs/1910.04098v2,"Biological evolution has distilled the experiences of many learners into the
general learning algorithms of humans. Our novel meta reinforcement learning
algorithm MetaGenRL is inspired by this process. MetaGenRL distills the
experiences of many complex agents to meta-learn a low-complexity neural
objective function that decides how future individuals will learn. Unlike
recent meta-RL algorithms, MetaGenRL can generalize to new environments that
are entirely different from those used for meta-training. In some cases, it
even outperforms human-engineered RL algorithms. MetaGenRL uses off-policy
second-order gradients during meta-training that greatly increase its sample
efficiency.",2022-11-13 16:02:09,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Erdem Bıyık, Malayandi Palan, Nicholas C. Landolfi, Dylan P. Losey, Dorsa Sadigh",Asking Easy Questions: A User-Friendly Approach to Active Reward Learning,,,,,http://arxiv.org/abs/1910.04365v1,"Robots can learn the right reward function by querying a human expert.
Existing approaches attempt to choose questions where the robot is most
uncertain about the human's response; however, they do not consider how easy it
will be for the human to answer! In this paper we explore an information gain
formulation for optimally selecting questions that naturally account for the
human's ability to answer. Our approach identifies questions that optimize the
trade-off between robot and human uncertainty, and determines when these
questions become redundant or costly. Simulations and a user study show our
method not only produces easy questions, but also ultimately results in faster
reward learning.",2022-11-13 16:02:09,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"D. Verma, S. Calo",Using AI/ML to gain situational understanding from passive network observations,,,,,http://arxiv.org/abs/1910.06266v1,"The data available in the network traffic fromany Government building
contains a significant amount ofinformation. An analysis of the traffic can
yield insightsand situational understanding about what is happening inthe
building. However, the use of traditional network packet inspection, either
deep or shallow, is useful for only a limited understanding of the environment,
with applicability limited to some aspects of network and security management.
If weuse AI/ML based techniques to understand the network traffic, we can gain
significant insights which increase our situational awareness of what is
happening in the environment.At IBM, we have created a system which uses a
combination of network domain knowledge and machine learning techniques to
convert network traffic into actionable insights about the on premise
environment. These insights include characterization of the communicating
devices, discovering unauthorized devices that may violate policy requirements,
identifying hidden components and vulnerability points, detecting leakage of
sensitive information, and identifying the presence of people and devices.In
this paper, we will describe the overall design of this system, the major
use-cases that have been identified for it, and the lessons learnt when
deploying this system for some of those use-cases",2022-11-13 16:02:10,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Bairavi Venkatesh, Tosha Shah, Antong Chen, Soheil Ghafurian",Restoration of marker occluded hematoxylin and eosin stained whole slide histology images using generative adversarial networks,,,,,http://arxiv.org/abs/1910.06428v1,"It is common for pathologists to annotate specific regions of the tissue,
such as tumor, directly on the glass slide with markers. Although this practice
was helpful prior to the advent of histology whole slide digitization, it often
occludes important details which are increasingly relevant to immuno-oncology
due to recent advancements in digital pathology imaging techniques. The current
work uses a generative adversarial network with cycle loss to remove these
annotations while still maintaining the underlying structure of the tissue by
solving an image-to-image translation problem. We train our network on up to
300 whole slide images with marker inks and show that 70% of the corrected
image patches are indistinguishable from originally uncontaminated image tissue
to a human expert. This portion increases 97% when we replace the human expert
with a deep residual network. We demonstrated the fidelity of the method to the
original image by calculating the correlation between image gradient
magnitudes. We observed a revival of up to 94,000 nuclei per slide in our
dataset, the majority of which were located on tissue border.",2022-11-13 16:02:10,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Dongge Han, Wendelin Boehmer, Michael Wooldridge, Alex Rogers",Multi-agent Hierarchical Reinforcement Learning with Dynamic Termination,,,,10.1007/978-3-030-29911-8_7,http://arxiv.org/abs/1910.09508v1,"In a multi-agent system, an agent's optimal policy will typically depend on
the policies chosen by others. Therefore, a key issue in multi-agent systems
research is that of predicting the behaviours of others, and responding
promptly to changes in such behaviours. One obvious possibility is for each
agent to broadcast their current intention, for example, the currently executed
option in a hierarchical reinforcement learning framework. However, this
approach results in inflexibility of agents if options have an extended
duration and are dynamic. While adjusting the executed option at each step
improves flexibility from a single-agent perspective, frequent changes in
options can induce inconsistency between an agent's actual behaviour and its
broadcast intention. In order to balance flexibility and predictability, we
propose a dynamic termination Bellman equation that allows the agents to
flexibly terminate their options. We evaluate our model empirically on a set of
multi-agent pursuit and taxi tasks, and show that our agents learn to adapt
flexibly across scenarios that require different termination behaviours.",2022-11-13 16:02:11,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Yuval Heffetz, Roman Vainstein, Gilad Katz, Lior Rokach",DeepLine: AutoML Tool for Pipelines Generation using Deep Reinforcement Learning and Hierarchical Actions Filtering,,,,,http://arxiv.org/abs/1911.00061v1,"Automatic machine learning (AutoML) is an area of research aimed at
automating machine learning (ML) activities that currently require human
experts. One of the most challenging tasks in this field is the automatic
generation of end-to-end ML pipelines: combining multiple types of ML
algorithms into a single architecture used for end-to-end analysis of
previously-unseen data. This task has two challenging aspects: the first is the
need to explore a large search space of algorithms and pipeline architectures.
The second challenge is the computational cost of training and evaluating
multiple pipelines. In this study we present DeepLine, a reinforcement learning
based approach for automatic pipeline generation. Our proposed approach
utilizes an efficient representation of the search space and leverages past
knowledge gained from previously-analyzed datasets to make the problem more
tractable. Additionally, we propose a novel hierarchical-actions algorithm that
serves as a plugin, mediating the environment-agent interaction in deep
reinforcement learning problems. The plugin significantly speeds up the
training process of our model. Evaluation on 56 datasets shows that DeepLine
outperforms state-of-the-art approaches both in accuracy and in computational
cost.",2022-11-13 16:02:11,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Daniel Kasenberg, Antonio Roque, Ravenna Thielstrom, Meia Chita-Tegmark, Matthias Scheutz",Generating Justifications for Norm-Related Agent Decisions,,,,,http://arxiv.org/abs/1911.00226v1,"We present an approach to generating natural language justifications of
decisions derived from norm-based reasoning. Assuming an agent which maximally
satisfies a set of rules specified in an object-oriented temporal logic, the
user can ask factual questions (about the agent's rules, actions, and the
extent to which the agent violated the rules) as well as ""why"" questions that
require the agent comparing actual behavior to counterfactual trajectories with
respect to these rules. To produce natural-sounding explanations, we focus on
the subproblem of producing natural language clauses from statements in a
fragment of temporal logic, and then describe how to embed these clauses into
explanatory sentences. We use a human judgment evaluation on a testbed task to
compare our approach to variants in terms of intelligibility, mental model and
perceived trust.",2022-11-13 16:02:12,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Daniel Furelos-Blanco, Mark Law, Alessandra Russo, Krysia Broda, Anders Jonsson",Induction of Subgoal Automata for Reinforcement Learning,,,,,http://arxiv.org/abs/1911.13152v1,"In this work we present ISA, a novel approach for learning and exploiting
subgoals in reinforcement learning (RL). Our method relies on inducing an
automaton whose transitions are subgoals expressed as propositional formulas
over a set of observable events. A state-of-the-art inductive logic programming
system is used to learn the automaton from observation traces perceived by the
RL agent. The reinforcement learning and automaton learning processes are
interleaved: a new refined automaton is learned whenever the RL agent generates
a trace not recognized by the current automaton. We evaluate ISA in several
gridworld problems and show that it performs similarly to a method for which
automata are given in advance. We also show that the learned automata can be
exploited to speed up convergence through reward shaping and transfer learning
across multiple tasks. Finally, we analyze the running time and the number of
traces that ISA needs to learn an automata, and the impact that the number of
observable events has on the learner's performance.",2022-11-13 16:02:12,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Ross Gruetzemacher, Jess Whittlestone",The Transformative Potential of Artificial Intelligence,,,,,http://arxiv.org/abs/1912.00747v3,"The terms 'human-level artificial intelligence' and 'artificial general
intelligence' are widely used to refer to the possibility of advanced
artificial intelligence (AI) with potentially extreme impacts on society. These
terms are poorly defined and do not necessarily indicate what is most important
with respect to future societal impacts. We suggest that the term
'transformative AI' is a helpful alternative, reflecting the possibility that
advanced AI systems could have very large impacts on society without reaching
human-level cognitive abilities. To be most useful, however, more analysis of
what it means for AI to be 'transformative' is needed. In this paper, we
propose three different levels on which AI might be said to be transformative,
associated with different levels of societal change. We suggest that these
distinctions would improve conversations between policy makers and decision
makers concerning the mid- to long-term impacts of advances in AI. Further, we
feel this would have a positive effect on strategic foresight efforts involving
advanced AI, which we expect to illuminate paths to alternative futures. We
conclude with a discussion of the benefits of our new framework and by
highlighting directions for future work in this area.",2022-11-13 16:02:13,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Ehsan Toreini, Mhairi Aitken, Kovila Coopamootoo, Karen Elliott, Carlos Gonzalez Zelaya, Aad van Moorsel",The relationship between trust in AI and trustworthy machine learning technologies,,,,,http://arxiv.org/abs/1912.00782v2,"To build AI-based systems that users and the public can justifiably trust one
needs to understand how machine learning technologies impact trust put in these
services. To guide technology developments, this paper provides a systematic
approach to relate social science concepts of trust with the technologies used
in AI-based services and products. We conceive trust as discussed in the ABI
(Ability, Benevolence, Integrity) framework and use a recently proposed mapping
of ABI on qualities of technologies. We consider four categories of machine
learning technologies, namely these for Fairness, Explainability, Auditability
and Safety (FEAS) and discuss if and how these possess the required qualities.
Trust can be impacted throughout the life cycle of AI-based systems, and we
introduce the concept of Chain of Trust to discuss technological needs for
trust in different stages of the life cycle. FEAS has obvious relations with
known frameworks and therefore we relate FEAS to a variety of international
Principled AI policy and technology frameworks that have emerged in recent
years.",2022-11-13 16:02:13,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Kevin Lu, Igor Mordatch, Pieter Abbeel",Adaptive Online Planning for Continual Lifelong Learning,,,,,http://arxiv.org/abs/1912.01188v2,"We study learning control in an online reset-free lifelong learning scenario,
where mistakes can compound catastrophically into the future and the underlying
dynamics of the environment may change. Traditional model-free policy learning
methods have achieved successes in difficult tasks due to their broad
flexibility, but struggle in this setting, as they can activate failure modes
early in their lifetimes which are difficult to recover from and face
performance degradation as dynamics change. On the other hand, model-based
planning methods learn and adapt quickly, but require prohibitive levels of
computational resources. We present a new algorithm, Adaptive Online Planning
(AOP), that achieves strong performance in this setting by combining
model-based planning with model-free learning. By approximating the uncertainty
of the model-free components and the planner performance, AOP is able to call
upon more extensive planning only when necessary, leading to reduced
computation times, while still gracefully adapting behaviors in the face of
unpredictable changes in the world -- even when traditional RL fails.",2022-11-13 16:02:14,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Mustafa Mert Çelikok, Tomi Peltola, Pedram Daee, Samuel Kaski",Interactive AI with a Theory of Mind,,,,,http://arxiv.org/abs/1912.05284v1,"Understanding each other is the key to success in collaboration. For humans,
attributing mental states to others, the theory of mind, provides the crucial
advantage. We argue for formulating human--AI interaction as a multi-agent
problem, endowing AI with a computational theory of mind to understand and
anticipate the user. To differentiate the approach from previous work, we
introduce a categorisation of user modelling approaches based on the level of
agency learnt in the interaction. We describe our recent work in using nested
multi-agent modelling to formulate user models for multi-armed bandit based
interactive AI systems, including a proof-of-concept user study.",2022-11-13 16:02:14,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Krishn Bera, Yash Mandilwar, Bapi Raju",Value-of-Information based Arbitration between Model-based and Model-free Control,,,,,http://arxiv.org/abs/1912.05453v1,"There have been numerous attempts in explaining the general learning
behaviours using model-based and model-free methods. While the model-based
control is flexible yet computationally expensive in planning, the model-free
control is quick but inflexible. The model-based control is therefore immune
from reward devaluation and contingency degradation. Multiple arbitration
schemes have been suggested to achieve the data efficiency and computational
efficiency of model-based and model-free control respectively. In this context,
we propose a quantitative 'value of information' based arbitration between both
the controllers in order to establish a general computational framework for
skill learning. The interacting model-based and model-free reinforcement
learning processes are arbitrated using an uncertainty-based value of
information. We further show that our algorithm performs better than Q-learning
as well as Q-learning with experience replay.",2022-11-13 16:02:15,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,Thomas Bartz-Beielstein,Why we need an AI-resilient society,,,,,http://arxiv.org/abs/1912.08786v1,"Artificial intelligence is considered as a key technology. It has a huge
impact on our society. Besides many positive effects, there are also some
negative effects or threats. Some of these threats to society are well-known,
e.g., weapons or killer robots. But there are also threats that are ignored.
These unknown-knowns or blind spots affect privacy, and facilitate manipulation
and mistaken identities. We cannot trust data, audio, video, and identities any
more. Democracies are able to cope with known threats, the known-knowns.
Transforming unknown-knowns to known-knowns is one important cornerstone of
resilient societies. An AI-resilient society is able to transform threats
caused by new AI tecchnologies such as generative adversarial networks.
Resilience can be seen as a positive adaptation of these threats. We propose
three strategies how this adaptation can be achieved: awareness, agreements,
and red flags. This article accompanies the TEDx talk ""Why we urgently need an
AI-resilient society"", see https://youtu.be/f6c2ngp7rqY.",2022-11-13 16:02:15,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"P. M. Krafft, Meg Young, Michael Katell, Karen Huang, Ghislain Bugingo",Defining AI in Policy versus Practice,,,,,http://arxiv.org/abs/1912.11095v1,"Recent concern about harms of information technologies motivate consideration
of regulatory action to forestall or constrain certain developments in the
field of artificial intelligence (AI). However, definitional ambiguity hampers
the possibility of conversation about this urgent topic of public concern.
Legal and regulatory interventions require agreed-upon definitions, but
consensus around a definition of AI has been elusive, especially in policy
conversations. With an eye towards practical working definitions and a broader
understanding of positions on these issues, we survey experts and review
published policy documents to examine researcher and policy-maker conceptions
of AI. We find that while AI researchers favor definitions of AI that emphasize
technical functionality, policy-makers instead use definitions that compare
systems to human thinking and behavior. We point out that definitions adhering
closely to the functionality of AI systems are more inclusive of technologies
in use today, whereas definitions that emphasize human-like capabilities are
most applicable to hypothetical future technologies. As a result of this gap,
ethical and regulatory efforts may overemphasize concern about future
technologies at the expense of pressing issues with existing deployed
technologies.",2022-11-13 16:02:15,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,Haydn Belfield,Activism by the AI Community: Analysing Recent Achievements and Future Prospects,,,,,http://arxiv.org/abs/2001.06528v1,"The artificial intelligence community (AI) has recently engaged in activism
in relation to their employers, other members of the community, and their
governments in order to shape the societal and ethical implications of AI. It
has achieved some notable successes, but prospects for further political
organising and activism are uncertain. We survey activism by the AI community
over the last six years; apply two analytical frameworks drawing upon the
literature on epistemic communities, and worker organising and bargaining; and
explore what they imply for the future prospects of the AI community. Success
thus far has hinged on a coherent shared culture, and high bargaining power due
to the high demand for a limited supply of AI talent. Both are crucial to the
future of AI activism and worthy of sustained attention.",2022-11-13 16:02:16,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Christian Kästner, Eunsuk Kang",Teaching Software Engineering for AI-Enabled Systems,,,,,http://arxiv.org/abs/2001.06691v1,"Software engineers have significant expertise to offer when building
intelligent systems, drawing on decades of experience and methods for building
systems that are scalable, responsive and robust, even when built on unreliable
components. Systems with artificial-intelligence or machine-learning (ML)
components raise new challenges and require careful engineering. We designed a
new course to teach software-engineering skills to students with a background
in ML. We specifically go beyond traditional ML courses that teach modeling
techniques under artificial conditions and focus, in lecture and assignments,
on realism with large and changing datasets, robust and evolvable
infrastructure, and purposeful requirements engineering that considers ethics
and fairness as well. We describe the course and our infrastructure and share
experience and all material from teaching the course for the first time.",2022-11-13 16:02:16,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Martin Lindvall, Jesper Molin",Designing for the Long Tail of Machine Learning,,,,,http://arxiv.org/abs/2001.07455v1,"Recent technical advances has made machine learning (ML) a promising
component to include in end user facing systems. However, user experience (UX)
practitioners face challenges in relating ML to existing user-centered design
processes and how to navigate the possibilities and constraints of this design
space. Drawing on our own experience, we characterize designing within this
space as navigating trade-offs between data gathering, model development and
designing valuable interactions for a given model performance. We suggest that
the theoretical description of how machine learning performance scales with
training data can guide designers in these trade-offs as well as having
implications for prototyping. We exemplify the learning curve's usage by
arguing that a useful pattern is to design an initial system in a bootstrap
phase that aims to exploit the training effect of data collected at increasing
orders of magnitude.",2022-11-13 16:02:17,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Jan Bosch, Ivica Crnkovic, Helena Holmström Olsson",Engineering AI Systems: A Research Agenda,,,,,http://arxiv.org/abs/2001.07522v2,"Artificial intelligence (AI) and machine learning (ML) are increasingly
broadly adopted in industry, However, based on well over a dozen case studies,
we have learned that deploying industry-strength, production quality ML models
in systems proves to be challenging. Companies experience challenges related to
data quality, design methods and processes, performance of models as well as
deployment and compliance. We learned that a new, structured engineering
approach is required to construct and evolve systems that contain ML/DL
components. In this paper, we provide a conceptualization of the typical
evolution patterns that companies experience when employing ML as well as an
overview of the key problems experienced by the companies that we have studied.
The main contribution of the paper is a research agenda for AI engineering that
provides an overview of the key engineering challenges surrounding ML solutions
and an overview of open items that need to be addressed by the research
community at large.",2022-11-13 16:02:17,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Shikha Singh, Deepak Khemani",Subjective Knowledge and Reasoning about Agents in Multi-Agent Systems,,,,,http://arxiv.org/abs/2001.08016v1,"Though a lot of work in multi-agent systems is focused on reasoning about
knowledge and beliefs of artificial agents, an explicit representation and
reasoning about the presence/absence of agents, especially in the scenarios
where agents may be unaware of other agents joining in or going offline in a
multi-agent system, leading to partial knowledge/asymmetric knowledge of the
agents is mostly overlooked by the MAS community. Such scenarios lay the
foundations of cases where an agent can influence other agents' mental states
by (mis)informing them about the presence/absence of collaborators or
adversaries. In this paper, we investigate how Kripke structure-based epistemic
models can be extended to express the above notion based on an agent's
subjective knowledge and we discuss the challenges that come along.",2022-11-13 16:02:18,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Devleena Das, Sonia Chernova",Leveraging Rationales to Improve Human Task Performance,,,,10.1145/3377325.3377512,http://arxiv.org/abs/2002.04202v1,"Machine learning (ML) systems across many application areas are increasingly
demonstrating performance that is beyond that of humans. In response to the
proliferation of such models, the field of Explainable AI (XAI) has sought to
develop techniques that enhance the transparency and interpretability of
machine learning methods. In this work, we consider a question not previously
explored within the XAI and ML communities: Given a computational system whose
performance exceeds that of its human user, can explainable AI capabilities be
leveraged to improve the performance of the human? We study this question in
the context of the game of Chess, for which computational game engines that
surpass the performance of the average player are widely available. We
introduce the Rationale-Generating Algorithm, an automated technique for
generating rationales for utility-based computational methods, which we
evaluate with a multi-day user study against two baselines. The results show
that our approach produces rationales that lead to statistically significant
improvement in human task performance, demonstrating that rationales
automatically generated from an AI's internal task model can be used not only
to explain what the system is doing, but also to instruct the user and
ultimately improve their task performance.",2022-11-13 16:02:18,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Mohammadhosein Hasanbeig, Alessandro Abate, Daniel Kroening",Cautious Reinforcement Learning with Logical Constraints,,,,,http://arxiv.org/abs/2002.12156v2,"This paper presents the concept of an adaptive safe padding that forces
Reinforcement Learning (RL) to synthesise optimal control policies while
ensuring safety during the learning process. Policies are synthesised to
satisfy a goal, expressed as a temporal logic formula, with maximal
probability. Enforcing the RL agent to stay safe during learning might limit
the exploration, however we show that the proposed architecture is able to
automatically handle the trade-off between efficient progress in exploration
(towards goal satisfaction) and ensuring safety. Theoretical guarantees are
available on the optimality of the synthesised policies and on the convergence
of the learning algorithm. Experimental results are provided to showcase the
performance of the proposed method.",2022-11-13 16:02:18,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Abhishek Kumar, Benjamin Finley, Tristan Braud, Sasu Tarkoma, Pan Hui",Marketplace for AI Models,,,,,http://arxiv.org/abs/2003.01593v1,"Artificial intelligence shows promise for solving many practical societal
problems in areas such as healthcare and transportation. However, the current
mechanisms for AI model diffusion such as Github code repositories, academic
project webpages, and commercial AI marketplaces have some limitations; for
example, a lack of monetization methods, model traceability, and model
auditabilty. In this work, we sketch guidelines for a new AI diffusion method
based on a decentralized online marketplace. We consider the technical,
economic, and regulatory aspects of such a marketplace including a discussion
of solutions for problems in these areas. Finally, we include a comparative
analysis of several current AI marketplaces that are already available or in
development. We find that most of these marketplaces are centralized commercial
marketplaces with relatively few models.",2022-11-13 16:02:19,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Gabriel Lima, Meeyoung Cha, Chihyung Jeon, Kyungsin Park",The Conflict Between People's Urge to Punish AI and Legal Systems,,,,10.3389/frobt.2021.756242,http://arxiv.org/abs/2003.06507v3,"Regulating artificial intelligence (AI) has become necessary in light of its
deployment in high-risk scenarios. This paper explores the proposal to extend
legal personhood to AI and robots, which had not yet been examined through the
lens of the general public. We present two studies (N = 3,559) to obtain
people's views of electronic legal personhood vis-\`a-vis existing liability
models. Our study reveals people's desire to punish automated agents even
though these entities are not recognized any mental state. Furthermore, people
did not believe automated agents' punishment would fulfill deterrence nor
retribution and were unwilling to grant them legal punishment preconditions,
namely physical independence and assets. Collectively, these findings suggest a
conflict between the desire to punish automated agents and its perceived
impracticability. We conclude by discussing how future design and legal
decisions may influence how the public reacts to automated agents' wrongdoings.",2022-11-13 16:02:20,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,Spyridon Samothrakis,Open Loop In Natura Economic Planning,,,,,http://arxiv.org/abs/2005.01539v2,"The debate between the optimal way of allocating societal surplus (i.e.
products and services) has been raging, in one form or another, practically
forever; following the collapse of the Soviet Union in 1991, the market became
the only legitimate form of organisation -- there was no other alternative.
Working within the tradition of Marx, Leontief, Kantorovich, Beer and
Cockshott, we propose what we deem an automated planning system that aims to
operate on unit level (e.g., factories and citizens), rather than on aggregate
demand and sectors. We explain why it is both a viable and desirable
alternative to current market conditions and position our solution within
current societal structures. Our experiments show that it would be trivial to
plan for up to 50K industrial goods and 5K final goods in commodity hardware.",2022-11-13 16:02:20,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"J. K. Terry, Nathaniel Grammel",Multi-Agent Informational Learning Processes,,,,,http://arxiv.org/abs/2006.06870v4,"We introduce a new mathematical model of multi-agent reinforcement learning,
the Multi-Agent Informational Learning Processor ""MAILP"" model. The model is
based on the notion that agents have policies for a certain amount of
information, models how this information iteratively evolves and propagates
through many agents. This model is very general, and the only meaningful
assumption made is that learning for individual agents progressively slows over
time.",2022-11-13 16:02:21,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Mirka Snyder Caron, Abhishek Gupta",The Social Contract for AI,,,,,http://arxiv.org/abs/2006.08140v1,"Like any technology, AI systems come with inherent risks and potential
benefits. It comes with potential disruption of established norms and methods
of work, societal impacts and externalities. One may think of the adoption of
technology as a form of social contract, which may evolve or fluctuate in time,
scale, and impact. It is important to keep in mind that for AI, meeting the
expectations of this social contract is critical, because recklessly driving
the adoption and implementation of unsafe, irresponsible, or unethical AI
systems may trigger serious backlash against industry and academia involved
which could take decades to resolve, if not actually seriously harm society.
For the purpose of this paper, we consider that a social contract arises when
there is sufficient consensus within society to adopt and implement this new
technology. As such, to enable a social contract to arise for the adoption and
implementation of AI, developing: 1) A socially accepted purpose, through 2) A
safe and responsible method, with 3) A socially aware level of risk involved,
for 4) A socially beneficial outcome, is key.",2022-11-13 16:02:22,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Nathan Fulton, Nathan Hunt, Nghia Hoang, Subhro Das",Formal Verification of End-to-End Learning in Cyber-Physical Systems: Progress and Challenges,,,,,http://arxiv.org/abs/2006.09181v1,"Autonomous systems -- such as self-driving cars, autonomous drones, and
automated trains -- must come with strong safety guarantees. Over the past
decade, techniques based on formal methods have enjoyed some success in
providing strong correctness guarantees for large software systems including
operating system kernels, cryptographic protocols, and control software for
drones. These successes suggest it might be possible to ensure the safety of
autonomous systems by constructing formal, computer-checked correctness proofs.
This paper identifies three assumptions underlying existing formal verification
techniques, explains how each of these assumptions limits the applicability of
verification in autonomous systems, and summarizes preliminary work toward
improving the strength of evidence provided by formal verification.",2022-11-13 16:02:22,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Alexander I. Cowen-Rivers, Daniel Palenicek, Vincent Moens, Mohammed Abdullah, Aivar Sootla, Jun Wang, Haitham Ammar",SAMBA: Safe Model-Based & Active Reinforcement Learning,,,,,http://arxiv.org/abs/2006.09436v1,"In this paper, we propose SAMBA, a novel framework for safe reinforcement
learning that combines aspects from probabilistic modelling, information
theory, and statistics. Our method builds upon PILCO to enable active
exploration using novel(semi-)metrics for out-of-sample Gaussian process
evaluation optimised through a multi-objective problem that supports
conditional-value-at-risk constraints. We evaluate our algorithm on a variety
of safe dynamical system benchmarks involving both low and high-dimensional
state representations. Our results show orders of magnitude reductions in
samples and violations compared to state-of-the-art methods. Lastly, we provide
intuition as to the effectiveness of the framework by a detailed analysis of
our active metrics and safety constraints.",2022-11-13 16:02:23,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Hossein Hajipour, Mateusz Malinowski, Mario Fritz",IReEn: Reverse-Engineering of Black-Box Functions via Iterative Neural Program Synthesis,,,,,http://arxiv.org/abs/2006.10720v2,"In this work, we investigate the problem of revealing the functionality of a
black-box agent. Notably, we are interested in the interpretable and formal
description of the behavior of such an agent. Ideally, this description would
take the form of a program written in a high-level language. This task is also
known as reverse engineering and plays a pivotal role in software engineering,
computer security, but also most recently in interpretability. In contrast to
prior work, we do not rely on privileged information on the black box, but
rather investigate the problem under a weaker assumption of having only access
to inputs and outputs of the program. We approach this problem by iteratively
refining a candidate set using a generative neural program synthesis approach
until we arrive at a functionally equivalent program. We assess the performance
of our approach on the Karel dataset. Our results show that the proposed
approach outperforms the state-of-the-art on this challenge by finding an
approximately functional equivalent program in 78% of cases -- even exceeding
prior work that had privileged information on the black-box.",2022-11-13 16:02:24,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Deepak Gopinath, Mahdieh Nejati Javaremi, Brenna D. Argall",Customized Handling of Unintended Interface Operation in Assistive Robots,,,,,http://arxiv.org/abs/2007.02092v2,"We present an assistance system that reasons about a human's intended actions
during robot teleoperation in order to provide appropriate corrections for
unintended behavior. We model the human's physical interaction with a control
interface during robot teleoperation and distinguish between intended and
measured physical actions explicitly. By reasoning over the unobserved
intentions using model-based inference techniques, our assistive system
provides customized corrections on a user's issued commands. We validate our
algorithm with a 10-person human subject study in which we evaluate the
performance of the proposed assistance paradigms. Our results show that the
assistance paradigms helped to significantly reduce task completion time,
number of mode switches, cognitive workload, and user frustration and improve
overall user satisfaction.",2022-11-13 16:02:25,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Alvaro Ovalle, Simon M. Lucas",Modulation of viability signals for self-regulatory control,,,,,http://arxiv.org/abs/2007.09297v2,"We revisit the role of instrumental value as a driver of adaptive behavior.
In active inference, instrumental or extrinsic value is quantified by the
information-theoretic surprisal of a set of observations measuring the extent
to which those observations conform to prior beliefs or preferences. That is,
an agent is expected to seek the type of evidence that is consistent with its
own model of the world. For reinforcement learning tasks, the distribution of
preferences replaces the notion of reward. We explore a scenario in which the
agent learns this distribution in a self-supervised manner. In particular, we
highlight the distinction between observations induced by the environment and
those pertaining more directly to the continuity of an agent in time. We
evaluate our methodology in a dynamic environment with discrete time and
actions. First with a surprisal minimizing model-free agent (in the RL sense)
and then expanding to the model-based case to minimize the expected free
energy.",2022-11-13 16:02:25,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Gabriel Paludo Licks, Felipe Meneguzzi",Automated Database Indexing using Model-free Reinforcement Learning,,,,,http://arxiv.org/abs/2007.14244v1,"Configuring databases for efficient querying is a complex task, often carried
out by a database administrator. Solving the problem of building indexes that
truly optimize database access requires a substantial amount of database and
domain knowledge, the lack of which often results in wasted space and memory
for irrelevant indexes, possibly jeopardizing database performance for querying
and certainly degrading performance for updating. We develop an architecture to
solve the problem of automatically indexing a database by using reinforcement
learning to optimize queries by indexing data throughout the lifetime of a
database. In our experimental evaluation, our architecture shows superior
performance compared to related work on reinforcement learning and genetic
algorithms, maintaining near-optimal index configurations and efficiently
scaling to large databases.",2022-11-13 16:02:26,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Chidiebere Onyedinma, Patrick Gavigan, Babak Esfandiari",Toward Campus Mail Delivery Using BDI,"EPTCS 319, 2020, pp. 127-143",,,10.4204/EPTCS.319.10,http://arxiv.org/abs/2007.16089v1,"Autonomous systems developed with the Belief-Desire-Intention (BDI)
architecture are usually mostly implemented in simulated environments. In this
project we sought to build a BDI agent for use in the real world for campus
mail delivery in the tunnel system at Carleton University. Ideally, the robot
should receive a delivery order via a mobile application, pick up the mail at a
station, navigate the tunnels to the destination station, and notify the
recipient.
  We linked the Robot Operating System (ROS) with a BDI reasoning system to
achieve a subset of the required use cases. ROS handles the low-level sensing
and actuation, while the BDI reasoning system handles the high-level reasoning
and decision making. Sensory data is orchestrated and sent from ROS to the
reasoning system as perceptions. These perceptions are then deliberated upon,
and an action string is sent back to ROS for interpretation and driving of the
necessary actuator for the action to be performed.
  In this paper we present our current implementation, which closes the loop on
the hardware-software integration, and implements a subset of the use cases
required for the full system.",2022-11-13 16:02:26,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Gabriel Lima, Changyeon Kim, Seungho Ryu, Chihyung Jeon, Meeyoung Cha",Collecting the Public Perception of AI and Robot Rights,,,,,http://arxiv.org/abs/2008.01339v1,"Whether to give rights to artificial intelligence (AI) and robots has been a
sensitive topic since the European Parliament proposed advanced robots could be
granted ""electronic personalities."" Numerous scholars who favor or disfavor its
feasibility have participated in the debate. This paper presents an experiment
(N=1270) that 1) collects online users' first impressions of 11 possible rights
that could be granted to autonomous electronic agents of the future and 2)
examines whether debunking common misconceptions on the proposal modifies one's
stance toward the issue. The results indicate that even though online users
mainly disfavor AI and robot rights, they are supportive of protecting
electronic agents from cruelty (i.e., favor the right against cruel treatment).
Furthermore, people's perceptions became more positive when given information
about rights-bearing non-human entities or myth-refuting statements. The style
used to introduce AI and robot rights significantly affected how the
participants perceived the proposal, similar to the way metaphors function in
creating laws. For robustness, we repeated the experiment over a more
representative sample of U.S. residents (N=164) and found that perceptions
gathered from online users and those by the general population are similar.",2022-11-13 16:02:27,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Alexandra Luccioni, Joseph Bullock, Katherine Hoffmann Pham, Cynthia Sin Nga Lam, Miguel Luengo-Oroz","Considerations, Good Practices, Risks and Pitfalls in Developing AI Solutions Against COVID-19","Harvard CRCS Workshop on AI for Social Good, United States, 2020",,,,http://arxiv.org/abs/2008.09043v1,"The COVID-19 pandemic has been a major challenge to humanity, with 12.7
million confirmed cases as of July 13th, 2020 [1]. In previous work, we
described how Artificial Intelligence can be used to tackle the pandemic with
applications at the molecular, clinical, and societal scales [2]. In the
present follow-up article, we review these three research directions, and
assess the level of maturity and feasibility of the approaches used, as well as
their potential for operationalization. We also summarize some commonly
encountered risks and practical pitfalls, as well as guidelines and best
practices for formulating and deploying AI applications at different scales.",2022-11-13 16:02:27,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Kishor Jothimurugan, Rajeev Alur, Osbert Bastani",A Composable Specification Language for Reinforcement Learning Tasks,"In Advances in Neural Information Processing Systems, pp.
  13041-13051. 2019",,,,http://arxiv.org/abs/2008.09293v2,"Reinforcement learning is a promising approach for learning control policies
for robot tasks. However, specifying complex tasks (e.g., with multiple
objectives and safety constraints) can be challenging, since the user must
design a reward function that encodes the entire task. Furthermore, the user
often needs to manually shape the reward to ensure convergence of the learning
algorithm. We propose a language for specifying complex control tasks, along
with an algorithm that compiles specifications in our language into a reward
function and automatically performs reward shaping. We implement our approach
in a tool called SPECTRL, and show that it outperforms several state-of-the-art
baselines.",2022-11-13 16:02:28,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Sandhya Saisubramanian, Shlomo Zilberstein, Ece Kamar",Avoiding Negative Side Effects due to Incomplete Knowledge of AI Systems,,,,,http://arxiv.org/abs/2008.12146v3,"Autonomous agents acting in the real-world often operate based on models that
ignore certain aspects of the environment. The incompleteness of any given
model -- handcrafted or machine acquired -- is inevitable due to practical
limitations of any modeling technique for complex real-world settings. Due to
the limited fidelity of its model, an agent's actions may have unexpected,
undesirable consequences during execution. Learning to recognize and avoid such
negative side effects of an agent's actions is critical to improve the safety
and reliability of autonomous systems. Mitigating negative side effects is an
emerging research topic that is attracting increased attention due to the rapid
growth in the deployment of AI systems and their broad societal impacts. This
article provides a comprehensive overview of different forms of negative side
effects and the recent research efforts to address them. We identify key
characteristics of negative side effects, highlight the challenges in avoiding
negative side effects, and discuss recently developed approaches, contrasting
their benefits and limitations. The article concludes with a discussion of open
questions and suggestions for future research directions.",2022-11-13 16:02:29,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Guy Clarke Marshall, André Freitas, Caroline Jay",How Researchers Use Diagrams in Communicating Neural Network Systems,,,,,http://arxiv.org/abs/2008.12566v2,"Neural networks are a prevalent and effective machine learning component, and
their application is leading to significant scientific progress in many
domains. As the field of neural network systems is fast growing, it is
important to understand how advances are communicated. Diagrams are key to
this, appearing in almost all papers describing novel systems. This paper
reports on a study into the use of neural network system diagrams, through
interviews, card sorting, and qualitative feedback structured around
ecologically-derived examples. We find high diversity of usage, perception and
preference in both creation and interpretation of diagrams, examining this in
the context of existing design, information visualisation, and user experience
guidelines. Considering the interview data alongside existing guidance, we
propose guidelines aiming to improve the way in which neural network system
diagrams are constructed.",2022-11-13 16:02:29,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Alexandre Galashov, Jakub Sygnowski, Guillaume Desjardins, Jan Humplik, Leonard Hasenclever, Rae Jeong, Yee Whye Teh, Nicolas Heess",Importance Weighted Policy Learning and Adaptation,,,,,http://arxiv.org/abs/2009.04875v2,"The ability to exploit prior experience to solve novel problems rapidly is a
hallmark of biological learning systems and of great practical importance for
artificial ones. In the meta reinforcement learning literature much recent work
has focused on the problem of optimizing the learning process itself. In this
paper we study a complementary approach which is conceptually simple, general,
modular and built on top of recent improvements in off-policy learning. The
framework is inspired by ideas from the probabilistic inference literature and
combines robust off-policy learning with a behavior prior, or default behavior
that constrains the space of solutions and serves as a bias for exploration; as
well as a representation for the value function, both of which are easily
learned from a number of training tasks in a multi-task scenario. Our approach
achieves competitive adaptation performance on hold-out tasks compared to meta
reinforcement learning baselines and can scale to complex sparse-reward
scenarios.",2022-11-13 16:02:30,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Pallavi Bagga, Nicola Paoletti, Kostas Stathis",Learnable Strategies for Bilateral Agent Negotiation over Multiple Issues,,,,,http://arxiv.org/abs/2009.08302v2,"We present a novel bilateral negotiation model that allows a self-interested
agent to learn how to negotiate over multiple issues in the presence of user
preference uncertainty. The model relies upon interpretable strategy templates
representing the tactics the agent should employ during the negotiation and
learns template parameters to maximize the average utility received over
multiple negotiations, thus resulting in optimal bid acceptance and generation.
Our model also uses deep reinforcement learning to evaluate threshold utility
values, for those tactics that require them, thereby deriving optimal utilities
for every environment state. To handle user preference uncertainty, the model
relies on a stochastic search to find user model that best agrees with a given
partial preference profile. Multi-objective optimization and multi-criteria
decision-making methods are applied at negotiation time to generate
Pareto-optimal outcomes thereby increasing the number of successful (win-win)
negotiations. Rigorous experimental evaluations show that the agent employing
our model outperforms the winning agents of the 10th Automated Negotiating
Agents Competition (ANAC'19) in terms of individual as well as social-welfare
utilities.",2022-11-13 16:02:30,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,Johannes Schneider,Humans learn too: Better Human-AI Interaction using Optimized Human Inputs,,,,,http://arxiv.org/abs/2009.09266v1,"Humans rely more and more on systems with AI components. The AI community
typically treats human inputs as a given and optimizes AI models only. This
thinking is one-sided and it neglects the fact that humans can learn, too. In
this work, human inputs are optimized for better interaction with an AI model
while keeping the model fixed. The optimized inputs are accompanied by
instructions on how to create them. They allow humans to save time and cut on
errors, while keeping required changes to original inputs limited. We propose
continuous and discrete optimization methods modifying samples in an iterative
fashion. Our quantitative and qualitative evaluation including a human study on
different hand-generated inputs shows that the generated proposals lead to
lower error rates, require less effort to create and differ only modestly from
the original samples.",2022-11-13 16:02:31,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,U. Kerzel,Enterprise AI Canvas -- Integrating Artificial Intelligence into Business,,,,10.1080/08839514.2020.1826146,http://arxiv.org/abs/2009.11190v1,"Artificial Intelligence (AI) and Machine Learning have enormous potential to
transform businesses and disrupt entire industry sectors. However, companies
wishing to integrate algorithmic decisions into their face multiple challenges:
They have to identify use-cases in which artificial intelligence can create
value, as well as decisions that can be supported or executed automatically.
Furthermore, the organization will need to be transformed to be able to
integrate AI based systems into their human work-force. Furthermore, the more
technical aspects of the underlying machine learning model have to be discussed
in terms of how they impact the various units of a business: Where do the
relevant data come from, which constraints have to be considered, how is the
quality of the data and the prediction evaluated?
  The Enterprise AI canvas is designed to bring Data Scientist and business
expert together to discuss and define all relevant aspects which need to be
clarified in order to integrate AI based systems into a digital enterprise. It
consists of two parts where part one focuses on the business view and
organizational aspects, whereas part two focuses on the underlying machine
learning model and the data it uses.",2022-11-13 16:02:32,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Kai-En Yang, Chia-Yu Tsai, Hung-Hao Shen, Chen-Feng Chiang, Feng-Ming Tsai, Chung-An Wang, Yiju Ting, Chia-Shun Yeh, Chin-Tang Lai",Trust-Region Method with Deep Reinforcement Learning in Analog Design Space Exploration,,,,10.1109/DAC18074.2021.9586087,http://arxiv.org/abs/2009.13772v4,"This paper introduces new perspectives on analog design space search. To
minimize the time-to-market, this endeavor better cast as constraint
satisfaction problem than global optimization defined in prior arts. We
incorporate model-based agents, contrasted with model-free learning, to
implement a trust-region strategy. As such, simple feed-forward networks can be
trained with supervised learning, where the convergence is relatively trivial.
Experiment results demonstrate orders of magnitude improvement on search
iterations. Additionally, the unprecedented consideration of PVT conditions are
accommodated. On circuits with TSMC 5/6nm process, our method achieve
performance surpassing human designers. Furthermore, this framework is in
production in industrial settings.",2022-11-13 16:02:32,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Xusen Yin, Ralph Weischedel, Jonathan May",Learning to Generalize for Sequential Decision Making,,,,,http://arxiv.org/abs/2010.02229v1,"We consider problems of making sequences of decisions to accomplish tasks,
interacting via the medium of language. These problems are often tackled with
reinforcement learning approaches. We find that these models do not generalize
well when applied to novel task domains. However, the large amount of
computation necessary to adequately train and explore the search space of
sequential decision making, under a reinforcement learning paradigm, precludes
the inclusion of large contextualized language models, which might otherwise
enable the desired generalization ability. We introduce a teacher-student
imitation learning methodology and a means of converting a reinforcement
learning model into a natural language understanding model. Together, these
methodologies enable the introduction of contextualized language models into
the sequential decision making problem space. We show that models can learn
faster and generalize more, leveraging both the imitation learning and the
reformulation. Our models exceed teacher performance on various held-out
decision problems, by up to 7% on in-domain problems and 24% on out-of-domain
problems.",2022-11-13 16:02:33,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Daniel Nemirovsky, Nicolas Thiebaut, Ye Xu, Abhishek Gupta",Providing Actionable Feedback in Hiring Marketplaces using Generative Adversarial Networks,,,,,http://arxiv.org/abs/2010.02419v1,"Machine learning predictors have been increasingly applied in production
settings, including in one of the world's largest hiring platforms, Hired, to
provide a better candidate and recruiter experience. The ability to provide
actionable feedback is desirable for candidates to improve their chances of
achieving success in the marketplace. Until recently, however, methods aimed at
providing actionable feedback have been limited in terms of realism and
latency. In this work, we demonstrate how, by applying a newly introduced
method based on Generative Adversarial Networks (GANs), we are able to overcome
these limitations and provide actionable feedback in real-time to candidates in
production settings. Our experimental results highlight the significant
benefits of utilizing a GAN-based approach on our dataset relative to two other
state-of-the-art approaches (including over 1000x latency gains). We also
illustrate the potential impact of this approach in detail on two real
candidate profile examples.",2022-11-13 16:02:33,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Abdulmajid Murad, Frank Alexander Kraemer, Kerstin Bach, Gavin Taylor",Information-Driven Adaptive Sensing Based on Deep Reinforcement Learning,"10th International Conference on the Internet of Things (IoT20),
  October 6-9, 2020, Malmo, Sweden",,,10.1145/3410992.3411001,http://arxiv.org/abs/2010.04112v1,"In order to make better use of deep reinforcement learning in the creation of
sensing policies for resource-constrained IoT devices, we present and study a
novel reward function based on the Fisher information value. This reward
function enables IoT sensor devices to learn to spend available energy on
measurements at otherwise unpredictable moments, while conserving energy at
times when measurements would provide little new information. This is a highly
general approach, which allows for a wide range of use cases without
significant human design effort or hyper-parameter tuning. We illustrate the
approach in a scenario of workplace noise monitoring, where results show that
the learned behavior outperforms a uniform sampling strategy and comes close to
a near-optimal oracle solution.",2022-11-13 16:02:34,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Alessandro Giuseppi, Antonio Pietrabissa",Chance-Constrained Control with Lexicographic Deep Reinforcement Learning,"IEEE Control Systems Letters, vol. 4, no. 3, pp. 755-760, July
  2020",,,10.1109/LCSYS.2020.2979635,http://arxiv.org/abs/2010.09468v1,"This paper proposes a lexicographic Deep Reinforcement Learning
(DeepRL)-based approach to chance-constrained Markov Decision Processes, in
which the controller seeks to ensure that the probability of satisfying the
constraint is above a given threshold. Standard DeepRL approaches require i)
the constraints to be included as additional weighted terms in the cost
function, in a multi-objective fashion, and ii) the tuning of the introduced
weights during the training phase of the Deep Neural Network (DNN) according to
the probability thresholds. The proposed approach, instead, requires to
separately train one constraint-free DNN and one DNN associated to each
constraint and then, at each time-step, to select which DNN to use depending on
the system observed state. The presented solution does not require any
hyper-parameter tuning besides the standard DNN ones, even if the probability
thresholds changes. A lexicographic version of the well-known DeepRL algorithm
DQN is also proposed and validated via simulations.",2022-11-13 16:02:34,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Niya Stoimenova, Rebecca Price",Exploring the Nuances of Designing (with/for) Artificial Intelligence,"Design Issues, 36(4), 45-55 (2020)",,,10.1162/desi_a_00613,http://arxiv.org/abs/2010.15578v1,"Solutions relying on artificial intelligence are devised to predict data
patterns and answer questions that are clearly defined, involve an enumerable
set of solutions, clear rules, and inherently binary decision mechanisms. Yet,
as they become exponentially implemented in our daily activities, they begin to
transcend these initial boundaries and to affect the larger sociotechnical
system in which they are situated. In this arrangement, a solution is under
pressure to surpass true or false criteria and move to an ethical evaluation of
right and wrong. Neither algorithmic solutions, nor purely humanistic ones will
be enough to fully mitigate undesirable outcomes in the narrow state of AI or
its future incarnations. We must take a holistic view. In this paper we explore
the construct of infrastructure as a means to simultaneously address
algorithmic and societal issues when designing AI.",2022-11-13 16:02:35,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Paul Schwerdtner, Florens Greßner, Nikhil Kapoor, Felix Assion, René Sass, Wiebke Günther, Fabian Hüger, Peter Schlicht",Risk Assessment for Machine Learning Models,,,,,http://arxiv.org/abs/2011.04328v1,"In this paper we propose a framework for assessing the risk associated with
deploying a machine learning model in a specified environment. For that we
carry over the risk definition from decision theory to machine learning. We
develop and implement a method that allows to define deployment scenarios, test
the machine learning model under the conditions specified in each scenario, and
estimate the damage associated with the output of the machine learning model
under test. Using the likelihood of each scenario together with the estimated
damage we define \emph{key risk indicators} of a machine learning model.
  The definition of scenarios and weighting by their likelihood allows for
standardized risk assessment in machine learning throughout multiple domains of
application. In particular, in our framework, the robustness of a machine
learning model to random input corruptions, distributional shifts caused by a
changing environment, and adversarial perturbations can be assessed.",2022-11-13 16:02:35,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Annie Xie, Dylan P. Losey, Ryan Tolsma, Chelsea Finn, Dorsa Sadigh",Learning Latent Representations to Influence Multi-Agent Interaction,,,,,http://arxiv.org/abs/2011.06619v1,"Seamlessly interacting with humans or robots is hard because these agents are
non-stationary. They update their policy in response to the ego agent's
behavior, and the ego agent must anticipate these changes to co-adapt. Inspired
by humans, we recognize that robots do not need to explicitly model every
low-level action another agent will make; instead, we can capture the latent
strategy of other agents through high-level representations. We propose a
reinforcement learning-based framework for learning latent representations of
an agent's policy, where the ego agent identifies the relationship between its
behavior and the other agent's future strategy. The ego agent then leverages
these latent dynamics to influence the other agent, purposely guiding them
towards policies suitable for co-adaptation. Across several simulated domains
and a real-world air hockey game, our approach outperforms the alternatives and
learns to influence the other agent.",2022-11-13 16:02:36,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Ramana Kumar, Jonathan Uesato, Richard Ngo, Tom Everitt, Victoria Krakovna, Shane Legg",REALab: An Embedded Perspective on Tampering,,,,,http://arxiv.org/abs/2011.08820v1,"This paper describes REALab, a platform for embedded agency research in
reinforcement learning (RL). REALab is designed to model the structure of
tampering problems that may arise in real-world deployments of RL. Standard
Markov Decision Process (MDP) formulations of RL and simulated environments
mirroring the MDP structure assume secure access to feedback (e.g., rewards).
This may be unrealistic in settings where agents are embedded and can corrupt
the processes producing feedback (e.g., human supervisors, or an implemented
reward function). We describe an alternative Corrupt Feedback MDP formulation
and the REALab environment platform, which both avoid the secure feedback
assumption. We hope the design of REALab provides a useful perspective on
tampering problems, and that the platform may serve as a unit test for the
presence of tampering incentives in RL agent designs.",2022-11-13 16:02:36,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Pedro Freire, Adam Gleave, Sam Toyer, Stuart Russell",DERAIL: Diagnostic Environments for Reward And Imitation Learning,,,,,http://arxiv.org/abs/2012.01365v1,"The objective of many real-world tasks is complex and difficult to
procedurally specify. This makes it necessary to use reward or imitation
learning algorithms to infer a reward or policy directly from human data.
Existing benchmarks for these algorithms focus on realism, testing in complex
environments. Unfortunately, these benchmarks are slow, unreliable and cannot
isolate failures. As a complementary approach, we develop a suite of simple
diagnostic tasks that test individual facets of algorithm performance in
isolation. We evaluate a range of common reward and imitation learning
algorithms on our tasks. Our results confirm that algorithm performance is
highly sensitive to implementation details. Moreover, in a case-study into a
popular preference-based reward learning implementation, we illustrate how the
suite can pinpoint design flaws and rapidly evaluate candidate solutions. The
environments are available at https://github.com/HumanCompatibleAI/seals .",2022-11-13 16:02:37,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Pratyay Banerjee, Chitta Baral, Man Luo, Arindam Mitra, Kuntal Pal, Tran C. Son, Neeraj Varshney",Can Transformers Reason About Effects of Actions?,,,,,http://arxiv.org/abs/2012.09938v1,"A recent work has shown that transformers are able to ""reason"" with facts and
rules in a limited setting where the rules are natural language expressions of
conjunctions of conditions implying a conclusion. Since this suggests that
transformers may be used for reasoning with knowledge given in natural
language, we do a rigorous evaluation of this with respect to a common form of
knowledge and its corresponding reasoning -- the reasoning about effects of
actions. Reasoning about action and change has been a top focus in the
knowledge representation subfield of AI from the early days of AI and more
recently it has been a highlight aspect in common sense question answering. We
consider four action domains (Blocks World, Logistics, Dock-Worker-Robots and a
Generic Domain) in natural language and create QA datasets that involve
reasoning about the effects of actions in these domains. We investigate the
ability of transformers to (a) learn to reason in these domains and (b)
transfer that learning from the generic domains to the other domains.",2022-11-13 16:02:37,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Jerry Zikun Chen, Shi Yu, Haoran Wang",Exploring Fluent Query Reformulations with Text-to-Text Transformers and Reinforcement Learning,,,,,http://arxiv.org/abs/2012.10033v2,"Query reformulation aims to alter noisy or ambiguous text sequences into
coherent ones closer to natural language questions. This is to prevent errors
from propagating in a client-facing pipeline and promote better communication
with users. Besides, it is crucial to maintain performance in downstream
environments like question answering when rephrased queries are given as input.
We show that under the previous framework (AQA), attempts to alter RL
algorithms do not bring significant benefits to either reward acquisition or
sequence fluency. Instead, we leverage a query-reformulating text-to-text
transformer (QRT5) and apply policy-based RL algorithms to further nudge this
reformulator and obtain better answers downstream by generating
reward-acquiring query trajectories. QRT5 shows better sample efficiency in RL
to achieve the same level of QA performance as the previous approach. It can
generate reformulations with more readability based on query well-formedness
evaluations and can generalize to out-of-sample data. Our framework is
demonstrated to be flexible, allowing reward signals to be sourced from
different downstream environments such as intent classification.",2022-11-13 16:02:38,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Zelin Zhao, Chuang Gan, Jiajun Wu, Xiaoxiao Guo, Joshua B. Tenenbaum",Augmenting Policy Learning with Routines Discovered from a Single Demonstration,,,,,http://arxiv.org/abs/2012.12469v4,"Humans can abstract prior knowledge from very little data and use it to boost
skill learning. In this paper, we propose routine-augmented policy learning
(RAPL), which discovers routines composed of primitive actions from a single
demonstration and uses discovered routines to augment policy learning. To
discover routines from the demonstration, we first abstract routine candidates
by identifying grammar over the demonstrated action trajectory. Then, the best
routines measured by length and frequency are selected to form a routine
library. We propose to learn policy simultaneously at primitive-level and
routine-level with discovered routines, leveraging the temporal structure of
routines. Our approach enables imitating expert behavior at multiple temporal
scales for imitation learning and promotes reinforcement learning exploration.
Extensive experiments on Atari games demonstrate that RAPL improves the
state-of-the-art imitation learning method SQIL and reinforcement learning
method A2C. Further, we show that discovered routines can generalize to unseen
levels and difficulties on the CoinRun benchmark.",2022-11-13 16:02:38,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,Lance Eliot,Antitrust and Artificial Intelligence (AAI): Antitrust Vigilance Lifecycle and AI Legal Reasoning Autonomy,,,,,http://arxiv.org/abs/2012.13016v1,"There is an increasing interest in the entwining of the field of antitrust
with the field of Artificial Intelligence (AI), frequently referred to jointly
as Antitrust and AI (AAI) in the research literature. This study focuses on the
synergies entangling antitrust and AI, doing so to extend the literature by
proffering the primary ways that these two fields intersect, consisting of: (1)
the application of antitrust to AI, and (2) the application of AI to antitrust.
To date, most of the existing research on this intermixing has concentrated on
the former, namely the application of antitrust to AI, entailing how the
marketplace will be altered by the advent of AI and the potential for adverse
antitrust behaviors arising accordingly. Opting to explore more deeply the
other side of this coin, this research closely examines the application of AI
to antitrust and establishes an antitrust vigilance lifecycle to which AI is
predicted to be substantively infused for purposes of enabling and bolstering
antitrust detection, enforcement, and post-enforcement monitoring. Furthermore,
a gradual and incremental injection of AI into antitrust vigilance is
anticipated to occur as significant advances emerge amidst the Levels of
Autonomy (LoA) for AI Legal Reasoning (AILR).",2022-11-13 16:02:39,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Arnaud Fickinger, Simon Zhuang, Andrew Critch, Dylan Hadfield-Menell, Stuart Russell",Multi-Principal Assistance Games: Definition and Collegial Mechanisms,,,,,http://arxiv.org/abs/2012.14536v1,"We introduce the concept of a multi-principal assistance game (MPAG), and
circumvent an obstacle in social choice theory, Gibbard's theorem, by using a
sufficiently collegial preference inference mechanism. In an MPAG, a single
agent assists N human principals who may have widely different preferences.
MPAGs generalize assistance games, also known as cooperative inverse
reinforcement learning games. We analyze in particular a generalization of
apprenticeship learning in which the humans first perform some work to obtain
utility and demonstrate their preferences, and then the robot acts to further
maximize the sum of human payoffs. We show in this setting that if the game is
sufficiently collegial, i.e. if the humans are responsible for obtaining a
sufficient fraction of the rewards through their own actions, then their
preferences are straightforwardly revealed through their work. This revelation
mechanism is non-dictatorial, does not limit the possible outcomes to two
alternatives, and is dominant-strategy incentive-compatible.",2022-11-13 16:02:39,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Engkarat Techapanurak, Anh-Chuong Dang, Takayuki Okatani",Bridging In- and Out-of-distribution Samples for Their Better Discriminability,,,,,http://arxiv.org/abs/2101.02500v1,"This paper proposes a method for OOD detection. Questioning the premise of
previous studies that ID and OOD samples are separated distinctly, we consider
samples lying in the intermediate of the two and use them for training a
network. We generate such samples using multiple image transformations that
corrupt inputs in various ways and with different severity levels. We estimate
where the generated samples by a single image transformation lie between ID and
OOD using a network trained on clean ID samples. To be specific, we make the
network classify the generated samples and calculate their mean classification
accuracy, using which we create a soft target label for them. We train the same
network from scratch using the original ID samples and the generated samples
with the soft labels created for them. We detect OOD samples by thresholding
the entropy of the predicted softmax probability. The experimental results show
that our method outperforms the previous state-of-the-art in the standard
benchmark tests. We also analyze the effect of the number and particular
combinations of image corrupting transformations on the performance.",2022-11-13 16:02:40,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Vivek Veeriah, Tom Zahavy, Matteo Hessel, Zhongwen Xu, Junhyuk Oh, Iurii Kemaev, Hado van Hasselt, David Silver, Satinder Singh",Discovery of Options via Meta-Learned Subgoals,,,,,http://arxiv.org/abs/2102.06741v1,"Temporal abstractions in the form of options have been shown to help
reinforcement learning (RL) agents learn faster. However, despite prior work on
this topic, the problem of discovering options through interaction with an
environment remains a challenge. In this paper, we introduce a novel
meta-gradient approach for discovering useful options in multi-task RL
environments. Our approach is based on a manager-worker decomposition of the RL
agent, in which a manager maximises rewards from the environment by learning a
task-dependent policy over both a set of task-independent discovered-options
and primitive actions. The option-reward and termination functions that define
a subgoal for each option are parameterised as neural networks and trained via
meta-gradients to maximise their usefulness. Empirical analysis on gridworld
and DeepMind Lab tasks show that: (1) our approach can discover meaningful and
diverse temporally-extended options in multi-task RL domains, (2) the
discovered options are frequently used by the agent while learning to solve the
training tasks, and (3) that the discovered options help a randomly initialised
manager learn faster in completely new tasks.",2022-11-13 16:02:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Khanh Nguyen, Dipendra Misra, Robert Schapire, Miro Dudík, Patrick Shafto",Interactive Learning from Activity Description,,,,,http://arxiv.org/abs/2102.07024v2,"We present a novel interactive learning protocol that enables training
request-fulfilling agents by verbally describing their activities. Unlike
imitation learning (IL), our protocol allows the teaching agent to provide
feedback in a language that is most appropriate for them. Compared with reward
in reinforcement learning (RL), the description feedback is richer and allows
for improved sample complexity. We develop a probabilistic framework and an
algorithm that practically implements our protocol. Empirical results in two
challenging request-fulfilling problems demonstrate the strengths of our
approach: compared with RL baselines, it is more sample-efficient; compared
with IL baselines, it achieves competitive success rates without requiring the
teaching agent to be able to demonstrate the desired behavior using the
learning agent's actions. Apart from empirical evaluation, we also provide
theoretical guarantees for our algorithm under certain assumptions about the
teacher and the environment.",2022-11-13 16:02:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Giuliano Lorenzoni, Paulo Alencar, Nathalia Nascimento, Donald Cowan",Machine Learning Model Development from a Software Engineering Perspective: A Systematic Literature Review,,,,,http://arxiv.org/abs/2102.07574v1,"Data scientists often develop machine learning models to solve a variety of
problems in the industry and academy but not without facing several challenges
in terms of Model Development. The problems regarding Machine Learning
Development involves the fact that such professionals do not realize that they
usually perform ad-hoc practices that could be improved by the adoption of
activities presented in the Software Engineering Development Lifecycle. Of
course, since machine learning systems are different from traditional Software
systems, some differences in their respective development processes are to be
expected. In this context, this paper is an effort to investigate the
challenges and practices that emerge during the development of ML models from
the software engineering perspective by focusing on understanding how software
developers could benefit from applying or adapting the traditional software
engineering process to the Machine Learning workflow.",2022-11-13 16:02:42,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Selmer Bringsjord, Naveen Sundar Govindarajulu, Michael Giancola","AI Can Stop Mass Shootings, and More",,,,,http://arxiv.org/abs/2102.09343v1,"We propose to build directly upon our longstanding, prior r&d in AI/machine
ethics in order to attempt to make real the blue-sky idea of AI that can thwart
mass shootings, by bringing to bear its ethical reasoning. The r&d in question
is overtly and avowedly logicist in form, and since we are hardly the only ones
who have established a firm foundation in the attempt to imbue AI's with their
own ethical sensibility, the pursuit of our proposal by those in different
methodological camps should, we believe, be considered as well. We seek herein
to make our vision at least somewhat concrete by anchoring our exposition to
two simulations, one in which the AI saves the lives of innocents by locking
out a malevolent human's gun, and a second in which this malevolent agent is
allowed by the AI to be neutralized by law enforcement. Along the way, some
objections are anticipated, and rebutted.",2022-11-13 16:02:42,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Chao-Han Huck Yang, I-Te Danny Hung, Yi Ouyang, Pin-Yu Chen",Training a Resilient Q-Network against Observational Interference,,,,,http://arxiv.org/abs/2102.09677v3,"Deep reinforcement learning (DRL) has demonstrated impressive performance in
various gaming simulators and real-world applications. In practice, however, a
DRL agent may receive faulty observation by abrupt interferences such as
black-out, frozen-screen, and adversarial perturbation. How to design a
resilient DRL algorithm against these rare but mission-critical and
safety-crucial scenarios is an essential yet challenging task. In this paper,
we consider a deep q-network (DQN) framework training with an auxiliary task of
observational interferences such as artificial noises. Inspired by causal
inference for observational interference, we propose a causal inference based
DQN algorithm called causal inference Q-network (CIQ). We evaluate the
performance of CIQ in several benchmark DQN environments with different types
of interferences as auxiliary labels. Our experimental results show that the
proposed CIQ method could achieve higher performance and more resilience
against observational interferences.",2022-11-13 16:02:43,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Solon Barocas, Anhong Guo, Ece Kamar, Jacquelyn Krones, Meredith Ringel Morris, Jennifer Wortman Vaughan, Duncan Wadsworth, Hanna Wallach","Designing Disaggregated Evaluations of AI Systems: Choices, Considerations, and Tradeoffs",,,,,http://arxiv.org/abs/2103.06076v2,"Disaggregated evaluations of AI systems, in which system performance is
assessed and reported separately for different groups of people, are
conceptually simple. However, their design involves a variety of choices. Some
of these choices influence the results that will be obtained, and thus the
conclusions that can be drawn; others influence the impacts -- both beneficial
and harmful -- that a disaggregated evaluation will have on people, including
the people whose data is used to conduct the evaluation. We argue that a deeper
understanding of these choices will enable researchers and practitioners to
design careful and conclusive disaggregated evaluations. We also argue that
better documentation of these choices, along with the underlying considerations
and tradeoffs that have been made, will help others when interpreting an
evaluation's results and conclusions.",2022-11-13 16:02:43,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Matteo Camilli, Michael Felderer, Andrea Giusti, Dominik T. Matt, Anna Perini, Barbara Russo, Angelo Susi",Towards Risk Modeling for Collaborative AI,,,,,http://arxiv.org/abs/2103.07460v1,"Collaborative AI systems aim at working together with humans in a shared
space to achieve a common goal. This setting imposes potentially hazardous
circumstances due to contacts that could harm human beings. Thus, building such
systems with strong assurances of compliance with requirements domain specific
standards and regulations is of greatest importance. Challenges associated with
the achievement of this goal become even more severe when such systems rely on
machine learning components rather than such as top-down rule-based AI. In this
paper, we introduce a risk modeling approach tailored to Collaborative AI
systems. The risk model includes goals, risk events and domain specific
indicators that potentially expose humans to hazards. The risk model is then
leveraged to drive assurance methods that feed in turn the risk model through
insights extracted from run-time evidence. Our envisioned approach is described
by means of a running example in the domain of Industry 4.0, where a robotic
arm endowed with a visual perception component, implemented with machine
learning, collaborates with a human operator for a production-relevant task.",2022-11-13 16:02:44,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Harshit Sikchi, Wenxuan Zhou, David Held",Lyapunov Barrier Policy Optimization,,,,,http://arxiv.org/abs/2103.09230v1,"Deploying Reinforcement Learning (RL) agents in the real-world require that
the agents satisfy safety constraints. Current RL agents explore the
environment without considering these constraints, which can lead to damage to
the hardware or even other agents in the environment. We propose a new method,
LBPO, that uses a Lyapunov-based barrier function to restrict the policy update
to a safe set for each training iteration. Our method also allows the user to
control the conservativeness of the agent with respect to the constraints in
the environment. LBPO significantly outperforms state-of-the-art baselines in
terms of the number of constraint violations during training while being
competitive in terms of performance. Further, our analysis reveals that
baselines like CPO and SDDPG rely mostly on backtracking to ensure safety
rather than safe projection, which provides insight into why previous methods
might not have effectively limit the number of constraint violations.",2022-11-13 16:02:44,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Yuanhao Xie, Luís Cruz, Petra Heck, Jan S. Rellermeyer",Systematic Mapping Study on the Machine Learning Lifecycle,,,,,http://arxiv.org/abs/2103.10248v1,"The development of artificial intelligence (AI) has made various industries
eager to explore the benefits of AI. There is an increasing amount of research
surrounding AI, most of which is centred on the development of new AI
algorithms and techniques. However, the advent of AI is bringing an increasing
set of practical problems related to AI model lifecycle management that need to
be investigated. We address this gap by conducting a systematic mapping study
on the lifecycle of AI model. Through quantitative research, we provide an
overview of the field, identify research opportunities, and provide suggestions
for future research. Our study yields 405 publications published from 2005 to
2020, mapped in 5 different main research topics, and 31 sub-topics. We observe
that only a minority of publications focus on data management and model
production problems, and that more studies should address the AI lifecycle from
a holistic perspective.",2022-11-13 16:02:45,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Dmitrii Krasheninnikov, Rohin Shah, Herke van Hoof",Combining Reward Information from Multiple Sources,,,,,http://arxiv.org/abs/2103.12142v1,"Given two sources of evidence about a latent variable, one can combine the
information from both by multiplying the likelihoods of each piece of evidence.
However, when one or both of the observation models are misspecified, the
distributions will conflict. We study this problem in the setting with two
conflicting reward functions learned from different sources. In such a setting,
we would like to retreat to a broader distribution over reward functions, in
order to mitigate the effects of misspecification. We assume that an agent will
maximize expected reward given this distribution over reward functions, and
identify four desiderata for this setting. We propose a novel algorithm,
Multitask Inverse Reward Design (MIRD), and compare it to a range of simple
baselines. While all methods must trade off between conservatism and
informativeness, through a combination of theory and empirical results on a toy
environment, we find that MIRD and its variant MIRD-IF strike a good balance
between the two.",2022-11-13 16:02:45,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Francesco Ponzio, Enrico Macii, Elisa Ficarra, Santa Di Cataldo",W2WNet: a two-module probabilistic Convolutional Neural Network with embedded data cleansing functionality,,,,,http://arxiv.org/abs/2103.13107v1,"Convolutional Neural Networks (CNNs) are supposed to be fed with only
high-quality annotated datasets. Nonetheless, in many real-world scenarios,
such high quality is very hard to obtain, and datasets may be affected by any
sort of image degradation and mislabelling issues. This negatively impacts the
performance of standard CNNs, both during the training and the inference phase.
To address this issue we propose Wise2WipedNet (W2WNet), a new two-module
Convolutional Neural Network, where a Wise module exploits Bayesian inference
to identify and discard spurious images during the training, and a Wiped module
takes care of the final classification while broadcasting information on the
prediction confidence at inference time. The goodness of our solution is
demonstrated on a number of public benchmarks addressing different image
classification tasks, as well as on a real-world case study on histological
image analysis. Overall, our experiments demonstrate that W2WNet is able to
identify image degradation and mislabelling issues both at training and at
inference time, with a positive impact on the final classification accuracy.",2022-11-13 16:02:46,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Faraz Torabi, Garrett Warnell, Peter Stone",DEALIO: Data-Efficient Adversarial Learning for Imitation from Observation,,,,,http://arxiv.org/abs/2104.00163v1,"In imitation learning from observation IfO, a learning agent seeks to imitate
a demonstrating agent using only observations of the demonstrated behavior
without access to the control signals generated by the demonstrator. Recent
methods based on adversarial imitation learning have led to state-of-the-art
performance on IfO problems, but they typically suffer from high sample
complexity due to a reliance on data-inefficient, model-free reinforcement
learning algorithms. This issue makes them impractical to deploy in real-world
settings, where gathering samples can incur high costs in terms of time,
energy, and risk. In this work, we hypothesize that we can incorporate ideas
from model-based reinforcement learning with adversarial methods for IfO in
order to increase the data efficiency of these methods without sacrificing
performance. Specifically, we consider time-varying linear Gaussian policies,
and propose a method that integrates the linear-quadratic regulator with path
integral policy improvement into an existing adversarial IfO framework. The
result is a more data-efficient IfO algorithm with better performance, which we
show empirically in four simulation domains: using far fewer interactions with
the environment, the proposed method exhibits similar or better performance
than the existing technique.",2022-11-13 16:02:46,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Brittany Davis Pierson, Justine Ventura, Matthew E. Taylor",The Atari Data Scraper,,,,,http://arxiv.org/abs/2104.04893v1,"Reinforcement learning has made great strides in recent years due to the
success of methods using deep neural networks. However, such neural networks
act as a black box, obscuring the inner workings. While reinforcement learning
has the potential to solve unique problems, a lack of trust and understanding
of reinforcement learning algorithms could prevent their widespread adoption.
Here, we present a library that attaches a ""data scraper"" to deep reinforcement
learning agents, acting as an observer, and then show how the data collected by
the Atari Data Scraper can be used to understand and interpret deep
reinforcement learning agents. The code for the Atari Data Scraper can be found
here: https://github.com/IRLL/Atari-Data-Scraper",2022-11-13 16:02:47,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Ercument Ilhan, Jeremy Gow, Diego Perez-Liebana",Action Advising with Advice Imitation in Deep Reinforcement Learning,,,,,http://arxiv.org/abs/2104.08441v1,"Action advising is a peer-to-peer knowledge exchange technique built on the
teacher-student paradigm to alleviate the sample inefficiency problem in deep
reinforcement learning. Recently proposed student-initiated approaches have
obtained promising results. However, due to being in the early stages of
development, these also have some substantial shortcomings. One of the
abilities that are absent in the current methods is further utilising advice by
reusing, which is especially crucial in the practical settings considering the
budget and cost constraints in peer-to-peer. In this study, we present an
approach to enable the student agent to imitate previously acquired advice to
reuse them directly in its exploration policy, without any interventions in the
learning mechanism itself. In particular, we employ a behavioural cloning
module to imitate the teacher policy and use dropout regularisation to have a
notion of epistemic uncertainty to keep track of which state-advice pairs are
actually collected. As the results of experiments we conducted in three Atari
games show, advice reusing via generalisation is indeed a feasible option in
deep RL and our approach can successfully achieve this while significantly
improving the learning performance, even when paired with a simple early
advising heuristic.",2022-11-13 16:02:47,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,Lambert Hogenhout,A Framework for Ethical AI at the United Nations,,,,,http://arxiv.org/abs/2104.12547v1,"This paper aims to provide an overview of the ethical concerns in artificial
intelligence (AI) and the framework that is needed to mitigate those risks, and
to suggest a practical path to ensure the development and use of AI at the
United Nations (UN) aligns with our ethical values. The overview discusses how
AI is an increasingly powerful tool with potential for good, albeit one with a
high risk of negative side-effects that go against fundamental human rights and
UN values. It explains the need for ethical principles for AI aligned with
principles for data governance, as data and AI are tightly interwoven. It
explores different ethical frameworks that exist and tools such as assessment
lists. It recommends that the UN develop a framework consisting of ethical
principles, architectural standards, assessment methods, tools and
methodologies, and a policy to govern the implementation and adherence to this
framework, accompanied by an education program for staff.",2022-11-13 16:02:48,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Anirudhan Badrinath, Frederic Wang, Zachary Pardos",pyBKT: An Accessible Python Library of Bayesian Knowledge Tracing Models,,,,,http://arxiv.org/abs/2105.00385v2,"Bayesian Knowledge Tracing, a model used for cognitive mastery estimation,
has been a hallmark of adaptive learning research and an integral component of
deployed intelligent tutoring systems (ITS). In this paper, we provide a brief
history of knowledge tracing model research and introduce pyBKT, an accessible
and computationally efficient library of model extensions from the literature.
The library provides data generation, fitting, prediction, and cross-validation
routines, as well as a simple to use data helper interface to ingest typical
tutor log dataset formats. We evaluate the runtime with various dataset sizes
and compare to past implementations. Additionally, we conduct sanity checks of
the model using experiments with simulated data to evaluate the accuracy of its
EM parameter learning and use real-world data to validate its predictions,
comparing pyBKT's supported model variants with results from the papers in
which they were originally introduced. The library is open source and open
license for the purpose of making knowledge tracing more accessible to
communities of research and practice and to facilitate progress in the field
through easier replication of past approaches.",2022-11-13 16:02:48,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Sarah Dean, Thomas Krendl Gilbert, Nathan Lambert, Tom Zick",Axes for Sociotechnical Inquiry in AI Research,,,,10.1109/TTS.2021.3074097,http://arxiv.org/abs/2105.06551v1,"The development of artificial intelligence (AI) technologies has far exceeded
the investigation of their relationship with society. Sociotechnical inquiry is
needed to mitigate the harms of new technologies whose potential impacts remain
poorly understood. To date, subfields of AI research develop primarily
individual views on their relationship with sociotechnics, while tools for
external investigation, comparison, and cross-pollination are lacking. In this
paper, we propose four directions for inquiry into new and evolving areas of
technological development: value--what progress and direction does a field
promote, optimization--how the defined system within a problem formulation
relates to broader dynamics, consensus--how agreement is achieved and who is
included in building it, and failure--what methods are pursued when the problem
specification is found wanting. The paper provides a lexicon for sociotechnical
inquiry and illustrates it through the example of consumer drone technology.",2022-11-13 16:02:49,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Michael Crosscombe, Jonathan Lawry",The Impact of Network Connectivity on Collective Learning,,,,,http://arxiv.org/abs/2106.00655v2,"In decentralised autonomous systems it is the interactions between individual
agents which govern the collective behaviours of the system. These local-level
interactions are themselves often governed by an underlying network structure.
These networks are particularly important for collective learning and
decision-making whereby agents must gather evidence from their environment and
propagate this information to other agents in the system. Models for collective
behaviours may often rely upon the assumption of total connectivity between
agents to provide effective information sharing within the system, but this
assumption may be ill-advised. In this paper we investigate the impact that the
underlying network has on performance in the context of collective learning.
Through simulations we study small-world networks with varying levels of
connectivity and randomness and conclude that totally-connected networks result
in higher average error when compared to networks with less connectivity.
Furthermore, we show that networks of high regularity outperform networks with
increasing levels of random connectivity.",2022-11-13 16:02:49,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch",Decision Transformer: Reinforcement Learning via Sequence Modeling,,,,,http://arxiv.org/abs/2106.01345v2,"We introduce a framework that abstracts Reinforcement Learning (RL) as a
sequence modeling problem. This allows us to draw upon the simplicity and
scalability of the Transformer architecture, and associated advances in
language modeling such as GPT-x and BERT. In particular, we present Decision
Transformer, an architecture that casts the problem of RL as conditional
sequence modeling. Unlike prior approaches to RL that fit value functions or
compute policy gradients, Decision Transformer simply outputs the optimal
actions by leveraging a causally masked Transformer. By conditioning an
autoregressive model on the desired return (reward), past states, and actions,
our Decision Transformer model can generate future actions that achieve the
desired return. Despite its simplicity, Decision Transformer matches or exceeds
the performance of state-of-the-art model-free offline RL baselines on Atari,
OpenAI Gym, and Key-to-Door tasks.",2022-11-13 16:02:50,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Stephen McAleer, John Lanier, Michael Dennis, Pierre Baldi, Roy Fox",Improving Social Welfare While Preserving Autonomy via a Pareto Mediator,,,,,http://arxiv.org/abs/2106.03927v1,"Machine learning algorithms often make decisions on behalf of agents with
varied and sometimes conflicting interests. In domains where agents can choose
to take their own action or delegate their action to a central mediator, an
open question is how mediators should take actions on behalf of delegating
agents. The main existing approach uses delegating agents to punish
non-delegating agents in an attempt to get all agents to delegate, which tends
to be costly for all. We introduce a Pareto Mediator which aims to improve
outcomes for delegating agents without making any of them worse off. Our
experiments in random normal form games, a restaurant recommendation game, and
a reinforcement learning sequential social dilemma show that the Pareto
Mediator greatly increases social welfare. Also, even when the Pareto Mediator
is based on an incorrect model of agent utility, performance gracefully
degrades to the pre-intervention level, due to the individual autonomy
preserved by the voluntary mediator.",2022-11-13 16:02:50,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Gaurav Yengera, Rati Devidze, Parameswaran Kamalaruban, Adish Singla",Curriculum Design for Teaching via Demonstrations: Theory and Applications,,,,,http://arxiv.org/abs/2106.04696v3,"We consider the problem of teaching via demonstrations in sequential
decision-making settings. In particular, we study how to design a personalized
curriculum over demonstrations to speed up the learner's convergence. We
provide a unified curriculum strategy for two popular learner models: Maximum
Causal Entropy Inverse Reinforcement Learning (MaxEnt-IRL) and Cross-Entropy
Behavioral Cloning (CrossEnt-BC). Our unified strategy induces a ranking over
demonstrations based on a notion of difficulty scores computed w.r.t. the
teacher's optimal policy and the learner's current policy. Compared to the
state of the art, our strategy doesn't require access to the learner's internal
dynamics and still enjoys similar convergence guarantees under mild technical
conditions. Furthermore, we adapt our curriculum strategy to the setting where
no teacher agent is present using task-specific difficulty scores. Experiments
on a synthetic car driving environment and navigation-based environments
demonstrate the effectiveness of our curriculum strategy.",2022-11-13 16:02:51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Mythreyi Velmurugan, Chun Ouyang, Catarina Moreira, Renuka Sindhgatta",Developing a Fidelity Evaluation Approach for Interpretable Machine Learning,,,,,http://arxiv.org/abs/2106.08492v1,"Although modern machine learning and deep learning methods allow for complex
and in-depth data analytics, the predictive models generated by these methods
are often highly complex, and lack transparency. Explainable AI (XAI) methods
are used to improve the interpretability of these complex models, and in doing
so improve transparency. However, the inherent fitness of these explainable
methods can be hard to evaluate. In particular, methods to evaluate the
fidelity of the explanation to the underlying black box require further
development, especially for tabular data. In this paper, we (a) propose a three
phase approach to developing an evaluation method; (b) adapt an existing
evaluation method primarily for image and text data to evaluate models trained
on tabular data; and (c) evaluate two popular explainable methods using this
evaluation method. Our evaluations suggest that the internal mechanism of the
underlying predictive model, the internal mechanism of the explainable method
used and model and data complexity all affect explanation fidelity. Given that
explanation fidelity is so sensitive to context and tools and data used, we
could not clearly identify any specific explainable method as being superior to
another.",2022-11-13 16:02:51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Dominik Sisejkovic, Lennart M. Reimann, Elmira Moussavi, Farhad Merchant, Rainer Leupers",Logic Locking at the Frontiers of Machine Learning: A Survey on Developments and Opportunities,"2021 IFIP/IEEE 29th International Conference on Very Large Scale
  Integration (VLSI-SoC)",,,10.1109/VLSI-SoC53125.2021.9606979,http://arxiv.org/abs/2107.01915v4,"In the past decade, a lot of progress has been made in the design and
evaluation of logic locking; a premier technique to safeguard the integrity of
integrated circuits throughout the electronics supply chain. However, the
widespread proliferation of machine learning has recently introduced a new
pathway to evaluating logic locking schemes. This paper summarizes the recent
developments in logic locking attacks and countermeasures at the frontiers of
contemporary machine learning models. Based on the presented work, the key
takeaways, opportunities, and challenges are highlighted to offer
recommendations for the design of next-generation logic locking.",2022-11-13 16:02:52,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Raphael Y. Cohen, Aaron D. Sodickson",An Orchestration Platform that Puts Radiologists in the Driver's Seat of AI Innovation: A Methodological Approach,,,,,http://arxiv.org/abs/2107.04409v1,"Current AI-driven research in radiology requires resources and expertise that
are often inaccessible to small and resource-limited labs. The clinicians who
are able to participate in AI research are frequently well-funded,
well-staffed, and either have significant experience with AI and computing, or
have access to colleagues or facilities that do. Current imaging data is
clinician-oriented and is not easily amenable to machine learning initiatives,
resulting in inefficient, time consuming, and costly efforts that rely upon a
crew of data engineers and machine learning scientists, and all too often
preclude radiologists from driving AI research and innovation. We present the
system and methodology we have developed to address infrastructure and platform
needs, while reducing the staffing and resource barriers to entry. We emphasize
a data-first and modular approach that streamlines the AI development and
deployment process while providing efficient and familiar interfaces for
radiologists, such that they can be the drivers of new AI innovations.",2022-11-13 16:02:52,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Stepan Makarenko, Dmitry Sorokin, Alexander Ulanov, A. I. Lvovsky",Aligning an optical interferometer with beam divergence control and continuous action space,,,,,http://arxiv.org/abs/2107.04457v2,"Reinforcement learning is finding its way to real-world problem application,
transferring from simulated environments to physical setups. In this work, we
implement vision-based alignment of an optical Mach-Zehnder interferometer with
a confocal telescope in one arm, which controls the diameter and divergence of
the corresponding beam. We use a continuous action space; exponential scaling
enables us to handle actions within a range of over two orders of magnitude.
Our agent trains only in a simulated environment with domain randomizations. In
an experimental evaluation, the agent significantly outperforms an existing
solution and a human expert.",2022-11-13 16:02:53,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Jesse David Dinneen, Helen Bubinger","Not Quite 'Ask a Librarian': AI on the Nature, Value, and Future of LIS",,,,,http://arxiv.org/abs/2107.05383v1,"AI language models trained on Web data generate prose that reflects human
knowledge and public sentiments, but can also contain novel insights and
predictions. We asked the world's best language model, GPT-3, fifteen difficult
questions about the nature, value, and future of library and information
science (LIS), topics that receive perennial attention from LIS scholars. We
present highlights from its 45 different responses, which range from platitudes
and caricatures to interesting perspectives and worrisome visions of the
future, thus providing an LIS-tailored demonstration of the current performance
of AI language models. We also reflect on the viability of using AI to forecast
or generate research ideas in this way today. Finally, we have shared the full
response log online for readers to consider and evaluate for themselves.",2022-11-13 16:02:54,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Ariyan Bighashdel, Panagiotis Meletis, Pavol Jancura, Gijs Dubbelman",Deep Adaptive Multi-Intention Inverse Reinforcement Learning,,,,,http://arxiv.org/abs/2107.06692v1,"This paper presents a deep Inverse Reinforcement Learning (IRL) framework
that can learn an a priori unknown number of nonlinear reward functions from
unlabeled experts' demonstrations. For this purpose, we employ the tools from
Dirichlet processes and propose an adaptive approach to simultaneously account
for both complex and unknown number of reward functions. Using the conditional
maximum entropy principle, we model the experts' multi-intention behaviors as a
mixture of latent intention distributions and derive two algorithms to estimate
the parameters of the deep reward network along with the number of experts'
intentions from unlabeled demonstrations. The proposed algorithms are evaluated
on three benchmarks, two of which have been specifically extended in this study
for multi-intention IRL, and compared with well-known baselines. We demonstrate
through several experiments the advantages of our algorithms over the existing
approaches and the benefits of online inferring, rather than fixing beforehand,
the number of expert's intentions.",2022-11-13 16:02:55,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Sunder Ali Khowaja, Kapal Dev, Nawab Muhammad Faseeh Qureshi, Parus Khuwaja, Luca Foschini",Towards Industrial Private AI: A two-tier framework for data and model security,IEEE Wireless Communications 2022,,,,http://arxiv.org/abs/2107.12806v2,"With the advances in 5G and IoT devices, the industries are vastly adopting
artificial intelligence (AI) techniques for improving classification and
prediction-based services. However, the use of AI also raises concerns
regarding privacy and security that can be misused or leaked. Private AI was
recently coined to address the data security issue by combining AI with
encryption techniques, but existing studies have shown that model inversion
attacks can be used to reverse engineer the images from model parameters. In
this regard, we propose a Federated Learning and Encryption-based Private
(FLEP) AI framework that provides two-tier security for data and model
parameters in an IIoT environment. We proposed a three-layer encryption method
for data security and provide a hypothetical method to secure the model
parameters. Experimental results show that the proposed method achieves better
encryption quality at the expense of slightly increased execution time. We also
highlight several open issues and challenges regarding the FLEP AI framework's
realization.",2022-11-13 16:02:55,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Ahmad Hammoudeh, Sara Tedmori, Nadim Obeid",A Reflection on Learning from Data: Epistemology Issues and Limitations,,,,,http://arxiv.org/abs/2107.13270v1,"Although learning from data is effective and has achieved significant
milestones, it has many challenges and limitations. Learning from data starts
from observations and then proceeds to broader generalizations. This framework
is controversial in science, yet it has achieved remarkable engineering
successes. This paper reflects on some epistemological issues and some of the
limitations of the knowledge discovered in data. The document discusses the
common perception that getting more data is the key to achieving better machine
learning models from theoretical and practical perspectives. The paper sheds
some light on the shortcomings of using generic mathematical theories to
describe the process. It further highlights the need for theories specialized
in learning from data. While more data leverages the performance of machine
learning models in general, the relation in practice is shown to be logarithmic
at its best; After a specific limit, more data stabilize or degrade the machine
learning models. Recent work in reinforcement learning showed that the trend is
shifting away from data-oriented approaches and relying more on algorithms. The
paper concludes that learning from data is hindered by many limitations. Hence
an approach that has an intensional orientation is needed.",2022-11-13 16:02:56,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,Susan von Struensee,"The Role of Social Movements, Coalitions, and Workers in Resisting Harmful Artificial Intelligence and Contributing to the Development of Responsible AI",,,,,http://arxiv.org/abs/2107.14052v1,"There is mounting public concern over the influence that AI based systems has
in our society. Coalitions in all sectors are acting worldwide to resist hamful
applications of AI. From indigenous people addressing the lack of reliable
data, to smart city stakeholders, to students protesting the academic
relationships with sex trafficker and MIT donor Jeffery Epstein, the
questionable ethics and values of those heavily investing in and profiting from
AI are under global scrutiny. There are biased, wrongful, and disturbing
assumptions embedded in AI algorithms that could get locked in without
intervention. Our best human judgment is needed to contain AI's harmful impact.
Perhaps one of the greatest contributions of AI will be to make us ultimately
understand how important human wisdom truly is in life on earth.",2022-11-13 16:02:57,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,Ajay Kulkarni,Towards Understanding the Impact of Real-Time AI-Powered Educational Dashboards (RAED) on Providing Guidance to Instructors,,,,,http://arxiv.org/abs/2107.14414v1,"The objectives of this ongoing research are to build Real-Time AI-Powered
Educational Dashboard (RAED) as a decision support tool for instructors, and to
measure its impact on them while making decisions. Current developments in AI
can be combined with the educational dashboards to make them AI-Powered. Thus,
AI can help in providing recommendations based on the students' performances.
AI-Powered educational dashboards can also assist instructors in tracking
real-time student activities. In this ongoing research, our aim is to develop
the AI component as well as improve the existing design component of the RAED.
Further, we will conduct experiments to study its impact on instructors, and
understand how much they trust RAED to guide them while making decisions. This
paper elaborates on the ongoing research and future direction.",2022-11-13 16:02:58,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Jiahao Chen, Victor Storchan, Eren Kurshan",Beyond Fairness Metrics: Roadblocks and Challenges for Ethical AI in Practice,,,,,http://arxiv.org/abs/2108.06217v1,"We review practical challenges in building and deploying ethical AI at the
scale of contemporary industrial and societal uses. Apart from the purely
technical concerns that are the usual focus of academic research, the
operational challenges of inconsistent regulatory pressures, conflicting
business goals, data quality issues, development processes, systems integration
practices, and the scale of deployment all conspire to create new ethical
risks. Such ethical concerns arising from these practical considerations are
not adequately addressed by existing research results. We argue that a holistic
consideration of ethics in the development and deployment of AI systems is
necessary for building ethical AI in practice, and exhort researchers to
consider the full operational contexts of AI systems when assessing ethical
risks.",2022-11-13 16:02:58,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,Benjamin Cedric Larsen,A Framework for Understanding AI-Induced Field Change: How AI Technologies are Legitimized and Institutionalized,"In Proceedings of the 2021 AAAI ACM Conference on AI Ethics and
  Society",,,10.1145/3461702.3462591,http://arxiv.org/abs/2108.07804v1,"Artificial intelligence (AI) systems operate in increasingly diverse areas,
from healthcare to facial recognition, the stock market, autonomous vehicles,
and so on. While the underlying digital infrastructure of AI systems is
developing rapidly, each area of implementation is subject to different degrees
and processes of legitimization. By combining elements from institutional
theory and information systems-theory, this paper presents a conceptual
framework to analyze and understand AI-induced field-change. The introduction
of novel AI-agents into new or existing fields creates a dynamic in which
algorithms (re)shape organizations and institutions while existing
institutional infrastructures determine the scope and speed at which
organizational change is allowed to occur. Where institutional infrastructure
and governance arrangements, such as standards, rules, and regulations, still
are unelaborate, the field can move fast but is also more likely to be
contested. The institutional infrastructure surrounding AI-induced fields is
generally little elaborated, which could be an obstacle to the broader
institutionalization of AI-systems going forward.",2022-11-13 16:02:59,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Paolo Bova, Jonas Emanuel Müller, Benjamin Harack",Safe Transformative AI via a Windfall Clause,,,,,http://arxiv.org/abs/2108.09404v2,"Society could soon see transformative artificial intelligence (TAI). Models
of competition for TAI show firms face strong competitive pressure to deploy
TAI systems before they are safe. This paper explores a proposed solution to
this problem, a Windfall Clause, where developers commit to donating a
significant portion of any eventual extremely large profits to good causes.
However, a key challenge for a Windfall Clause is that firms must have reason
to join one. Firms must also believe these commitments are credible. We extend
a model of TAI competition with a Windfall Clause to show how firms and
policymakers can design a Windfall Clause which overcomes these challenges.
Encouragingly, firms benefit from joining a Windfall Clause under a wide range
of scenarios. We also find that firms join the Windfall Clause more often when
the competition is more dangerous. Even when firms learn each other's
capabilities, firms rarely wish to withdraw their support for the Windfall
Clause. These three findings strengthen the case for using a Windfall Clause to
promote the safe development of TAI.",2022-11-13 16:03:00,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Shachaf Poran, Gil Amsalem, Amit Beka, Dmitri Goldenberg",With One Voice: Composing a Travel Voice Assistant from Re-purposed Models,"2nd International Workshop on Industrial Recommendation Systems @
  KDD 2021",,,,http://arxiv.org/abs/2108.11463v1,"Voice assistants provide users a new way of interacting with digital
products, allowing them to retrieve information and complete tasks with an
increased sense of control and flexibility. Such products are comprised of
several machine learning models, like Speech-to-Text transcription, Named
Entity Recognition and Resolution, and Text Classification. Building a voice
assistant from scratch takes the prolonged efforts of several teams
constructing numerous models and orchestrating between components. Alternatives
such as using third-party vendors or re-purposing existing models may be
considered to shorten time-to-market and development costs. However, each
option has its benefits and drawbacks. We present key insights from building a
voice search assistant for Booking.com search and recommendation system. Our
paper compares the achieved performance and development efforts in dedicated
tailor-made solutions against existing re-purposed models. We share and discuss
our data-driven decisions about implementation trade-offs and their estimated
outcomes in hindsight, showing that a fully functional machine learning product
can be built from existing models.",2022-11-13 16:03:00,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Jess Whittlestone, Jack Clark",Why and How Governments Should Monitor AI Development,,,,,http://arxiv.org/abs/2108.12427v2,"In this paper we outline a proposal for improving the governance of
artificial intelligence (AI) by investing in government capacity to
systematically measure and monitor the capabilities and impacts of AI systems.
If adopted, this would give governments greater information about the AI
ecosystem, equipping them to more effectively direct AI development and
deployment in the most societally and economically beneficial directions. It
would also create infrastructure that could rapidly identify potential threats
or harms that could occur as a consequence of changes in the AI ecosystem, such
as the emergence of strategically transformative capabilities, or the
deployment of harmful systems.
  We begin by outlining the problem which motivates this proposal: in brief,
traditional governance approaches struggle to keep pace with the speed of
progress in AI. We then present our proposal for addressing this problem:
governments must invest in measurement and monitoring infrastructure. We
discuss this proposal in detail, outlining what specific things governments
could focus on measuring and monitoring, and the kinds of benefits this would
generate for policymaking. Finally, we outline some potential pilot projects
and some considerations for implementing this in practice.",2022-11-13 16:03:01,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Sebastian Flennerhag, Yannick Schroecker, Tom Zahavy, Hado van Hasselt, David Silver, Satinder Singh",Bootstrapped Meta-Learning,,,,,http://arxiv.org/abs/2109.04504v2,"Meta-learning empowers artificial intelligence to increase its efficiency by
learning how to learn. Unlocking this potential involves overcoming a
challenging meta-optimisation problem. We propose an algorithm that tackles
this problem by letting the meta-learner teach itself. The algorithm first
bootstraps a target from the meta-learner, then optimises the meta-learner by
minimising the distance to that target under a chosen (pseudo-)metric. Focusing
on meta-learning with gradients, we establish conditions that guarantee
performance improvements and show that the metric can control
meta-optimisation. Meanwhile, the bootstrapping mechanism can extend the
effective meta-learning horizon without requiring backpropagation through all
updates. We achieve a new state-of-the art for model-free agents on the Atari
ALE benchmark and demonstrate that it yields both performance and efficiency
gains in multi-task meta-learning. Finally, we explore how bootstrapping opens
up new possibilities and find that it can meta-learn efficient exploration in
an epsilon-greedy Q-learning agent, without backpropagating through the update
rule.",2022-11-13 16:03:02,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Oliver Eigner, Sebastian Eresheim, Peter Kieseberg, Lukas Daniel Klausner, Martin Pirker, Torsten Priebe, Simon Tjoa, Fiammetta Marulli, Francesco Mercaldo",Towards Resilient Artificial Intelligence: Survey and Research Issues,"Proceedings of the 2021 IEEE International Conference on Cyber
  Security and Resilience (CSR 2021), 2021, 536-542",,,10.1109/CSR51186.2021.9527986,http://arxiv.org/abs/2109.08904v1,"Artificial intelligence (AI) systems are becoming critical components of
today's IT landscapes. Their resilience against attacks and other environmental
influences needs to be ensured just like for other IT assets. Considering the
particular nature of AI, and machine learning (ML) in particular, this paper
provides an overview of the emerging field of resilient AI and presents
research issues the authors identify as potential future work.",2022-11-13 16:03:02,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Dan Hendrycks, Nicholas Carlini, John Schulman, Jacob Steinhardt",Unsolved Problems in ML Safety,,,,,http://arxiv.org/abs/2109.13916v5,"Machine learning (ML) systems are rapidly increasing in size, are acquiring
new capabilities, and are increasingly deployed in high-stakes settings. As
with other powerful technologies, safety for ML should be a leading research
priority. In response to emerging safety challenges in ML, such as those
introduced by recent large-scale models, we provide a new roadmap for ML Safety
and refine the technical problems that the field needs to address. We present
four problems ready for research, namely withstanding hazards (""Robustness""),
identifying hazards (""Monitoring""), reducing inherent model hazards
(""Alignment""), and reducing systemic hazards (""Systemic Safety""). Throughout,
we clarify each problem's motivation and provide concrete research directions.",2022-11-13 16:03:03,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Jing Bi, Jiebo Luo, Chenliang Xu",Procedure Planning in Instructional Videos via Contextual Modeling and Model-based Policy Learning,,,,,http://arxiv.org/abs/2110.01770v2,"Learning new skills by observing humans' behaviors is an essential capability
of AI. In this work, we leverage instructional videos to study humans'
decision-making processes, focusing on learning a model to plan goal-directed
actions in real-life videos. In contrast to conventional action recognition,
goal-directed actions are based on expectations of their outcomes requiring
causal knowledge of potential consequences of actions. Thus, integrating the
environment structure with goals is critical for solving this task. Previous
works learn a single world model will fail to distinguish various tasks,
resulting in an ambiguous latent space; planning through it will gradually
neglect the desired outcomes since the global information of the future goal
degrades quickly as the procedure evolves. We address these limitations with a
new formulation of procedure planning and propose novel algorithms to model
human behaviors through Bayesian Inference and model-based Imitation Learning.
Experiments conducted on real-world instructional videos show that our method
can achieve state-of-the-art performance in reaching the indicated goals.
Furthermore, the learned contextual information presents interesting features
for planning in a latent space.",2022-11-13 16:03:03,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Tian Dong, Han Qiu, Tianwei Zhang, Jiwei Li, Hewu Li, Jialiang Lu",Fingerprinting Multi-exit Deep Neural Network Models via Inference Time,,,,,http://arxiv.org/abs/2110.03175v1,"Transforming large deep neural network (DNN) models into the multi-exit
architectures can overcome the overthinking issue and distribute a large DNN
model on resource-constrained scenarios (e.g. IoT frontend devices and backend
servers) for inference and transmission efficiency. Nevertheless, intellectual
property (IP) protection for the multi-exit models in the wild is still an
unsolved challenge. Previous efforts to verify DNN model ownership mainly rely
on querying the model with specific samples and checking the responses, e.g.,
DNN watermarking and fingerprinting. However, they are vulnerable to
adversarial settings such as adversarial training and are not suitable for the
IP verification for multi-exit DNN models. In this paper, we propose a novel
approach to fingerprint multi-exit models via inference time rather than
inference predictions. Specifically, we design an effective method to generate
a set of fingerprint samples to craft the inference process with a unique and
robust inference time cost as the evidence for model ownership. We conduct
extensive experiments to prove the uniqueness and robustness of our method on
three structures (ResNet-56, VGG-16, and MobileNet) and three datasets
(CIFAR-10, CIFAR-100, and Tiny-ImageNet) under comprehensive adversarial
settings.",2022-11-13 16:03:04,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Andreas Madsen, Nicholas Meade, Vaibhav Adlakha, Siva Reddy",Evaluating the Faithfulness of Importance Measures in NLP by Recursively Masking Allegedly Important Tokens and Retraining,,,,,http://arxiv.org/abs/2110.08412v3,"To explain NLP models a popular approach is to use importance measures, such
as attention, which inform input tokens are important for making a prediction.
However, an open question is how well these explanations accurately reflect a
model's logic, a property called faithfulness.
  To answer this question, we propose Recursive ROAR, a new faithfulness
metric. This works by recursively masking allegedly important tokens and then
retraining the model. The principle is that this should result in worse model
performance compared to masking random tokens. The result is a performance
curve given a masking-ratio. Furthermore, we propose a summarizing metric using
relative area-between-curves (RACU), which allows for easy comparison across
papers, models, and tasks.
  We evaluate 4 different importance measures on 8 different datasets, using
both LSTM-attention models and RoBERTa models. We find that the faithfulness of
importance measures is both model-dependent and task-dependent. This conclusion
contradicts previous evaluations in both computer vision and faithfulness of
attention literature.",2022-11-13 16:03:05,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Su Lin Blodgett, Michael Madaio",Risks of AI Foundation Models in Education,,,,,http://arxiv.org/abs/2110.10024v1,"If the authors of a recent Stanford report (Bommasani et al., 2021) on the
opportunities and risks of ""foundation models"" are to be believed, these models
represent a paradigm shift for AI and for the domains in which they will
supposedly be used, including education. Although the name is new (and
contested (Field, 2021)), the term describes existing types of algorithmic
models that are ""trained on broad data at scale"" and ""fine-tuned"" (i.e.,
adapted) for particular downstream tasks, and is intended to encompass large
language models such as BERT or GPT-3 and computer vision models such as CLIP.
Such technologies have the potential for harm broadly speaking (e.g., Bender et
al., 2021), but their use in the educational domain is particularly fraught,
despite the potential benefits for learners claimed by the authors. In section
3.3 of the Stanford report, Malik et al. argue that achieving the goal of
providing education for all learners requires more efficient computational
approaches that can rapidly scale across educational domains and across
educational contexts, for which they argue foundation models are uniquely
well-suited. However, evidence suggests that not only are foundation models not
likely to achieve the stated benefits for learners, but their use may also
introduce new risks for harm.",2022-11-13 16:03:05,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Muhammad Usman, Divya Gopinath, Corina S. Păsăreanu",QuantifyML: How Good is my Machine Learning Model?,"EPTCS 348, 2021, pp. 92-100",,,10.4204/EPTCS.348.6,http://arxiv.org/abs/2110.12588v1,"The efficacy of machine learning models is typically determined by computing
their accuracy on test data sets. However, this may often be misleading, since
the test data may not be representative of the problem that is being studied.
With QuantifyML we aim to precisely quantify the extent to which machine
learning models have learned and generalized from the given data. Given a
trained model, QuantifyML translates it into a C program and feeds it to the
CBMC model checker to produce a formula in Conjunctive Normal Form (CNF). The
formula is analyzed with off-the-shelf model counters to obtain precise counts
with respect to different model behavior. QuantifyML enables i) evaluating
learnability by comparing the counts for the outputs to ground truth, expressed
as logical predicates, ii) comparing the performance of models built with
different machine learning algorithms (decision-trees vs. neural networks), and
iii) quantifying the safety and robustness of models.",2022-11-13 16:03:06,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Zeerak Talat, Hagen Blix, Josef Valvoda, Maya Indira Ganesh, Ryan Cotterell, Adina Williams",A Word on Machine Ethics: A Response to Jiang et al. (2021),,,,,http://arxiv.org/abs/2111.04158v1,"Ethics is one of the longest standing intellectual endeavors of humanity. In
recent years, the fields of AI and NLP have attempted to wrangle with how
learning systems that interact with humans should be constrained to behave
ethically. One proposal in this vein is the construction of morality models
that can take in arbitrary text and output a moral judgment about the situation
described. In this work, we focus on a single case study of the recently
proposed Delphi model and offer a critique of the project's proposed method of
automating morality judgments. Through an audit of Delphi, we examine broader
issues that would be applicable to any similar attempt. We conclude with a
discussion of how machine ethics could usefully proceed, by focusing on current
and near-future uses of technology, in a way that centers around transparency,
democratic values, and allows for straightforward accountability.",2022-11-13 16:03:06,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Patric M. Fulop, Vincent Danos",Efficient estimates of optimal transport via low-dimensional embeddings,,,,,http://arxiv.org/abs/2111.04838v1,"Optimal transport distances (OT) have been widely used in recent work in
Machine Learning as ways to compare probability distributions. These are costly
to compute when the data lives in high dimension. Recent work by Paty et al.,
2019, aims specifically at reducing this cost by computing OT using low-rank
projections of the data (seen as discrete measures). We extend this approach
and show that one can approximate OT distances by using more general families
of maps provided they are 1-Lipschitz. The best estimate is obtained by
maximising OT over the given family. As OT calculations are done after mapping
data to a lower dimensional space, our method scales well with the original
data dimension. We demonstrate the idea with neural networks.",2022-11-13 16:03:07,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Ying Zhang, Matthew A. Gitzendanner, Dan S. Maxwell, Justin W. Richardson, Kaleb E. Smith, Eric A. Stubbs, Brian J. Stucky, Jingchao Zhang, Erik Deumens",Building an AI-ready RSE Workforce,,,,,http://arxiv.org/abs/2111.04916v1,"Artificial Intelligence has been transforming industries and academic
research across the globe, and research software development is no exception.
Machine learning and deep learning are being applied in every aspect of the
research software development lifecycles, from new algorithm design paradigms
to software development processes. In this paper, we discuss our views on
today's challenges and opportunities that AI has presented on research software
development and engineers, and the approaches we, at the University of Florida,
are taking to prepare our workforce for the new era of AI.",2022-11-13 16:03:07,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Waddah Saeed, Christian Omlin",Explainable AI (XAI): A Systematic Meta-Survey of Current Challenges and Future Opportunities,,,,,http://arxiv.org/abs/2111.06420v1,"The past decade has seen significant progress in artificial intelligence
(AI), which has resulted in algorithms being adopted for resolving a variety of
problems. However, this success has been met by increasing model complexity and
employing black-box AI models that lack transparency. In response to this need,
Explainable AI (XAI) has been proposed to make AI more transparent and thus
advance the adoption of AI in critical domains. Although there are several
reviews of XAI topics in the literature that identified challenges and
potential research directions in XAI, these challenges and research directions
are scattered. This study, hence, presents a systematic meta-survey for
challenges and future research directions in XAI organized in two themes: (1)
general challenges and research directions in XAI and (2) challenges and
research directions in XAI based on machine learning life cycle's phases:
design, development, and deployment. We believe that our meta-survey
contributes to XAI literature by providing a guide for future exploration in
the XAI area.",2022-11-13 16:03:08,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Alex Kearney, Anna Koop, Johannes Günther, Patrick M. Pilarski",Finding Useful Predictions by Meta-gradient Descent to Improve Decision-making,"NeurIPS 2021 Workshop on Self-Supervised Learning: Theory and
  Practice",,,,http://arxiv.org/abs/2111.11212v1,"In computational reinforcement learning, a growing body of work seeks to
express an agent's model of the world through predictions about future
sensations. In this manuscript we focus on predictions expressed as General
Value Functions: temporally extended estimates of the accumulation of a future
signal. One challenge is determining from the infinitely many predictions that
the agent could possibly make which might support decision-making. In this
work, we contribute a meta-gradient descent method by which an agent can
directly specify what predictions it learns, independent of designer
instruction. To that end, we introduce a partially observable domain suited to
this investigation. We then demonstrate that through interaction with the
environment an agent can independently select predictions that resolve the
partial-observability, resulting in performance similar to expertly chosen
value functions. By learning, rather than manually specifying these
predictions, we enable the agent to identify useful predictions in a
self-supervised manner, taking a step towards truly autonomous systems.",2022-11-13 16:03:09,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,Shashank Yadav,Machines & Influence: An Information Systems Lens,,,,,http://arxiv.org/abs/2111.13365v4,"Policymakers face a broader challenge of how to view AI capabilities today
and where does society stand in terms of those capabilities. This paper surveys
AI capabilities and tackles this very issue, exploring it in context of
political security in digitally networked societies. We extend the ideas of
Information Management to better understand contemporary AI systems as part of
a larger and more complex information system. Comprehensively reviewing AI
capabilities and contemporary man-machine interactions, we undertake conceptual
development to suggest that better information management could allow states to
more optimally offset the risks of AI enabled influence and better utilise the
emerging capabilities which these systems have to offer to policymakers and
political institutions across the world. Hopefully this long essay will actuate
further debates and discussions over these ideas, and prove to be a useful
contribution towards governing the future of AI.",2022-11-13 16:03:09,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Pablo Villanueva-Domingo, Francisco Villaescusa-Navarro, Shy Genel, Daniel Anglés-Alcázar, Lars Hernquist, Federico Marinacci, David N. Spergel, Mark Vogelsberger, Desika Narayanan",Weighing the Milky Way and Andromeda with Artificial Intelligence,,,,,http://arxiv.org/abs/2111.14874v1,"We present new constraints on the masses of the halos hosting the Milky Way
and Andromeda galaxies derived using graph neural networks. Our models, trained
on thousands of state-of-the-art hydrodynamic simulations of the CAMELS
project, only make use of the positions, velocities and stellar masses of the
galaxies belonging to the halos, and are able to perform likelihood-free
inference on halo masses while accounting for both cosmological and
astrophysical uncertainties. Our constraints are in agreement with estimates
from other traditional methods.",2022-11-13 16:03:10,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Inioluwa Deborah Raji, Emily M. Bender, Amandalynne Paullada, Emily Denton, Alex Hanna",AI and the Everything in the Whole Wide World Benchmark,,,,,http://arxiv.org/abs/2111.15366v1,"There is a tendency across different subfields in AI to valorize a small
collection of influential benchmarks. These benchmarks operate as stand-ins for
a range of anointed common problems that are frequently framed as foundational
milestones on the path towards flexible and generalizable AI systems.
State-of-the-art performance on these benchmarks is widely understood as
indicative of progress towards these long-term goals. In this position paper,
we explore the limits of such benchmarks in order to reveal the construct
validity issues in their framing as the functionally ""general"" broad measures
of progress they are set up to be.",2022-11-13 16:03:10,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Weichao Zhou, Wenchao Li",Programmatic Reward Design by Example,,,,,http://arxiv.org/abs/2112.08438v2,"Reward design is a fundamental problem in reinforcement learning (RL). A
misspecified or poorly designed reward can result in low sample efficiency and
undesired behaviors. In this paper, we propose the idea of programmatic reward
design, i.e. using programs to specify the reward functions in RL environments.
Programs allow human engineers to express sub-goals and complex task scenarios
in a structured and interpretable way. The challenge of programmatic reward
design, however, is that while humans can provide the high-level structures,
properly setting the low-level details, such as the right amount of reward for
a specific sub-task, remains difficult. A major contribution of this paper is a
probabilistic framework that can infer the best candidate programmatic reward
function from expert demonstrations. Inspired by recent generative-adversarial
approaches, our framework searches for the most likely programmatic reward
function under which the optimally generated trajectories cannot be
differentiated from the demonstrated trajectories. Experimental results show
that programmatic reward functionslearned using this framework can
significantly outperform those learned using existing reward learning
algo-rithms, and enable RL agents to achieve state-of-the-artperformance on
highly complex tasks.",2022-11-13 16:03:11,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,Immanuel Trummer,"DB-BERT: a Database Tuning Tool that ""Reads the Manual""",,,,,http://arxiv.org/abs/2112.10925v1,"DB-BERT is a database tuning tool that exploits information gained via
natural language analysis of manuals and other relevant text documents. It uses
text to identify database system parameters to tune as well as recommended
parameter values. DB-BERT applies large, pre-trained language models
(specifically, the BERT model) for text analysis. During an initial training
phase, it fine-tunes model weights in order to translate natural language hints
into recommended settings. At run time, DB-BERT learns to aggregate, adapt, and
prioritize hints to achieve optimal performance for a specific database system
and benchmark. Both phases are iterative and use reinforcement learning to
guide the selection of tuning settings to evaluate (penalizing settings that
the database system rejects while rewarding settings that improve performance).
In our experiments, we leverage hundreds of text documents about database
tuning as input for DB-BERT. We compare DB-BERT against various baselines,
considering different benchmarks (TPC-C and TPC-H), metrics (throughput and run
time), as well as database systems (Postgres and MySQL). In all cases, DB-BERT
finds the best parameter settings among all compared methods. The code of
DB-BERT is available online at https://itrummer.github.io/dbbert/.",2022-11-13 16:03:12,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Wei Ye, Francesco Bullo, Noah Friedkin, Ambuj K Singh",Modeling Human-AI Team Decision Making,,,,,http://arxiv.org/abs/2201.02759v1,"AI and humans bring complementary skills to group deliberations. Modeling
this group decision making is especially challenging when the deliberations
include an element of risk and an exploration-exploitation process of
appraising the capabilities of the human and AI agents. To investigate this
question, we presented a sequence of intellective issues to a set of human
groups aided by imperfect AI agents. A group's goal was to appraise the
relative expertise of the group's members and its available AI agents, evaluate
the risks associated with different actions, and maximize the overall reward by
reaching consensus. We propose and empirically validate models of human-AI team
decision making under such uncertain circumstances, and show the value of
socio-cognitive constructs of prospect theory, influence dynamics, and Bayesian
learning in predicting the behavior of human-AI groups.",2022-11-13 16:03:12,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Alexander Pan, Kush Bhatia, Jacob Steinhardt",The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models,,,,,http://arxiv.org/abs/2201.03544v2,"Reward hacking -- where RL agents exploit gaps in misspecified reward
functions -- has been widely observed, but not yet systematically studied. To
understand how reward hacking arises, we construct four RL environments with
misspecified rewards. We investigate reward hacking as a function of agent
capabilities: model capacity, action space resolution, observation space noise,
and training time. More capable agents often exploit reward misspecifications,
achieving higher proxy reward and lower true reward than less capable agents.
Moreover, we find instances of phase transitions: capability thresholds at
which the agent's behavior qualitatively shifts, leading to a sharp decrease in
the true reward. Such phase transitions pose challenges to monitoring the
safety of ML systems. To address this, we propose an anomaly detection task for
aberrant policies and offer several baseline detectors.",2022-11-13 16:03:13,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Yitzhak Spielberg, Amos Azaria",The Concept of Criticality in AI Safety,,,,,http://arxiv.org/abs/2201.04632v1,"When AI agents don't align their actions with human values they may cause
serious harm. One way to solve the value alignment problem is by including a
human operator who monitors all of the agent's actions. Despite the fact, that
this solution guarantees maximal safety, it is very inefficient, since it
requires the human operator to dedicate all of his attention to the agent. In
this paper, we propose a much more efficient solution that allows an operator
to be engaged in other activities without neglecting his monitoring task. In
our approach the AI agent requests permission from the operator only for
critical actions, that is, potentially harmful actions. We introduce the
concept of critical actions with respect to AI safety and discuss how to build
a model that measures action criticality. We also discuss how the operator's
feedback could be used to make the agent smarter.",2022-11-13 16:03:14,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Yitzhak Spielberg, Amos Azaria",Revelation of Task Difficulty in AI-aided Education,,,,,http://arxiv.org/abs/2201.04633v1,"When a student is asked to perform a given task, her subjective estimate of
the difficulty of that task has a strong influence on her performance. There
exists a rich literature on the impact of perceived task difficulty on
performance and motivation. Yet, there is another topic that is closely related
to the subject of the influence of perceived task difficulty that did not
receive any attention in previous research - the influence of revealing the
true difficulty of a task to the student. This paper investigates the impact of
revealing the task difficulty on the student's performance, motivation,
self-efficacy and subjective task value via an experiment in which workers are
asked to solve matchstick riddles. Furthermore, we discuss how the experiment
results might be relevant for AI-aided education. Specifically, we elaborate on
the question of how a student's learning experience might be improved by
supporting her with two types of AI systems: an AI system that predicts task
difficulty and an AI system that determines when task difficulty should be
revealed and when not.",2022-11-13 16:03:14,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Biplav Srivastava, Tarmo Koppel, Ronak Shah, Owen Bond, Sai Teja Paladi, Rohit Sharma, Austin Hetherington",ULTRA: A Data-driven Approach for Recommending Team Formation in Response to Proposal Calls,,,,,http://arxiv.org/abs/2201.05646v1,"We introduce an emerging AI-based approach and prototype system for assisting
team formation when researchers respond to calls for proposals from funding
agencies. This is an instance of the general problem of building teams when
demand opportunities come periodically and potential members may vary over
time. The novelties of our approach are that we: (a) extract technical skills
needed about researchers and calls from multiple data sources and normalize
them using Natural Language Processing (NLP) techniques, (b) build a prototype
solution based on matching and teaming based on constraints, (c) describe
initial feedback about system from researchers at a University to deploy, and
(d) create and publish a dataset that others can use.",2022-11-13 16:03:15,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Ryan Soklaski, Justin Goodwin, Olivia Brown, Michael Yee, Jason Matterer",Tools and Practices for Responsible AI Engineering,,,,,http://arxiv.org/abs/2201.05647v1,"Responsible Artificial Intelligence (AI) - the practice of developing,
evaluating, and maintaining accurate AI systems that also exhibit essential
properties such as robustness and explainability - represents a multifaceted
challenge that often stretches standard machine learning tooling, frameworks,
and testing methods beyond their limits. In this paper, we present two new
software libraries - hydra-zen and the rAI-toolbox - that address critical
needs for responsible AI engineering. hydra-zen dramatically simplifies the
process of making complex AI applications configurable, and their behaviors
reproducible. The rAI-toolbox is designed to enable methods for evaluating and
enhancing the robustness of AI-models in a way that is scalable and that
composes naturally with other popular ML frameworks. We describe the design
principles and methodologies that make these tools effective, including the use
of property-based testing to bolster the reliability of the tools themselves.
Finally, we demonstrate the composability and flexibility of the tools by
showing how various use cases from adversarial robustness and explainable AI
can be concisely implemented with familiar APIs.",2022-11-13 16:03:15,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Julius Frost, Olivia Watkins, Eric Weiner, Pieter Abbeel, Trevor Darrell, Bryan Plummer, Kate Saenko",Explaining Reinforcement Learning Policies through Counterfactual Trajectories,,,,,http://arxiv.org/abs/2201.12462v2,"In order for humans to confidently decide where to employ RL agents for
real-world tasks, a human developer must validate that the agent will perform
well at test-time. Some policy interpretability methods facilitate this by
capturing the policy's decision making in a set of agent rollouts. However,
even the most informative trajectories of training time behavior may give
little insight into the agent's behavior out of distribution. In contrast, our
method conveys how the agent performs under distribution shifts by showing the
agent's behavior across a wider trajectory distribution. We generate these
trajectories by guiding the agent to more diverse unseen states and showing the
agent's behavior there. In a user study, we demonstrate that our method enables
users to score better than baseline methods on one of two agent validation
tasks.",2022-11-13 16:03:16,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Michael Laskin, Hao Liu, Xue Bin Peng, Denis Yarats, Aravind Rajeswaran, Pieter Abbeel",CIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery,,,,,http://arxiv.org/abs/2202.00161v3,"We introduce Contrastive Intrinsic Control (CIC), an algorithm for
unsupervised skill discovery that maximizes the mutual information between
state-transitions and latent skill vectors. CIC utilizes contrastive learning
between state-transitions and skills to learn behavior embeddings and maximizes
the entropy of these embeddings as an intrinsic reward to encourage behavioral
diversity. We evaluate our algorithm on the Unsupervised Reinforcement Learning
Benchmark, which consists of a long reward-free pre-training phase followed by
a short adaptation phase to downstream tasks with extrinsic rewards. CIC
substantially improves over prior methods in terms of adaptation efficiency,
outperforming prior unsupervised skill discovery methods by 1.79x and the next
leading overall exploration algorithm by 1.18x.",2022-11-13 16:03:17,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,Ben Green,Technology Ethics in Action: Critical and Interdisciplinary Perspectives,Special Issue of the Journal of Social Computing (2021),,,,http://arxiv.org/abs/2202.01351v2,"This special issue interrogates the meaning and impacts of ""tech ethics"": the
embedding of ethics into digital technology research, development, use, and
governance. In response to concerns about the social harms associated with
digital technologies, many individuals and institutions have articulated the
need for a greater emphasis on ethics in digital technology. Yet as more groups
embrace the concept of ethics, critical discourses have emerged questioning
whose ethics are being centered, whether ""ethics"" is the appropriate frame for
improving technology, and what it means to develop ""ethical"" technology in
practice. This interdisciplinary issue takes up these questions, interrogating
the relationships among ethics, technology, and society in action. This special
issue engages with the normative and contested notions of ethics itself, how
ethics has been integrated with technology across domains, and potential paths
forward to support more just and egalitarian technology. Rather than starting
from philosophical theories, the authors in this issue orient their articles
around the real-world discourses and impacts of tech ethics--i.e., tech ethics
in action.",2022-11-13 16:03:17,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,Remy Demichelis,Science Facing Interoperability as a Necessary Condition of Success and Evil,,,,,http://arxiv.org/abs/2202.02540v1,"Artificial intelligence (AI) systems, such as machine learning algorithms,
have allowed scientists, marketers and governments to shed light on
correlations that remained invisible until now. Beforehand, the dots that we
had to connect in order to imagine a new knowledge were either too numerous,
too sparse or not even detected. Sometimes, the information was not stored in
the same data lake or format and was not able to communicate. But in creating
new bridges with AI, many problems appeared such as bias reproduction, unfair
inferences or mass surveillance. Our aim is to show that, on one hand, the AI's
deep ethical problem lays essentially in these new connections made possible by
systems interoperability. In connecting the spheres of our life, these systems
undermine the notion of justice particular to each of them, because the new
interactions create dominances of social goods from a sphere to another. These
systems make therefore spheres permeable to one another and, in doing so, they
open to progress as well as to tyranny. On another hand, however, we would like
to emphasize that the act to connect what used to seem a priori disjoint is a
necessary move of knowledge and scientific progress.",2022-11-13 16:03:18,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Ronny Luss, Amit Dhurandhar, Miao Liu",Local Explanations for Reinforcement Learning,,,,,http://arxiv.org/abs/2202.03597v1,"Many works in explainable AI have focused on explaining black-box
classification models. Explaining deep reinforcement learning (RL) policies in
a manner that could be understood by domain users has received much less
attention. In this paper, we propose a novel perspective to understanding RL
policies based on identifying important states from automatically learned
meta-states. The key conceptual difference between our approach and many
previous ones is that we form meta-states based on locality governed by the
expert policy dynamics rather than based on similarity of actions, and that we
do not assume any particular knowledge of the underlying topology of the state
space. Theoretically, we show that our algorithm to find meta-states converges
and the objective that selects important states from each meta-state is
submodular leading to efficient high quality greedy selection. Experiments on
four domains (four rooms, door-key, minipacman, and pong) and a carefully
conducted user study illustrate that our perspective leads to better
understanding of the policy. We conjecture that this is a result of our
meta-states being more intuitive in that the corresponding important states are
strong indicators of tractable intermediate goals that are easier for humans to
interpret and follow.",2022-11-13 16:03:18,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Qinqing Zheng, Amy Zhang, Aditya Grover",Online Decision Transformer,,,,,http://arxiv.org/abs/2202.05607v2,"Recent work has shown that offline reinforcement learning (RL) can be
formulated as a sequence modeling problem (Chen et al., 2021; Janner et al.,
2021) and solved via approaches similar to large-scale language modeling.
However, any practical instantiation of RL also involves an online component,
where policies pretrained on passive offline datasets are finetuned via
taskspecific interactions with the environment. We propose Online Decision
Transformers (ODT), an RL algorithm based on sequence modeling that blends
offline pretraining with online finetuning in a unified framework. Our
framework uses sequence-level entropy regularizers in conjunction with
autoregressive modeling objectives for sample-efficient exploration and
finetuning. Empirically, we show that ODT is competitive with the
state-of-the-art in absolute performance on the D4RL benchmark but shows much
more significant gains during the finetuning procedure.",2022-11-13 16:03:19,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,Christoph Benzmüller,A Simplified Variant of Gödel's Ontological Argument,,,,,http://arxiv.org/abs/2202.06264v2,"A simplified variant of G\""odel's ontological argument is presented. The
simplified argument is valid already in basic modal logics K or KT, it does not
suffer from modal collapse, and it avoids the rather complex predicates of
essence (Ess.) and necessary existence (NE) as used by G\""odel. The variant
presented has been obtained as a side result of a series of theory
simplification experiments conducted in interaction with a modern proof
assistant system. The starting point for these experiments was the computer
encoding of G\""odel's argument, and then automated reasoning techniques were
systematically applied to arrive at the simplified variant presented. The
presented work thus exemplifies a fruitful human-computer interaction in
computational metaphysics. Whether the presented result increases or decreases
the attractiveness and persuasiveness of the ontological argument is a question
I would like to pass on to philosophy and theology.",2022-11-13 16:03:20,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Sebastiaan De Peuter, Samuel Kaski",Zero-Shot Assistance in Novel Decision Problems,,,,,http://arxiv.org/abs/2202.07364v2,"We consider the problem of creating assistants that can help agents - often
humans - solve novel sequential decision problems, assuming the agent is not
able to specify the reward function explicitly to the assistant. Instead of
aiming to automate, and act in place of the agent as in current approaches, we
give the assistant an advisory role and keep the agent in the loop as the main
decision maker. The difficulty is that we must account for potential biases
induced by limitations or constraints of the agent which may cause it to
seemingly irrationally reject advice. To do this we introduce a novel
formalization of assistance that models these biases, allowing the assistant to
infer and adapt to them. We then introduce a new method for planning the
assistant's advice which can scale to large decision making problems. Finally,
we show experimentally that our approach adapts to these agent biases, and
results in higher cumulative reward for the agent than automation-based
alternatives.",2022-11-13 16:03:20,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Kanak Tekwani, Manojkumar Parmar",Critical Checkpoints for Evaluating Defence Models Against Adversarial Attack and Robustness,,,,,http://arxiv.org/abs/2202.09039v1,"From past couple of years there is a cycle of researchers proposing a defence
model for adversaries in machine learning which is arguably defensible to most
of the existing attacks in restricted condition (they evaluate on some bounded
inputs or datasets). And then shortly another set of researcher finding the
vulnerabilities in that defence model and breaking it by proposing a stronger
attack model. Some common flaws are been noticed in the past defence models
that were broken in very short time. Defence models being broken so easily is a
point of concern as decision of many crucial activities are taken with the help
of machine learning models. So there is an utter need of some defence
checkpoints that any researcher should keep in mind while evaluating the
soundness of technique and declaring it to be decent defence technique. In this
paper, we have suggested few checkpoints that should be taken into
consideration while building and evaluating the soundness of defence models.
All these points are recommended after observing why some past defence models
failed and how some model remained adamant and proved their soundness against
some of the very strong attacks.",2022-11-13 16:03:21,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Alihan Hüyük, William R. Zame, Mihaela van der Schaar",Inferring Lexicographically-Ordered Rewards from Preferences,,,,,http://arxiv.org/abs/2202.10153v2,"Modeling the preferences of agents over a set of alternatives is a principal
concern in many areas. The dominant approach has been to find a single
reward/utility function with the property that alternatives yielding higher
rewards are preferred over alternatives yielding lower rewards. However, in
many settings, preferences are based on multiple, often competing, objectives;
a single reward function is not adequate to represent such preferences. This
paper proposes a method for inferring multi-objective reward-based
representations of an agent's observed preferences. We model the agent's
priorities over different objectives as entering lexicographically, so that
objectives with lower priorities matter only when the agent is indifferent with
respect to objectives with higher priorities. We offer two example applications
in healthcare, one inspired by cancer treatment, the other inspired by organ
transplantation, to illustrate how the lexicographically-ordered rewards we
learn can provide a better understanding of a decision-maker's preferences and
help improve policies when used in reinforcement learning.",2022-11-13 16:03:21,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Kai Arulkumaran, Dylan R. Ashley, Jürgen Schmidhuber, Rupesh K. Srivastava",All You Need Is Supervised Learning: From Imitation Learning to Meta-RL With Upside Down RL,,,,,http://arxiv.org/abs/2202.11960v1,"Upside down reinforcement learning (UDRL) flips the conventional use of the
return in the objective function in RL upside down, by taking returns as input
and predicting actions. UDRL is based purely on supervised learning, and
bypasses some prominent issues in RL: bootstrapping, off-policy corrections,
and discount factors. While previous work with UDRL demonstrated it in a
traditional online RL setting, here we show that this single algorithm can also
work in the imitation learning and offline RL settings, be extended to the
goal-conditioned RL setting, and even the meta-RL setting. With a general agent
architecture, a single UDRL agent can learn across all paradigms.",2022-11-13 16:03:22,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Ali Furkan Biten, Rubèn Tito, Lluis Gomez, Ernest Valveny, Dimosthenis Karatzas",OCR-IDL: OCR Annotations for Industry Document Library Dataset,,,,,http://arxiv.org/abs/2202.12985v1,"Pretraining has proven successful in Document Intelligence tasks where deluge
of documents are used to pretrain the models only later to be finetuned on
downstream tasks. One of the problems of the pretraining approaches is the
inconsistent usage of pretraining data with different OCR engines leading to
incomparable results between models. In other words, it is not obvious whether
the performance gain is coming from diverse usage of amount of data and
distinct OCR engines or from the proposed models. To remedy the problem, we
make public the OCR annotations for IDL documents using commercial OCR engine
given their superior performance over open source OCR models. The contributed
dataset (OCR-IDL) has an estimated monetary value over 20K US$. It is our hope
that OCR-IDL can be a starting point for future works on Document Intelligence.
All of our data and its collection process with the annotations can be found in
https://github.com/furkanbiten/idl_data.",2022-11-13 16:03:23,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Hongzhi Wen, Jiayuan Ding, Wei Jin, Yiqi Wang, Yuying Xie, Jiliang Tang",Graph Neural Networks for Multimodal Single-Cell Data Integration,,,,10.1145/3534678.3539213,http://arxiv.org/abs/2203.01884v3,"Recent advances in multimodal single-cell technologies have enabled
simultaneous acquisitions of multiple omics data from the same cell, providing
deeper insights into cellular states and dynamics. However, it is challenging
to learn the joint representations from the multimodal data, model the
relationship between modalities, and, more importantly, incorporate the vast
amount of single-modality datasets into the downstream analyses. To address
these challenges and correspondingly facilitate multimodal single-cell data
analyses, three key tasks have been introduced: $\textit{modality prediction}$,
$\textit{modality matching}$ and $\textit{joint embedding}$. In this work, we
present a general Graph Neural Network framework $\textit{scMoGNN}$ to tackle
these three tasks and show that $\textit{scMoGNN}$ demonstrates superior
results in all three tasks compared with the state-of-the-art and conventional
approaches. Our method is an official winner in the overall ranking of
$\textit{Modality prediction}$ from NeurIPS 2021 Competition, and all
implementations of our methods have been integrated into DANCE
package~\url{https://github.com/OmicsML/dance}.",2022-11-13 16:03:23,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Armin Moin, Ukrit Wattanavaekin, Alexandra Lungu, Moharram Challenger, Atta Badii, Stephan Günnemann",Enabling Automated Machine Learning for Model-Driven AI Engineering,,,,,http://arxiv.org/abs/2203.02927v1,"Developing smart software services requires both Software Engineering and
Artificial Intelligence (AI) skills. AI practitioners, such as data scientists
often focus on the AI side, for example, creating and training Machine Learning
(ML) models given a specific use case and data. They are typically not
concerned with the entire software development life-cycle, architectural
decisions for the system and performance issues beyond the predictive ML models
(e.g., regarding the security, privacy, throughput, scalability, availability,
as well as ethical, legal and regulatory compliance). In this manuscript, we
propose a novel approach to enable Model-Driven Software Engineering and
Model-Driven AI Engineering. In particular, we support Automated ML, thus
assisting software engineers without deep AI knowledge in developing
AI-intensive systems by choosing the most appropriate ML model, algorithm and
techniques with suitable hyper-parameters for the task at hand. To validate our
work, we carry out a case study in the smart energy domain.",2022-11-13 16:03:24,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Felix Friedrich, Wolfgang Stammer, Patrick Schramowski, Kristian Kersting",A Typology to Explore the Mitigation of Shortcut Behavior,,,,,http://arxiv.org/abs/2203.03668v2,"As machine learning models become increasingly larger, trained weakly
supervised on large, possibly uncurated data sets, it becomes increasingly
important to establish mechanisms for inspecting, interacting, and revising
models to mitigate learning shortcuts and guarantee their learned knowledge is
aligned with human knowledge. The recently proposed XIL framework was developed
for this purpose, and several such methods have been introduced, each with
individual motivations and methodological details. In this work, we provide a
unification of various XIL methods into a single typology by establishing a
common set of basic modules. In doing so, we pave the way for a principled
comparison of existing, but, importantly, also future XIL approaches. In
addition, we discuss existing and introduce novel measures and benchmarks for
evaluating the overall abilities of a XIL method. Given this extensive toolbox,
including our typology, measures, and benchmarks, we finally compare several
recent XIL methods methodologically and quantitatively. In our evaluations, all
methods prove to revise a model successfully. However, we found remarkable
differences in individual benchmark tasks, revealing valuable
application-relevant aspects for integrating these benchmarks in developing
future methods.",2022-11-13 16:03:25,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Yuan Gong, Sameer Khurana, Andrew Rouditchenko, James Glass",CMKD: CNN/Transformer-Based Cross-Model Knowledge Distillation for Audio Classification,,,,,http://arxiv.org/abs/2203.06760v1,"Audio classification is an active research area with a wide range of
applications. Over the past decade, convolutional neural networks (CNNs) have
been the de-facto standard building block for end-to-end audio classification
models. Recently, neural networks based solely on self-attention mechanisms
such as the Audio Spectrogram Transformer (AST) have been shown to outperform
CNNs. In this paper, we find an intriguing interaction between the two very
different models - CNN and AST models are good teachers for each other. When we
use either of them as the teacher and train the other model as the student via
knowledge distillation (KD), the performance of the student model noticeably
improves, and in many cases, is better than the teacher model. In our
experiments with this CNN/Transformer Cross-Model Knowledge Distillation (CMKD)
method we achieve new state-of-the-art performance on FSD50K, AudioSet, and
ESC-50.",2022-11-13 16:03:26,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Michael Bohlke-Schneider, Shubham Kapoor, Tim Januschowski",Resilient Neural Forecasting Systems,,,,,http://arxiv.org/abs/2203.08492v1,"Industrial machine learning systems face data challenges that are often
under-explored in the academic literature. Common data challenges are data
distribution shifts, missing values and anomalies. In this paper, we discuss
data challenges and solutions in the context of a Neural Forecasting
application on labor planning.We discuss how to make this forecasting system
resilient to these data challenges. We address changes in data distribution
with a periodic retraining scheme and discuss the critical importance of model
stability in this setting. Furthermore, we show how our deep learning model
deals with missing values natively without requiring imputation. Finally, we
describe how we detect anomalies in the input data and mitigate their effect
before they impact the forecasts. This results in a fully autonomous
forecasting system that compares favorably to a hybrid system consisting of the
algorithm and human overrides.",2022-11-13 16:03:26,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Qinghua Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Zhenchang Xing",Towards a Roadmap on Software Engineering for Responsible AI,,,,,http://arxiv.org/abs/2203.08594v1,"Although AI is transforming the world, there are serious concerns about its
ability to behave and make decisions responsibly. Many ethical regulations,
principles, and frameworks for responsible AI have been issued recently.
However, they are high level and difficult to put into practice. On the other
hand, most AI researchers focus on algorithmic solutions, while the responsible
AI challenges actually crosscut the entire engineering lifecycle and components
of AI systems. To close the gap in operationalizing responsible AI, this paper
aims to develop a roadmap on software engineering for responsible AI. The
roadmap focuses on (i) establishing multi-level governance for responsible AI
systems, (ii) setting up the development processes incorporating
process-oriented practices for responsible AI systems, and (iii) building
responsible-AI-by-design into AI systems through system-level architectural
style, patterns and techniques.",2022-11-13 16:03:27,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Zhe Zhang, Yaozhong Gan, Xiaoyang Tan",Robust Action Gap Increasing with Clipped Advantage Learning,,,,,http://arxiv.org/abs/2203.11677v1,"Advantage Learning (AL) seeks to increase the action gap between the optimal
action and its competitors, so as to improve the robustness to estimation
errors. However, the method becomes problematic when the optimal action induced
by the approximated value function does not agree with the true optimal action.
In this paper, we present a novel method, named clipped Advantage Learning
(clipped AL), to address this issue. The method is inspired by our observation
that increasing the action gap blindly for all given samples while not taking
their necessities into account could accumulate more errors in the performance
loss bound, leading to a slow value convergence, and to avoid that, we should
adjust the advantage value adaptively. We show that our simple clipped AL
operator not only enjoys fast convergence guarantee but also retains proper
action gaps, hence achieving a good balance between the large action gap and
the fast convergence. The feasibility and effectiveness of the proposed method
are verified empirically on several RL benchmarks with promising performance.",2022-11-13 16:03:27,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Markus Borg, Johan Bengtsson, Harald Österling, Alexander Hagelborn, Isabella Gagner, Piotr Tomaszewski",Quality Assurance of Generative Dialog Models in an Evolving Conversational Agent Used for Swedish Language Practice,,,,,http://arxiv.org/abs/2203.15414v1,"Due to the migration megatrend, efficient and effective second-language
acquisition is vital. One proposed solution involves AI-enabled conversational
agents for person-centered interactive language practice. We present results
from ongoing action research targeting quality assurance of proprietary
generative dialog models trained for virtual job interviews. The action team
elicited a set of 38 requirements for which we designed corresponding automated
test cases for 15 of particular interest to the evolving solution. Our results
show that six of the test case designs can detect meaningful differences
between candidate models. While quality assurance of natural language
processing applications is complex, we provide initial steps toward an
automated framework for machine learning model selection in the context of an
evolving conversational agent. Future work will focus on model selection in an
MLOps setting.",2022-11-13 16:03:28,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Daniel Jarne Ornia, Manuel Mazo Jr",Robust Event-Driven Interactions in Cooperative Multi-Agent Learning,"Formal Modeling and Analysis of Timed Systems. FORMATS 2022.
  Lecture Notes in Computer Science, vol 13465. Springer, Cham",,,10.1007/978-3-031-15839-1_16,http://arxiv.org/abs/2204.03361v2,"We present an approach to reduce the communication required between agents in
a Multi-Agent learning system by exploiting the inherent robustness of the
underlying Markov Decision Process. We compute so-called robustness surrogate
functions (off-line), that give agents a conservative indication of how far
their state measurements can deviate before they need to update other agents in
the system. This results in fully distributed decision functions, enabling
agents to decide when it is necessary to update others. We derive bounds on the
optimality of the resulting systems in terms of the discounted sum of rewards
obtained, and show these bounds are a function of the design parameters.
Additionally, we extend the results for the case where the robustness surrogate
functions are learned from data, and present experimental results demonstrating
a significant reduction in communication events between agents.",2022-11-13 16:03:28,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Chao Li, Jia Ning, Han Hu, Kun He","Enhancing the Robustness, Efficiency, and Diversity of Differentiable Architecture Search",,,,,http://arxiv.org/abs/2204.04681v1,"Differentiable architecture search (DARTS) has attracted much attention due
to its simplicity and significant improvement in efficiency. However, the
excessive accumulation of the skip connection makes it suffer from long-term
weak stability and low robustness. Many works attempt to restrict the
accumulation of skip connections by indicators or manual design, however, these
methods are susceptible to thresholds and human priors. In this work, we
suggest a more subtle and direct approach that removes skip connections from
the operation space. Then, by introducing an adaptive channel allocation
strategy, we redesign the DARTS framework to automatically refill the skip
connections in the evaluation stage, resolving the performance degradation
caused by the absence of skip connections. Our method, dubbed
Adaptive-Channel-Allocation-DARTS (ACA-DRATS), could eliminate the
inconsistency in operation strength and significantly expand the architecture
diversity. We continue to explore smaller search space under our framework, and
offer a direct search on the entire ImageNet dataset. Experiments show that
ACA-DRATS improves the search stability and significantly speeds up DARTS by
more than ten times while yielding higher accuracy.",2022-11-13 16:03:29,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Fu-Chieh Chang, Yu-Wei Tseng, Ya-Wen Yu, Ssu-Rui Lee, Alexandru Cioba, I-Lun Tseng, Da-shan Shiu, Jhih-Wei Hsu, Cheng-Yuan Wang, Chien-Yi Yang, Ren-Chu Wang, Yao-Wen Chang, Tai-Chen Chen, Tung-Chieh Chen",Flexible Multiple-Objective Reinforcement Learning for Chip Placement,,,,,http://arxiv.org/abs/2204.06407v1,"Recently, successful applications of reinforcement learning to chip placement
have emerged. Pretrained models are necessary to improve efficiency and
effectiveness. Currently, the weights of objective metrics (e.g., wirelength,
congestion, and timing) are fixed during pretraining. However, fixed-weighed
models cannot generate the diversity of placements required for engineers to
accommodate changing requirements as they arise. This paper proposes flexible
multiple-objective reinforcement learning (MORL) to support objective functions
with inference-time variable weights using just a single pretrained model. Our
macro placement results show that MORL can generate the Pareto frontier of
multiple objectives effectively.",2022-11-13 16:03:29,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Sahir, Ercüment İlhan, Srijita Das, Matthew E. Taylor",Methodical Advice Collection and Reuse in Deep Reinforcement Learning,,,,,http://arxiv.org/abs/2204.07254v1,"Reinforcement learning (RL) has shown great success in solving many
challenging tasks via use of deep neural networks. Although using deep learning
for RL brings immense representational power, it also causes a well-known
sample-inefficiency problem. This means that the algorithms are data-hungry and
require millions of training samples to converge to an adequate policy. One way
to combat this issue is to use action advising in a teacher-student framework,
where a knowledgeable teacher provides action advice to help the student. This
work considers how to better leverage uncertainties about when a student should
ask for advice and if the student can model the teacher to ask for less advice.
The student could decide to ask for advice when it is uncertain or when both it
and its model of the teacher are uncertain. In addition to this investigation,
this paper introduces a new method to compute uncertainty for a deep RL agent
using a secondary neural network. Our empirical results show that using dual
uncertainties to drive advice collection and reuse may improve learning
performance across several Atari games.",2022-11-13 16:03:30,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Anna Yeaton, Rahul G. Krishnan, Rebecca Mieloszyk, David Alvarez-Melis, Grace Huynh",Hierarchical Optimal Transport for Comparing Histopathology Datasets,,,,,http://arxiv.org/abs/2204.08324v2,"Scarcity of labeled histopathology data limits the applicability of deep
learning methods to under-profiled cancer types and labels. Transfer learning
allows researchers to overcome the limitations of small datasets by
pre-training machine learning models on larger datasets similar to the small
target dataset. However, similarity between datasets is often determined
heuristically. In this paper, we propose a principled notion of distance
between histopathology datasets based on a hierarchical generalization of
optimal transport distances. Our method does not require any training, is
agnostic to model type, and preserves much of the hierarchical structure in
histopathology datasets imposed by tiling. We apply our method to H&E stained
slides from The Cancer Genome Atlas from six different cancer types. We show
that our method outperforms a baseline distance in a cancer-type prediction
task. Our results also show that our optimal transport distance predicts
difficulty of transferability in a tumor vs.normal prediction setting.",2022-11-13 16:03:31,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Anton Korinek, Avital Balwit",Aligned with Whom? Direct and social goals for AI systems,,,,,http://arxiv.org/abs/2205.04279v1,"As artificial intelligence (AI) becomes more powerful and widespread, the AI
alignment problem - how to ensure that AI systems pursue the goals that we want
them to pursue - has garnered growing attention. This article distinguishes two
types of alignment problems depending on whose goals we consider, and analyzes
the different solutions necessitated by each. The direct alignment problem
considers whether an AI system accomplishes the goals of the entity operating
it. In contrast, the social alignment problem considers the effects of an AI
system on larger groups or on society more broadly. In particular, it also
considers whether the system imposes externalities on others. Whereas solutions
to the direct alignment problem center around more robust implementation,
social alignment problems typically arise because of conflicts between
individual and group-level goals, elevating the importance of AI governance to
mediate such conflicts. Addressing the social alignment problem requires both
enforcing existing norms on their developers and operators and designing new
norms that apply directly to AI systems.",2022-11-13 16:03:31,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,Virginia Dignum,Responsible Artificial Intelligence -- from Principles to Practice,,,,,http://arxiv.org/abs/2205.10785v1,"The impact of Artificial Intelligence does not depend only on fundamental
research and technological developments, but for a large part on how these
systems are introduced into society and used in everyday situations. AI is
changing the way we work, live and solve challenges but concerns about
fairness, transparency or privacy are also growing. Ensuring responsible,
ethical AI is more than designing systems whose result can be trusted. It is
about the way we design them, why we design them, and who is involved in
designing them. In order to develop and use AI responsibly, we need to work
towards technical, societal, institutional and legal methods and tools which
provide concrete support to AI practitioners, as well as awareness and training
to enable participation of all, to ensure the alignment of AI systems with our
societies' principles and values.",2022-11-13 16:03:32,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Giovanni De Toni, Paolo Viappiani, Bruno Lepri, Andrea Passerini",Generating personalized counterfactual interventions for algorithmic recourse by eliciting user preferences,,,,,http://arxiv.org/abs/2205.13743v1,"Counterfactual interventions are a powerful tool to explain the decisions of
a black-box decision process, and to enable algorithmic recourse. They are a
sequence of actions that, if performed by a user, can overturn an unfavourable
decision made by an automated decision system. However, most of the current
methods provide interventions without considering the user's preferences. For
example, a user might prefer doing certain actions with respect to others. In
this work, we present the first human-in-the-loop approach to perform
algorithmic recourse by eliciting user preferences. We introduce a polynomial
procedure to ask choice-set questions which maximize the Expected Utility of
Selection (EUS), and use it to iteratively refine our cost estimates in a
Bayesian setting. We integrate this preference elicitation strategy into a
reinforcement learning agent coupled with Monte Carlo Tree Search for efficient
exploration, so as to provide personalized interventions achieving algorithmic
recourse. An experimental evaluation on synthetic and real-world datasets shows
that a handful of queries allows to achieve a substantial reduction in the cost
of interventions with respect to user-independent alternatives.",2022-11-13 16:03:32,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Xuan Bac Nguyen, Apoorva Bisht, Hugh Churchill, Khoa Luu",Two-Dimensional Quantum Material Identification via Self-Attention and Soft-labeling in Deep Learning,,,,,http://arxiv.org/abs/2205.15948v1,"In quantum machine field, detecting two-dimensional (2D) materials in Silicon
chips is one of the most critical problems. Instance segmentation can be
considered as a potential approach to solve this problem. However, similar to
other deep learning methods, the instance segmentation requires a large scale
training dataset and high quality annotation in order to achieve a considerable
performance. In practice, preparing the training dataset is a challenge since
annotators have to deal with a large image, e.g 2K resolution, and extremely
dense objects in this problem. In this work, we present a novel method to
tackle the problem of missing annotation in instance segmentation in 2D quantum
material identification. We propose a new mechanism for automatically detecting
false negative objects and an attention based loss strategy to reduce the
negative impact of these objects contributing to the overall loss function. We
experiment on the 2D material detection datasets, and the experiments show our
method outperforms previous works.",2022-11-13 16:03:33,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Omer Antverg, Eyal Ben-David, Yonatan Belinkov",IDANI: Inference-time Domain Adaptation via Neuron-level Interventions,,,,,http://arxiv.org/abs/2206.00259v1,"Large pre-trained models are usually fine-tuned on downstream task data, and
tested on unseen data. When the train and test data come from different
domains, the model is likely to struggle, as it is not adapted to the test
domain. We propose a new approach for domain adaptation (DA), using
neuron-level interventions: We modify the representation of each test example
in specific neurons, resulting in a counterfactual example from the source
domain, which the model is more familiar with. The modified example is then fed
back into the model. While most other DA methods are applied during training
time, ours is applied during inference only, making it more efficient and
applicable. Our experiments show that our method improves performance on unseen
domains.",2022-11-13 16:03:33,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Tero Karras, Miika Aittala, Timo Aila, Samuli Laine",Elucidating the Design Space of Diffusion-Based Generative Models,,,,,http://arxiv.org/abs/2206.00364v2,"We argue that the theory and practice of diffusion-based generative models
are currently unnecessarily convoluted and seek to remedy the situation by
presenting a design space that clearly separates the concrete design choices.
This lets us identify several changes to both the sampling and training
processes, as well as preconditioning of the score networks. Together, our
improvements yield new state-of-the-art FID of 1.79 for CIFAR-10 in a
class-conditional setting and 1.97 in an unconditional setting, with much
faster sampling (35 network evaluations per image) than prior designs. To
further demonstrate their modular nature, we show that our design changes
dramatically improve both the efficiency and quality obtainable with
pre-trained score networks from previous work, including improving the FID of a
previously trained ImageNet-64 model from 2.07 to near-SOTA 1.55, and after
re-training with our proposed improvements to a new SOTA of 1.36.",2022-11-13 16:03:34,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Bao-Sinh Nguyen, Quang-Bach Tran, Tuan-Anh Nguyen Dang, Duc Nguyen, Hung Le",HYCEDIS: HYbrid Confidence Engine for Deep Document Intelligence System,,,,,http://arxiv.org/abs/2206.02628v2,"Measuring the confidence of AI models is critical for safely deploying AI in
real-world industrial systems. One important application of confidence
measurement is information extraction from scanned documents. However, there
exists no solution to provide reliable confidence score for current
state-of-the-art deep-learning-based information extractors. In this paper, we
propose a complete and novel architecture to measure confidence of current deep
learning models in document information extraction task. Our architecture
consists of a Multi-modal Conformal Predictor and a Variational
Cluster-oriented Anomaly Detector, trained to faithfully estimate its
confidence on its outputs without the need of host models modification. We
evaluate our architecture on real-wold datasets, not only outperforming
competing confidence estimators by a huge margin but also demonstrating
generalization ability to out-of-distribution data.",2022-11-13 16:03:35,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Benjamin Eysenbach, Soumith Udatha, Sergey Levine, Ruslan Salakhutdinov",Imitating Past Successes can be Very Suboptimal,,,,,http://arxiv.org/abs/2206.03378v1,"Prior work has proposed a simple strategy for reinforcement learning (RL):
label experience with the outcomes achieved in that experience, and then
imitate the relabeled experience. These outcome-conditioned imitation learning
methods are appealing because of their simplicity, strong performance, and
close ties with supervised learning. However, it remains unclear how these
methods relate to the standard RL objective, reward maximization. In this
paper, we prove that existing outcome-conditioned imitation learning methods do
not necessarily improve the policy; rather, in some settings they can decrease
the expected reward. Nonetheless, we show that a simple modification results in
a method that does guarantee policy improvement, under some assumptions. Our
aim is not to develop an entirely new method, but rather to explain how a
variant of outcome-conditioned imitation learning can be used to maximize
rewards.",2022-11-13 16:03:35,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Chengyang Ying, Xinning Zhou, Hang Su, Dong Yan, Ning Chen, Jun Zhu",Towards Safe Reinforcement Learning via Constraining Conditional Value-at-Risk,IJCAI 2022,,,,http://arxiv.org/abs/2206.04436v2,"Though deep reinforcement learning (DRL) has obtained substantial success, it
may encounter catastrophic failures due to the intrinsic uncertainty of both
transition and observation. Most of the existing methods for safe reinforcement
learning can only handle transition disturbance or observation disturbance
since these two kinds of disturbance affect different parts of the agent;
besides, the popular worst-case return may lead to overly pessimistic policies.
To address these issues, we first theoretically prove that the performance
degradation under transition disturbance and observation disturbance depends on
a novel metric of Value Function Range (VFR), which corresponds to the gap in
the value function between the best state and the worst state. Based on the
analysis, we adopt conditional value-at-risk (CVaR) as an assessment of risk
and propose a novel reinforcement learning algorithm of
CVaR-Proximal-Policy-Optimization (CPPO) which formalizes the risk-sensitive
constrained optimization problem by keeping its CVaR under a given threshold.
Experimental results show that CPPO achieves a higher cumulative reward and is
more robust against both observation and transition disturbances on a series of
continuous control tasks in MuJoCo.",2022-11-13 16:03:36,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Yakov Miron, Chana Ross, Yuval Goldfracht, Chen Tessler, Dotan Di Castro",Towards Autonomous Grading In The Real World,,,,,http://arxiv.org/abs/2206.06091v2,"In this work, we aim to tackle the problem of autonomous grading, where a
dozer is required to flatten an uneven area. In addition, we explore methods
for bridging the gap between a simulated environment and real scenarios. We
design both a realistic physical simulation and a scaled real prototype
environment mimicking the real dozer dynamics and sensory information. We
establish heuristics and learning strategies in order to solve the problem.
Through extensive experimentation, we show that although heuristics are capable
of tackling the problem in a clean and noise-free simulated environment, they
fail catastrophically when facing real world scenarios. As the heuristics are
capable of successfully solving the task in the simulated environment, we show
they can be leveraged to guide a learning agent which can generalize and solve
the task both in simulation and in a scaled prototype environment.",2022-11-13 16:03:36,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Maribeth Rauh, John Mellor, Jonathan Uesato, Po-Sen Huang, Johannes Welbl, Laura Weidinger, Sumanth Dathathri, Amelia Glaese, Geoffrey Irving, Iason Gabriel, William Isaac, Lisa Anne Hendricks",Characteristics of Harmful Text: Towards Rigorous Benchmarking of Language Models,,,,,http://arxiv.org/abs/2206.08325v2,"Large language models produce human-like text that drive a growing number of
applications. However, recent literature and, increasingly, real world
observations, have demonstrated that these models can generate language that is
toxic, biased, untruthful or otherwise harmful. Though work to evaluate
language model harms is under way, translating foresight about which harms may
arise into rigorous benchmarks is not straightforward. To facilitate this
translation, we outline six ways of characterizing harmful text which merit
explicit consideration when designing new benchmarks. We then use these
characteristics as a lens to identify trends and gaps in existing benchmarks.
Finally, we apply them in a case study of the Perspective API, a toxicity
classifier that is widely used in harm benchmarks. Our characteristics provide
one piece of the bridge that translates between foresight and effective
evaluation.",2022-11-13 16:03:37,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Tengyang Xie, Akanksha Saran, Dylan J. Foster, Lekan Molu, Ida Momennejad, Nan Jiang, Paul Mineiro, John Langford",Interaction-Grounded Learning with Action-inclusive Feedback,,,,,http://arxiv.org/abs/2206.08364v2,"Consider the problem setting of Interaction-Grounded Learning (IGL), in which
a learner's goal is to optimally interact with the environment with no explicit
reward to ground its policies. The agent observes a context vector, takes an
action, and receives a feedback vector, using this information to effectively
optimize a policy with respect to a latent reward function. Prior analyzed
approaches fail when the feedback vector contains the action, which
significantly limits IGL's success in many potential scenarios such as
Brain-computer interface (BCI) or Human-computer interface (HCI) applications.
We address this by creating an algorithm and analysis which allows IGL to work
even when the feedback vector contains the action, encoded in any fashion. We
provide theoretical guarantees and large-scale experiments based on supervised
datasets to demonstrate the effectiveness of the new approach.",2022-11-13 16:03:38,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Jean-Stanislas Denain, Jacob Steinhardt",Auditing Visualizations: Transparency Methods Struggle to Detect Anomalous Behavior,,,,,http://arxiv.org/abs/2206.13498v1,"Transparency methods such as model visualizations provide information that
outputs alone might miss, since they describe the internals of neural networks.
But can we trust that model explanations reflect model behavior? For instance,
can they diagnose abnormal behavior such as backdoors or shape bias? To
evaluate model explanations, we define a model as anomalous if it differs from
a reference set of normal models, and we test whether transparency methods
assign different explanations to anomalous and normal models. We find that
while existing methods can detect stark anomalies such as shape bias or
adversarial training, they struggle to identify more subtle anomalies such as
models trained on incomplete data. Moreover, they generally fail to distinguish
the inputs that induce anomalous behavior, e.g. images containing a backdoor
trigger. These results reveal new blind spots in existing model explanations,
pointing to the need for further method development.",2022-11-13 16:03:38,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"James McCarthy, Rahul Nair, Elizabeth Daly, Radu Marinescu, Ivana Dusparic",Boolean Decision Rules for Reinforcement Learning Policy Summarisation,,,,,http://arxiv.org/abs/2207.08651v1,"Explainability of Reinforcement Learning (RL) policies remains a challenging
research problem, particularly when considering RL in a safety context.
Understanding the decisions and intentions of an RL policy offer avenues to
incorporate safety into the policy by limiting undesirable actions. We propose
the use of a Boolean Decision Rules model to create a post-hoc rule-based
summary of an agent's policy. We evaluate our proposed approach using a DQN
agent trained on an implementation of a lava gridworld and show that, given a
hand-crafted feature representation of this gridworld, simple generalised rules
can be created, giving a post-hoc explainable summary of the agent's policy. We
discuss possible avenues to introduce safety into a RL agent's policy by using
rules generated by this rule-based model as constraints imposed on the agent's
policy, as well as discuss how creating simple rule summaries of an agent's
policy may help in the debugging process of RL agents.",2022-11-13 16:03:39,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Stalin Muñoz Gutiérrez, Gerald Steinbauer-Wagner",The Need for a Meta-Architecture for Robot Autonomy,"EPTCS 362, 2022, pp. 81-97",,,10.4204/EPTCS.362.9,http://arxiv.org/abs/2207.09712v1,"Long-term autonomy of robotic systems implicitly requires dependable
platforms that are able to naturally handle hardware and software faults,
problems in behaviors, or lack of knowledge. Model-based dependable platforms
additionally require the application of rigorous methodologies during the
system development, including the use of correct-by-construction techniques to
implement robot behaviors. As the level of autonomy in robots increases, so do
the cost of offering guarantees about the dependability of the system.
Certifiable dependability of autonomous robots, we argue, can benefit from
formal models of the integration of several cognitive functions, knowledge
processing, reasoning, and meta-reasoning. Here we put forward the case for a
generative model of cognitive architectures for autonomous robotic agents that
subscribes to the principles of model-based engineering and certifiable
dependability, autonomic computing, and knowledge-enabled robotics.",2022-11-13 16:03:39,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Haoran Xu, Xianyuan Zhan, Honglei Yin, Huiling Qin",Discriminator-Weighted Offline Imitation Learning from Suboptimal Demonstrations,,,,,http://arxiv.org/abs/2207.10050v1,"We study the problem of offline Imitation Learning (IL) where an agent aims
to learn an optimal expert behavior policy without additional online
environment interactions. Instead, the agent is provided with a supplementary
offline dataset from suboptimal behaviors. Prior works that address this
problem either require that expert data occupies the majority proportion of the
offline dataset, or need to learn a reward function and perform offline
reinforcement learning (RL) afterwards. In this paper, we aim to address the
problem without additional steps of reward learning and offline RL training for
the case when demonstrations contain a large proportion of suboptimal data.
Built upon behavioral cloning (BC), we introduce an additional discriminator to
distinguish expert and non-expert data. We propose a cooperation framework to
boost the learning of both tasks, Based on this framework, we design a new IL
algorithm, where the outputs of discriminator serve as the weights of the BC
loss. Experimental results show that our proposed algorithm achieves higher
returns and faster training speed compared to baseline algorithms.",2022-11-13 16:03:40,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Kenneth Holstein, Maria De-Arteaga, Lakshmi Tumati, Yanghuidi Cheng",Toward Supporting Perceptual Complementarity in Human-AI Collaboration via Reflection on Unobservables,,,,,http://arxiv.org/abs/2207.13834v1,"In many real world contexts, successful human-AI collaboration requires
humans to productively integrate complementary sources of information into
AI-informed decisions. However, in practice human decision-makers often lack
understanding of what information an AI model has access to in relation to
themselves. There are few available guidelines regarding how to effectively
communicate about unobservables: features that may influence the outcome, but
which are unavailable to the model. In this work, we conducted an online
experiment to understand whether and how explicitly communicating potentially
relevant unobservables influences how people integrate model outputs and
unobservables when making predictions. Our findings indicate that presenting
prompts about unobservables can change how humans integrate model outputs and
unobservables, but do not necessarily lead to improved performance.
Furthermore, the impacts of these prompts can vary depending on
decision-makers' prior domain expertise. We conclude by discussing implications
for future research and design of AI-based decision support tools.",2022-11-13 16:03:40,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Corban Rivera, Chace Ashcraft, Alexander New, James Schmidt, Gautam Vallabha",Latent Properties of Lifelong Learning Systems,,,,,http://arxiv.org/abs/2207.14378v1,"Creating artificial intelligence (AI) systems capable of demonstrating
lifelong learning is a fundamental challenge, and many approaches and metrics
have been proposed to analyze algorithmic properties. However, for existing
lifelong learning metrics, algorithmic contributions are confounded by task and
scenario structure. To mitigate this issue, we introduce an algorithm-agnostic
explainable surrogate-modeling approach to estimate latent properties of
lifelong learning algorithms. We validate the approach for estimating these
properties via experiments on synthetic data. To validate the structure of the
surrogate model, we analyze real performance data from a collection of popular
lifelong learning approaches and baselines adapted for lifelong classification
and lifelong reinforcement learning.",2022-11-13 16:03:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,Jamie Harris,The History of AI Rights Research,,,,,http://arxiv.org/abs/2208.04714v2,"This report documents the history of research on AI rights and other moral
consideration of artificial entities. It highlights key intellectual influences
on this literature as well as research and academic discussion addressing the
topic more directly. We find that researchers addressing AI rights have often
seemed to be unaware of the work of colleagues whose interests overlap with
their own. Academic interest in this topic has grown substantially in recent
years; this reflects wider trends in academic research, but it seems that
certain influential publications, the gradual, accumulating ubiquity of AI and
robotic technology, and relevant news events may all have encouraged increased
academic interest in this specific topic. We suggest four levers that, if
pulled on in the future, might increase interest further: the adoption of
publication strategies similar to those of the most successful previous
contributors; increased engagement with adjacent academic fields and debates;
the creation of specialized journals, conferences, and research institutions;
and more exploration of legal rights for artificial entities.",2022-11-13 16:03:42,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Olusola T. Odeyomi, Olubiyi O. Akintade, Temitayo O. Olowu, Gergely Zaruba",A Review of the Convergence of 5G/6G Architecture and Deep Learning,,,,,http://arxiv.org/abs/2208.07643v1,"The convergence of 5G architecture and deep learning has gained a lot of
research interests in both the fields of wireless communication and artificial
intelligence. This is because deep learning technologies have been identified
to be the potential driver of the 5G technologies, that make up the 5G
architecture. Hence, there have been extensive surveys on the convergence of 5G
architecture and deep learning. However, most of the existing survey papers
mainly focused on how deep learning can converge with a specific 5G technology,
thus, not covering the full spectrum of the 5G architecture. Although there is
a recent survey paper that appears to be robust, a review of that paper shows
that it is not well structured to specifically cover the convergence of deep
learning and the 5G technologies. Hence, this paper provides a robust overview
of the convergence of the key 5G technologies and deep learning. The challenges
faced by such convergence are discussed. In addition, a brief overview of the
future 6G architecture, and how it can converge with deep learning is also
discussed.",2022-11-13 16:03:42,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Charlotte Siegmann, Markus Anderljung",The Brussels Effect and Artificial Intelligence: How EU regulation will impact the global AI market,,,,,http://arxiv.org/abs/2208.12645v1,"The European Union is likely to introduce among the first, most stringent,
and most comprehensive AI regulatory regimes of the world's major
jurisdictions. In this report, we ask whether the EU's upcoming regulation for
AI will diffuse globally, producing a so-called ""Brussels Effect"". Building on
and extending Anu Bradford's work, we outline the mechanisms by which such
regulatory diffusion may occur. We consider both the possibility that the EU's
AI regulation will incentivise changes in products offered in non-EU countries
(a de facto Brussels Effect) and the possibility it will influence regulation
adopted by other jurisdictions (a de jure Brussels Effect). Focusing on the
proposed EU AI Act, we tentatively conclude that both de facto and de jure
Brussels effects are likely for parts of the EU regulatory regime. A de facto
effect is particularly likely to arise in large US tech companies with AI
systems that the AI Act terms ""high-risk"". We argue that the upcoming
regulation might be particularly important in offering the first and most
influential operationalisation of what it means to develop and deploy
trustworthy or human-centred AI. If the EU regime is likely to see significant
diffusion, ensuring it is well-designed becomes a matter of global importance.",2022-11-13 16:03:43,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Satwik Patnaik, Vasudev Gohil, Hao Guo, Jeyavijayan, Rajendran","Reinforcement Learning for Hardware Security: Opportunities, Developments, and Challenges",,,,,http://arxiv.org/abs/2208.13885v1,"Reinforcement learning (RL) is a machine learning paradigm where an
autonomous agent learns to make an optimal sequence of decisions by interacting
with the underlying environment. The promise demonstrated by RL-guided
workflows in unraveling electronic design automation problems has encouraged
hardware security researchers to utilize autonomous RL agents in solving
domain-specific problems. From the perspective of hardware security, such
autonomous agents are appealing as they can generate optimal actions in an
unknown adversarial environment. On the other hand, the continued globalization
of the integrated circuit supply chain has forced chip fabrication to
off-shore, untrustworthy entities, leading to increased concerns about the
security of the hardware. Furthermore, the unknown adversarial environment and
increasing design complexity make it challenging for defenders to detect subtle
modifications made by attackers (a.k.a. hardware Trojans). In this brief, we
outline the development of RL agents in detecting hardware Trojans, one of the
most challenging hardware security problems. Additionally, we outline potential
opportunities and enlist the challenges of applying RL to solve hardware
security problems.",2022-11-13 16:03:43,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"James R. Kirk, Robert E. Wray, Peter Lindes, John E. Laird",Improving Language Model Prompting in Support of Semi-autonomous Task Learning,,,,,http://arxiv.org/abs/2209.07636v1,"Language models (LLMs) offer potential as a source of knowledge for agents
that need to acquire new task competencies within a performance environment. We
describe efforts toward a novel agent capability that can construct cues (or
""prompts"") that result in useful LLM responses for an agent learning a new
task. Importantly, responses must not only be ""reasonable"" (a measure used
commonly in research on knowledge extraction from LLMs) but also specific to
the agent's task context and in a form that the agent can interpret given its
native language capacities. We summarize a series of empirical investigations
of prompting strategies and evaluate responses against the goals of targeted
and actionable responses for task learning. Our results demonstrate that
actionable task knowledge can be obtained from LLMs in support of online agent
task learning.",2022-11-13 16:03:43,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Hosein Hasanbeig, Daniel Kroening, Alessandro Abate",LCRL: Certified Policy Synthesis via Logically-Constrained Reinforcement Learning,,,,,http://arxiv.org/abs/2209.10341v1,"LCRL is a software tool that implements model-free Reinforcement Learning
(RL) algorithms over unknown Markov Decision Processes (MDPs), synthesising
policies that satisfy a given linear temporal specification with maximal
probability. LCRL leverages partially deterministic finite-state machines known
as Limit Deterministic Buchi Automata (LDBA) to express a given linear temporal
specification. A reward function for the RL algorithm is shaped on-the-fly,
based on the structure of the LDBA. Theoretical guarantees under proper
assumptions ensure the convergence of the RL algorithm to an optimal policy
that maximises the satisfaction probability. We present case studies to
demonstrate the applicability, ease of use, scalability, and performance of
LCRL. Owing to the LDBA-guided exploration and LCRL model-free architecture, we
observe robust performance, which also scales well when compared to standard RL
approaches (whenever applicable to LTL specifications). Full instructions on
how to execute all the case studies in this paper are provided on a GitHub page
that accompanies the LCRL distribution www.github.com/grockious/lcrl.",2022-11-13 16:03:44,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Lunjun Zhang, Bradly C. Stadie",Understanding Hindsight Goal Relabeling Requires Rethinking Divergence Minimization,,,,,http://arxiv.org/abs/2209.13046v1,"Hindsight goal relabeling has become a foundational technique for multi-goal
reinforcement learning (RL). The idea is quite simple: any arbitrary trajectory
can be seen as an expert demonstration for reaching the trajectory's end state.
Intuitively, this procedure trains a goal-conditioned policy to imitate a
sub-optimal expert. However, this connection between imitation and hindsight
relabeling is not well understood. Modern imitation learning algorithms are
described in the language of divergence minimization, and yet it remains an
open problem how to recast hindsight goal relabeling into that framework. In
this work, we develop a unified objective for goal-reaching that explains such
a connection, from which we can derive goal-conditioned supervised learning
(GCSL) and the reward function in hindsight experience replay (HER) from first
principles. Experimentally, we find that despite recent advances in
goal-conditioned behaviour cloning (BC), multi-goal Q-learning can still
outperform BC-like methods; moreover, a vanilla combination of both actually
hurts model performance. Under our framework, we study when BC is expected to
help, and empirically validate our findings. Our work further bridges
goal-reaching and generative modeling, illustrating the nuances and new
pathways of extending the success of generative models to RL.",2022-11-13 16:03:45,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Jialu Zhang, José Cambronero, Sumit Gulwani, Vu Le, Ruzica Piskac, Gustavo Soares, Gust Verbruggen",Repairing Bugs in Python Assignments Using Large Language Models,,,,,http://arxiv.org/abs/2209.14876v1,"Students often make mistakes on their introductory programming assignments as
part of their learning process. Unfortunately, providing custom repairs for
these mistakes can require a substantial amount of time and effort from class
instructors. Automated program repair (APR) techniques can be used to
synthesize such fixes. Prior work has explored the use of symbolic and neural
techniques for APR in the education domain. Both types of approaches require
either substantial engineering efforts or large amounts of data and training.
We propose to use a large language model trained on code, such as Codex, to
build an APR system -- MMAPR -- for introductory Python programming
assignments. Our system can fix both syntactic and semantic mistakes by
combining multi-modal prompts, iterative querying, test-case-based selection of
few-shots, and program chunking. We evaluate MMAPR on 286 real student programs
and compare to a baseline built by combining a state-of-the-art Python syntax
repair engine, BIFI, and state-of-the-art Python semantic repair engine for
student assignments, Refactory. We find that MMAPR can fix more programs and
produce smaller patches on average.",2022-11-13 16:03:46,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Burcu Sayin, Fabio Casati, Andrea Passerini, Jie Yang, Xinyue Chen",Rethinking and Recomputing the Value of ML Models,,,,,http://arxiv.org/abs/2209.15157v1,"In this paper, we argue that the way we have been training and evaluating ML
models has largely forgotten the fact that they are applied in an organization
or societal context as they provide value to people. We show that with this
perspective we fundamentally change how we evaluate, select and deploy ML
models - and to some extent even what it means to learn. Specifically, we
stress that the notion of value plays a central role in learning and
evaluating, and different models may require different learning practices and
provide different values based on the application context they are applied. We
also show that this concretely impacts how we select and embed models into
human workflows based on experimental datasets. Nothing of what is presented
here is hard: to a large extent is a series of fairly trivial observations with
massive practical implications.",2022-11-13 16:03:46,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Arjun Krishnakumar, Colin White, Arber Zela, Renbo Tu, Mahmoud Safari, Frank Hutter",NAS-Bench-Suite-Zero: Accelerating Research on Zero Cost Proxies,,,,,http://arxiv.org/abs/2210.03230v1,"Zero-cost proxies (ZC proxies) are a recent architecture performance
prediction technique aiming to significantly speed up algorithms for neural
architecture search (NAS). Recent work has shown that these techniques show
great promise, but certain aspects, such as evaluating and exploiting their
complementary strengths, are under-studied. In this work, we create
NAS-Bench-Suite: we evaluate 13 ZC proxies across 28 tasks, creating by far the
largest dataset (and unified codebase) for ZC proxies, enabling
orders-of-magnitude faster experiments on ZC proxies, while avoiding
confounding factors stemming from different implementations. To demonstrate the
usefulness of NAS-Bench-Suite, we run a large-scale analysis of ZC proxies,
including a bias analysis, and the first information-theoretic analysis which
concludes that ZC proxies capture substantial complementary information.
Motivated by these findings, we present a procedure to improve the performance
of ZC proxies by reducing biases such as cell size, and we also show that
incorporating all 13 ZC proxies into the surrogate models used by NAS
algorithms can improve their predictive performance by up to 42%. Our code and
datasets are available at https://github.com/automl/naslib/tree/zerocost.",2022-11-13 16:03:47,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Zih-Yun Chiu, Yi-Lin Tuan, William Yang Wang, Michael C. Yip",Knowledge-Grounded Reinforcement Learning,,,,,http://arxiv.org/abs/2210.03729v1,"Receiving knowledge, abiding by laws, and being aware of regulations are
common behaviors in human society. Bearing in mind that reinforcement learning
(RL) algorithms benefit from mimicking humanity, in this work, we propose that
an RL agent can act on external guidance in both its learning process and model
deployment, making the agent more socially acceptable. We introduce the
concept, Knowledge-Grounded RL (KGRL), with a formal definition that an agent
learns to follow external guidelines and develop its own policy. Moving towards
the goal of KGRL, we propose a novel actor model with an embedding-based
attention mechanism that can attend to either a learnable internal policy or
external knowledge. The proposed method is orthogonal to training algorithms,
and the external knowledge can be flexibly recomposed, rearranged, and reused
in both training and inference stages. Through experiments on tasks with
discrete and continuous action space, our KGRL agent is shown to be more sample
efficient and generalizable, and it has flexibly rearrangeable knowledge
embeddings and interpretable behaviors.",2022-11-13 16:03:47,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Maitrey Gramopadhye, Daniel Szafir",Generating Executable Action Plans with Environmentally-Aware Language Models,,,,,http://arxiv.org/abs/2210.04964v1,"Large Language Models (LLMs) trained using massive text datasets have
recently shown promise in generating action plans for robotic agents from high
level text queries. However, these models typically do not consider the robot's
environment, resulting in generated plans that may not actually be executable
due to ambiguities in the planned actions or environmental constraints. In this
paper, we propose an approach to generate environmentally-aware action plans
that can be directly mapped to executable agent actions. Our approach involves
integrating environmental objects and object relations as additional inputs
into LLM action plan generation to provide the system with an awareness of its
surroundings, resulting in plans where each generated action is mapped to
objects present in the scene. We also design a novel scoring function that,
along with generating the action steps and associating them with objects, helps
the system disambiguate among object instances and take into account their
states. We evaluate our approach using the VirtualHome simulator and the
ActivityPrograms knowledge base. Our results show that the action plans
generated from our system outperform prior work in terms of their correctness
and executability by 5.3% and 8.9% respectively.",2022-11-13 16:03:47,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Vaibhav Bajaj, Guni Sharon, Peter Stone",Task Phasing: Automated Curriculum Learning from Demonstrations,,,,,http://arxiv.org/abs/2210.10999v1,"Applying reinforcement learning (RL) to sparse reward domains is notoriously
challenging due to insufficient guiding signals. Common techniques for
addressing such domains include (1) learning from demonstrations and (2)
curriculum learning. While these two approaches have been studied in detail,
they have rarely been considered together. This paper aims to do so by
introducing a principled task phasing approach that uses demonstrations to
automatically generate a curriculum sequence. Using inverse RL from
(suboptimal) demonstrations we define a simple initial task. Our task phasing
approach then provides a framework to gradually increase the complexity of the
task all the way to the target task, while retuning the RL agent in each
phasing iteration. Two approaches for phasing are considered: (1) gradually
increasing the proportion of time steps an RL agent is in control, and (2)
phasing out a guiding informative reward function. We present conditions that
guarantee the convergence of these approaches to an optimal policy.
Experimental results on 3 sparse reward domains demonstrate that our task
phasing approaches outperform state-of-the-art approaches with respect to their
asymptotic performance.",2022-11-13 16:03:48,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,Yanick Schraner,Teacher-student curriculum learning for reinforcement learning,,,,,http://arxiv.org/abs/2210.17368v1,"Reinforcement learning (rl) is a popular paradigm for sequential decision
making problems. The past decade's advances in rl have led to breakthroughs in
many challenging domains such as video games, board games, robotics, and chip
design. The sample inefficiency of deep reinforcement learning methods is a
significant obstacle when applying rl to real-world problems. Transfer learning
has been applied to reinforcement learning such that the knowledge gained in
one task can be applied when training in a new task. Curriculum learning is
concerned with sequencing tasks or data samples such that knowledge can be
transferred between those tasks to learn a target task that would otherwise be
too difficult to solve. Designing a curriculum that improves sample efficiency
is a complex problem. In this thesis, we propose a teacher-student curriculum
learning setting where we simultaneously train a teacher that selects tasks for
the student while the student learns how to solve the selected task. Our method
is independent of human domain knowledge and manual curriculum design. We
evaluated our methods on two reinforcement learning benchmarks: grid world and
the challenging Google Football environment. With our method, we can improve
the sample efficiency and generality of the student compared to tabula-rasa
reinforcement learning.",2022-11-13 16:03:48,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2011,"S. S. Fatima, N. R. Jennings, M. J. Wooldridge",Multi-Issue Negotiation with Deadlines,"Journal Of Artificial Intelligence Research, Volume 27, pages
  381-417, 2006",,,10.1613/jair.2056,http://arxiv.org/abs/1110.2765v1,"This paper studies bilateral multi-issue negotiation between self-interested
autonomous agents. Now, there are a number of different procedures that can be
used for this process; the three main ones being the package deal procedure in
which all the issues are bundled and discussed together, the simultaneous
procedure in which the issues are discussed simultaneously but independently of
each other, and the sequential procedure in which the issues are discussed one
after another. Since each of them yields a different outcome, a key problem is
to decide which one to use in which circumstances. Specifically, we consider
this question for a model in which the agents have time constraints (in the
form of both deadlines and discount factors) and information uncertainty (in
that the agents do not know the opponents utility function). For this model, we
consider issues that are both independent and those that are interdependent and
determine equilibria for each case for each procedure. In so doing, we show
that the package deal is in fact the optimal procedure for each party. We then
go on to show that, although the package deal may be computationally more
complex than the other two procedures, it generates Pareto optimal outcomes
(unlike the other two), it has similar earliest and latest possible times of
agreement to the simultaneous procedure (which is better than the sequential
procedure), and that it (like the other two procedures) generates a unique
outcome only under certain conditions (which we define).",2022-11-13 16:03:49,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2012,"Umar Syed, Robert E. Schapire",Imitation Learning with a Value-Based Prior,,,,,http://arxiv.org/abs/1206.5290v1,"The goal of imitation learning is for an apprentice to learn how to behave in
a stochastic environment by observing a mentor demonstrating the correct
behavior. Accurate prior knowledge about the correct behavior can reduce the
need for demonstrations from the mentor. We present a novel approach to
encoding prior knowledge about the correct behavior, where we assume that this
prior knowledge takes the form of a Markov Decision Process (MDP) that is used
by the apprentice as a rough and imperfect model of the mentor's behavior.
Specifically, taking a Bayesian approach, we treat the value of a policy in
this modeling MDP as the log prior probability of the policy. In other words,
we assume a priori that the mentor's behavior is likely to be a high value
policy in the modeling MDP, though quite possibly different from the optimal
policy. We describe an efficient algorithm that, given a modeling MDP and a set
of demonstrations by a mentor, provably converges to a stationary point of the
log posterior of the mentor's policy, where the posterior is computed with
respect to the ""value based"" prior. We also present empirical evidence that
this prior does in fact speed learning of the mentor's policy, and is an
improvement in our experiments over similar previous methods.",2022-11-13 16:03:49,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2016,"Ben McCamish, Vahid Ghadakchi, Arash Termehchy, Behrouz Touri",A Signaling Game Approach to Databases Querying and Interaction,,,,,http://arxiv.org/abs/1603.04068v5,"As most users do not precisely know the structure and/or the content of
databases, their queries do not exactly reflect their information needs. The
database management systems (DBMS) may interact with users and use their
feedback on the returned results to learn the information needs behind their
queries. Current query interfaces assume that users do not learn and modify the
way way they express their information needs in form of queries during their
interaction with the DBMS. Using a real-world interaction workload, we show
that users learn and modify how to express their information needs during their
interactions with the DBMS and their learning is accurately modeled by a
well-known reinforcement learning mechanism. As current data interaction
systems assume that users do not modify their strategies, they cannot discover
the information needs behind users' queries effectively. We model the
interaction between users and DBMS as a game with identical interest between
two rational agents whose goal is to establish a common language for
representing information needs in form of queries. We propose a reinforcement
learning method that learns and answers the information needs behind queries
and adapts to the changes in users' strategies and prove that it improves the
effectiveness of answering queries stochastically speaking. We propose two
efficient implementation of this method over large relational databases. Our
extensive empirical studies over real-world query workloads indicate that our
algorithms are efficient and effective.",2022-11-13 16:03:50,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,"Azalia Mirhoseini, Hieu Pham, Quoc V. Le, Benoit Steiner, Rasmus Larsen, Yuefeng Zhou, Naveen Kumar, Mohammad Norouzi, Samy Bengio, Jeff Dean",Device Placement Optimization with Reinforcement Learning,,,,,http://arxiv.org/abs/1706.04972v2,"The past few years have witnessed a growth in size and computational
requirements for training and inference with neural networks. Currently, a
common approach to address these requirements is to use a heterogeneous
distributed environment with a mixture of hardware devices such as CPUs and
GPUs. Importantly, the decision of placing parts of the neural models on
devices is often made by human experts based on simple heuristics and
intuitions. In this paper, we propose a method which learns to optimize device
placement for TensorFlow computational graphs. Key to our method is the use of
a sequence-to-sequence model to predict which subsets of operations in a
TensorFlow graph should run on which of the available devices. The execution
time of the predicted placements is then used as the reward signal to optimize
the parameters of the sequence-to-sequence model. Our main result is that on
Inception-V3 for ImageNet classification, and on RNN LSTM, for language
modeling and neural machine translation, our model finds non-trivial device
placements that outperform hand-crafted heuristics and traditional algorithmic
methods.",2022-11-13 16:03:50,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2017,"Anirban Santara, Abhishek Naik, Balaraman Ravindran, Dipankar Das, Dheevatsa Mudigere, Sasikanth Avancha, Bharat Kaul",RAIL: Risk-Averse Imitation Learning,,,,,http://arxiv.org/abs/1707.06658v4,"Imitation learning algorithms learn viable policies by imitating an expert's
behavior when reward signals are not available. Generative Adversarial
Imitation Learning (GAIL) is a state-of-the-art algorithm for learning policies
when the expert's behavior is available as a fixed set of trajectories. We
evaluate in terms of the expert's cost function and observe that the
distribution of trajectory-costs is often more heavy-tailed for GAIL-agents
than the expert at a number of benchmark continuous-control tasks. Thus,
high-cost trajectories, corresponding to tail-end events of catastrophic
failure, are more likely to be encountered by the GAIL-agents than the expert.
This makes the reliability of GAIL-agents questionable when it comes to
deployment in risk-sensitive applications like robotic surgery and autonomous
driving. In this work, we aim to minimize the occurrence of tail-end events by
minimizing tail risk within the GAIL framework. We quantify tail risk by the
Conditional-Value-at-Risk (CVaR) of trajectories and develop the Risk-Averse
Imitation Learning (RAIL) algorithm. We observe that the policies learned with
RAIL show lower tail-end risk than those of vanilla GAIL. Thus the proposed
RAIL algorithm appears as a potent alternative to GAIL for improved reliability
in risk-sensitive applications.",2022-11-13 16:03:51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,"Abhinav Verma, Vijayaraghavan Murali, Rishabh Singh, Pushmeet Kohli, Swarat Chaudhuri",Programmatically Interpretable Reinforcement Learning,PMLR 80:5045-5054,,,,http://arxiv.org/abs/1804.02477v3,"We present a reinforcement learning framework, called Programmatically
Interpretable Reinforcement Learning (PIRL), that is designed to generate
interpretable and verifiable agent policies. Unlike the popular Deep
Reinforcement Learning (DRL) paradigm, which represents policies by neural
networks, PIRL represents policies using a high-level, domain-specific
programming language. Such programmatic policies have the benefits of being
more easily interpreted than neural networks, and being amenable to
verification by symbolic methods. We propose a new method, called Neurally
Directed Program Search (NDPS), for solving the challenging nonsmooth
optimization problem of finding a programmatic policy with maximal reward. NDPS
works by first learning a neural policy network using DRL, and then performing
a local search over programmatic policies that seeks to minimize a distance
from this neural ""oracle"". We evaluate NDPS on the task of learning to drive a
simulated car in the TORCS car-racing environment. We demonstrate that NDPS is
able to discover human-readable policies that pass some significant performance
bars. We also show that PIRL policies can have smoother trajectories, and can
be more easily transferred to environments not encountered during training,
than corresponding policies discovered by DRL.",2022-11-13 16:03:51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,"Nanqing Dong, Michael Kampffmeyer, Xiaodan Liang, Zeya Wang, Wei Dai, Eric P. Xing",Reinforced Auto-Zoom Net: Towards Accurate and Fast Breast Cancer Segmentation in Whole-slide Images,,,,,http://arxiv.org/abs/1807.11113v1,"Convolutional neural networks have led to significant breakthroughs in the
domain of medical image analysis. However, the task of breast cancer
segmentation in whole-slide images (WSIs) is still underexplored. WSIs are
large histopathological images with extremely high resolution. Constrained by
the hardware and field of view, using high-magnification patches can slow down
the inference process and using low-magnification patches can cause the loss of
information. In this paper, we aim to achieve two seemingly conflicting goals
for breast cancer segmentation: accurate and fast prediction. We propose a
simple yet efficient framework Reinforced Auto-Zoom Net (RAZN) to tackle this
task. Motivated by the zoom-in operation of a pathologist using a digital
microscope, RAZN learns a policy network to decide whether zooming is required
in a given region of interest. Because the zoom-in action is selective, RAZN is
robust to unbalanced and noisy ground truth labels and can efficiently reduce
overfitting. We evaluate our method on a public breast cancer dataset. RAZN
outperforms both single-scale and multi-scale baseline approaches, achieving
better accuracy at low inference cost.",2022-11-13 16:03:52,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,"Nir Douer, Joachim Meyer",The Responsibility Quantification (ResQu) Model of Human Interaction with Automation,"IEEE Transactions on Automation Science and Engineering, 17 (2),
  1044-1060 (2020)",,,10.1109/TASE.2020.2965466,http://arxiv.org/abs/1810.12644v4,"Intelligent systems and advanced automation are involved in information
collection and evaluation, in decision-making and in the implementation of
chosen actions. In such systems, human responsibility becomes equivocal.
Understanding human casual responsibility is particularly important when
intelligent autonomous systems can harm people, as with autonomous vehicles or,
most notably, with autonomous weapon systems (AWS). Using Information Theory,
we develop a responsibility quantification (ResQu) model of human involvement
in intelligent automated systems and demonstrate its applications on decisions
regarding AWS. The analysis reveals that human comparative responsibility to
outcomes is often low, even when major functions are allocated to the human.
Thus, broadly stated policies of keeping humans in the loop and having
meaningful human control are misleading and cannot truly direct decisions on
how to involve humans in intelligent systems and advanced automation. The
current model is an initial step in the complex goal to create a comprehensive
responsibility model, that will enable quantification of human causal
responsibility. It assumes stationarity, full knowledge regarding the
characteristic of the human and automation and ignores temporal aspects.
Despite these limitations, it can aid in the analysis of systems designs
alternatives and policy decisions regarding human responsibility in intelligent
systems and advanced automation.",2022-11-13 16:03:52,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,Alexander Peysakhovich,Reinforcement Learning and Inverse Reinforcement Learning with System 1 and System 2,,,,,http://arxiv.org/abs/1811.08549v2,"Inferring a person's goal from their behavior is an important problem in
applications of AI (e.g. automated assistants, recommender systems). The
workhorse model for this task is the rational actor model - this amounts to
assuming that people have stable reward functions, discount the future
exponentially, and construct optimal plans. Under the rational actor assumption
techniques such as inverse reinforcement learning (IRL) can be used to infer a
person's goals from their actions. A competing model is the dual-system model.
Here decisions are the result of an interplay between a fast, automatic,
heuristic-based system 1 and a slower, deliberate, calculating system 2. We
generalize the dual system framework to the case of Markov decision problems
and show how to compute optimal plans for dual-system agents. We show that
dual-system agents exhibit behaviors that are incompatible with rational actor
assumption. We show that naive applications of rational-actor IRL to the
behavior of dual-system agents can generate wrong inference about the agents'
goals and suggest interventions that actually reduce the agent's overall
utility. Finally, we adapt a simple IRL algorithm to correctly infer the goals
of dual system decision-makers. This allows us to make interventions that help,
rather than hinder, the dual-system agent's ability to reach their true goals.",2022-11-13 16:03:53,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,"Julia A. Meister, Raja Naeem Akram, Konstantinos Markantonakis",Deep Learning Application in Security and Privacy -- Theory and Practice: A Position Paper,,,,,http://arxiv.org/abs/1812.00190v1,"Technology is shaping our lives in a multitude of ways. This is fuelled by a
technology infrastructure, both legacy and state of the art, composed of a
heterogeneous group of hardware, software, services and organisations. Such
infrastructure faces a diverse range of challenges to its operations that
include security, privacy, resilience, and quality of services. Among these,
cybersecurity and privacy are taking the centre-stage, especially since the
General Data Protection Regulation (GDPR) came into effect. Traditional
security and privacy techniques are overstretched and adversarial actors have
evolved to design exploitation techniques that circumvent protection. With the
ever-increasing complexity of technology infrastructure, security and
privacy-preservation specialists have started to look for adaptable and
flexible protection methods that can evolve (potentially autonomously) as the
adversarial actor changes its techniques. For this, Artificial Intelligence
(AI), Machine Learning (ML) and Deep Learning (DL) were put forward as
saviours. In this paper, we look at the promises of AI, ML, and DL stated in
academic and industrial literature and evaluate how realistic they are. We also
put forward potential challenges a DL based security and privacy protection
technique has to overcome. Finally, we conclude the paper with a discussion on
what steps the DL and the security and privacy-preservation community have to
take to ensure that DL is not just going to be hype, but an opportunity to
build a secure, reliable, and trusted technology infrastructure on which we can
rely on for so much in our lives.",2022-11-13 16:03:53,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2018,"Minghao Guo, Zhao Zhong, Wei Wu, Dahua Lin, Junjie Yan",IRLAS: Inverse Reinforcement Learning for Architecture Search,,,,,http://arxiv.org/abs/1812.05285v5,"In this paper, we propose an inverse reinforcement learning method for
architecture search (IRLAS), which trains an agent to learn to search network
structures that are topologically inspired by human-designed network. Most
existing architecture search approaches totally neglect the topological
characteristics of architectures, which results in complicated architecture
with a high inference latency. Motivated by the fact that human-designed
networks are elegant in topology with a fast inference speed, we propose a
mirror stimuli function inspired by biological cognition theory to extract the
abstract topological knowledge of an expert human-design network (ResNeXt). To
avoid raising a too strong prior over the search space, we introduce inverse
reinforcement learning to train the mirror stimuli function and exploit it as a
heuristic guidance for architecture search, easily generalized to different
architecture search algorithms. On CIFAR-10, the best architecture searched by
our proposed IRLAS achieves 2.60% error rate. For ImageNet mobile setting, our
model achieves a state-of-the-art top-1 accuracy 75.28%, while being 2~4x
faster than most auto-generated architectures. A fast version of this model
achieves 10% faster than MobileNetV2, while maintaining a higher accuracy.",2022-11-13 16:03:54,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Ross Gruetzemacher, David Paradice, Kang Bok Lee",Forecasting Transformative AI: An Expert Survey,,,,,http://arxiv.org/abs/1901.08579v2,"Transformative AI technologies have the potential to reshape critical aspects
of society in the near future. However, in order to properly prepare policy
initiatives for the arrival of such technologies accurate forecasts and
timelines are necessary. A survey was administered to attendees of three AI
conferences during the summer of 2018 (ICML, IJCAI and the HLAI conference).
The survey included questions for estimating AI capabilities over the next
decade, questions for forecasting five scenarios of transformative AI and
questions concerning the impact of computational resources in AI research.
Respondents indicated a median of 21.5% of human tasks (i.e., all tasks that
humans are currently paid to do) can be feasibly automated now, and that this
figure would rise to 40% in 5 years and 60% in 10 years. Median forecasts
indicated a 50% probability of AI systems being capable of automating 90% of
current human tasks in 25 years and 99% of current human tasks in 50 years. The
conference of attendance was found to have a statistically significant impact
on all forecasts, with attendees of HLAI providing more optimistic timelines
with less uncertainty. These findings suggest that AI experts expect major
advances in AI technology to continue over the next decade to a degree that
will likely have profound transformative impacts on society.",2022-11-13 16:03:55,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Laura von Rueden, Sebastian Mayer, Katharina Beckh, Bogdan Georgiev, Sven Giesselbach, Raoul Heese, Birgit Kirsch, Julius Pfrommer, Annika Pick, Rajkumar Ramamurthy, Michal Walczak, Jochen Garcke, Christian Bauckhage, Jannis Schuecker",Informed Machine Learning -- A Taxonomy and Survey of Integrating Knowledge into Learning Systems,,,,10.1109/TKDE.2021.3079836,http://arxiv.org/abs/1903.12394v3,"Despite its great success, machine learning can have its limits when dealing
with insufficient training data. A potential solution is the additional
integration of prior knowledge into the training process which leads to the
notion of informed machine learning. In this paper, we present a structured
overview of various approaches in this field. We provide a definition and
propose a concept for informed machine learning which illustrates its building
blocks and distinguishes it from conventional machine learning. We introduce a
taxonomy that serves as a classification framework for informed machine
learning approaches. It considers the source of knowledge, its representation,
and its integration into the machine learning pipeline. Based on this taxonomy,
we survey related research and describe how different knowledge representations
such as algebraic equations, logic rules, or simulation results can be used in
learning systems. This evaluation of numerous papers on the basis of our
taxonomy uncovers key methods in the field of informed machine learning.",2022-11-13 16:03:55,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Alex Kearney, Patrick M. Pilarski",When is a Prediction Knowledge?,,,,,http://arxiv.org/abs/1904.09024v1,"Within Reinforcement Learning, there is a growing collection of research
which aims to express all of an agent's knowledge of the world through
predictions about sensation, behaviour, and time. This work can be seen not
only as a collection of architectural proposals, but also as the beginnings of
a theory of machine knowledge in reinforcement learning. Recent work has
expanded what can be expressed using predictions, and developed applications
which use predictions to inform decision-making on a variety of synthetic and
real-world problems. While promising, we here suggest that the notion of
predictions as knowledge in reinforcement learning is as yet underdeveloped:
some work explicitly refers to predictions as knowledge, what the requirements
are for considering a prediction to be knowledge have yet to be well explored.
This specification of the necessary and sufficient conditions of knowledge is
important; even if claims about the nature of knowledge are left implicit in
technical proposals, the underlying assumptions of such claims have
consequences for the systems we design. These consequences manifest in both the
way we choose to structure predictive knowledge architectures, and how we
evaluate them. In this paper, we take a first step to formalizing predictive
knowledge by discussing the relationship of predictive knowledge learning
methods to existing theories of knowledge in epistemology. Specifically, we
explore the relationships between Generalized Value Functions and epistemic
notions of Justification and Truth.",2022-11-13 16:03:56,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Chenglong Wang, Rudy Bunel, Krishnamurthy Dvijotham, Po-Sen Huang, Edward Grefenstette, Pushmeet Kohli",Knowing When to Stop: Evaluation and Verification of Conformity to Output-size Specifications,,,,,http://arxiv.org/abs/1904.12004v1,"Models such as Sequence-to-Sequence and Image-to-Sequence are widely used in
real world applications. While the ability of these neural architectures to
produce variable-length outputs makes them extremely effective for problems
like Machine Translation and Image Captioning, it also leaves them vulnerable
to failures of the form where the model produces outputs of undesirable length.
This behavior can have severe consequences such as usage of increased
computation and induce faults in downstream modules that expect outputs of a
certain length. Motivated by the need to have a better understanding of the
failures of these models, this paper proposes and studies the novel output-size
modulation problem and makes two key technical contributions. First, to
evaluate model robustness, we develop an easy-to-compute differentiable proxy
objective that can be used with gradient-based algorithms to find
output-lengthening inputs. Second and more importantly, we develop a
verification approach that can formally verify whether a network always
produces outputs within a certain length. Experimental results on Machine
Translation and Image Captioning show that our output-lengthening approach can
produce outputs that are 50 times longer than the input, while our verification
approach can, given a model and input domain, prove that the output length is
below a certain size.",2022-11-13 16:03:56,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Otello Ardovino, Jacopo Arpetti, Marco Delmastro",Regulating AI: do we need new tools?,,,,,http://arxiv.org/abs/1904.12134v1,"The Artificial Intelligence paradigm (hereinafter referred to as ""AI"") builds
on the analysis of data able, among other things, to snap pictures of the
individuals' behaviors and preferences. Such data represent the most valuable
currency in the digital ecosystem, where their value derives from their being a
fundamental asset in order to train machines with a view to developing AI
applications. In this environment, online providers attract users by offering
them services for free and getting in exchange data generated right through the
usage of such services. This swap, characterized by an implicit nature,
constitutes the focus of the present paper, in the light of the disequilibria,
as well as market failures, that it may bring about. We use mobile apps and the
related permission system as an ideal environment to explore, via econometric
tools, those issues. The results, stemming from a dataset of over one million
observations, show that both buyers and sellers are aware that access to
digital services implicitly implies an exchange of data, although this does not
have a considerable impact neither on the level of downloads (demand), nor on
the level of the prices (supply). In other words, the implicit nature of this
exchange does not allow market indicators to work efficiently. We conclude that
current policies (e.g. transparency rules) may be inherently biased and we put
forward suggestions for a new approach.",2022-11-13 16:03:57,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Lihi Dery, Svetlana Obraztsova, Zinovi Rabinovich, Meir Kalech",Lie on the Fly: Strategic Voting in an Iterative Preference Elicitation Process,,,,10.1007/s10726-019-09637-2,http://arxiv.org/abs/1905.04933v1,"A voting center is in charge of collecting and aggregating voter preferences.
In an iterative process, the center sends comparison queries to voters,
requesting them to submit their preference between two items. Voters might
discuss the candidates among themselves, figuring out during the elicitation
process which candidates stand a chance of winning and which do not.
Consequently, strategic voters might attempt to manipulate by deviating from
their true preferences and instead submit a different response in order to
attempt to maximize their profit. We provide a practical algorithm for
strategic voters which computes the best manipulative vote and maximizes the
voter's selfish outcome when such a vote exists. We also provide a careful
voting center which is aware of the possible manipulations and avoids
manipulative queries when possible. In an empirical study on four real-world
domains, we show that in practice manipulation occurs in a low percentage of
settings and has a low impact on the final outcome. The careful voting center
reduces manipulation even further, thus allowing for a non-distorted group
decision process to take place. We thus provide a core technology study of a
voting process that can be adopted in opinion or information aggregation
systems and in crowdsourcing applications, e.g., peer grading in Massive Open
Online Courses (MOOCs).",2022-11-13 16:03:57,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Jessica Morley, Luciano Floridi, Libby Kinsey, Anat Elhalal","From What to How: An Initial Review of Publicly Available AI Ethics Tools, Methods and Research to Translate Principles into Practices",,,,,http://arxiv.org/abs/1905.06876v2,"The debate about the ethical implications of Artificial Intelligence dates
from the 1960s. However, in recent years symbolic AI has been complemented and
sometimes replaced by Neural Networks and Machine Learning techniques. This has
vastly increased its potential utility and impact on society, with the
consequence that the ethical debate has gone mainstream. Such debate has
primarily focused on principles - the what of AI ethics - rather than on
practices, the how. Awareness of the potential issues is increasing at a fast
rate, but the AI community's ability to take action to mitigate the associated
risks is still at its infancy. Therefore, our intention in presenting this
research is to contribute to closing the gap between principles and practices
by constructing a typology that may help practically-minded developers apply
ethics at each stage of the pipeline, and to signal to researchers where
further work is needed. The focus is exclusively on Machine Learning, but it is
hoped that the results of this research may be easily applicable to other
branches of AI. The article outlines the research method for creating this
typology, the initial findings, and provides a summary of future research
needs.",2022-11-13 16:03:58,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Rohin Shah, Noah Gundotra, Pieter Abbeel, Anca D. Dragan","On the Feasibility of Learning, Rather than Assuming, Human Biases for Reward Inference",,,,,http://arxiv.org/abs/1906.09624v1,"Our goal is for agents to optimize the right reward function, despite how
difficult it is for us to specify what that is. Inverse Reinforcement Learning
(IRL) enables us to infer reward functions from demonstrations, but it usually
assumes that the expert is noisily optimal. Real people, on the other hand,
often have systematic biases: risk-aversion, myopia, etc. One option is to try
to characterize these biases and account for them explicitly during learning.
But in the era of deep learning, a natural suggestion researchers make is to
avoid mathematical models of human behavior that are fraught with specific
assumptions, and instead use a purely data-driven approach. We decided to put
this to the test -- rather than relying on assumptions about which specific
bias the demonstrator has when planning, we instead learn the demonstrator's
planning algorithm that they use to generate demonstrations, as a
differentiable planner. Our exploration yielded mixed findings: on the one
hand, learning the planner can lead to better reward inference than relying on
the wrong assumption; on the other hand, this benefit is dwarfed by the loss we
incur by going from an exact to a differentiable planner. This suggest that at
least for the foreseeable future, agents need a middle ground between the
flexibility of data-driven methods and the useful bias of known human biases.
Code is available at https://tinyurl.com/learningbiases.",2022-11-13 16:03:58,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Akira Kinose, Tadahiro Taniguchi",Integration of Imitation Learning using GAIL and Reinforcement Learning using Task-achievement Rewards via Probabilistic Graphical Model,,,,,http://arxiv.org/abs/1907.02140v2,"Integration of reinforcement learning and imitation learning is an important
problem that has been studied for a long time in the field of intelligent
robotics. Reinforcement learning optimizes policies to maximize the cumulative
reward, whereas imitation learning attempts to extract general knowledge about
the trajectories demonstrated by experts, i.e., demonstrators. Because each of
them has their own drawbacks, methods combining them and compensating for each
set of drawbacks have been explored thus far. However, many of the methods are
heuristic and do not have a solid theoretical basis. In this paper, we present
a new theory for integrating reinforcement and imitation learning by extending
the probabilistic generative model framework for reinforcement learning, {\it
plan by inference}. We develop a new probabilistic graphical model for
reinforcement learning with multiple types of rewards and a probabilistic
graphical model for Markov decision processes with multiple optimality
emissions (pMDP-MO). Furthermore, we demonstrate that the integrated learning
method of reinforcement learning and imitation learning can be formulated as a
probabilistic inference of policies on pMDP-MO by considering the output of the
discriminator in generative adversarial imitation learning as an additional
optimal emission observation. We adapt the generative adversarial imitation
learning and task-achievement reward to our proposed framework, achieving
significantly better performance than agents trained with reinforcement
learning or imitation learning alone. Experiments demonstrate that our
framework successfully integrates imitation and reinforcement learning even
when the number of demonstrators is only a few.",2022-11-13 16:03:59,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Pedro Fernandes, Francisco C. Santos, Manuel Lopes",Norms for Beneficial A.I.: A Computational Analysis of the Societal Value Alignment Problem,"AI Communications, vol. 33, no. 3-6, pp. 155-171, 2020",,,10.3233/AIC-201502,http://arxiv.org/abs/1907.03843v2,"The rise of artificial intelligence (A.I.) based systems is already offering
substantial benefits to the society as a whole. However, these systems may also
enclose potential conflicts and unintended consequences. Notably, people will
tend to adopt an A.I. system if it confers them an advantage, at which point
non-adopters might push for a strong regulation if that advantage for adopters
is at a cost for them. Here we propose an agent-based game-theoretical model
for these conflicts, where agents may decide to resort to A.I. to use and
acquire additional information on the payoffs of a stochastic game, striving to
bring insights from simulation to what has been, hitherto, a mostly
philosophical discussion. We frame our results under the current discussion on
ethical A.I. and the conflict between individual and societal gains: the
societal value alignment problem. We test the arising equilibria in the
adoption of A.I. technology under different norms followed by artificial
agents, their ensuing benefits, and the emergent levels of wealth inequality.
We show that without any regulation, purely selfish A.I. systems will have the
strongest advantage, even when a utilitarian A.I. provides significant benefits
for the individual and the society. Nevertheless, we show that it is possible
to develop A.I. systems following human conscious policies that, when
introduced in society, lead to an equilibrium where the gains for the adopters
are not at a cost for non-adopters, thus increasing the overall wealth of the
population and lowering inequality. However, as shown, a self-organised
adoption of such policies would require external regulation.",2022-11-13 16:03:59,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Felix Leibfried, Sergio Pascual-Diaz, Jordi Grau-Moya",A Unified Bellman Optimality Principle Combining Reward Maximization and Empowerment,,,,,http://arxiv.org/abs/1907.12392v5,"Empowerment is an information-theoretic method that can be used to
intrinsically motivate learning agents. It attempts to maximize an agent's
control over the environment by encouraging visiting states with a large number
of reachable next states. Empowered learning has been shown to lead to complex
behaviors, without requiring an explicit reward signal. In this paper, we
investigate the use of empowerment in the presence of an extrinsic reward
signal. We hypothesize that empowerment can guide reinforcement learning (RL)
agents to find good early behavioral solutions by encouraging highly empowered
states. We propose a unified Bellman optimality principle for empowered reward
maximization. Our empowered reward maximization approach generalizes both
Bellman's optimality principle as well as recent information-theoretical
extensions to it. We prove uniqueness of the empowered values and show
convergence to the optimal solution. We then apply this idea to develop
off-policy actor-critic RL algorithms which we validate in high-dimensional
continuous robotics domains (MuJoCo). Our methods demonstrate improved initial
and competitive final performance compared to model-free state-of-the-art
techniques.",2022-11-13 16:04:00,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Úlfar Erlingsson, Ilya Mironov, Ananth Raghunathan, Shuang Song",That which we call private,,,,,http://arxiv.org/abs/1908.03566v2,"The guarantees of security and privacy defenses are often strengthened by
relaxing the assumptions made about attackers or the context in which defenses
are deployed. Such relaxations can be a highly worthwhile topic of
exploration---even though they typically entail assuming a weaker, less
powerful adversary---because there may indeed be great variability in both
attackers' powers and their context.
  However, no weakening or contextual discounting of attackers' power is
assumed for what some have called ""relaxed definitions"" in the analysis of
differential-privacy guarantees. Instead, the definitions so named are the
basis of refinements and more advanced analyses of the worst-case implications
of attackers---without any change assumed in attackers' powers.
  Because they more precisely bound the worst-case privacy loss, these improved
analyses can greatly strengthen the differential-privacy upper-bound
guarantees---sometimes lowering the differential-privacy epsilon by
orders-of-magnitude. As such, to the casual eye, these analyses may appear to
imply a reduced privacy loss. This is a false perception: the privacy loss of
any concrete mechanism cannot change with the choice of a worst-case-loss
upper-bound analysis technique. Practitioners must be careful not to equate
real-world privacy with differential-privacy epsilon values, at least not
without full consideration of the context.",2022-11-13 16:04:00,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Sebastien Racaniere, Andrew K. Lampinen, Adam Santoro, David P. Reichert, Vlad Firoiu, Timothy P. Lillicrap",Automated curricula through setter-solver interactions,"International Conference on Learning Representations, 2020",,,,http://arxiv.org/abs/1909.12892v2,"Reinforcement learning algorithms use correlations between policies and
rewards to improve agent performance. But in dynamic or sparsely rewarding
environments these correlations are often too small, or rewarding events are
too infrequent to make learning feasible. Human education instead relies on
curricula--the breakdown of tasks into simpler, static challenges with dense
rewards--to build up to complex behaviors. While curricula are also useful for
artificial agents, hand-crafting them is time consuming. This has lead
researchers to explore automatic curriculum generation. Here we explore
automatic curriculum generation in rich, dynamic environments. Using a
setter-solver paradigm we show the importance of considering goal validity,
goal feasibility, and goal coverage to construct useful curricula. We
demonstrate the success of our approach in rich but sparsely rewarding 2D and
3D environments, where an agent is tasked to achieve a single goal selected
from a set of possible goals that varies between episodes, and identify
challenges for future work. Finally, we demonstrate the value of a novel
technique that guides agents towards a desired goal distribution. Altogether,
these results represent a substantial step towards applying automatic task
curricula to learn complex, otherwise unlearnable goals, and to our knowledge
are the first to demonstrate automated curriculum generation for
goal-conditioned agents in environments where the possible goals vary between
episodes.",2022-11-13 16:04:01,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Vicki Bier, Paul B. Kantor, Gary Lupyan, Xiaojin Zhu",Can We Distinguish Machine Learning from Human Learning?,,,,,http://arxiv.org/abs/1910.03466v1,"What makes a task relatively more or less difficult for a machine compared to
a human? Much AI/ML research has focused on expanding the range of tasks that
machines can do, with a focus on whether machines can beat humans. Allowing for
differences in scale, we can seek interesting (anomalous) pairs of tasks T, T'.
We define interesting in this way: The ""harder to learn"" relation is reversed
when comparing human intelligence (HI) to AI. While humans seems to be able to
understand problems by formulating rules, ML using neural networks does not
rely on constructing rules. We discuss a novel approach where the challenge is
to ""perform well under rules that have been created by human beings."" We
suggest that this provides a rigorous and precise pathway for understanding the
difference between the two kinds of learning. Specifically, we suggest a large
and extensible class of learning tasks, formulated as learning under rules.
With these tasks, both the AI and HI will be studied with rigor and precision.
The immediate goal is to find interesting groundtruth rule pairs. In the long
term, the goal will be to understand, in a generalizable way, what
distinguishes interesting pairs from ordinary pairs, and to define saliency
behind interesting pairs. This may open new ways of thinking about AI, and
provide unexpected insights into human learning.",2022-11-13 16:04:01,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2019,"Andreas Sedlmeier, Thomas Gabor, Thomy Phan, Lenz Belzner, Claudia Linnhoff-Popien",Uncertainty-Based Out-of-Distribution Classification in Deep Reinforcement Learning,"Proceedings of the 12th International Conference on Agents and
  Artificial Intelligence - Volume 2: ICAART, 2020, ISBN 978-989-758-395-7,
  pages 522-529",,,10.5220/0008949905220529,http://arxiv.org/abs/2001.00496v1,"Robustness to out-of-distribution (OOD) data is an important goal in building
reliable machine learning systems. Especially in autonomous systems, wrong
predictions for OOD inputs can cause safety critical situations. As a first
step towards a solution, we consider the problem of detecting such data in a
value-based deep reinforcement learning (RL) setting. Modelling this problem as
a one-class classification problem, we propose a framework for
uncertainty-based OOD classification: UBOOD. It is based on the effect that an
agent's epistemic uncertainty is reduced for situations encountered during
training (in-distribution), and thus lower than for unencountered (OOD)
situations. Being agnostic towards the approach used for estimating epistemic
uncertainty, combinations with different uncertainty estimation methods, e.g.
approximate Bayesian inference methods or ensembling techniques are possible.
We further present a first viable solution for calculating a dynamic
classification threshold, based on the uncertainty distribution of the training
data. Evaluation shows that the framework produces reliable classification
results when combined with ensemble-based estimators, while the combination
with concrete dropout-based estimators fails to reliably detect OOD situations.
In summary, UBOOD presents a viable approach for OOD classification in deep RL
settings by leveraging the epistemic uncertainty of the agent's value function.",2022-11-13 16:04:02,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Roozbeh Yousefzadeh, Dianne P. O'Leary",Auditing and Debugging Deep Learning Models via Decision Boundaries: Individual-level and Group-level Analysis,,,,,http://arxiv.org/abs/2001.00682v1,"Deep learning models have been criticized for their lack of easy
interpretation, which undermines confidence in their use for important
applications. Nevertheless, they are consistently utilized in many
applications, consequential to humans' lives, mostly because of their better
performance. Therefore, there is a great need for computational methods that
can explain, audit, and debug such models. Here, we use flip points to
accomplish these goals for deep learning models with continuous output scores
(e.g., computed by softmax), used in social applications. A flip point is any
point that lies on the boundary between two output classes: e.g. for a model
with a binary yes/no output, a flip point is any input that generates equal
scores for ""yes"" and ""no"". The flip point closest to a given input is of
particular importance because it reveals the least changes in the input that
would change a model's classification, and we show that it is the solution to a
well-posed optimization problem. Flip points also enable us to systematically
study the decision boundaries of a deep learning classifier. The resulting
insight into the decision boundaries of a deep model can clearly explain the
model's output on the individual-level, via an explanation report that is
understandable by non-experts. We also develop a procedure to understand and
audit model behavior towards groups of people. Flip points can also be used to
alter the decision boundaries in order to improve undesirable behaviors. We
demonstrate our methods by investigating several models trained on standard
datasets used in social applications of machine learning. We also identify the
features that are most responsible for particular classifications and
misclassifications.",2022-11-13 16:04:03,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Shakkeel Ahmed, Ravi S. Mula, Soma S. Dhavala",A Framework for Democratizing AI,,,,,http://arxiv.org/abs/2001.00818v1,"Machine Learning and Artificial Intelligence are considered an integral part
of the Fourth Industrial Revolution. Their impact, and far-reaching
consequences, while acknowledged, are yet to be comprehended. These
technologies are very specialized, and few organizations and select highly
trained professionals have the wherewithal, in terms of money, manpower, and
might, to chart the future. However, concentration of power can lead to
marginalization, causing severe inequalities. Regulatory agencies and
governments across the globe are creating national policies, and laws around
these technologies to protect the rights of the digital citizens, as well as to
empower them. Even private, not-for-profit organizations are also contributing
to democratizing the technologies by making them \emph{accessible} and
\emph{affordable}. However, accessibility and affordability are all but a few
of the facets of democratizing the field. Others include, but not limited to,
\emph{portability}, \emph{explainability}, \emph{credibility}, \emph{fairness},
among others. As one can imagine, democratizing AI is a multi-faceted problem,
and it requires advancements in science, technology and policy. At
\texttt{mlsquare}, we are developing scientific tools in this space.
Specifically, we introduce an opinionated, extensible, \texttt{Python}
framework that provides a single point of interface to a variety of solutions
in each of the categories mentioned above. We present the design details, APIs
of the framework, reference implementations, road map for development, and
guidelines for contributions.",2022-11-13 16:04:03,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Carlos Fernández-Loría, Foster Provost, Xintian Han",Explaining Data-Driven Decisions made by AI Systems: The Counterfactual Approach,,,,,http://arxiv.org/abs/2001.07417v5,"We examine counterfactual explanations for explaining the decisions made by
model-based AI systems. The counterfactual approach we consider defines an
explanation as a set of the system's data inputs that causally drives the
decision (i.e., changing the inputs in the set changes the decision) and is
irreducible (i.e., changing any subset of the inputs does not change the
decision). We (1) demonstrate how this framework may be used to provide
explanations for decisions made by general, data-driven AI systems that may
incorporate features with arbitrary data types and multiple predictive models,
and (2) propose a heuristic procedure to find the most useful explanations
depending on the context. We then contrast counterfactual explanations with
methods that explain model predictions by weighting features according to their
importance (e.g., SHAP, LIME) and present two fundamental reasons why we should
carefully consider whether importance-weight explanations are well-suited to
explain system decisions. Specifically, we show that (i) features that have a
large importance weight for a model prediction may not affect the corresponding
decision, and (ii) importance weights are insufficient to communicate whether
and how features influence decisions. We demonstrate this with several concise
examples and three detailed case studies that compare the counterfactual
approach with SHAP to illustrate various conditions under which counterfactual
explanations explain data-driven decisions better than importance weights.",2022-11-13 16:04:04,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Yehia Elrakaiby, Paola Spoletini, Bashar Nuseibeh",Optimal by Design: Model-Driven Synthesis of Adaptation Strategies for Autonomous Systems,,,,,http://arxiv.org/abs/2001.08525v1,"Many software systems have become too large and complex to be managed
efficiently by human administrators, particularly when they operate in
uncertain and dynamic environments and require frequent changes.
Requirements-driven adaptation techniques have been proposed to endow systems
with the necessary means to autonomously decide ways to satisfy their
requirements. However, many current approaches rely on general-purpose
languages, models and/or frameworks to design, develop and analyze autonomous
systems. Unfortunately, these tools are not tailored towards the
characteristics of adaptation problems in autonomous systems. In this paper, we
present Optimal by Design (ObD ), a framework for model-based
requirements-driven synthesis of optimal adaptation strategies for autonomous
systems. ObD proposes a model (and a language) for the high-level description
of the basic elements of self-adaptive systems, namely the system,
capabilities, requirements and environment. Based on those elements, a Markov
Decision Process (MDP) is constructed to compute the optimal strategy or the
most rewarding system behaviour. Furthermore, this defines a reflex controller
that can ensure timely responses to changes. One novel feature of the framework
is that it benefits both from goal-oriented techniques, developed for
requirement elicitation, refinement and analysis, and synthesis capabilities
and extensive research around MDPs, their extensions and tools. Our preliminary
evaluation results demonstrate the practicality and advantages of the
framework.",2022-11-13 16:04:04,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Mislav Juric, Agneza Sandic, Mario Brcic",AI safety: state of the field through quantitative lens,,,,,http://arxiv.org/abs/2002.05671v2,"Last decade has seen major improvements in the performance of artificial
intelligence which has driven wide-spread applications. Unforeseen effects of
such mass-adoption has put the notion of AI safety into the public eye. AI
safety is a relatively new field of research focused on techniques for building
AI beneficial for humans. While there exist survey papers for the field of AI
safety, there is a lack of a quantitative look at the research being conducted.
The quantitative aspect gives a data-driven insight about the emerging trends,
knowledge gaps and potential areas for future research. In this paper,
bibliometric analysis of the literature finds significant increase in research
activity since 2015. Also, the field is so new that most of the technical
issues are open, including: explainability with its long-term utility, and
value alignment which we have identified as the most important long-term
research topic. Equally, there is a severe lack of research into concrete
policies regarding AI. As we expect AI to be the one of the main driving forces
of changes in society, AI safety is the field under which we need to decide the
direction of humanity's future.",2022-11-13 16:04:05,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Akanksha Saran, Ruohan Zhang, Elaine Schaertl Short, Scott Niekum",Efficiently Guiding Imitation Learning Agents with Human Gaze,,,,,http://arxiv.org/abs/2002.12500v4,"Human gaze is known to be an intention-revealing signal in human
demonstrations of tasks. In this work, we use gaze cues from human
demonstrators to enhance the performance of agents trained via three popular
imitation learning methods -- behavioral cloning (BC), behavioral cloning from
observation (BCO), and Trajectory-ranked Reward EXtrapolation (T-REX). Based on
similarities between the attention of reinforcement learning agents and human
gaze, we propose a novel approach for utilizing gaze data in a computationally
efficient manner, as part of an auxiliary loss function, which guides a network
to have higher activations in image regions where the human's gaze fixated.
This work is a step towards augmenting any existing convolutional imitation
learning agent's training with auxiliary gaze data. Our auxiliary
coverage-based gaze loss (CGL) guides learning toward a better reward function
or policy, without adding any additional learnable parameters and without
requiring gaze data at test time. We find that our proposed approach improves
the performance by 95% for BC, 343% for BCO, and 390% for T-REX, averaged over
20 different Atari games. We also find that compared to a prior
state-of-the-art imitation learning method assisted by human gaze (AGIL), our
method achieves better performance, and is more efficient in terms of learning
with fewer demonstrations. We further interpret trained CGL agents with a
saliency map visualization method to explain their performance. At last, we
show that CGL can help alleviate a well-known causal confusion problem in
imitation learning.",2022-11-13 16:04:05,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Peter Hase, Mohit Bansal",Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?,,,,,http://arxiv.org/abs/2005.01831v1,"Algorithmic approaches to interpreting machine learning models have
proliferated in recent years. We carry out human subject tests that are the
first of their kind to isolate the effect of algorithmic explanations on a key
aspect of model interpretability, simulatability, while avoiding important
confounding experimental factors. A model is simulatable when a person can
predict its behavior on new inputs. Through two kinds of simulation tests
involving text and tabular data, we evaluate five explanations methods: (1)
LIME, (2) Anchor, (3) Decision Boundary, (4) a Prototype model, and (5) a
Composite approach that combines explanations from each method. Clear evidence
of method effectiveness is found in very few cases: LIME improves
simulatability in tabular classification, and our Prototype method is effective
in counterfactual simulation tests. We also collect subjective ratings of
explanations, but we do not find that ratings are predictive of how helpful
explanations are. Our results provide the first reliable and comprehensive
estimates of how explanations influence simulatability across a variety of
explanation methods and data domains. We show that (1) we need to be careful
about the metrics we use to evaluate explanation methods, and (2) there is
significant room for improvement in current methods. All our supporting code,
data, and models are publicly available at:
https://github.com/peterbhase/InterpretableNLP-ACL2020",2022-11-13 16:04:06,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Michael K. Cohen, Elliot Catt, Marcus Hutter",Curiosity Killed or Incapacitated the Cat and the Asymptotically Optimal Agent,Journal of Selected Areas in Information Theory 2 (2021),,,,http://arxiv.org/abs/2006.03357v2,"Reinforcement learners are agents that learn to pick actions that lead to
high reward. Ideally, the value of a reinforcement learner's policy approaches
optimality--where the optimal informed policy is the one which maximizes
reward. Unfortunately, we show that if an agent is guaranteed to be
""asymptotically optimal"" in any (stochastically computable) environment, then
subject to an assumption about the true environment, this agent will be either
""destroyed"" or ""incapacitated"" with probability 1. Much work in reinforcement
learning uses an ergodicity assumption to avoid this problem. Often, doing
theoretical research under simplifying assumptions prepares us to provide
practical solutions even in the absence of those assumptions, but the
ergodicity assumption in reinforcement learning may have led us entirely astray
in preparing safe and effective exploration strategies for agents in dangerous
environments. Rather than assuming away the problem, we present an agent,
Mentee, with the modest guarantee of approaching the performance of a mentor,
doing safe exploration instead of reckless exploration. Critically, Mentee's
exploration probability depends on the expected information gain from
exploring. In a simple non-ergodic environment with a weak mentor, we find
Mentee outperforms existing asymptotically optimal agents and its mentor.",2022-11-13 16:04:06,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Matteo Turchetta, Andrey Kolobov, Shital Shah, Andreas Krause, Alekh Agarwal",Safe Reinforcement Learning via Curriculum Induction,,,,,http://arxiv.org/abs/2006.12136v2,"In safety-critical applications, autonomous agents may need to learn in an
environment where mistakes can be very costly. In such settings, the agent
needs to behave safely not only after but also while learning. To achieve this,
existing safe reinforcement learning methods make an agent rely on priors that
let it avoid dangerous situations during exploration with high probability, but
both the probabilistic guarantees and the smoothness assumptions inherent in
the priors are not viable in many scenarios of interest such as autonomous
driving. This paper presents an alternative approach inspired by human
teaching, where an agent learns under the supervision of an automatic
instructor that saves the agent from violating constraints during learning. In
this model, we introduce the monitor that neither needs to know how to do well
at the task the agent is learning nor needs to know how the environment works.
Instead, it has a library of reset controllers that it activates when the agent
starts behaving dangerously, preventing it from doing damage. Crucially, the
choices of which reset controller to apply in which situation affect the speed
of agent learning. Based on observing agents' progress, the teacher itself
learns a policy for choosing the reset controllers, a curriculum, to optimize
the agent's final policy reward. Our experiments use this framework in two
environments to induce curricula for safe and efficient learning.",2022-11-13 16:04:07,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Paul Barde, Julien Roy, Wonseok Jeon, Joelle Pineau, Christopher Pal, Derek Nowrouzezahrai",Adversarial Soft Advantage Fitting: Imitation Learning without Policy Optimization,Advances in Neural Information Processing Systems 33 (2020),,,,http://arxiv.org/abs/2006.13258v6,"Adversarial Imitation Learning alternates between learning a discriminator --
which tells apart expert's demonstrations from generated ones -- and a
generator's policy to produce trajectories that can fool this discriminator.
This alternated optimization is known to be delicate in practice since it
compounds unstable adversarial training with brittle and sample-inefficient
reinforcement learning. We propose to remove the burden of the policy
optimization steps by leveraging a novel discriminator formulation.
Specifically, our discriminator is explicitly conditioned on two policies: the
one from the previous generator's iteration and a learnable policy. When
optimized, this discriminator directly learns the optimal generator's policy.
Consequently, our discriminator's update solves the generator's optimization
problem for free: learning a policy that imitates the expert does not require
an additional optimization loop. This formulation effectively cuts by half the
implementation and computational burden of Adversarial Imitation Learning
algorithms by removing the Reinforcement Learning phase altogether. We show on
a variety of tasks that our simpler approach is competitive to prevalent
Imitation Learning methods.",2022-11-13 16:04:07,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Ehsan Toreini, Mhairi Aitken, Kovila P. L. Coopamootoo, Karen Elliott, Vladimiro Gonzalez Zelaya, Paolo Missier, Magdalene Ng, Aad van Moorsel",Technologies for Trustworthy Machine Learning: A Survey in a Socio-Technical Context,,,,,http://arxiv.org/abs/2007.08911v3,"Concerns about the societal impact of AI-based services and systems has
encouraged governments and other organisations around the world to propose AI
policy frameworks to address fairness, accountability, transparency and related
topics. To achieve the objectives of these frameworks, the data and software
engineers who build machine-learning systems require knowledge about a variety
of relevant supporting tools and techniques. In this paper we provide an
overview of technologies that support building trustworthy machine learning
systems, i.e., systems whose properties justify that people place trust in
them. We argue that four categories of system properties are instrumental in
achieving the policy objectives, namely fairness, explainability, auditability
and safety & security (FEAS). We discuss how these properties need to be
considered across all stages of the machine learning life cycle, from data
collection through run-time model inference. As a consequence, we survey in
this paper the main technologies with respect to all four of the FEAS
properties, for data-centric as well as model-centric stages of the machine
learning system life cycle. We conclude with an identification of open research
problems, with a particular focus on the connection between trustworthy machine
learning technologies and their implications for individuals and society.",2022-11-13 16:04:08,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Luca Weihs, Unnat Jain, Iou-Jen Liu, Jordi Salvador, Svetlana Lazebnik, Aniruddha Kembhavi, Alexander Schwing",Bridging the Imitation Gap by Adaptive Insubordination,,,,,http://arxiv.org/abs/2007.12173v3,"In practice, imitation learning is preferred over pure reinforcement learning
whenever it is possible to design a teaching agent to provide expert
supervision. However, we show that when the teaching agent makes decisions with
access to privileged information that is unavailable to the student, this
information is marginalized during imitation learning, resulting in an
""imitation gap"" and, potentially, poor results. Prior work bridges this gap via
a progression from imitation learning to reinforcement learning. While often
successful, gradual progression fails for tasks that require frequent switches
between exploration and memorization. To better address these tasks and
alleviate the imitation gap we propose 'Adaptive Insubordination' (ADVISOR).
ADVISOR dynamically weights imitation and reward-based reinforcement learning
losses during training, enabling on-the-fly switching between imitation and
exploration. On a suite of challenging tasks set within gridworlds, multi-agent
particle environments, and high-fidelity 3D simulators, we show that on-the-fly
switching with ADVISOR outperforms pure imitation, pure reinforcement learning,
as well as their sequential and parallel combinations.",2022-11-13 16:04:08,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Evan Zheran Liu, Aditi Raghunathan, Percy Liang, Chelsea Finn",Decoupling Exploration and Exploitation for Meta-Reinforcement Learning without Sacrifices,,,,,http://arxiv.org/abs/2008.02790v4,"The goal of meta-reinforcement learning (meta-RL) is to build agents that can
quickly learn new tasks by leveraging prior experience on related tasks.
Learning a new task often requires both exploring to gather task-relevant
information and exploiting this information to solve the task. In principle,
optimal exploration and exploitation can be learned end-to-end by simply
maximizing task performance. However, such meta-RL approaches struggle with
local optima due to a chicken-and-egg problem: learning to explore requires
good exploitation to gauge the exploration's utility, but learning to exploit
requires information gathered via exploration. Optimizing separate objectives
for exploration and exploitation can avoid this problem, but prior meta-RL
exploration objectives yield suboptimal policies that gather information
irrelevant to the task. We alleviate both concerns by constructing an
exploitation objective that automatically identifies task-relevant information
and an exploration objective to recover only this information. This avoids
local optima in end-to-end training, without sacrificing optimal exploration.
Empirically, DREAM substantially outperforms existing approaches on complex
meta-RL problems, such as sparse-reward 3D visual navigation. Videos of DREAM:
https://ezliu.github.io/dream/",2022-11-13 16:04:09,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Eiji Uchibe, Kenji Doya",Forward and inverse reinforcement learning sharing network weights and hyperparameters,"Neural Networks, December 2021, Pages 138-153",,,10.1016/j.neunet.2021.08.017,http://arxiv.org/abs/2008.07284v2,"This paper proposes model-free imitation learning named Entropy-Regularized
Imitation Learning (ERIL) that minimizes the reverse Kullback-Leibler (KL)
divergence. ERIL combines forward and inverse reinforcement learning (RL) under
the framework of an entropy-regularized Markov decision process. An inverse RL
step computes the log-ratio between two distributions by evaluating two binary
discriminators. The first discriminator distinguishes the state generated by
the forward RL step from the expert's state. The second discriminator, which is
structured by the theory of entropy regularization, distinguishes the
state-action-next-state tuples generated by the learner from the expert ones.
One notable feature is that the second discriminator shares hyperparameters
with the forward RL, which can be used to control the discriminator's ability.
A forward RL step minimizes the reverse KL estimated by the inverse RL step. We
show that minimizing the reverse KL divergence is equivalent to finding an
optimal policy. Our experimental results on MuJoCo-simulated environments and
vision-based reaching tasks with a robotic arm show that ERIL is more
sample-efficient than the baseline methods. We apply the method to human
behaviors that perform a pole-balancing task and describe how the estimated
reward functions show how every subject achieves her goal.",2022-11-13 16:04:09,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,John Mark Bishop,Artificial Intelligence is stupid and causal reasoning won't fix it,,,,,http://arxiv.org/abs/2008.07371v1,"Artificial Neural Networks have reached Grandmaster and even super-human
performance across a variety of games: from those involving perfect-information
(such as Go) to those involving imperfect-information (such as Starcraft). Such
technological developments from AI-labs have ushered concomitant applications
across the world of business - where an AI brand tag is fast becoming
ubiquitous. A corollary of such widespread commercial deployment is that when
AI gets things wrong - an autonomous vehicle crashes; a chatbot exhibits racist
behaviour; automated credit scoring processes discriminate on gender etc. -
there are often significant financial, legal and brand consequences and the
incident becomes major news. As Judea Pearl sees it, the underlying reason for
such mistakes is that, 'all the impressive achievements of deep learning amount
to just curve fitting'. The key, Judea Pearl suggests, is to replace reasoning
by association with causal-reasoning - the ability to infer causes from
observed phenomena. It is a point that was echoed by Gary Marcus and Ernest
Davis in a recent piece for the New York Times: 'we need to stop building
computer systems that merely get better and better at detecting statistical
patterns in data sets - often using an approach known as Deep Learning - and
start building computer systems that from the moment of their assembly innately
grasp three basic concepts: time, space and causality'. In this paper,
foregrounding what in 1949 Gilbert Ryle termed a category mistake, I will offer
an alternative explanation for AI errors: it is not so much that AI machinery
cannot grasp causality, but that AI machinery - qua computation - cannot
understand anything at all.",2022-11-13 16:04:10,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Peipei Xu, Wenjie Ruan, Xiaowei Huang",Towards the Quantification of Safety Risks in Deep Neural Networks,,,,,http://arxiv.org/abs/2009.06114v1,"Safety concerns on the deep neural networks (DNNs) have been raised when they
are applied to critical sectors. In this paper, we define safety risks by
requesting the alignment of the network's decision with human perception. To
enable a general methodology for quantifying safety risks, we define a generic
safety property and instantiate it to express various safety risks. For the
quantification of risks, we take the maximum radius of safe norm balls, in
which no safety risk exists. The computation of the maximum safe radius is
reduced to the computation of their respective Lipschitz metrics - the
quantities to be computed. In addition to the known adversarial example,
reachability example, and invariant example, in this paper we identify a new
class of risk - uncertainty example - on which humans can tell easily but the
network is unsure. We develop an algorithm, inspired by derivative-free
optimization techniques and accelerated by tensor-based parallelization on
GPUs, to support efficient computation of the metrics. We perform evaluations
on several benchmark neural networks, including ACSC-Xu, MNIST, CIFAR-10, and
ImageNet networks. The experiments show that, our method can achieve
competitive performance on safety quantification in terms of the tightness and
the efficiency of computation. Importantly, as a generic approach, our method
can work with a broad class of safety risks and without restrictions on the
structure of neural networks.",2022-11-13 16:04:10,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Minhae Kwon, Saurabh Daptardar, Paul Schrater, Xaq Pitkow",Inverse Rational Control with Partially Observable Continuous Nonlinear Dynamics,,,,,http://arxiv.org/abs/2009.12576v2,"A fundamental question in neuroscience is how the brain creates an internal
model of the world to guide actions using sequences of ambiguous sensory
information. This is naturally formulated as a reinforcement learning problem
under partial observations, where an agent must estimate relevant latent
variables in the world from its evidence, anticipate possible future states,
and choose actions that optimize total expected reward. This problem can be
solved by control theory, which allows us to find the optimal actions for a
given system dynamics and objective function. However, animals often appear to
behave suboptimally. Why? We hypothesize that animals have their own flawed
internal model of the world, and choose actions with the highest expected
subjective reward according to that flawed model. We describe this behavior as
rational but not optimal. The problem of Inverse Rational Control (IRC) aims to
identify which internal model would best explain an agent's actions. Our
contribution here generalizes past work on Inverse Rational Control which
solved this problem for discrete control in partially observable Markov
decision processes. Here we accommodate continuous nonlinear dynamics and
continuous actions, and impute sensory observations corrupted by unknown noise
that is private to the animal. We first build an optimal Bayesian agent that
learns an optimal policy generalized over the entire model space of dynamics
and subjective rewards using deep reinforcement learning. Crucially, this
allows us to compute a likelihood over models for experimentally observable
action trajectories acquired from a suboptimal agent. We then find the model
parameters that maximize the likelihood using gradient ascent.",2022-11-13 16:04:11,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Chintan Donda, Sayan Dasgupta, Soma S Dhavala, Keyur Faldu, Aditi Avasthi","A framework for predicting, interpreting, and improving Learning Outcomes",,,,,http://arxiv.org/abs/2010.02629v2,"It has long been recognized that academic success is a result of both
cognitive and non-cognitive dimensions acting together. Consequently, any
intelligent learning platform designed to improve learning outcomes (LOs) must
provide actionable inputs to the learner in these dimensions. However,
operationalizing such inputs in a production setting that is scalable is not
trivial. We develop an Embibe Score Quotient model (ESQ) to predict test scores
based on observed academic, behavioral and test-taking features of a student.
ESQ can be used to predict the future scoring potential of a student as well as
offer personalized learning nudges, both critical to improving LOs. Multiple
machine learning models are evaluated for the prediction task. In order to
provide meaningful feedback to the learner, individualized Shapley feature
attributions for each feature are computed. Prediction intervals are obtained
by applying non-parametric quantile regression, in an attempt to quantify the
uncertainty in the predictions. We apply the above modelling strategy on a
dataset consisting of more than a hundred million learner interactions on the
Embibe learning platform. We observe that the Median Absolute Error between the
observed and predicted scores is 4.58% across several user segments, and the
correlation between predicted and observed responses is 0.93. Game-like what-if
scenarios are played out to see the changes in LOs, on counterfactual examples.
We briefly discuss how a rational agent can then apply an optimal policy to
affect the learning outcomes by treating the above model like an Oracle.",2022-11-13 16:04:11,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Santiago Miret, Somdeb Majumdar, Carroll Wainwright",Safety Aware Reinforcement Learning (SARL),,,,,http://arxiv.org/abs/2010.02846v1,"As reinforcement learning agents become increasingly integrated into complex,
real-world environments, designing for safety becomes a critical consideration.
We specifically focus on researching scenarios where agents can cause undesired
side effects while executing a policy on a primary task. Since one can define
multiple tasks for a given environment dynamics, there are two important
challenges. First, we need to abstract the concept of safety that applies
broadly to that environment independent of the specific task being executed.
Second, we need a mechanism for the abstracted notion of safety to modulate the
actions of agents executing different policies to minimize their side-effects.
In this work, we propose Safety Aware Reinforcement Learning (SARL) - a
framework where a virtual safe agent modulates the actions of a main
reward-based agent to minimize side effects. The safe agent learns a
task-independent notion of safety for a given environment. The main agent is
then trained with a regularization loss given by the distance between the
native action probabilities of the two agents. Since the safe agent effectively
abstracts a task-independent notion of safety via its action probabilities, it
can be ported to modulate multiple policies solving different tasks within the
given environment without further training. We contrast this with solutions
that rely on task-specific regularization metrics and test our framework on the
SafeLife Suite, based on Conway's Game of Life, comprising a number of complex
tasks in dynamic environments. We show that our solution is able to match the
performance of solutions that rely on task-specific side-effect penalties on
both the primary and safety objectives while additionally providing the benefit
of generalizability and portability.",2022-11-13 16:04:12,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Heribert Wankerl, Maike L. Stern, Ali Mahdavi, Christoph Eichler, Elmar W. Lang",Parameterized Reinforcement Learning for Optical System Optimization,J. Phys. D: Appl. Phys. 54 305104 (2021),,,10.1088/1361-6463/abfddb,http://arxiv.org/abs/2010.05769v2,"Designing a multi-layer optical system with designated optical
characteristics is an inverse design problem in which the resulting design is
determined by several discrete and continuous parameters. In particular, we
consider three design parameters to describe a multi-layer stack: Each layer's
dielectric material and thickness as well as the total number of layers. Such a
combination of both, discrete and continuous parameters is a challenging
optimization problem that often requires a computationally expensive search for
an optimal system design. Hence, most methods merely determine the optimal
thicknesses of the system's layers. To incorporate layer material and the total
number of layers as well, we propose a method that considers the stacking of
consecutive layers as parameterized actions in a Markov decision process. We
propose an exponentially transformed reward signal that eases policy
optimization and adapt a recent variant of Q-learning for inverse design
optimization. We demonstrate that our method outperforms human experts and a
naive reinforcement learning algorithm concerning the achieved optical
characteristics. Moreover, the learned Q-values contain information about the
optical properties of multi-layer optical systems, thereby allowing physical
interpretation or what-if analysis.",2022-11-13 16:04:12,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Victoria Krakovna, Laurent Orseau, Richard Ngo, Miljan Martic, Shane Legg",Avoiding Side Effects By Considering Future Tasks,,,,,http://arxiv.org/abs/2010.07877v1,"Designing reward functions is difficult: the designer has to specify what to
do (what it means to complete the task) as well as what not to do (side effects
that should be avoided while completing the task). To alleviate the burden on
the reward designer, we propose an algorithm to automatically generate an
auxiliary reward function that penalizes side effects. This auxiliary objective
rewards the ability to complete possible future tasks, which decreases if the
agent causes side effects during the current task. The future task reward can
also give the agent an incentive to interfere with events in the environment
that make future tasks less achievable, such as irreversible actions by other
agents. To avoid this interference incentive, we introduce a baseline policy
that represents a default course of action (such as doing nothing), and use it
to filter out future tasks that are not achievable by default. We formally
define interference incentives and show that the future task approach with a
baseline policy avoids these incentives in the deterministic case. Using
gridworld environments that test for side effects and interference, we show
that our method avoids interference and is more effective for avoiding side
effects than the common approach of penalizing irreversible actions.",2022-11-13 16:04:12,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Avik Pal, Jonah Philion, Yuan-Hong Liao, Sanja Fidler",Emergent Road Rules In Multi-Agent Driving Environments,"International Conference on Learning Representations, 2021",,,,http://arxiv.org/abs/2011.10753v2,"For autonomous vehicles to safely share the road with human drivers,
autonomous vehicles must abide by specific ""road rules"" that human drivers have
agreed to follow. ""Road rules"" include rules that drivers are required to
follow by law -- such as the requirement that vehicles stop at red lights -- as
well as more subtle social rules -- such as the implicit designation of fast
lanes on the highway. In this paper, we provide empirical evidence that
suggests that -- instead of hard-coding road rules into self-driving algorithms
-- a scalable alternative may be to design multi-agent environments in which
road rules emerge as optimal solutions to the problem of maximizing traffic
flow. We analyze what ingredients in driving environments cause the emergence
of these road rules and find that two crucial factors are noisy perception and
agents' spatial density. We provide qualitative and quantitative evidence of
the emergence of seven social driving behaviors, ranging from obeying traffic
signals to following lanes, all of which emerge from training agents to drive
quickly to destinations without colliding. Our results add empirical support
for the social road rules that countries worldwide have agreed on for safe,
efficient driving.",2022-11-13 16:04:13,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2020,"Suresh Venkatasubramanian, Nadya Bliss, Helen Nissenbaum, Melanie Moses",Interdisciplinary Approaches to Understanding Artificial Intelligence's Impact on Society,,,,,http://arxiv.org/abs/2012.06057v1,"Innovations in AI have focused primarily on the questions of ""what"" and
""how""-algorithms for finding patterns in web searches, for instance-without
adequate attention to the possible harms (such as privacy, bias, or
manipulation) and without adequate consideration of the societal context in
which these systems operate. In part, this is driven by incentives and forces
in the tech industry, where a more product-driven focus tends to drown out
broader reflective concerns about potential harms and misframings. But this
focus on what and how is largely a reflection of the engineering and
mathematics-focused training in computer science, which emphasizes the building
of tools and development of computational concepts.
  As a result of this tight technical focus, and the rapid, worldwide explosion
in its use, AI has come with a storm of unanticipated socio-technical problems,
ranging from algorithms that act in racially or gender-biased ways, get caught
in feedback loops that perpetuate inequalities, or enable unprecedented
behavioral monitoring surveillance that challenges the fundamental values of
free, democratic societies.
  Given that AI is no longer solely the domain of technologists but rather of
society as a whole, we need tighter coupling of computer science and those
disciplines that study society and societal values.",2022-11-13 16:04:14,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Lu Cheng, Kush R. Varshney, Huan Liu","Socially Responsible AI Algorithms: Issues, Purposes, and Challenges",Journal of Artificial Intelligence Research 71 (2021) 1137-1181,,,,http://arxiv.org/abs/2101.02032v5,"In the current era, people and society have grown increasingly reliant on
artificial intelligence (AI) technologies. AI has the potential to drive us
towards a future in which all of humanity flourishes. It also comes with
substantial risks for oppression and calamity. Discussions about whether we
should (re)trust AI have repeatedly emerged in recent years and in many
quarters, including industry, academia, healthcare, services, and so on.
Technologists and AI researchers have a responsibility to develop trustworthy
AI systems. They have responded with great effort to design more responsible AI
algorithms. However, existing technical solutions are narrow in scope and have
been primarily directed towards algorithms for scoring or classification tasks,
with an emphasis on fairness and unwanted bias. To build long-lasting trust
between AI and human beings, we argue that the key is to think beyond
algorithmic fairness and connect major aspects of AI that potentially cause
AI's indifferent behavior. In this survey, we provide a systematic framework of
Socially Responsible AI Algorithms that aims to examine the subjects of AI
indifference and the need for socially responsible AI algorithms, define the
objectives, and introduce the means by which we may achieve these objectives.
We further discuss how to leverage this framework to improve societal
well-being through protection, information, and prevention/mitigation.",2022-11-13 16:04:14,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Siyi Hu, Fengda Zhu, Xiaojun Chang, Xiaodan Liang",UPDeT: Universal Multi-agent Reinforcement Learning via Policy Decoupling with Transformers,,,,,http://arxiv.org/abs/2101.08001v3,"Recent advances in multi-agent reinforcement learning have been largely
limited in training one model from scratch for every new task. The limitation
is due to the restricted model architecture related to fixed input and output
dimensions. This hinders the experience accumulation and transfer of the
learned agent over tasks with diverse levels of difficulty (e.g. 3 vs 3 or 5 vs
6 multi-agent games). In this paper, we make the first attempt to explore a
universal multi-agent reinforcement learning pipeline, designing one single
architecture to fit tasks with the requirement of different observation and
action configurations. Unlike previous RNN-based models, we utilize a
transformer-based model to generate a flexible policy by decoupling the policy
distribution from the intertwined input observation with an importance weight
measured by the merits of the self-attention mechanism. Compared to a standard
transformer block, the proposed model, named as Universal Policy Decoupling
Transformer (UPDeT), further relaxes the action restriction and makes the
multi-agent task's decision process more explainable. UPDeT is general enough
to be plugged into any multi-agent reinforcement learning pipeline and equip
them with strong generalization abilities that enables the handling of multiple
tasks at a time. Extensive experiments on large-scale SMAC multi-agent
competitive games demonstrate that the proposed UPDeT-based multi-agent
reinforcement learning achieves significant results relative to
state-of-the-art approaches, demonstrating advantageous transfer capability in
terms of both performance and training speed (10 times faster).",2022-11-13 16:04:15,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,Abhishek Gupta,Making Responsible AI the Norm rather than the Exception,,,,,http://arxiv.org/abs/2101.11832v2,"This report prepared by the Montreal AI Ethics Institute provides
recommendations in response to the National Security Commission on Artificial
Intelligence (NSCAI) Key Considerations for Responsible Development and
Fielding of Artificial Intelligence document. The report centres on the idea
that Responsible AI should be made the Norm rather than an Exception. It does
so by utilizing the guiding principles of: (1) alleviating friction in existing
workflows, (2) empowering stakeholders to get buy-in, and (3) conducting an
effective translation of abstract standards into actionable engineering
practices. After providing some overarching comments on the document from the
NSCAI, the report dives into the primary contribution of an actionable
framework to help operationalize the ideas presented in the document from the
NSCAI. The framework consists of: (1) a learning, knowledge, and information
exchange (LKIE), (2) the Three Ways of Responsible AI, (3) an
empirically-driven risk-prioritization matrix, and (4) achieving the right
level of complexity. All components reinforce each other to move from
principles to practice in service of making Responsible AI the norm rather than
the exception.",2022-11-13 16:04:15,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Mingqi Yuan, Mao-on Pun",Exploring Beyond-Demonstrator via Meta Learning-Based Reward Extrapolation,,,,,http://arxiv.org/abs/2102.02454v12,"Extrapolating beyond-demonstrator (BD) performance through the imitation
learning (IL) algorithm aims to learn from and subsequently outperform the
demonstrator. To that end, a representative approach is to leverage inverse
reinforcement learning (IRL) to infer a reward function from demonstrations
before performing RL on the learned reward function. However, most existing
reward extrapolation methods require massive demonstrations, making it
difficult to be applied in tasks of limited training data. To address this
problem, one simple solution is to perform data augmentation to artificially
generate more training data, which may incur severe inductive bias and policy
performance loss. In this paper, we propose a novel meta learning-based reward
extrapolation (MLRE) algorithm, which can effectively approximate the
ground-truth rewards using limited demonstrations. More specifically, MLRE
first learns an initial reward function from a set of tasks that have abundant
training data. Then the learned reward function will be fine-tuned using data
of the target task. Extensive simulation results demonstrated that the proposed
MLRE can achieve impressive performance improvement as compared to other
similar BDIL algorithms.",2022-11-13 16:04:16,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Markus Kneer, Michael T. Stuart",Playing the Blame Game with Robots,,,,10.1145/3434074.3447202,http://arxiv.org/abs/2102.04527v1,"Recent research shows -- somewhat astonishingly -- that people are willing to
ascribe moral blame to AI-driven systems when they cause harm [1]-[4]. In this
paper, we explore the moral-psychological underpinnings of these findings. Our
hypothesis was that the reason why people ascribe moral blame to AI systems is
that they consider them capable of entertaining inculpating mental states (what
is called mens rea in the law). To explore this hypothesis, we created a
scenario in which an AI system runs a risk of poisoning people by using a novel
type of fertilizer. Manipulating the computational (or quasi-cognitive)
abilities of the AI system in a between-subjects design, we tested whether
people's willingness to ascribe knowledge of a substantial risk of harm (i.e.,
recklessness) and blame to the AI system. Furthermore, we investigated whether
the ascription of recklessness and blame to the AI system would influence the
perceived blameworthiness of the system's user (or owner). In an experiment
with 347 participants, we found (i) that people are willing to ascribe blame to
AI systems in contexts of recklessness, (ii) that blame ascriptions depend
strongly on the willingness to attribute recklessness and (iii) that the
latter, in turn, depends on the perceived ""cognitive"" capacities of the system.
Furthermore, our results suggest (iv) that the higher the computational
sophistication of the AI system, the more blame is shifted from the human user
to the AI system.",2022-11-13 16:04:16,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,Wenjing Chu,A Decentralized Approach towards Responsible AI in Social Ecosystems,,,,,http://arxiv.org/abs/2102.06362v3,"For AI technology to fulfill its full promises, we must have effective means
to ensure Responsible AI behavior and curtail potential irresponsible use,
e.g., in areas of privacy protection, human autonomy, robustness, and
prevention of biases and discrimination in automated decision making. Recent
literature in the field has identified serious shortcomings of narrow
technology focused and formalism-oriented research and has proposed an
interdisciplinary approach that brings the social context into the scope of
study. In this paper, we take a sociotechnical approach to propose a more
expansive framework of thinking about the Responsible AI challenges in both
technical and social context. Effective solutions need to bridge the gap
between a technical system with the social system that it will be deployed to.
To this end, we propose human agency and regulation as main mechanisms of
intervention and propose a decentralized computational infrastructure, or a set
of public utilities, as the computational means to bridge this gap. A
decentralized infrastructure is uniquely suited for meeting this challenge and
enable technical solutions and social institutions in a mutually reinforcing
dynamic to achieve Responsible AI goals. Our approach is novel in its
sociotechnical approach and its aim in tackling the structural issues that
cannot be solved within the narrow confines of AI technical research. We then
explore possible features of the proposed infrastructure and discuss how it may
help solve example problems recently studied in the field.",2022-11-13 16:04:16,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Michiel A. Bakker, Richard Everett, Laura Weidinger, Iason Gabriel, William S. Isaac, Joel Z. Leibo, Edward Hughes",Modelling Cooperation in Network Games with Spatio-Temporal Complexity,,,,,http://arxiv.org/abs/2102.06911v1,"The real world is awash with multi-agent problems that require collective
action by self-interested agents, from the routing of packets across a computer
network to the management of irrigation systems. Such systems have local
incentives for individuals, whose behavior has an impact on the global outcome
for the group. Given appropriate mechanisms describing agent interaction,
groups may achieve socially beneficial outcomes, even in the face of short-term
selfish incentives. In many cases, collective action problems possess an
underlying graph structure, whose topology crucially determines the
relationship between local decisions and emergent global effects. Such
scenarios have received great attention through the lens of network games.
However, this abstraction typically collapses important dimensions, such as
geometry and time, relevant to the design of mechanisms promoting cooperation.
In parallel work, multi-agent deep reinforcement learning has shown great
promise in modelling the emergence of self-organized cooperation in complex
gridworld domains. Here we apply this paradigm in graph-structured collective
action problems. Using multi-agent deep reinforcement learning, we simulate an
agent society for a variety of plausible mechanisms, finding clear transitions
between different equilibria over time. We define analytic tools inspired by
related literatures to measure the social outcomes, and use these to draw
conclusions about the efficacy of different environmental interventions. Our
methods have implications for mechanism design in both human and artificial
agent systems.",2022-11-13 16:04:17,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Tao Zhang, Quanyan Zhu",On the Equilibrium Elicitation of Markov Games Through Information Design,,,,,http://arxiv.org/abs/2102.07152v1,"This work considers a novel information design problem and studies how the
craft of payoff-relevant environmental signals solely can influence the
behaviors of intelligent agents. The agents' strategic interactions are
captured by an incomplete-information Markov game, in which each agent first
selects one environmental signal from multiple signal sources as additional
payoff-relevant information and then takes an action. There is a rational
information designer (designer) who possesses one signal source and aims to
control the equilibrium behaviors of the agents by designing the information
structure of her signals sent to the agents. An obedient principle is
established which states that it is without loss of generality to focus on the
direct information design when the information design incentivizes each agent
to select the signal sent by the designer, such that the design process avoids
the predictions of the agents' strategic selection behaviors. We then introduce
the design protocol given a goal of the designer referred to as obedient
implementability (OIL) and characterize the OIL in a class of obedient perfect
Bayesian Markov Nash equilibria (O-PBME). A new framework for information
design is proposed based on an approach of maximizing the optimal slack
variables. Finally, we formulate the designer's goal selection problem and
characterize it in terms of information design by establishing a relationship
between the O-PBME and the Bayesian Markov correlated equilibria, in which we
build upon the revelation principle in classic information design in economics.
The proposed approach can be applied to elicit desired behaviors of multi-agent
systems in competing as well as cooperating settings and be extended to
heterogeneous stochastic games in the complete- and the incomplete-information
environments.",2022-11-13 16:04:17,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Benjamin Patrick Evans, Mikhail Prokopenko",A maximum entropy model of bounded rational decision-making with prior beliefs and market feedback,,,,10.3390/e23060669,http://arxiv.org/abs/2102.09180v3,"Bounded rationality is an important consideration stemming from the fact that
agents often have limits on their processing abilities, making the assumption
of perfect rationality inapplicable to many real tasks. We propose an
information-theoretic approach to the inference of agent decisions under
Smithian competition. The model explicitly captures the boundedness of agents
(limited in their information-processing capacity) as the cost of information
acquisition for expanding their prior beliefs. The expansion is measured as the
Kullblack-Leibler divergence between posterior decisions and prior beliefs.
When information acquisition is free, the homo economicus agent is recovered,
while in cases when information acquisition becomes costly, agents instead
revert to their prior beliefs. The maximum entropy principle is used to infer
least-biased decisions based upon the notion of Smithian competition formalised
within the Quantal Response Statistical Equilibrium framework. The
incorporation of prior beliefs into such a framework allowed us to
systematically explore the effects of prior beliefs on decision-making in the
presence of market feedback, as well as importantly adding a temporal
interpretation to the framework. We verified the proposed model using
Australian housing market data, showing how the incorporation of prior
knowledge alters the resulting agent decisions. Specifically, it allowed for
the separation of past beliefs and utility maximisation behaviour of the agent
as well as the analysis into the evolution of agent beliefs.",2022-11-13 16:04:18,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Rui Yang, Jiafei Lyu, Yu Yang, Jiangpeng Yan, Feng Luo, Dijun Luo, Lanqing Li, Xiu Li",Bias-reduced Multi-step Hindsight Experience Replay for Efficient Multi-goal Reinforcement Learning,,,,,http://arxiv.org/abs/2102.12962v3,"Multi-goal reinforcement learning is widely applied in planning and robot
manipulation. Two main challenges in multi-goal reinforcement learning are
sparse rewards and sample inefficiency. Hindsight Experience Replay (HER) aims
to tackle the two challenges via goal relabeling. However, HER-related works
still need millions of samples and a huge computation. In this paper, we
propose Multi-step Hindsight Experience Replay (MHER), incorporating multi-step
relabeled returns based on $n$-step relabeling to improve sample efficiency.
Despite the advantages of $n$-step relabeling, we theoretically and
experimentally prove the off-policy $n$-step bias introduced by $n$-step
relabeling may lead to poor performance in many environments. To address the
above issue, two bias-reduced MHER algorithms, MHER($\lambda$) and Model-based
MHER (MMHER) are presented. MHER($\lambda$) exploits the $\lambda$ return while
MMHER benefits from model-based value expansions. Experimental results on
numerous multi-goal robotic tasks show that our solutions can successfully
alleviate off-policy $n$-step bias and achieve significantly higher sample
efficiency than HER and Curriculum-guided HER with little additional
computation beyond HER.",2022-11-13 16:04:18,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Víctor Campos, Pablo Sprechmann, Steven Hansen, Andre Barreto, Steven Kapturowski, Alex Vitvitskyi, Adrià Puigdomènech Badia, Charles Blundell",Beyond Fine-Tuning: Transferring Behavior in Reinforcement Learning,,,,,http://arxiv.org/abs/2102.13515v3,"Designing agents that acquire knowledge autonomously and use it to solve new
tasks efficiently is an important challenge in reinforcement learning.
Knowledge acquired during an unsupervised pre-training phase is often
transferred by fine-tuning neural network weights once rewards are exposed, as
is common practice in supervised domains. Given the nature of the reinforcement
learning problem, we argue that standard fine-tuning strategies alone are not
enough for efficient transfer in challenging domains. We introduce Behavior
Transfer (BT), a technique that leverages pre-trained policies for exploration
and that is complementary to transferring neural network weights. Our
experiments show that, when combined with large-scale pre-training in the
absence of rewards, existing intrinsic motivation objectives can lead to the
emergence of complex behaviors. These pre-trained policies can then be
leveraged by BT to discover better solutions than without pre-training, and
combining BT with standard fine-tuning strategies results in additional
benefits. The largest gains are generally observed in domains requiring
structured exploration, including settings where the behavior of the
pre-trained policies is misaligned with the downstream task.",2022-11-13 16:04:19,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Leandro Eichenberger, Michael Cochez, Benjamin Heitmann, Stefan Decker",Secure Evaluation of Knowledge Graph Merging Gain,,,,,http://arxiv.org/abs/2103.00082v1,"Finding out the differences and commonalities between the knowledge of two
parties is an important task. Such a comparison becomes necessary, when one
party wants to determine how much it is worth to acquire the knowledge of the
second party, or similarly when two parties try to determine, whether a
collaboration could be beneficial. When these two parties cannot trust each
other (for example, due to them being competitors) performing such a comparison
is challenging as neither of them would be willing to share any of their
assets. This paper addresses this problem for knowledge graphs, without a need
for non-disclosure agreements nor a third party during the protocol.
  During the protocol, the intersection between the two knowledge graphs is
determined in a privacy preserving fashion. This is followed by the computation
of various metrics, which give an indication of the potential gain from
obtaining the other parties knowledge graph, while still keeping the actual
knowledge graph contents secret. The protocol makes use of blind signatures and
(counting) Bloom filters to reduce the amount of leaked information. Finally,
the party who wants to obtain the other's knowledge graph can get a part of
such in a way that neither party is able to know beforehand which parts of the
graph are obtained (i.e., they cannot choose to only get or share the good
parts). After inspection of the quality of this part, the Buyer can decide to
proceed with the transaction.
  The analysis of the protocol indicates that the developed protocol is secure
against malicious participants. Further experimental analysis shows that the
resource consumption scales linear with the number of statements in the
knowledge graph.",2022-11-13 16:04:19,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"André Artelt, Valerie Vaquet, Riza Velioglu, Fabian Hinder, Johannes Brinkrolf, Malte Schilling, Barbara Hammer",Evaluating Robustness of Counterfactual Explanations,,,,,http://arxiv.org/abs/2103.02354v3,"Transparency is a fundamental requirement for decision making systems when
these should be deployed in the real world. It is usually achieved by providing
explanations of the system's behavior. A prominent and intuitive type of
explanations are counterfactual explanations. Counterfactual explanations
explain a behavior to the user by proposing actions -- as changes to the input
-- that would cause a different (specified) behavior of the system. However,
such explanation methods can be unstable with respect to small changes to the
input -- i.e. even a small change in the input can lead to huge or arbitrary
changes in the output and of the explanation. This could be problematic for
counterfactual explanations, as two similar individuals might get very
different explanations. Even worse, if the recommended actions differ
considerably in their complexity, one would consider such unstable
(counterfactual) explanations as individually unfair.
  In this work, we formally and empirically study the robustness of
counterfactual explanations in general, as well as under different models and
different kinds of perturbations. Furthermore, we propose that plausible
counterfactual explanations can be used instead of closest counterfactual
explanations to improve the robustness and consequently the individual fairness
of counterfactual explanations.",2022-11-13 16:04:20,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Naoki Yokoyama, Sehoon Ha, Dhruv Batra",Success Weighted by Completion Time: A Dynamics-Aware Evaluation Criteria for Embodied Navigation,,,,,http://arxiv.org/abs/2103.08022v1,"We present Success weighted by Completion Time (SCT), a new metric for
evaluating navigation performance for mobile robots. Several related works on
navigation have used Success weighted by Path Length (SPL) as the primary
method of evaluating the path an agent makes to a goal location, but SPL is
limited in its ability to properly evaluate agents with complex dynamics. In
contrast, SCT explicitly takes the agent's dynamics model into consideration,
and aims to accurately capture how well the agent has approximated the fastest
navigation behavior afforded by its dynamics. While several embodied navigation
works use point-turn dynamics, we focus on unicycle-cart dynamics for our
agent, which better exemplifies the dynamics model of popular mobile robotics
platforms (e.g., LoCoBot, TurtleBot, Fetch, etc.). We also present
RRT*-Unicycle, an algorithm for unicycle dynamics that estimates the fastest
collision-free path and completion time from a starting pose to a goal location
in an environment containing obstacles. We experiment with deep reinforcement
learning and reward shaping to train and compare the navigation performance of
agents with different dynamics models. In evaluating these agents, we show that
in contrast to SPL, SCT is able to capture the advantages in navigation speed a
unicycle model has over a simpler point-turn model of dynamics. Lastly, we show
that we can successfully deploy our trained models and algorithms outside of
simulation in the real world. We embody our agents in an real robot to navigate
an apartment, and show that they can generalize in a zero-shot manner.",2022-11-13 16:04:20,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Grace A. Lewis, Stephany Bellomo, Ipek Ozkaya",Characterizing and Detecting Mismatch in Machine-Learning-Enabled Systems,,,,,http://arxiv.org/abs/2103.14101v1,"Increasing availability of machine learning (ML) frameworks and tools, as
well as their promise to improve solutions to data-driven decision problems,
has resulted in popularity of using ML techniques in software systems. However,
end-to-end development of ML-enabled systems, as well as their seamless
deployment and operations, remain a challenge. One reason is that development
and deployment of ML-enabled systems involves three distinct workflows,
perspectives, and roles, which include data science, software engineering, and
operations. These three distinct perspectives, when misaligned due to incorrect
assumptions, cause ML mismatches which can result in failed systems. We
conducted an interview and survey study where we collected and validated common
types of mismatches that occur in end-to-end development of ML-enabled systems.
Our analysis shows that how each role prioritizes the importance of relevant
mismatches varies, potentially contributing to these mismatched assumptions. In
addition, the mismatch categories we identified can be specified as machine
readable descriptors contributing to improved ML-enabled system development. In
this paper, we report our findings and their implications for improving
end-to-end ML-enabled system development.",2022-11-13 16:04:21,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"David Leslie, Christopher Burr, Mhairi Aitken, Josh Cowls, Michael Katell, Morgan Briggs","Artificial intelligence, human rights, democracy, and the rule of law: a primer",,,,10.5281/zenodo.4639743,http://arxiv.org/abs/2104.04147v1,"In September 2019, the Council of Europe's Committee of Ministers adopted the
terms of reference for the Ad Hoc Committee on Artificial Intelligence (CAHAI).
The CAHAI is charged with examining the feasibility and potential elements of a
legal framework for the design, development, and deployment of AI systems that
accord with Council of Europe standards across the interrelated areas of human
rights, democracy, and the rule of law. As a first and necessary step in
carrying out this responsibility, the CAHAI's Feasibility Study, adopted by its
plenary in December 2020, has explored options for an international legal
response that fills existing gaps in legislation and tailors the use of binding
and non-binding legal instruments to the specific risks and opportunities
presented by AI systems. The Study examines how the fundamental rights and
freedoms that are already codified in international human rights law can be
used as the basis for such a legal framework. The purpose of this primer is to
introduce the main concepts and principles presented in the CAHAI's Feasibility
Study for a general, non-technical audience. It also aims to provide some
background information on the areas of AI innovation, human rights law,
technology policy, and compliance mechanisms covered therein. In keeping with
the Council of Europe's commitment to broad multi-stakeholder consultations,
outreach, and engagement, this primer has been designed to help facilitate the
meaningful and informed participation of an inclusive group of stakeholders as
the CAHAI seeks feedback and guidance regarding the essential issues raised by
the Feasibility Study.",2022-11-13 16:04:22,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Ercument Ilhan, Jeremy Gow, Diego Perez-Liebana",Learning on a Budget via Teacher Imitation,,,,,http://arxiv.org/abs/2104.08440v3,"Deep Reinforcement Learning (RL) techniques can benefit greatly from
leveraging prior experience, which can be either self-generated or acquired
from other entities. Action advising is a framework that provides a flexible
way to transfer such knowledge in the form of actions between teacher-student
peers. However, due to the realistic concerns, the number of these interactions
is limited with a budget; therefore, it is crucial to perform these in the most
appropriate moments. There have been several promising studies recently that
address this problem setting especially from the student's perspective. Despite
their success, they have some shortcomings when it comes to the practical
applicability and integrity as an overall solution to the learning from advice
challenge. In this paper, we extend the idea of advice reusing via teacher
imitation to construct a unified approach that addresses both advice collection
and advice utilisation problems. We also propose a method to automatically tune
the relevant hyperparameters of these components on-the-fly to make it able to
adapt to any task with minimal human intervention. The experiments we performed
in 5 different Atari games verify that our algorithm either surpasses or
performs on-par with its top competitors while being far simpler to be
employed. Furthermore, its individual components are also found to be providing
significant advantages alone.",2022-11-13 16:04:22,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Giulia Milan, Luca Vassio, Idilio Drago, Marco Mellia",RL-IoT: Reinforcement Learning to Interact with IoT Devices,,,,10.1109/COINS51742.2021.9524260,http://arxiv.org/abs/2105.00884v3,"Our life is getting filled by Internet of Things (IoT) devices. These devices
often rely on closed or poorly documented protocols, with unknown formats and
semantics. Learning how to interact with such devices in an autonomous manner
is the key for interoperability and automatic verification of their
capabilities. In this paper, we propose RL-IoT, a system that explores how to
automatically interact with possibly unknown IoT devices. We leverage
reinforcement learning (RL) to recover the semantics of protocol messages and
to take control of the device to reach a given goal, while minimizing the
number of interactions. We assume to know only a database of possible IoT
protocol messages, whose semantics are however unknown. RL-IoT exchanges
messages with the target IoT device, learning those commands that are useful to
reach the given goal. Our results show that RL-IoT is able to solve both simple
and complex tasks. With properly tuned parameters, RL-IoT learns how to perform
actions with the target device, a Yeelight smart bulb in our case study,
completing non-trivial patterns with as few as 400 interactions. RL-IoT paves
the road for automatic interactions with poorly documented IoT protocols, thus
enabling interoperable systems.",2022-11-13 16:04:23,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Giorgio Angelotti, Nicolas Drougard, Caroline Ponzoni Carvalho Chanel",Exploitation vs Caution: Risk-sensitive Policies for Offline Learning,,,,,http://arxiv.org/abs/2105.13431v1,"Offline model learning for planning is a branch of machine learning that
trains agents to perform actions in an unknown environment using a fixed batch
of previously collected experiences. The limited size of the data set hinders
the estimate of the Value function of the relative Markov Decision Process
(MDP), bounding the performance of the obtained policy in the real world. In
this context, recent works showed that planning with a discount factor lower
than the one used during the evaluation phase yields more performing policies.
However, the optimal discount factor is finally chosen by cross-validation. Our
aim is to show that looking for a sub-optimal solution of a Bayesian MDP might
lead to better performances with respect to the current baselines that work in
the offline setting. Hence, we propose Exploitation vs Caution (EvC), an
algorithm that automatically selects the policy that solves a Risk-sensitive
Bayesian MDP in a set of policies obtained by solving several MDPs
characterized by different discount factors and transition dynamics. On one
hand, the Bayesian formalism elegantly includes model uncertainty and on
another hand the introduction of a risk-sensitive utility function guarantees
robustness. We evaluated the proposed approach in different discrete simple
environments offering a fair variety of MDP classes. We also compared the
obtained results with state-of-the-art offline learning for planning baselines
such as MOPO and MOReL. In the tested scenarios EvC is more robust than the
said approaches suggesting that sub-optimally solving an Offline Risk-sensitive
Bayesian MDP (ORBMDP) could define a sound framework for planning under model
uncertainty.",2022-11-13 16:04:24,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Cassidy Laidlaw, Stuart Russell",Uncertain Decisions Facilitate Better Preference Learning,,,,,http://arxiv.org/abs/2106.10394v2,"Existing observational approaches for learning human preferences, such as
inverse reinforcement learning, usually make strong assumptions about the
observability of the human's environment. However, in reality, people make many
important decisions under uncertainty. To better understand preference learning
in these cases, we study the setting of inverse decision theory (IDT), a
previously proposed framework where a human is observed making non-sequential
binary decisions under uncertainty. In IDT, the human's preferences are
conveyed through their loss function, which expresses a tradeoff between
different types of mistakes. We give the first statistical analysis of IDT,
providing conditions necessary to identify these preferences and characterizing
the sample complexity -- the number of decisions that must be observed to learn
the tradeoff the human is making to a desired precision. Interestingly, we show
that it is actually easier to identify preferences when the decision problem is
more uncertain. Furthermore, uncertain decision problems allow us to relax the
unrealistic assumption that the human is an optimal decision maker but still
identify their exact preferences; we give sample complexities in this
suboptimal case as well. Our analysis contradicts the intuition that partial
observability should make preference learning more difficult. It also provides
a first step towards understanding and improving preference learning methods
for uncertain and suboptimal humans.",2022-11-13 16:04:25,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Roel Dobbe, Thomas Krendl Gilbert, Yonatan Mintz",Hard Choices in Artificial Intelligence,,,,,http://arxiv.org/abs/2106.11022v1,"As AI systems are integrated into high stakes social domains, researchers now
examine how to design and operate them in a safe and ethical manner. However,
the criteria for identifying and diagnosing safety risks in complex social
contexts remain unclear and contested. In this paper, we examine the vagueness
in debates about the safety and ethical behavior of AI systems. We show how
this vagueness cannot be resolved through mathematical formalism alone, instead
requiring deliberation about the politics of development as well as the context
of deployment. Drawing from a new sociotechnical lexicon, we redefine vagueness
in terms of distinct design challenges at key stages in AI system development.
The resulting framework of Hard Choices in Artificial Intelligence (HCAI)
empowers developers by 1) identifying points of overlap between design
decisions and major sociotechnical challenges; 2) motivating the creation of
stakeholder feedback channels so that safety issues can be exhaustively
addressed. As such, HCAI contributes to a timely debate about the status of AI
development in democratic societies, arguing that deliberation should be the
goal of AI Safety, not just the procedure by which it is ensured.",2022-11-13 16:04:25,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Carina Prunkl, Carolyn Ashurst, Markus Anderljung, Helena Webb, Jan Leike, Allan Dafoe",Institutionalising Ethics in AI through Broader Impact Requirements,Nature Machine Intelligence 3.2 (2021): 104-110,,,10.1038/s42256-021-00298-y,http://arxiv.org/abs/2106.11039v1,"Turning principles into practice is one of the most pressing challenges of
artificial intelligence (AI) governance. In this article, we reflect on a novel
governance initiative by one of the world's largest AI conferences. In 2020,
the Conference on Neural Information Processing Systems (NeurIPS) introduced a
requirement for submitting authors to include a statement on the broader
societal impacts of their research. Drawing insights from similar governance
initiatives, including institutional review boards (IRBs) and impact
requirements for funding applications, we investigate the risks, challenges and
potential benefits of such an initiative. Among the challenges, we list a lack
of recognised best practice and procedural transparency, researcher opportunity
costs, institutional and social pressures, cognitive biases, and the inherently
difficult nature of the task. The potential benefits, on the other hand,
include improved anticipation and identification of impacts, better
communication with policy and governance experts, and a general strengthening
of the norms around responsible research. To maximise the chance of success, we
recommend measures to increase transparency, improve guidance, create
incentives to engage earnestly with the process, and facilitate public
deliberation on the requirement's merits and future. Perhaps the most important
contribution from this analysis are the insights we can gain regarding
effective community-based governance and the role and responsibility of the AI
research community more broadly.",2022-11-13 16:04:26,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Armin Moin, Andrei Mituca, Moharram Challenger, Atta Badii, Stephan Günnemann",ML-Quadrat & DriotData: A Model-Driven Engineering Tool and a Low-Code Platform for Smart IoT Services,,,,10.1109/ICSE-Companion55297.2022.9793752,http://arxiv.org/abs/2107.02692v4,"In this paper, we present ML-Quadrat, an open-source research prototype that
is based on the Eclipse Modeling Framework (EMF) and the state of the art in
the literature of Model-Driven Software Engineering (MDSE) for smart
Cyber-Physical Systems (CPS) and the Internet of Things (IoT). Its envisioned
users are mostly software developers who might not have deep knowledge and
skills in the heterogeneous IoT platforms and the diverse Artificial
Intelligence (AI) technologies, specifically regarding Machine Learning (ML).
ML-Quadrat is released under the terms of the Apache 2.0 license on Github.
Additionally, we demonstrate an early tool prototype of DriotData, a web-based
Low-Code platform targeting citizen data scientists and citizen/end-user
software developers. DriotData exploits and adopts ML-Quadrat in the industry
by offering an extended version of it as a subscription-based service to
companies, mainly Small- and Medium-Sized Enterprises (SME). The current
preliminary version of DriotData has three web-based model editors: text-based,
tree-/form-based and diagram-based. The latter is designed for domain experts
in the problem or use case domains (namely the IoT vertical domains) who might
not have knowledge and skills in the field of IT. Finally, a short video
demonstrating the tools is available on YouTube: https://youtu.be/VAuz25w0a5k",2022-11-13 16:04:27,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Mohamed Abdelhack, Jiaming Zhang, Sandhya Tripathi, Bradley A Fritz, Daniel Felsky, Michael S Avidan, Yixin Chen, Christopher R King",A Modulation Layer to Increase Neural Network Robustness Against Data Quality Issues,,,,,http://arxiv.org/abs/2107.08574v2,"Data missingness and quality are common problems in machine learning,
especially for high-stakes applications such as healthcare. Developers often
train machine learning models on carefully curated datasets using only high
quality data; however, this reduces the utility of such models in production
environments. We propose a novel neural network modification to mitigate the
impacts of low quality and missing data which involves replacing the fixed
weights of a fully-connected layer with a function of an additional input. This
is inspired from neuromodulation in biological neural networks where the cortex
can up- and down-regulate inputs based on their reliability and the presence of
other data. In testing, with reliability scores as a modulating signal, models
with modulating layers were found to be more robust against degradation of data
quality, including additional missingness. These models are superior to
imputation as they save on training time by completely skipping the imputation
process and further allow the introduction of other data quality measures that
imputation cannot handle. Our results suggest that explicitly accounting for
reduced information quality with a modulating fully connected layer can enable
the deployment of artificial intelligence systems in real-time applications.",2022-11-13 16:04:27,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Verena Praher, Katharina Prinz, Arthur Flexer, Gerhard Widmer","On the Veracity of Local, Model-agnostic Explanations in Audio Classification: Targeted Investigations with Adversarial Examples",,,,,http://arxiv.org/abs/2107.09045v2,"Local explanation methods such as LIME have become popular in MIR as tools
for generating post-hoc, model-agnostic explanations of a model's
classification decisions. The basic idea is to identify a small set of
human-understandable features of the classified example that are most
influential on the classifier's prediction. These are then presented as an
explanation. Evaluation of such explanations in publications often resorts to
accepting what matches the expectation of a human without actually being able
to verify if what the explanation shows is what really caused the model's
prediction. This paper reports on targeted investigations where we try to get
more insight into the actual veracity of LIME's explanations in an audio
classification task. We deliberately design adversarial examples for the
classifier, in a way that gives us knowledge about which parts of the input are
potentially responsible for the model's (wrong) prediction. Asking LIME to
explain the predictions for these adversaries permits us to study whether local
explanations do indeed detect these regions of interest. We also look at
whether LIME is more successful in finding perturbations that are more
prominent and easily noticeable for a human. Our results suggest that LIME does
not necessarily manage to identify the most relevant input features and hence
it remains unclear whether explanations are useful or even misleading.",2022-11-13 16:04:28,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Elena Baninemeh, Siamak Farshidi, Slinger Jansen",A Decision Model for Decentralized Autonomous Organization Platform Selection: Three Industry Case Studies,,,,,http://arxiv.org/abs/2107.14093v1,"Decentralized autonomous organizations as a new form of online governance
arecollections of smart contracts deployed on a blockchain platform that
intercede groupsof people. A growing number of Decentralized Autonomous
Organization Platforms,such as Aragon and Colony, have been introduced in the
market to facilitate thedevelopment process of such organizations. Selecting
the best fitting platform ischallenging for the organizations, as a significant
number of decision criteria, such aspopularity, developer availability,
governance issues, and consistent documentation ofsuch platforms, should be
considered. Additionally, decision-makers at theorganizations are not experts
in every domain, so they must continuously acquirevolatile knowledge regarding
such platforms and keep themselves updated.Accordingly, a decision model is
required to analyze the decision criteria usingsystematic identification and
evaluation of potential alternative solutions for adevelopment project. We have
developed a theoretical framework to assist softwareengineers with a set of
Multi-Criteria Decision-Making problems in software production.This study
presents a decision model as a Multi-Criteria Decision-Making problem forthe
decentralized autonomous organization platform selection problem. Weconducted
three industry case studies in the context of three decentralizedautonomous
organizations to evaluate the effectiveness and efficiency of the decisionmodel
in assisting decision-makers.",2022-11-13 16:04:28,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Ryan Hoque, Ashwin Balakrishna, Ellen Novoseller, Albert Wilcox, Daniel S. Brown, Ken Goldberg",ThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,,,,,http://arxiv.org/abs/2109.08273v1,"Effective robot learning often requires online human feedback and
interventions that can cost significant human time, giving rise to the central
challenge in interactive imitation learning: is it possible to control the
timing and length of interventions to both facilitate learning and limit burden
on the human supervisor? This paper presents ThriftyDAgger, an algorithm for
actively querying a human supervisor given a desired budget of human
interventions. ThriftyDAgger uses a learned switching policy to solicit
interventions only at states that are sufficiently (1) novel, where the robot
policy has no reference behavior to imitate, or (2) risky, where the robot has
low confidence in task completion. To detect the latter, we introduce a novel
metric for estimating risk under the current robot policy. Experiments in
simulation and on a physical cable routing experiment suggest that
ThriftyDAgger's intervention criteria balances task performance and supervisor
burden more effectively than prior algorithms. ThriftyDAgger can also be
applied at execution time, where it achieves a 100% success rate on both the
simulation and physical tasks. A user study (N=10) in which users control a
three-robot fleet while also performing a concentration task suggests that
ThriftyDAgger increases human and robot performance by 58% and 80% respectively
compared to the next best algorithm while reducing supervisor burden.",2022-11-13 16:04:29,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,Vishal Rajput,Robustness of different loss functions and their impact on networks learning capability,,,,,http://arxiv.org/abs/2110.08322v2,"Recent developments in AI have made it ubiquitous, every industry is trying
to adopt some form of intelligent processing of their data. Despite so many
advances in the field, AIs full capability is yet to be exploited by the
industry. Industries that involve some risk factors still remain cautious about
the usage of AI due to the lack of trust in such autonomous systems.
Present-day AI might be very good in a lot of things but it is very bad in
reasoning and this behavior of AI can lead to catastrophic results. Autonomous
cars crashing into a person or a drone getting stuck in a tree are a few
examples where AI decisions lead to catastrophic results. To develop insight
and generate an explanation about the learning capability of AI, we will try to
analyze the working of loss functions. For our case, we will use two sets of
loss functions, generalized loss functions like Binary cross-entropy or BCE and
specialized loss functions like Dice loss or focal loss. Through a series of
experiments, we will establish whether combining different loss functions is
better than using a single loss function and if yes, then what is the reason
behind it. In order to establish the difference between generalized loss and
specialized losses, we will train several models using the above-mentioned
losses and then compare their robustness on adversarial examples. In
particular, we will look at how fast the accuracy of different models decreases
when we change the pixels corresponding to the most salient gradients.",2022-11-13 16:04:29,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Tien-Hong Lo, Yao-Ting Sung, Berlin Chen",Improving End-To-End Modeling for Mispronunciation Detection with Effective Augmentation Mechanisms,,,,,http://arxiv.org/abs/2110.08731v1,"Recently, end-to-end (E2E) models, which allow to take spectral vector
sequences of L2 (second-language) learners' utterances as input and produce the
corresponding phone-level sequences as output, have attracted much research
attention in developing mispronunciation detection (MD) systems. However, due
to the lack of sufficient labeled speech data of L2 speakers for model
estimation, E2E MD models are prone to overfitting in relation to conventional
ones that are built on DNN-HMM acoustic models. To alleviate this critical
issue, we in this paper propose two modeling strategies to enhance the
discrimination capability of E2E MD models, each of which can implicitly
leverage the phonetic and phonological traits encoded in a pretrained acoustic
model and contained within reference transcripts of the training data,
respectively. The first one is input augmentation, which aims to distill
knowledge about phonetic discrimination from a DNN-HMM acoustic model. The
second one is label augmentation, which manages to capture more phonological
patterns from the transcripts of training data. A series of empirical
experiments conducted on the L2-ARCTIC English dataset seem to confirm the
efficacy of our E2E MD model when compared to some top-of-the-line E2E MD
models and a classic pronunciation-scoring based method built on a DNN-HMM
acoustic model.",2022-11-13 16:04:30,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Mo Yu, Yang Zhang, Shiyu Chang, Tommi S. Jaakkola",Understanding Interlocking Dynamics of Cooperative Rationalization,,,,,http://arxiv.org/abs/2110.13880v1,"Selective rationalization explains the prediction of complex neural networks
by finding a small subset of the input that is sufficient to predict the neural
model output. The selection mechanism is commonly integrated into the model
itself by specifying a two-component cascaded system consisting of a rationale
generator, which makes a binary selection of the input features (which is the
rationale), and a predictor, which predicts the output based only on the
selected features. The components are trained jointly to optimize prediction
performance. In this paper, we reveal a major problem with such cooperative
rationalization paradigm -- model interlocking. Interlocking arises when the
predictor overfits to the features selected by the generator thus reinforcing
the generator's selection even if the selected rationales are sub-optimal. The
fundamental cause of the interlocking problem is that the rationalization
objective to be minimized is concave with respect to the generator's selection
policy. We propose a new rationalization framework, called A2R, which
introduces a third component into the architecture, a predictor driven by soft
attention as opposed to selection. The generator now realizes both soft and
hard attention over the features and these are fed into the two different
predictors. While the generator still seeks to support the original predictor
performance, it also minimizes a gap between the two predictors. As we will
show theoretically, since the attention-based predictor exhibits a better
convexity property, A2R can overcome the concavity barrier. Our experiments on
two synthetic benchmarks and two real datasets demonstrate that A2R can
significantly alleviate the interlock problem and find explanations that better
align with human judgments. We release our code at
https://github.com/Gorov/Understanding_Interlocking.",2022-11-13 16:04:31,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Tejas Sudharshan Mathai, Sungwon Lee, Daniel C. Elton, Thomas C. Shen, Yifan Peng, Zhiyong Lu, Ronald M. Summers",Lymph Node Detection in T2 MRI with Transformers,,,,,http://arxiv.org/abs/2111.04885v1,"Identification of lymph nodes (LN) in T2 Magnetic Resonance Imaging (MRI) is
an important step performed by radiologists during the assessment of
lymphoproliferative diseases. The size of the nodes play a crucial role in
their staging, and radiologists sometimes use an additional contrast sequence
such as diffusion weighted imaging (DWI) for confirmation. However, lymph nodes
have diverse appearances in T2 MRI scans, making it tough to stage for
metastasis. Furthermore, radiologists often miss smaller metastatic lymph nodes
over the course of a busy day. To deal with these issues, we propose to use the
DEtection TRansformer (DETR) network to localize suspicious metastatic lymph
nodes for staging in challenging T2 MRI scans acquired by different scanners
and exam protocols. False positives (FP) were reduced through a bounding box
fusion technique, and a precision of 65.41\% and sensitivity of 91.66\% at 4 FP
per image was achieved. To the best of our knowledge, our results improve upon
the current state-of-the-art for lymph node detection in T2 MRI scans.",2022-11-13 16:04:31,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Luca Pion-Tonachini, Kristofer Bouchard, Hector Garcia Martin, Sean Peisert, W. Bradley Holtz, Anil Aswani, Dipankar Dwivedi, Haruko Wainwright, Ghanshyam Pilania, Benjamin Nachman, Babetta L. Marrone, Nicola Falco, Prabhat, Daniel Arnold, Alejandro Wolf-Yadlin, Sarah Powers, Sharlee Climer, Quinn Jackson, Ty Carlson, Michael Sohn, Petrus Zwart, Neeraj Kumar, Amy Justice, Claire Tomlin, Daniel Jacobson, Gos Micklem, Georgios V. Gkoutos, Peter J. Bickel, Jean-Baptiste Cazier, Juliane Müller, Bobbie-Jo Webb-Robertson, Rick Stevens, Mark Anderson, Ken Kreutz-Delgado, Michael W. Mahoney, James B. Brown",Learning from learning machines: a new generation of AI technology to meet the needs of science,,,,,http://arxiv.org/abs/2111.13786v1,"We outline emerging opportunities and challenges to enhance the utility of AI
for scientific discovery. The distinct goals of AI for industry versus the
goals of AI for science create tension between identifying patterns in data
versus discovering patterns in the world from data. If we address the
fundamental challenges associated with ""bridging the gap"" between domain-driven
scientific models and data-driven AI learning machines, then we expect that
these AI models can transform hypothesis generation, scientific discovery, and
the scientific process itself.",2022-11-13 16:04:32,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Michael Luo, Ashwin Balakrishna, Brijen Thananjeyan, Suraj Nair, Julian Ibarz, Jie Tan, Chelsea Finn, Ion Stoica, Ken Goldberg",MESA: Offline Meta-RL for Safe Adaptation and Fault Tolerance,"Workshop on Safe and Robust Control of Uncertain Systems at the
  35th Conference on Neural Information Processing Systems (NeurIPS 2021),
  Online",,,,http://arxiv.org/abs/2112.03575v1,"Safe exploration is critical for using reinforcement learning (RL) in
risk-sensitive environments. Recent work learns risk measures which measure the
probability of violating constraints, which can then be used to enable safety.
However, learning such risk measures requires significant interaction with the
environment, resulting in excessive constraint violations during learning.
Furthermore, these measures are not easily transferable to new environments. We
cast safe exploration as an offline meta-RL problem, where the objective is to
leverage examples of safe and unsafe behavior across a range of environments to
quickly adapt learned risk measures to a new environment with previously unseen
dynamics. We then propose MEta-learning for Safe Adaptation (MESA), an approach
for meta-learning a risk measure for safe RL. Simulation experiments across 5
continuous control domains suggest that MESA can leverage offline data from a
range of different environments to reduce constraint violations in unseen
environments by up to a factor of 2 while maintaining task performance. See
https://tinyurl.com/safe-meta-rl for code and supplementary material.",2022-11-13 16:04:32,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2021,"Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, John Schulman",WebGPT: Browser-assisted question-answering with human feedback,,,,,http://arxiv.org/abs/2112.09332v3,"We fine-tune GPT-3 to answer long-form questions using a text-based
web-browsing environment, which allows the model to search and navigate the
web. By setting up the task so that it can be performed by humans, we are able
to train models on the task using imitation learning, and then optimize answer
quality with human feedback. To make human evaluation of factual accuracy
easier, models must collect references while browsing in support of their
answers. We train and evaluate our models on ELI5, a dataset of questions asked
by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior
cloning, and then performing rejection sampling against a reward model trained
to predict human preferences. This model's answers are preferred by humans 56%
of the time to those of our human demonstrators, and 69% of the time to the
highest-voted answer from Reddit.",2022-11-13 16:04:33,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Tiago Gaspar Oliveira, Arlindo L. Oliveira","Assessing Policy, Loss and Planning Combinations in Reinforcement Learning using a New Modular Architecture",,,,,http://arxiv.org/abs/2201.02874v1,"The model-based reinforcement learning paradigm, which uses planning
algorithms and neural network models, has recently achieved unprecedented
results in diverse applications, leading to what is now known as deep
reinforcement learning. These agents are quite complex and involve multiple
components, factors that can create challenges for research. In this work, we
propose a new modular software architecture suited for these types of agents,
and a set of building blocks that can be easily reused and assembled to
construct new model-based reinforcement learning agents. These building blocks
include planning algorithms, policies, and loss functions.
  We illustrate the use of this architecture by combining several of these
building blocks to implement and test agents that are optimized to three
different test environments: Cartpole, Minigrid, and Tictactoe. One particular
planning algorithm, made available in our implementation and not previously
used in reinforcement learning, which we called averaged minimax, achieved good
results in the three tested environments.
  Experiments performed with this architecture have shown that the best
combination of planning algorithm, policy, and loss function is heavily problem
dependent. This result provides evidence that the proposed architecture, which
is modular and reusable, is useful for reinforcement learning researchers who
want to study new environments and techniques.",2022-11-13 16:04:33,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,Erik Brynjolfsson,The Turing Trap: The Promise & Peril of Human-Like Artificial Intelligence,,,,,http://arxiv.org/abs/2201.04200v1,"In 1950, Alan Turing proposed an imitation game as the ultimate test of
whether a machine was intelligent: could a machine imitate a human so well that
its answers to questions indistinguishable from a human. Ever since, creating
intelligence that matches human intelligence has implicitly or explicitly been
the goal of thousands of researchers, engineers, and entrepreneurs. The
benefits of human-like artificial intelligence (HLAI) include soaring
productivity, increased leisure, and perhaps most profoundly, a better
understanding of our own minds.
  But not all types of AI are human-like. In fact, many of the most powerful
systems are very different from humans. So an excessive focus on developing and
deploying HLAI can lead us into a trap. As machines become better substitutes
for human labor, workers lose economic and political bargaining power and
become increasingly dependent on those who control the technology. In contrast,
when AI is focused on augmenting humans rather than mimicking them, then humans
retain the power to insist on a share of the value created. Furthermore,
augmentation creates new capabilities and new products and services, ultimately
generating far more value than merely human-like AI. While both types of AI can
be enormously beneficial, there are currently excess incentives for automation
rather than augmentation among technologists, business executives, and
policymakers.",2022-11-13 16:04:34,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Mustafa O. Karabag, Cyrus Neary, Ufuk Topcu",Planning Not to Talk: Multiagent Systems that are Robust to Communication Loss,,,,,http://arxiv.org/abs/2201.06619v1,"In a cooperative multiagent system, a collection of agents executes a joint
policy in order to achieve some common objective. The successful deployment of
such systems hinges on the availability of reliable inter-agent communication.
However, many sources of potential disruption to communication exist in
practice, such as radio interference, hardware failure, and adversarial
attacks. In this work, we develop joint policies for cooperative multiagent
systems that are robust to potential losses in communication. More
specifically, we develop joint policies for cooperative Markov games with
reach-avoid objectives. First, we propose an algorithm for the decentralized
execution of joint policies during periods of communication loss. Next, we use
the total correlation of the state-action process induced by a joint policy as
a measure of the intrinsic dependencies between the agents. We then use this
measure to lower-bound the performance of a joint policy when communication is
lost. Finally, we present an algorithm that maximizes a proxy to this lower
bound in order to synthesize minimum-dependency joint policies that are robust
to communication loss. Numerical experiments show that the proposed
minimum-dependency policies require minimal coordination between the agents
while incurring little to no loss in performance; the total correlation value
of the synthesized policy is one fifth of the total correlation value of the
baseline policy which does not take potential communication losses into
account. As a result, the performance of the minimum-dependency policies
remains consistently high regardless of whether or not communication is
available. By contrast, the performance of the baseline policy decreases by
twenty percent when communication is lost.",2022-11-13 16:04:34,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Pierre Carbonnelle, Simon Vandevelde, Joost Vennekens, Marc Denecker",IDP-Z3: a reasoning engine for FO(.),,,,,http://arxiv.org/abs/2202.00343v2,"An important sign of intelligence is the capacity to apply a body of
knowledge to a particular situation in order to not only derive new knowledge,
but also to determine relevant questions or provide explanations. Developing
interactive systems capable of performing such a variety of reasoning tasks for
the benefits of its users has proved difficult, notably for performance and/or
development cost reasons. Still, recently, a reasoning engine, called IDP3, has
been used to build such systems, but it lacked support for arithmetic
operations, seriously limiting its usefulness. We have developed a new
reasoning engine, IDP-Z3, that removes this limitation, and we put it to the
test in four knowledge-intensive industrial use cases.
  This paper describes FO(.) (aka FO-dot), the language used to represent
knowledge in the IDP3 and IDP-Z3 system. It then describes the generic
reasoning tasks that IDP-Z3 can perform, and how we used them to build a
generic user interface, called the Interactive Consultant. Finally, it reports
on the four use cases.
  In these four use cases, the interactive applications based on IDP-Z3 were
capable of intelligent behavior of value to users, while having a low
development cost (typically 10 days) and an acceptable response time (typically
below 3 seconds). Performance could be further improved, in particular for
problems on larger domains.",2022-11-13 16:04:35,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Tong Mu, Stephan Zheng, Alexander Trott",Solving Dynamic Principal-Agent Problems with a Rationally Inattentive Principal,,,,,http://arxiv.org/abs/2202.01691v2,"Principal-Agent (PA) problems describe a broad class of economic
relationships characterized by misaligned incentives and asymmetric
information. The Principal's problem is to find optimal incentives given the
available information, e.g., a manager setting optimal wages for its employees.
Whereas the Principal is often assumed rational, comparatively little is known
about solutions when the Principal is boundedly rational, especially in the
sequential setting, with multiple Agents, and with multiple information
channels. Here, we develop RIRL, a deep reinforcement learning framework that
solves such complex PA problems with a rationally inattentive Principal. Such a
Principal incurs a cost for paying attention to information, which can model
forms of bounded rationality. We use RIRL to analyze rich economic phenomena in
manager-employee relationships. In the single-step setting, 1) RIRL yields
wages that are consistent with theoretical predictions; and 2) non-zero
attention costs lead to simpler but less profitable wage structures, and
increased Agent welfare. In a sequential setting with multiple Agents, RIRL
shows opposing consequences of the Principal's inattention to different
information channels: 1) inattention to Agents' outputs closes wage gaps based
on ability differences; and 2) inattention to Agents' efforts induces a social
dilemma dynamic in which Agents work harder, but essentially for free.
Moreover, RIRL reveals non-trivial relationships between the Principal's
inattention and Agent types, e.g., if Agents are prone to sub-optimal effort
choices, payment schedules are more sensitive to the Principal's attention
cost. As such, RIRL can reveal novel economic relationships and enables
progress towards understanding the effects of bounded rationality in dynamic
settings.",2022-11-13 16:04:36,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Leonardo Lucio Custode, Giovanni Iacca",Interpretable pipelines with evolutionarily optimized modules for RL tasks with visual inputs,,,,10.1145/3520304.3528897,http://arxiv.org/abs/2202.04943v1,"The importance of explainability in AI has become a pressing concern, for
which several explainable AI (XAI) approaches have been recently proposed.
However, most of the available XAI techniques are post-hoc methods, which
however may be only partially reliable, as they do not reflect exactly the
state of the original models. Thus, a more direct way for achieving XAI is
through interpretable (also called glass-box) models. These models have been
shown to obtain comparable (and, in some cases, better) performance with
respect to black-boxes models in various tasks such as classification and
reinforcement learning. However, they struggle when working with raw data,
especially when the input dimensionality increases and the raw inputs alone do
not give valuable insights on the decision-making process. Here, we propose to
use end-to-end pipelines composed of multiple interpretable models co-optimized
by means of evolutionary algorithms, that allows us to decompose the
decision-making process into two parts: computing high-level features from raw
data, and reasoning on the extracted high-level features. We test our approach
in reinforcement learning environments from the Atari benchmark, where we
obtain comparable results (with respect to black-box approaches) in settings
without stochastic frame-skipping, while performance degrades in frame-skipping
settings.",2022-11-13 16:04:36,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,Max W. Shen,"Trust in AI: Interpretability is not necessary or sufficient, while black-box interaction is necessary and sufficient",,,,,http://arxiv.org/abs/2202.05302v1,"The problem of human trust in artificial intelligence is one of the most
fundamental problems in applied machine learning. Our processes for evaluating
AI trustworthiness have substantial ramifications for ML's impact on science,
health, and humanity, yet confusion surrounds foundational concepts. What does
it mean to trust an AI, and how do humans assess AI trustworthiness? What are
the mechanisms for building trustworthy AI? And what is the role of
interpretable ML in trust? Here, we draw from statistical learning theory and
sociological lenses on human-automation trust to motivate an AI-as-tool
framework, which distinguishes human-AI trust from human-AI-human trust.
Evaluating an AI's contractual trustworthiness involves predicting future model
behavior using behavior certificates (BCs) that aggregate behavioral evidence
from diverse sources including empirical out-of-distribution and out-of-task
evaluation and theoretical proofs linking model architecture to behavior. We
clarify the role of interpretability in trust with a ladder of model access.
Interpretability (level 3) is not necessary or even sufficient for trust, while
the ability to run a black-box model at-will (level 2) is necessary and
sufficient. While interpretability can offer benefits for trust, it can also
incur costs. We clarify ways interpretability can contribute to trust, while
questioning the perceived centrality of interpretability to trust in popular
discourse. How can we empower people with tools to evaluate trust? Instead of
trying to understand how a model works, we argue for understanding how a model
behaves. Instead of opening up black boxes, we should create more behavior
certificates that are more correct, relevant, and understandable. We discuss
how to build trusted and trustworthy AI responsibly.",2022-11-13 16:04:37,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Jan Balaguer, Raphael Koster, Ari Weinstein, Lucy Campbell-Gillingham, Christopher Summerfield, Matthew Botvinick, Andrea Tacchetti",HCMD-zero: Learning Value Aligned Mechanisms from Data,,,,,http://arxiv.org/abs/2202.10122v2,"Artificial learning agents are mediating a larger and larger number of
interactions among humans, firms, and organizations, and the intersection
between mechanism design and machine learning has been heavily investigated in
recent years. However, mechanism design methods often make strong assumptions
on how participants behave (e.g. rationality), on the kind of knowledge
designers have access to a priori (e.g. access to strong baseline mechanisms),
or on what the goal of the mechanism should be (e.g. total welfare). Here we
introduce HCMD-zero, a general purpose method to construct mechanisms making
none of these three assumptions. HCMD-zero learns to mediate interactions among
participants and adjusts the mechanism parameters to make itself more likely to
be preferred by participants. It does so by remaining engaged in an electoral
contest with copies of itself, thereby accessing direct feedback from
participants. We test our method on a stylized resource allocation game that
highlights the tension between productivity, equality and the temptation to
free ride. HCMD-zero produces a mechanism that is preferred by human
participants over a strong baseline, it does so automatically, without
requiring prior knowledge, and using human behavioral trajectories sparingly
and effectively. Our analysis shows HCMD-zero consistently makes the mechanism
policy more and more likely to be preferred by human participants over the
course of training, and that it results in a mechanism with an interpretable
and intuitive policy.",2022-11-13 16:04:37,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Andi Peng, Besmira Nushi, Emre Kiciman, Kori Inkpen, Ece Kamar",Investigations of Performance and Bias in Human-AI Teamwork in Hiring,,,,,http://arxiv.org/abs/2202.11812v1,"In AI-assisted decision-making, effective hybrid (human-AI) teamwork is not
solely dependent on AI performance alone, but also on its impact on human
decision-making. While prior work studies the effects of model accuracy on
humans, we endeavour here to investigate the complex dynamics of how both a
model's predictive performance and bias may transfer to humans in a
recommendation-aided decision task. We consider the domain of ML-assisted
hiring, where humans -- operating in a constrained selection setting -- can
choose whether they wish to utilize a trained model's inferences to help select
candidates from written biographies. We conduct a large-scale user study
leveraging a re-created dataset of real bios from prior work, where humans
predict the ground truth occupation of given candidates with and without the
help of three different NLP classifiers (random, bag-of-words, and deep neural
network). Our results demonstrate that while high-performance models
significantly improve human performance in a hybrid setting, some models
mitigate hybrid bias while others accentuate it. We examine these findings
through the lens of decision conformity and observe that our model architecture
choices have an impact on human-AI conformity and bias, motivating the explicit
need to assess these complex dynamics prior to deployment.",2022-11-13 16:04:38,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Michael S. Lee, Henny Admoni, Reid Simmons",Reasoning about Counterfactuals to Improve Human Inverse Reinforcement Learning,,,,,http://arxiv.org/abs/2203.01855v3,"To collaborate well with robots, we must be able to understand their decision
making. Humans naturally infer other agents' beliefs and desires by reasoning
about their observable behavior in a way that resembles inverse reinforcement
learning (IRL). Thus, robots can convey their beliefs and desires by providing
demonstrations that are informative for a human learner's IRL. An informative
demonstration is one that differs strongly from the learner's expectations of
what the robot will do given their current understanding of the robot's
decision making. However, standard IRL does not model the learner's existing
expectations, and thus cannot do this counterfactual reasoning. We propose to
incorporate the learner's current understanding of the robot's decision making
into our model of human IRL, so that a robot can select demonstrations that
maximize the human's understanding. We also propose a novel measure for
estimating the difficulty for a human to predict instances of a robot's
behavior in unseen environments. A user study finds that our test difficulty
measure correlates well with human performance and confidence. Interestingly,
considering human beliefs and counterfactuals when selecting demonstrations
decreases human performance on easy tests, but increases performance on
difficult tests, providing insight on how to best utilize such models.",2022-11-13 16:04:38,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe",Training language models to follow instructions with human feedback,,,,,http://arxiv.org/abs/2203.02155v1,"Making language models bigger does not inherently make them better at
following a user's intent. For example, large language models can generate
outputs that are untruthful, toxic, or simply not helpful to the user. In other
words, these models are not aligned with their users. In this paper, we show an
avenue for aligning language models with user intent on a wide range of tasks
by fine-tuning with human feedback. Starting with a set of labeler-written
prompts and prompts submitted through the OpenAI API, we collect a dataset of
labeler demonstrations of the desired model behavior, which we use to fine-tune
GPT-3 using supervised learning. We then collect a dataset of rankings of model
outputs, which we use to further fine-tune this supervised model using
reinforcement learning from human feedback. We call the resulting models
InstructGPT. In human evaluations on our prompt distribution, outputs from the
1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3,
despite having 100x fewer parameters. Moreover, InstructGPT models show
improvements in truthfulness and reductions in toxic output generation while
having minimal performance regressions on public NLP datasets. Even though
InstructGPT still makes simple mistakes, our results show that fine-tuning with
human feedback is a promising direction for aligning language models with human
intent.",2022-11-13 16:04:39,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Ingmar Kanitscheider, Harri Edwards",AutoDIME: Automatic Design of Interesting Multi-Agent Environments,,,,,http://arxiv.org/abs/2203.02481v1,"Designing a distribution of environments in which RL agents can learn
interesting and useful skills is a challenging and poorly understood task, for
multi-agent environments the difficulties are only exacerbated. One approach is
to train a second RL agent, called a teacher, who samples environments that are
conducive for the learning of student agents. However, most previous proposals
for teacher rewards do not generalize straightforwardly to the multi-agent
setting. We examine a set of intrinsic teacher rewards derived from prediction
problems that can be applied in multi-agent settings and evaluate them in
Mujoco tasks such as multi-agent Hide and Seek as well as a diagnostic
single-agent maze task. Of the intrinsic rewards considered we found value
disagreement to be most consistent across tasks, leading to faster and more
reliable emergence of advanced skills in Hide and Seek and the maze task.
Another candidate intrinsic reward considered, value prediction error, also
worked well in Hide and Seek but was susceptible to noisy-TV style distractions
in stochastic environments. Policy disagreement performed well in the maze task
but did not speed up learning in Hide and Seek. Our results suggest that
intrinsic teacher rewards, and in particular value disagreement, are a
promising approach for automating both single and multi-agent environment
design.",2022-11-13 16:04:40,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Dayong Ye, Tianqing Zhu, Shuai Zhou, Bo Liu, Wanlei Zhou",Label-only Model Inversion Attack: The Attack that Requires the Least Information,,,,,http://arxiv.org/abs/2203.06555v1,"In a model inversion attack, an adversary attempts to reconstruct the data
records, used to train a target model, using only the model's output. In
launching a contemporary model inversion attack, the strategies discussed are
generally based on either predicted confidence score vectors, i.e., black-box
attacks, or the parameters of a target model, i.e., white-box attacks. However,
in the real world, model owners usually only give out the predicted labels; the
confidence score vectors and model parameters are hidden as a defense mechanism
to prevent such attacks. Unfortunately, we have found a model inversion method
that can reconstruct the input data records based only on the output labels. We
believe this is the attack that requires the least information to succeed and,
therefore, has the best applicability. The key idea is to exploit the error
rate of the target model to compute the median distance from a set of data
records to the decision boundary of the target model. The distance, then, is
used to generate confidence score vectors which are adopted to train an attack
model to reconstruct the data records. The experimental results show that
highly recognizable data records can be reconstructed with far less information
than existing methods.",2022-11-13 16:04:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,Tong Owen Yang,Algebraic Learning: Towards Interpretable Information Modeling,,,,,http://arxiv.org/abs/2203.06690v1,"Along with the proliferation of digital data collected using sensor
technologies and a boost of computing power, Deep Learning (DL) based
approaches have drawn enormous attention in the past decade due to their
impressive performance in extracting complex relations from raw data and
representing valuable information. Meanwhile, though, rooted in its notorious
black-box nature, the appreciation of DL has been highly debated due to the
lack of interpretability. On the one hand, DL only utilizes statistical
features contained in raw data while ignoring human knowledge of the underlying
system, which results in both data inefficiency and trust issues; on the other
hand, a trained DL model does not provide to researchers any extra insight
about the underlying system beyond its output, which, however, is the essence
of most fields of science, e.g. physics and economics.
  This thesis addresses the issue of interpretability in general information
modeling and endeavors to ease the problem from two scopes. Firstly, a
problem-oriented perspective is applied to incorporate knowledge into modeling
practice, where interesting mathematical properties emerge naturally which cast
constraints on modeling. Secondly, given a trained model, various methods could
be applied to extract further insights about the underlying system. These two
pathways are termed as guided model design and secondary measurements.
Remarkably, a novel scheme emerges for the modeling practice in statistical
learning: Algebraic Learning (AgLr). Instead of being restricted to the
discussion of any specific model, AgLr starts from idiosyncrasies of a learning
task itself and studies the structure of a legitimate model class. This novel
scheme demonstrates the noteworthy value of abstract algebra for general AI,
which has been overlooked in recent progress, and could shed further light on
interpretable information modeling.",2022-11-13 16:04:41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Jens Heidrich, Andreas Jedlitschka, Adam Trendowicz, Anna Maria Vollmer",Building AI Innovation Labs together with Companies,,,,,http://arxiv.org/abs/2203.08465v1,"In the future, most companies will be confronted with the topic of Artificial
Intelligence (AI) and will have to decide on their strategy in this regards.
Currently, a lot of companies are thinking about whether and how AI and the
usage of data will impact their business model and what potential use cases
could look like. One of the biggest challenges lies in coming up with
innovative solution ideas with a clear business value. This requires business
competencies on the one hand and technical competencies in AI and data
analytics on the other hand. In this article, we present the concept of AI
innovation labs and demonstrate a comprehensive framework, from coming up with
the right ideas to incrementally implementing and evaluating them regarding
their business value and their feasibility based on a company's capabilities.
The concept is the result of nine years of working on data-driven innovations
with companies from various domains. Furthermore, we share some lessons learned
from its practical applications. Even though a lot of technical publications
can be found in the literature regarding the development of AI models and many
consultancy companies provide corresponding services for building AI
innovations, we found very few publications sharing details about what an
end-to-end framework could look like.",2022-11-13 16:04:42,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,Brian Subirana,"An Artificial Intelligence Browser Architecture (AIBA) For Our Kind and Others: A Voice Name System Speech implementation with two warrants, Wake Neutrality and Value Preservation of Personally Identifiable Information",,,,,http://arxiv.org/abs/2203.16497v2,"Conversational commerce, first pioneered by Apple's Siri, is the first of may
applications based on always-on artificial intelligence systems that decide on
its own when to interact with the environment, potentially collecting 24x7
longitudinal training data that is often Personally Identifiable Information
(PII). A large body of scholarly papers, on the order of a million according to
a simple Google Scholar search, suggests that the treatment of many health
conditions, including COVID-19 and dementia, can be vastly improved by this
data if the dataset is large enough as it has happened in other domains (e.g.
GPT3). In contrast, current dominant systems are closed garden solutions
without wake neutrality and that can't fully exploit the PII data they have
because of IRB and Cohues-type constraints.
  We present a voice browser-and-server architecture that aims to address these
two limitations by offering wake neutrality and the possibility to handle PII
aiming to maximize its value. We have implemented this browser for the
collection of speech samples and have successfully demonstrated it can capture
over 200.000 samples of COVID-19 coughs. The architecture we propose is
designed so it can grow beyond our kind into other domains such as collecting
sound samples from vehicles, video images from nature, ingestible robotics,
multi-modal signals (EEG, EKG,...), or even interacting with other kinds such
as dogs and cats.",2022-11-13 16:04:42,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Kamilia Mullakaeva, Luca Cosmo, Anees Kazi, Seyed-Ahmad Ahmadi, Nassir Navab, Michael M. Bronstein",Graph-in-Graph (GiG): Learning interpretable latent graphs in non-Euclidean domain for biological and healthcare applications,,,,,http://arxiv.org/abs/2204.00323v1,"Graphs are a powerful tool for representing and analyzing unstructured,
non-Euclidean data ubiquitous in the healthcare domain. Two prominent examples
are molecule property prediction and brain connectome analysis. Importantly,
recent works have shown that considering relationships between input data
samples have a positive regularizing effect for the downstream task in
healthcare applications. These relationships are naturally modeled by a
(possibly unknown) graph structure between input samples. In this work, we
propose Graph-in-Graph (GiG), a neural network architecture for protein
classification and brain imaging applications that exploits the graph
representation of the input data samples and their latent relation. We assume
an initially unknown latent-graph structure between graph-valued input data and
propose to learn end-to-end a parametric model for message passing within and
across input graph samples, along with the latent structure connecting the
input graphs. Further, we introduce a degree distribution loss that helps
regularize the predicted latent relationships structure. This regularization
can significantly improve the downstream task. Moreover, the obtained latent
graph can represent patient population models or networks of molecule clusters,
providing a level of interpretability and knowledge discovery in the input
domain of particular value in healthcare.",2022-11-13 16:04:43,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Andrew Fuchs, Andrea Passarella, Marco Conti",A Cognitive Framework for Delegation Between Error-Prone AI and Human Agents,,,,,http://arxiv.org/abs/2204.02889v2,"With humans interacting with AI-based systems at an increasing rate, it is
necessary to ensure the artificial systems are acting in a manner which
reflects understanding of the human. In the case of humans and artificial AI
agents operating in the same environment, we note the significance of
comprehension and response to the actions or capabilities of a human from an
agent's perspective, as well as the possibility to delegate decisions either to
humans or to agents, depending on who is deemed more suitable at a certain
point in time. Such capabilities will ensure an improved responsiveness and
utility of the entire human-AI system. To that end, we investigate the use of
cognitively inspired models of behavior to predict the behavior of both human
and AI agents. The predicted behavior, and associated performance with respect
to a certain goal, is used to delegate control between humans and AI agents
through the use of an intermediary entity. As we demonstrate, this allows
overcoming potential shortcomings of either humans or agents in the pursuit of
a goal.",2022-11-13 16:04:43,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Travis LaCroix, Alexandra Sasha Luccioni",Metaethical Perspectives on 'Benchmarking' AI Ethics,,,,,http://arxiv.org/abs/2204.05151v1,"Benchmarks are seen as the cornerstone for measuring technical progress in
Artificial Intelligence (AI) research and have been developed for a variety of
tasks ranging from question answering to facial recognition. An increasingly
prominent research area in AI is ethics, which currently has no set of
benchmarks nor commonly accepted way for measuring the 'ethicality' of an AI
system. In this paper, drawing upon research in moral philosophy and
metaethics, we argue that it is impossible to develop such a benchmark. As
such, alternative mechanisms are necessary for evaluating whether an AI system
is 'ethical'. This is especially pressing in light of the prevalence of
applied, industrial AI research. We argue that it makes more sense to talk
about 'values' (and 'value alignment') rather than 'ethics' when considering
the possible actions of present and future AI systems. We further highlight
that, because values are unambiguously relative, focusing on values forces us
to consider explicitly what the values are and whose values they are. Shifting
the emphasis from ethics to values therefore gives rise to several new ways of
understanding how researchers might advance research programmes for robustly
safe or beneficial AI. We conclude by highlighting a number of possible ways
forward for the field as a whole, and we advocate for different approaches
towards more value-aligned AI research.",2022-11-13 16:04:44,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Samson Tan, Araz Taeihagh, Kathy Baxter",The Risks of Machine Learning Systems,,,,,http://arxiv.org/abs/2204.09852v1,"The speed and scale at which machine learning (ML) systems are deployed are
accelerating even as an increasing number of studies highlight their potential
for negative impact. There is a clear need for companies and regulators to
manage the risk from proposed ML systems before they harm people. To achieve
this, private and public sector actors first need to identify the risks posed
by a proposed ML system. A system's overall risk is influenced by its direct
and indirect effects. However, existing frameworks for ML risk/impact
assessment often address an abstract notion of risk or do not concretize this
dependence.
  We propose to address this gap with a context-sensitive framework for
identifying ML system risks comprising two components: a taxonomy of the first-
and second-order risks posed by ML systems, and their contributing factors.
First-order risks stem from aspects of the ML system, while second-order risks
stem from the consequences of first-order risks. These consequences are system
failures that result from design and development choices. We explore how
different risks may manifest in various types of ML systems, the factors that
affect each risk, and how first-order risks may lead to second-order effects
when the system interacts with the real world.
  Throughout the paper, we show how real events and prior research fit into our
Machine Learning System Risk framework (MLSR). MLSR operates on ML systems
rather than technologies or domains, recognizing that a system's design,
implementation, and use case all contribute to its risk. In doing so, it
unifies the risks that are commonly discussed in the ethical AI community
(e.g., ethical/human rights risks) with system-level risks (e.g., application,
design, control risks), paving the way for holistic risk assessments of ML
systems.",2022-11-13 16:04:44,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Yi Wan, Ali Rahimi-Kalahroudi, Janarthanan Rajendran, Ida Momennejad, Sarath Chandar, Harm van Seijen",Towards Evaluating Adaptivity of Model-Based Reinforcement Learning Methods,,,,,http://arxiv.org/abs/2204.11464v2,"In recent years, a growing number of deep model-based reinforcement learning
(RL) methods have been introduced. The interest in deep model-based RL is not
surprising, given its many potential benefits, such as higher sample efficiency
and the potential for fast adaption to changes in the environment. However, we
demonstrate, using an improved version of the recently introduced Local Change
Adaptation (LoCA) setup, that well-known model-based methods such as PlaNet and
DreamerV2 perform poorly in their ability to adapt to local environmental
changes. Combined with prior work that made a similar observation about the
other popular model-based method, MuZero, a trend appears to emerge, suggesting
that current deep model-based methods have serious limitations. We dive deeper
into the causes of this poor performance, by identifying elements that hurt
adaptive behavior and linking these to underlying techniques frequently used in
deep model-based RL. We empirically validate these insights in the case of
linear function approximation by demonstrating that a modified version of
linear Dyna achieves effective adaptation to local changes. Furthermore, we
provide detailed insights into the challenges of building an adaptive nonlinear
model-based method, by experimenting with a nonlinear version of Dyna.",2022-11-13 16:04:45,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Thulitha Senevirathna, Zujany Salazar, Vinh Hoa La, Samuel Marchal, Bartlomiej Siniarski, Madhusanka Liyanage, Shen Wang","A Survey on XAI for Beyond 5G Security: Technical Aspects, Use Cases, Challenges and Research Directions",,,,,http://arxiv.org/abs/2204.12822v1,"With the advent of 5G commercialization, the need for more reliable, faster,
and intelligent telecommunication systems are envisaged for the next generation
beyond 5G (B5G) radio access technologies. Artificial Intelligence (AI) and
Machine Learning (ML) are not just immensely popular in the service layer
applications but also have been proposed as essential enablers in many aspects
of B5G networks, from IoT devices and edge computing to cloud-based
infrastructures. However, most of the existing surveys in B5G security focus on
the performance of AI/ML models and their accuracy, but they often overlook the
accountability and trustworthiness of the models' decisions. Explainable AI
(XAI) methods are promising techniques that would allow system developers to
identify the internal workings of AI/ML black-box models. The goal of using XAI
in the security domain of B5G is to allow the decision-making processes of the
security of systems to be transparent and comprehensible to stakeholders making
the systems accountable for automated actions. In every facet of the
forthcoming B5G era, including B5G technologies such as RAN, zero-touch network
management, E2E slicing, this survey emphasizes the role of XAI in them and the
use cases that the general users would ultimately enjoy. Furthermore, we
presented the lessons learned from recent efforts and future research
directions on top of the currently conducted projects involving XAI.",2022-11-13 16:04:45,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Tingting Zheng, Weixing chen, Shuqin Li, Hao Quan, Qun Bai, Tianhang Nan, Song Zheng, Xinghua Gao, Yue Zhao, Xiaoyu Cui",A Deep Reinforcement Learning Framework for Rapid Diagnosis of Whole Slide Pathological Images,,,,,http://arxiv.org/abs/2205.02850v1,"The deep neural network is a research hotspot for histopathological image
analysis, which can improve the efficiency and accuracy of diagnosis for
pathologists or be used for disease screening. The whole slide pathological
image can reach one gigapixel and contains abundant tissue feature information,
which needs to be divided into a lot of patches in the training and inference
stages. This will lead to a long convergence time and large memory consumption.
Furthermore, well-annotated data sets are also in short supply in the field of
digital pathology. Inspired by the pathologist's clinical diagnosis process, we
propose a weakly supervised deep reinforcement learning framework, which can
greatly reduce the time required for network inference. We use neural network
to construct the search model and decision model of reinforcement learning
agent respectively. The search model predicts the next action through the image
features of different magnifications in the current field of view, and the
decision model is used to return the predicted probability of the current field
of view image. In addition, an expert-guided model is constructed by
multi-instance learning, which not only provides rewards for search model, but
also guides decision model learning by the knowledge distillation method.
Experimental results show that our proposed method can achieve fast inference
and accurate prediction of whole slide images without any pixel-level
annotations.",2022-11-13 16:04:46,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Maurice Jakesch, Zana Buçinca, Saleema Amershi, Alexandra Olteanu",How Different Groups Prioritize Ethical Values for Responsible AI,"2022 ACM Conference on Fairness, Accountability, and Transparency
  (FAccT '22), June 21-24, 2022, Seoul, Republic of Korea",,,10.1145/3531146.3533097,http://arxiv.org/abs/2205.07722v1,"Private companies, public sector organizations, and academic groups have
outlined ethical values they consider important for responsible artificial
intelligence technologies. While their recommendations converge on a set of
central values, little is known about the values a more representative public
would find important for the AI technologies they interact with and might be
affected by. We conducted a survey examining how individuals perceive and
prioritize responsible AI values across three groups: a representative sample
of the US population (N=743), a sample of crowdworkers (N=755), and a sample of
AI practitioners (N=175). Our results empirically confirm a common concern: AI
practitioners' value priorities differ from those of the general public.
Compared to the US-representative sample, AI practitioners appear to consider
responsible AI values as less important and emphasize a different set of
values. In contrast, self-identified women and black respondents found
responsible AI values more important than other groups. Surprisingly, more
liberal-leaning participants, rather than participants reporting experiences
with discrimination, were more likely to prioritize fairness than other groups.
Our findings highlight the importance of paying attention to who gets to define
responsible AI.",2022-11-13 16:04:46,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XXX,journalArticle,2022,"Javier Del Ser, Alejandro Barredo-Arrieta, Natalia Díaz-Rodríguez, Francisco Herrera, Andreas Holzinger","Exploring the Trade-off between Plausibility, Change Intensity and Adversarial Power in Counterfactual Explanations using Multi-objective Optimization",,,,,http://arxiv.org/abs/2205.10232v1,"There is a broad consensus on the importance of deep learning models in tasks
involving complex data. Often, an adequate understanding of these models is
required when focusing on the transparency of decisions in human-critical
applications. Besides other explainability techniques, trustworthiness can be
achieved by using counterfactuals, like the way a human becomes familiar with
an unknown process: by understanding the hypothetical circumstances under which
the output changes. In this work we argue that automated counterfactual
generation should regard several aspects of the produced adversarial instances,
not only their adversarial capability. To this end, we present a novel
framework for the generation of counterfactual examples which formulates its
goal as a multi-objective optimization problem balancing three different
objectives: 1) plausibility, i.e., the likeliness of the counterfactual of
being possible as per the distribution of the input data; 2) intensity of the
changes to the original input; and 3) adversarial power, namely, the
variability of the model's output induced by the counterfactual. The framework
departs from a target model to be audited and uses a Generative Adversarial
Network to model the distribution of input data, together with a
multi-objective solver for the discovery of counterfactuals balancing among
these objectives. The utility of the framework is showcased over six
classification tasks comprising image and three-dimensional data. The
experiments verify that the framework unveils counterfactuals that comply with
intuition, increasing the trustworthiness of the user, and leading to further
insights, such as the detection of bias and data misrepresentation.",2022-11-13 16:04:47,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GHMMCFVM,magazineArticle,2018,"Arkin, Ronald; Russell, Stuart; Min-Seok, Kim",The new weapons of mass destruction?,The Security Times,,,,,,2018,2022-01-30 04:51:10,2022-01-30 04:51:10,,1,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: NoCitationData[s3]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/XXVH9ATC/Arkin et al. - The new weapons of mass destruction.pdf,,MetaSafety; CHAI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4HM6IEQ3,magazineArticle,2016,"Russell, Stuart",Robots in war: the next weapons of mass destruction?,World Economic Forum,,,,https://www.weforum.org/agenda/2016/01/robots-in-war-the-next-weapons-of-mass-destruction/,"Davos 2016: There is no doubt that as the technology improves, autonomous weapons will be highly effective. But does that necessarily mean they’re a good idea?",2016,2022-01-30 04:51:08,2022-01-30 04:51:08,2019-12-18 01:17:33,,,,,,,Robots in war,,,,,,,,,,,,,,ZSCC: 0000005,,/Users/jacquesthibodeau/Zotero/storage/UCHEJG2V/robots-in-war-the-next-weapons-of-mass-destruction.html,,MetaSafety; CHAI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,magazineArticle,2018,"Piper, Kelsey",The case for taking AI seriously as a threat to humanity,Vox,,,,https://www.vox.com/future-perfect/2018/12/21/18126576/ai-artificial-intelligence-machine-learning-safety-alignment,"Why some people fear AI, explained.",2018-12-21,2020-11-21 16:50,2020-12-20 15:54,2020-11-21 16:50,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/A,,/Users/angelica/Zotero/storage/E5RJBYQA/ai-artificial-intelligence-machine-learning-safety-alignment.html,,Other-org; NotSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This is an introduction to the problem of AI safety, from the perspective that it is hard to specify the ""right"" goal, and that goal-driven behavior leads to convergent instrumental subgoals that will likely be dangerous. It also addresses several common initial reactions that people have."
2C8U9GC5,manuscript,2020,"Garcez, Artur d'Avila; Lamb, Luis C.",Neurosymbolic AI: The 3rd Wave,,,,,http://arxiv.org/abs/2012.05876,"Current advances in Artificial Intelligence (AI) and Machine Learning (ML) have achieved unprecedented impact across research communities and industry. Nevertheless, concerns about trust, safety, interpretability and accountability of AI were raised by influential thinkers. Many have identified the need for well-founded knowledge representation and reasoning to be integrated with deep learning and for sound explainability. Neural-symbolic computing has been an active area of research for many years seeking to bring together robust learning in neural networks with reasoning and explainability via symbolic representations for network models. In this paper, we relate recent and early research results in neurosymbolic AI with the objective of identifying the key ingredients of the next wave of AI systems. We focus on research that integrates in a principled way neural network-based learning with symbolic knowledge representation and logical reasoning. The insights provided by 20 years of neural-symbolic computing are shown to shed new light onto the increasingly prominent role of trust, safety, interpretability and accountability of AI. We also identify promising directions and challenges for the next decade of AI research from the perspective of neural-symbolic systems.",2020-12-16,2022-01-30 04:48:46,2022-01-30 04:48:46,2021-11-13 21:56:59,,,,,,,Neurosymbolic AI,,,,,,,,,,,,arXiv.org,,ZSCC: 0000029  arXiv: 2012.05876,,/Users/jacquesthibodeau/Zotero/storage/4EQI3ZIW/Garcez and Lamb - 2020 - Neurosymbolic AI The 3rd Wave.pdf; /Users/jacquesthibodeau/Zotero/storage/JISGMD7C/2012.html,,UnsortedSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; I.2.4; I.2.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8XXTW2U4,manuscript,2020,"Srinivasan, Krishnan; Eysenbach, Benjamin; Ha, Sehoon; Tan, Jie; Finn, Chelsea",Learning to be Safe: Deep RL with a Safety Critic,,,,,http://arxiv.org/abs/2010.14603,"Safety is an essential component for deploying reinforcement learning (RL) algorithms in real-world scenarios, and is critical during the learning process itself. A natural first approach toward safe RL is to manually specify constraints on the policy's behavior. However, just as learning has enabled progress in large-scale development of AI systems, learning safety specifications may also be necessary to ensure safety in messy open-world environments where manual safety specifications cannot scale. Akin to how humans learn incrementally starting in child-safe environments, we propose to learn how to be safe in one set of tasks and environments, and then use that learned intuition to constrain future behaviors when learning new, modified tasks. We empirically study this form of safety-constrained transfer learning in three challenging domains: simulated navigation, quadruped locomotion, and dexterous in-hand manipulation. In comparison to standard deep RL techniques and prior approaches to safe RL, we find that our method enables the learning of new tasks and in new environments with both substantially fewer safety incidents, such as falling or dropping an object, and faster, more stable learning. This suggests a path forward not only for safer RL systems, but also for more effective RL systems.",2020-10-27,2022-01-30 04:48:46,2022-01-30 04:48:46,2021-11-13 14:06:18,,,,,,,Learning to be Safe,,,,,,,,,,,,arXiv.org,,ZSCC: 0000022  arXiv: 2010.14603,,/Users/jacquesthibodeau/Zotero/storage/GDXUUMCR/Srinivasan et al. - 2020 - Learning to be Safe Deep RL with a Safety Critic.pdf,,UnsortedSafety,Computer Science - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SX36BZ9U,manuscript,2019,"Chandra, Kartik; Meijer, Erik; Andow, Samantha; Arroyo-Fang, Emilio; Dea, Irene; George, Johann; Grueter, Melissa; Hosmer, Basil; Stumpos, Steffi; Tempest, Alanna; Yang, Shannon",Gradient Descent: The Ultimate Optimizer,,,,,http://arxiv.org/abs/1909.13371,"Working with any gradient-based machine learning algorithm involves the tedious task of tuning the optimizer's hyperparameters, such as the learning rate. There exist many techniques for automated hyperparameter optimization, but they typically introduce even more hyperparameters to control the hyperparameter optimization process. We propose to instead learn the hyperparameters themselves by gradient descent, and furthermore to learn the hyper-hyperparameters by gradient descent as well, and so on ad infinitum. As these towers of gradient-based optimizers grow, they become significantly less sensitive to the choice of top-level hyperparameters, hence decreasing the burden on the user to search for optimal values.",2019-09-29,2022-01-30 04:48:45,2022-01-30 04:48:45,2021-11-13 13:43:50,,,,,,,Gradient Descent,,,,,,,,,,,,arXiv.org,,ZSCC: 0000006  arXiv: 1909.13371,,/Users/jacquesthibodeau/Zotero/storage/TWKCH5RQ/Chandra et al. - 2019 - Gradient Descent The Ultimate Optimizer.pdf,,UnsortedSafety,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5PAQDW3C,manuscript,2020,"Shuster, Kurt; Urbanek, Jack; Dinan, Emily; Szlam, Arthur; Weston, Jason",Deploying Lifelong Open-Domain Dialogue Learning,,,,,http://arxiv.org/abs/2008.08076,"Much of NLP research has focused on crowdsourced static datasets and the supervised learning paradigm of training once and then evaluating test performance. As argued in de Vries et al. (2020), crowdsourced data has the issues of lack of naturalness and relevance to real-world use cases, while the static dataset paradigm does not allow for a model to learn from its experiences of using language (Silver et al., 2013). In contrast, one might hope for machine learning systems that become more useful as they interact with people. In this work, we build and deploy a role-playing game, whereby human players converse with learning agents situated in an open-domain fantasy world. We show that by training models on the conversations they have with humans in the game the models progressively improve, as measured by automatic metrics and online engagement scores. This learning is shown to be more efficient than crowdsourced data when applied to conversations with real users, as well as being far cheaper to collect.",2020-08-19,2022-01-30 04:48:45,2022-01-30 04:48:45,2021-11-07 14:31:04,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000009  arXiv: 2008.08076,,/Users/jacquesthibodeau/Zotero/storage/8R93AX2Q/Shuster et al. - 2020 - Deploying Lifelong Open-Domain Dialogue Learning.pdf,,UnsortedSafety,Computer Science - Artificial Intelligence; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8NPI7H29,manuscript,2020,"Lohn, Andrew J.",Estimating the Brittleness of AI: Safety Integrity Levels and the Need for Testing Out-Of-Distribution Performance,,,,,http://arxiv.org/abs/2009.00802,"Test, Evaluation, Verification, and Validation (TEVV) for Artificial Intelligence (AI) is a challenge that threatens to limit the economic and societal rewards that AI researchers have devoted themselves to producing. A central task of TEVV for AI is estimating brittleness, where brittleness implies that the system functions well within some bounds and poorly outside of those bounds. This paper argues that neither of those criteria are certain of Deep Neural Networks. First, highly touted AI successes (eg. image classification and speech recognition) are orders of magnitude more failure-prone than are typically certified in critical systems even within design bounds (perfectly in-distribution sampling). Second, performance falls off only gradually as inputs become further Out-Of-Distribution (OOD). Enhanced emphasis is needed on designing systems that are resilient despite failure-prone AI components as well as on evaluating and improving OOD performance in order to get AI to where it can clear the challenging hurdles of TEVV and certification.",2020-09-01,2022-01-30 04:48:45,2022-01-30 04:48:45,2021-11-07 17:00:41,,,,,,,Estimating the Brittleness of AI,,,,,,,,,,,,arXiv.org,,ZSCC: 0000003  arXiv: 2009.00802,,/Users/jacquesthibodeau/Zotero/storage/VU4UZTXJ/Lohn - 2020 - Estimating the Brittleness of AI Safety Integrity.pdf; /Users/jacquesthibodeau/Zotero/storage/MS2FRD92/2009.html,,UnsortedSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Computer Science - Software Engineering; Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZADUIUIP,manuscript,2020,"Nakkiran, Preetum; Bansal, Yamini",Distributional Generalization: A New Kind of Generalization,,,,,http://arxiv.org/abs/2009.08092,"We introduce a new notion of generalization -- Distributional Generalization -- which roughly states that outputs of a classifier at train and test time are close *as distributions*, as opposed to close in just their average error. For example, if we mislabel 30% of dogs as cats in the train set of CIFAR-10, then a ResNet trained to interpolation will in fact mislabel roughly 30% of dogs as cats on the *test set* as well, while leaving other classes unaffected. This behavior is not captured by classical generalization, which would only consider the average error and not the distribution of errors over the input domain. Our formal conjectures, which are much more general than this example, characterize the form of distributional generalization that can be expected in terms of problem parameters: model architecture, training procedure, number of samples, and data distribution. We give empirical evidence for these conjectures across a variety of domains in machine learning, including neural networks, kernel machines, and decision trees. Our results thus advance our empirical understanding of interpolating classifiers.",2020-10-14,2022-01-30 04:48:45,2022-01-30 04:48:45,2021-11-14 18:11:08,,,,,,,Distributional Generalization,,,,,,,,,,,,arXiv.org,,ZSCC: 0000003  arXiv: 2009.08092,,/Users/jacquesthibodeau/Zotero/storage/GUN2A4W9/Nakkiran and Bansal - 2020 - Distributional Generalization A New Kind of Gener.pdf; /Users/jacquesthibodeau/Zotero/storage/RR243E5K/2009.html,,UnsortedSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Neural and Evolutionary Computing; Mathematics - Statistics Theory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TEZ5G3WJ,manuscript,2020,"Sharma, Utkarsh; Kaplan, Jared",A Neural Scaling Law from the Dimension of the Data Manifold,,,,,http://arxiv.org/abs/2004.10802,"When data is plentiful, the loss achieved by well-trained neural networks scales as a power-law $L \propto N^{-\alpha}$ in the number of network parameters $N$. This empirical scaling law holds for a wide variety of data modalities, and may persist over many orders of magnitude. The scaling law can be explained if neural models are effectively just performing regression on a data manifold of intrinsic dimension $d$. This simple theory predicts that the scaling exponents $\alpha \approx 4/d$ for cross-entropy and mean-squared error losses. We confirm the theory by independently measuring the intrinsic dimension and the scaling exponents in a teacher/student framework, where we can study a variety of $d$ and $\alpha$ by dialing the properties of random teacher networks. We also test the theory with CNN image classifiers on several datasets and with GPT-type language models.",2020-04-22,2022-01-30 04:48:43,2022-01-30 04:48:43,2021-11-13 23:00:10,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000012  arXiv: 2004.10802,,/Users/jacquesthibodeau/Zotero/storage/4IPPBR63/Sharma and Kaplan - 2020 - A Neural Scaling Law from the Dimension of the Dat.pdf; /Users/jacquesthibodeau/Zotero/storage/TEW3HG34/2004.html,,UnsortedSafety,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DTDRV7MR,manuscript,2020,"Yuan, Li; Xiao, Will; Kreiman, Gabriel; Tay, Francis E. H.; Feng, Jiashi; Livingstone, Margaret S.",Adversarial images for the primate brain,,,,,http://arxiv.org/abs/2011.05623,"Deep artificial neural networks have been proposed as a model of primate vision. However, these networks are vulnerable to adversarial attacks, whereby introducing minimal noise can fool networks into misclassifying images. Primate vision is thought to be robust to such adversarial images. We evaluated this assumption by designing adversarial images to fool primate vision. To do so, we first trained a model to predict responses of face-selective neurons in macaque inferior temporal cortex. Next, we modified images, such as human faces, to match their model-predicted neuronal responses to a target category, such as monkey faces. These adversarial images elicited neuronal responses similar to the target category. Remarkably, the same images fooled monkeys and humans at the behavioral level. These results challenge fundamental assumptions about the similarity between computer and primate vision and show that a model of neuronal activity can selectively direct primate visual behavior.",2020-11-11,2022-01-30 04:48:43,2022-01-30 04:48:43,2021-11-13 22:51:54,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000001  arXiv: 2011.05623,,/Users/jacquesthibodeau/Zotero/storage/SG5SCXZA/Yuan et al. - 2020 - Adversarial images for the primate brain.pdf; /Users/jacquesthibodeau/Zotero/storage/CVAPB22W/2011.html,,UnsortedSafety,Computer Science - Neural and Evolutionary Computing; Computer Science - Computer Vision and Pattern Recognition; Electrical Engineering and Systems Science - Image and Video Processing; Quantitative Biology - Neurons and Cognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
X3VIZNQR,manuscript,2020,"Klinger, Joel; Mateos-Garcia, Juan; Stathoulopoulos, Konstantinos",A narrowing of AI research?,,,,,http://arxiv.org/abs/2009.10385,"Artificial Intelligence (AI) is being hailed as the latest example of a General Purpose Technology that could transform productivity and help tackle important societal challenges. This outcome is however not guaranteed: a myopic focus on short-term benefits could lock AI into technologies that turn out to be sub-optimal in the longer-run. Recent controversies about the dominance of deep learning methods and private labs in AI research suggest that the field may be getting narrower, but the evidence base is lacking. We seek to address this gap with an analysis of the thematic diversity of AI research in arXiv, a widely used pre-prints site. Having identified 110,000 AI papers in this corpus, we use hierarchical topic modelling to estimate the thematic composition of AI research, and this composition to calculate various metrics of research diversity. Our analysis suggests that diversity in AI research has stagnated in recent years, and that AI research involving private sector organisations tends to be less diverse than research in academia. This appears to be driven by a small number of prolific and narrowly-focused technology companies. Diversity in academia is bolstered by smaller institutions and research groups that may have less incentives to race and lower levels of collaboration with the private sector. We also find that private sector AI researchers tend to specialise in data and computationally intensive deep learning methods at the expense of research involving other (symbolic and statistical) AI methods, and of research that considers the societal and ethical implications of AI or applies it in domains like health. Our results suggest that there may be a rationale for policy action to prevent a premature narrowing of AI research that could reduce its societal benefits, but we note the incentive, information and scale hurdles standing in the way of such interventions.",2020-11-17,2022-01-30 04:48:43,2022-01-30 04:48:43,2021-11-13 21:54:52,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000017  arXiv: 2009.10385,,/Users/jacquesthibodeau/Zotero/storage/R7VVT4NZ/Klinger et al. - 2020 - A narrowing of AI research.pdf; /Users/jacquesthibodeau/Zotero/storage/KZJD9H3Z/2009.html,,UnsortedSafety,Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
32WKIH7F,manuscript,2011,"Tomasik, Brian",Risks of Astronomical Future Suﬀering,,,,,,"It’s far from clear that human values will shape an Earth-based space-colonization wave, but even if they do, it seems more likely that space colonization will increase total suﬀering rather than decrease it. That said, other people care a lot about humanity’s survival and spread into the cosmos, so I think suﬀering reducers should let others pursue their spacefaring dreams in exchange for stronger safety measures against future suﬀering. In general, I encourage people to focus on making an intergalactic future more humane if it happens rather than making sure there will be an intergalactic future.",2011,2022-01-30 04:51:36,2022-01-30 04:51:36,,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000018,,/Users/jacquesthibodeau/Zotero/storage/RENG9486/Tomasik - Risks of Astronomical Future Suﬀering.pdf,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IIW23ZE3,manuscript,2020,"Michaud, Eric J.; Gleave, Adam; Russell, Stuart",Understanding Learned Reward Functions,,,,,http://arxiv.org/abs/2012.05862,"In many real-world tasks, it is not possible to procedurally specify an RL agent's reward function. In such cases, a reward function must instead be learned from interacting with and observing humans. However, current techniques for reward learning may fail to produce reward functions which accurately reflect user preferences. Absent significant advances in reward learning, it is thus important to be able to audit learned reward functions to verify whether they truly capture user preferences. In this paper, we investigate techniques for interpreting learned reward functions. In particular, we apply saliency methods to identify failure modes and predict the robustness of reward functions. We find that learned reward functions often implement surprising algorithms that rely on contingent aspects of the environment. We also discover that existing interpretability techniques often attend to irrelevant changes in reward output, suggesting that reward interpretability may need significantly different methods from policy interpretability.",2020-12-10,2022-01-30 04:51:11,2022-01-30 04:51:11,2020-12-18 00:37:06,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000005  arXiv: 2012.05862,,/Users/jacquesthibodeau/Zotero/storage/7RS4HDDE/Michaud et al. - 2020 - Understanding Learned Reward Functions.pdf; /Users/jacquesthibodeau/Zotero/storage/58H3Z5GF/2012.html,,CHAI; TechSafety,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R9QCI7FK,manuscript,2020,"Brundage, Miles; Avin, Shahar; Wang, Jasmine; Belfield, Haydn; Krueger, Gretchen; Hadfield, Gillian; Khlaaf, Heidy; Yang, Jingying; Toner, Helen; Fong, Ruth; Maharaj, Tegan; Koh, Pang Wei; Hooker, Sara; Leung, Jade; Trask, Andrew; Bluemke, Emma; Lebensold, Jonathan; O'Keefe, Cullen; Koren, Mark; Ryffel, Théo; Rubinovitz, J. B.; Besiroglu, Tamay; Carugati, Federica; Clark, Jack; Eckersley, Peter; de Haas, Sarah; Johnson, Maritza; Laurie, Ben; Ingerman, Alex; Krawczuk, Igor; Askell, Amanda; Cammarota, Rosario; Lohn, Andrew; Krueger, David; Stix, Charlotte; Henderson, Peter; Graham, Logan; Prunkl, Carina; Martin, Bianca; Seger, Elizabeth; Zilberman, Noa; hÉigeartaigh, Seán Ó; Kroeger, Frens; Sastry, Girish; Kagan, Rebecca; Weller, Adrian; Tse, Brian; Barnes, Elizabeth; Dafoe, Allan; Scharre, Paul; Herbert-Voss, Ariel; Rasser, Martijn; Sodhani, Shagun; Flynn, Carrick; Gilbert, Thomas Krendl; Dyer, Lisa; Khan, Saif; Bengio, Yoshua; Anderljung, Markus",Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims,,,,,http://arxiv.org/abs/2004.07213,"With the recent wave of progress in artificial intelligence (AI) has come a growing awareness of the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry and academia are insufficient to ensure responsible AI development. In order for AI developers to earn trust from system users, customers, civil society, governments, and other stakeholders that they are building AI responsibly, they will need to make verifiable claims to which they can be held accountable. Those outside of a given organization also need effective means of scrutinizing such claims. This report suggests various steps that different stakeholders can take to improve the verifiability of claims made about AI systems and their associated development processes, with a focus on providing evidence about the safety, security, fairness, and privacy protection of AI systems. We analyze ten mechanisms for this purpose--spanning institutions, software, and hardware--and make recommendations aimed at implementing, exploring, or improving those mechanisms.",2020-04-20,2022-01-30 04:51:11,2022-01-30 04:51:11,2020-08-18 21:36:21,,,,,,,Toward Trustworthy AI Development,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s1]  ACC: 92  arXiv: 2004.07213,,/Users/jacquesthibodeau/Zotero/storage/7V57AEI3/Brundage et al. - 2020 - Toward Trustworthy AI Development Mechanisms for .pdf; /Users/jacquesthibodeau/Zotero/storage/JWN7NA8T/2004.html; /Users/jacquesthibodeau/Zotero/storage/J44EXJ2E/2004.html,,MetaSafety; CHAI; CFI; CSER; CSET; FHI; Open-AI,Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GZZWAE9K,manuscript,2017,"Critch, Andrew",Toward negotiable reinforcement learning: shifting priorities in Pareto optimal sequential decision-making,,,,,http://arxiv.org/abs/1701.01302,"Existing multi-objective reinforcement learning (MORL) algorithms do not account for objectives that arise from players with differing beliefs. Concretely, consider two players with different beliefs and utility functions who may cooperate to build a machine that takes actions on their behalf. A representation is needed for how much the machine's policy will prioritize each player's interests over time. Assuming the players have reached common knowledge of their situation, this paper derives a recursion that any Pareto optimal policy must satisfy. Two qualitative observations can be made from the recursion: the machine must (1) use each player's own beliefs in evaluating how well an action will serve that player's utility function, and (2) shift the relative priority it assigns to each player's expected utilities over time, by a factor proportional to how well that player's beliefs predict the machine's inputs. Observation (2) represents a substantial divergence from na\""{i}ve linear utility aggregation (as in Harsanyi's utilitarian theorem, and existing MORL algorithms), which is shown here to be inadequate for Pareto optimal sequential decision-making on behalf of players with different beliefs.",2017-01-05,2022-01-30 04:51:10,2022-01-30 04:51:10,2018-12-09 18:04:21,,,,,,,Toward negotiable reinforcement learning,,,,,,,,,,,,arXiv.org,,ZSCC: 0000010  arXiv: 1701.01302,,/Users/jacquesthibodeau/Zotero/storage/G65SGP7V/Critch - 2017 - Toward negotiable reinforcement learning shifting.pdf; /Users/jacquesthibodeau/Zotero/storage/X9ZHKTCD/Critch - 2017 - Toward negotiable reinforcement learning shifting.pdf; /Users/jacquesthibodeau/Zotero/storage/ARS283R8/1701.html; /Users/jacquesthibodeau/Zotero/storage/XSPJ5PMX/1701.html; /Users/jacquesthibodeau/Zotero/storage/K885DNUJ/1701.html,,CHAI; TechSafety; MIRI,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
S5KR49MS,manuscript,2017,"Critch, Andrew; Russell, Stuart",Servant of Many Masters: Shifting priorities in Pareto-optimal sequential decision-making,,,,,http://arxiv.org/abs/1711.00363,"It is often argued that an agent making decisions on behalf of two or more principals who have different utility functions should adopt a {\em Pareto-optimal} policy, i.e., a policy that cannot be improved upon for one agent without making sacrifices for another. A famous theorem of Harsanyi shows that, when the principals have a common prior on the outcome distributions of all policies, a Pareto-optimal policy for the agent is one that maximizes a fixed, weighted linear combination of the principals' utilities. In this paper, we show that Harsanyi's theorem does not hold for principals with different priors, and derive a more precise generalization which does hold, which constitutes our main result. In this more general case, the relative weight given to each principal's utility should evolve over time according to how well the agent's observations conform with that principal's prior. The result has implications for the design of contracts, treaties, joint ventures, and robots.",2017-10-31,2022-01-30 04:51:09,2022-01-30 04:51:09,2018-12-09 18:04:24,,,,,,,Servant of Many Masters,,,,,,,,,,,,arXiv.org,,ZSCC: 0000001  arXiv: 1711.00363,,/Users/jacquesthibodeau/Zotero/storage/P2ISQ5FB/Critch and Russell - 2017 - Servant of Many Masters Shifting priorities in Pa.pdf; /Users/jacquesthibodeau/Zotero/storage/ZJ7FA6BB/1711.html; /Users/jacquesthibodeau/Zotero/storage/SNVKFDTA/1711.html,,CHAI; TechSafety,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4W6H9NQQ,manuscript,2017,"Dragan, Anca D.",Robot Planning with Mathematical Models of Human State and Action,,,,,http://arxiv.org/abs/1705.04226,"Robots interacting with the physical world plan with models of physics. We advocate that robots interacting with people need to plan with models of cognition. This writeup summarizes the insights we have gained in integrating computational cognitive models of people into robotics planning and control. It starts from a general game-theoretic formulation of interaction, and analyzes how different approximations result in different useful coordination behaviors for the robot during its interaction with people.",2017-05-11,2022-01-30 04:51:08,2022-01-30 04:51:08,2019-07-22 21:49:18,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000027  arXiv: 1705.04226,,,,CHAI; TechSafety,Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BTQZCG32,manuscript,2020,"Filan, Daniel; Hod, Shlomi; Wild, Cody; Critch, Andrew; Russell, Stuart",Pruned Neural Networks are Surprisingly Modular,,,,,https://arxiv.org/abs/2003.04881v4,"The learned weights of a neural network are often considered devoid of scrutable internal structure. To discern structure in these weights, we introduce a measurable notion of modularity for multi-layer perceptrons (MLPs), and investigate the modular structure of MLPs trained on datasets of small images. Our notion of modularity comes from the graph clustering literature: a ""module"" is a set of neurons with strong internal connectivity but weak external connectivity. We find that training and weight pruning produces MLPs that are more modular than randomly initialized ones, and often significantly more modular than random MLPs with the same (sparse) distribution of weights. Interestingly, they are much more modular when trained with dropout. We also present exploratory analyses of the importance of different modules for performance and how modules depend on each other. Understanding the modular structure of neural networks, when such structure exists, will hopefully render their inner workings more interpretable to engineers.",2020-03-10,2022-01-30 04:51:08,2022-01-30 04:51:08,2020-12-12 01:54:56,,,,,,,,,,,,,,en,,,,,arxiv.org,,ZSCC: 0000001,,/Users/jacquesthibodeau/Zotero/storage/MWESESHU/Filan et al. - 2020 - Pruned Neural Networks are Surprisingly Modular.pdf; /Users/jacquesthibodeau/Zotero/storage/MN3V4CEA/2003.html; /Users/jacquesthibodeau/Zotero/storage/IDPDA9W6/2003.html,,CHAI; TechSafety; AmbiguosSafety; BERI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GG3RHP5E,manuscript,2021,"Stastny, Julian; Treutlein, Johannes; Riché, Maxime; Clifton, Jesse",Multi-agent learning in mixed-motive coordination problems,,,,,https://longtermrisk.org/files/stastny_et_al_implicit_bargaining.pdf,"Cooperation in settings where agents have diﬀerent but overlapping preferences (mixed-motive settings) has recently received considerable attention in multi-agent learning. However, the mixed-motive environments typically studied are simplistic in that they have a single cooperative outcome on which all agents can agree. Multi-agent systems in general may exhibit many payoﬀ proﬁles which might be called cooperative, but which agents have diﬀerent preferences over. This causes problems for independently trained agents that do not arise in the case of that there is a unique cooperative payoﬀ proﬁle. In this note, we illustrate this problem with a class of games called mixed-motive coordination problems (MCPs). We demonstrate the failure of several methods for achieving cooperation in sequential social dilemmas when used to independently train policies in a simple MCP. We discuss some possible directions for ameliorating MCPs.",2021-03-08,2022-01-30 04:51:08,2022-01-30 04:51:08,,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/K84ENAGC/Stastny et al. - 2021 - Multi-agent learning in mixed-motive coordination .pdf,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
D4HJK6A2,manuscript,2017,"Oesterheld, Caspar",Multiverse-wide Cooperation via Correlated Decision Making,,,,,,"Some decision theorists argue that when playing a prisoner’s dilemma-type game against a suﬃciently similar opponent, we should cooperate to make it more likely that our opponent also cooperates. This idea, which Hofstadter calls superrationality, has strong implications when combined with the insight from modern physics that we probably live in a large universe or multiverse of some sort. If we care about what happens in civilizations located elsewhere in the multiverse, we can superrationally cooperate with some of their inhabitants. That is, if we take their values into account, this makes it more likely that they do the same for us. In this paper, I attempt to assess the practical implications of this idea. I argue that to reap the full gains from trade, everyone should maximize the same impartially weighted sum of the utility functions of all collaborators. I also argue that we can obtain at least weak evidence about the content of these utility functions. In practice, the application of superrationality implies that we should promote causal cooperation, moral pluralism, moral reﬂection, and ensure that our descendants, who will be smarter and thus better at ﬁnding out how to beneﬁt other superrationalists in the universe, engage in superrational cooperation.",2017-08-10,2022-01-30 04:51:08,2022-01-30 04:51:08,,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000001,,/Users/jacquesthibodeau/Zotero/storage/WSAM3743/Oesterheld - Multiverse-wide Cooperation via Correlated Decisio.pdf,,CLR; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8M48CF7J,manuscript,2020,"Hutter, Adrian",Learning in two-player games between transparent opponents,,,,,http://arxiv.org/abs/2012.02671,"We consider a scenario in which two reinforcement learning agents repeatedly play a matrix game against each other and update their parameters after each round. The agents' decision-making is transparent to each other, which allows each agent to predict how their opponent will play against them. To prevent an infinite regress of both agents recursively predicting each other indefinitely, each agent is required to give an opponent-independent response with some probability at least epsilon. Transparency also allows each agent to anticipate and shape the other agent's gradient step, i.e. to move to regions of parameter space in which the opponent's gradient points in a direction favourable to them. We study the resulting dynamics experimentally, using two algorithms from previous literature (LOLA and SOS) for opponent-aware learning. We find that the combination of mutually transparent decision-making and opponent-aware learning robustly leads to mutual cooperation in a single-shot prisoner's dilemma. In a game of chicken, in which both agents try to manoeuvre their opponent towards their preferred equilibrium, converging to a mutually beneficial outcome turns out to be much harder, and opponent-aware learning can even lead to worst-case outcomes for both agents. This highlights the need to develop opponent-aware learning algorithms that achieve acceptable outcomes in social dilemmas involving an equilibrium selection problem.",2020-12-04,2022-01-30 04:51:08,2022-01-30 04:51:08,2020-12-12 15:01:23,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000000  arXiv: 2012.02671,,/Users/jacquesthibodeau/Zotero/storage/EFFXSWJA/Hutter - 2020 - Learning in two-player games between transparent o.pdf; /Users/jacquesthibodeau/Zotero/storage/U9M27NJU/2012.html,,CLR; TechSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4P4VEEUM,manuscript,2016,"Tomasik, Brian",How the Simulation Argument Dampens Future Fanaticism,,,,,,"Some eﬀective altruists assume that most of the expected impact of our actions comes from how we inﬂuence the very long-term future of Earthoriginating intelligence over the coming ∼billions of years. According to this view, helping humans and animals in the short term matters, but it mainly only matters via eﬀects on far-future outcomes.",2016,2022-01-30 04:51:08,2022-01-30 04:51:08,,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/CEX9FT56/Tomasik - How the Simulation Argument Dampens Future Fanatic.pdf,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7IV657SA,manuscript,2019,"Plonsky, Ori; Apel, Reut; Ert, Eyal; Tennenholtz, Moshe; Bourgin, David; Peterson, Joshua C.; Reichman, Daniel; Griffiths, Thomas L.; Russell, Stuart J.; Carter, Evan C.; Cavanagh, James F.; Erev, Ido",Predicting human decisions with behavioral theories and machine learning,,,,,http://arxiv.org/abs/1904.06866,"Behavioral decision theories aim to explain human behavior. Can they help predict it? An open tournament for prediction of human choices in fundamental economic decision tasks is presented. The results suggest that integration of certain behavioral theories as features in machine learning systems provides the best predictions. Surprisingly, the most useful theories for prediction build on basic properties of human and animal learning and are very different from mainstream decision theories that focus on deviations from rational choice. Moreover, we find that theoretical features should be based not only on qualitative behavioral insights (e.g. loss aversion), but also on quantitative behavioral foresights generated by functional descriptive models (e.g. Prospect Theory). Our analysis prescribes a recipe for derivation of explainable, useful predictions of human decisions.",2019-04-15,2022-01-30 04:51:07,2022-01-30 04:51:07,2019-12-18 02:16:33,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s6]  ACC: 34  J: 10 arXiv: 1904.06866,,/Users/jacquesthibodeau/Zotero/storage/ZJE7ABQH/Plonsky et al. - 2019 - Predicting human decisions with behavioral theorie.pdf; /Users/jacquesthibodeau/Zotero/storage/BFKM6MGZ/1904.html,,CHAI; TechSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computer Science and Game Theory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NXDBDRKX,manuscript,2020,"Zhan, Albert; Tiomkin, Stas; Abbeel, Pieter",Preventing Imitation Learning with Adversarial Policy Ensembles,,,,,http://arxiv.org/abs/2002.01059,"Imitation learning can reproduce policies by observing experts, which poses a problem regarding policy privacy. Policies, such as human, or policies on deployed robots, can all be cloned without consent from the owners. How can we protect against external observers cloning our proprietary policies? To answer this question we introduce a new reinforcement learning framework, where we train an ensemble of near-optimal policies, whose demonstrations are guaranteed to be useless for an external observer. We formulate this idea by a constrained optimization problem, where the objective is to improve proprietary policies, and at the same time deteriorate the virtual policy of an eventual external observer. We design a tractable algorithm to solve this new optimization problem by modifying the standard policy gradient algorithm. Our formulation can be interpreted in lenses of confidentiality and adversarial behaviour, which enables a broader perspective of this work. We demonstrate the existence of ""non-clonable"" ensembles, providing a solution to the above optimization problem, which is calculated by our modified policy gradient algorithm. To our knowledge, this is the first work regarding the protection of policies in Reinforcement Learning.",2020-08-02,2022-01-30 04:51:07,2022-01-30 04:51:07,2020-11-21 18:40:40,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000004  arXiv: 2002.01059,,/Users/jacquesthibodeau/Zotero/storage/EUFJR46P/Zhan et al. - 2020 - Preventing Imitation Learning with Adversarial Pol.pdf; /Users/jacquesthibodeau/Zotero/storage/THDFQB7N/Zhan et al. - 2020 - Preventing Imitation Learning with Adversarial Pol.pdf; /Users/jacquesthibodeau/Zotero/storage/6WBXIJZN/2002.html; /Users/jacquesthibodeau/Zotero/storage/RR6ER6D9/2002.html,,CHAI; TechSafety; AmbiguosSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BD2HXECW,manuscript,2016,"Oesterheld, Caspar",Backup utility functions as a fail-safe AI technique,,,,,https://longtermrisk.org/files/backup-utility-functions.pdf,"Many experts believe that AIs will, within the not-too-distant future, become powerful enough for their decisions to have tremendous impact. Unfortunately, setting up AI goal systems in a way that results in benevolent behavior is expected to be diﬃcult, and we cannot be certain to get it completely right on the ﬁrst attempt. We should therefore account for the possibility that the goal systems fail to implement our values the intended way. In this paper, we propose the idea of backup utility functions: Secondary utility functions that are used in case the primary ones “fail”. We also describe how this approach can be generalized to the use of multi-layered utility functions, some of which can fail without aﬀecting the ﬁnal outcome as badly as without the backup mechanism.",2016-10,2022-01-30 04:51:06,2022-01-30 04:51:06,2020-12-18,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000003,,/Users/jacquesthibodeau/Zotero/storage/FWH7VQ85/Oesterheld - Backup utility functions as a fail-safe AI techniq.pdf,,CLR; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4AMXU6SC,manuscript,2020,"Andrychowicz, Marcin; Raichuk, Anton; Stańczyk, Piotr; Orsini, Manu; Girgin, Sertan; Marinier, Raphael; Hussenot, Léonard; Geist, Matthieu; Pietquin, Olivier; Michalski, Marcin; Gelly, Sylvain; Bachem, Olivier",What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study,,,,,http://arxiv.org/abs/2006.05990,"In recent years, on-policy reinforcement learning (RL) has been successfully applied to many different continuous control tasks. While RL algorithms are often conceptually simple, their state-of-the-art implementations take numerous low- and high-level design decisions that strongly affect the performance of the resulting agents. Those choices are usually not extensively discussed in the literature, leading to discrepancy between published descriptions of algorithms and their implementations. This makes it hard to attribute progress in RL and slows down overall progress [Engstrom'20]. As a step towards filling that gap, we implement >50 such ``choices'' in a unified on-policy RL framework, allowing us to investigate their impact in a large-scale empirical study. We train over 250'000 agents in five continuous control environments of different complexity and provide insights and practical recommendations for on-policy training of RL agents.",2020-06-10,2022-01-30 04:48:55,2022-01-30 04:48:55,2021-11-07 22:59:39,,,,,,,What Matters In On-Policy Reinforcement Learning?,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s1]  ACC: 44  arXiv: 2006.05990,,/Users/jacquesthibodeau/Zotero/storage/PMZ4SVSJ/Andrychowicz et al. - 2020 - What Matters In On-Policy Reinforcement Learning .pdf,,UnsortedSafety,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FN5EFZJ8,manuscript,2020,"D'Amour, Alexander; Heller, Katherine; Moldovan, Dan; Adlam, Ben; Alipanahi, Babak; Beutel, Alex; Chen, Christina; Deaton, Jonathan; Eisenstein, Jacob; Hoffman, Matthew D.; Hormozdiari, Farhad; Houlsby, Neil; Hou, Shaobo; Jerfel, Ghassen; Karthikesalingam, Alan; Lucic, Mario; Ma, Yian; McLean, Cory; Mincu, Diana; Mitani, Akinori; Montanari, Andrea; Nado, Zachary; Natarajan, Vivek; Nielson, Christopher; Osborne, Thomas F.; Raman, Rajiv; Ramasamy, Kim; Sayres, Rory; Schrouff, Jessica; Seneviratne, Martin; Sequeira, Shannon; Suresh, Harini; Veitch, Victor; Vladymyrov, Max; Wang, Xuezhi; Webster, Kellie; Yadlowsky, Steve; Yun, Taedong; Zhai, Xiaohua; Sculley, D.",Underspecification Presents Challenges for Credibility in Modern Machine Learning,,,,,http://arxiv.org/abs/2011.03395,"ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.",2020-11-24,2022-01-30 04:48:54,2022-01-30 04:48:54,2021-11-13 19:45:01,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s0]  ACC: 163  arXiv: 2011.03395,,/Users/jacquesthibodeau/Zotero/storage/X3D7X8IC/D'Amour et al. - 2020 - Underspecification Presents Challenges for Credibi.pdf; /Users/jacquesthibodeau/Zotero/storage/VP4HBEMV/2011.html,,UnsortedSafety,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZSEVCJIG,manuscript,2019,"Gruetzemacher, Ross; Whittlestone, Jess",The Transformative Potential of Artificial Intelligence,,,,,http://arxiv.org/abs/1912.00747,"Recently the concept of transformative AI (TAI) has begun to receive attention in the AI policy space. TAI is often framed as an alternative formulation to notions of strong AI (e.g. artificial general intelligence or superintelligence) and reflects increasing consensus that advanced AI which does not fit these definitions may nonetheless have extreme and long-lasting impacts on society. However, the term TAI is poorly defined and often used ambiguously. Some use the notion of TAI to describe levels of societal transformation associated with previous 'general purpose technologies' (GPTs) such as electricity or the internal combustion engine. Others use the term to refer to more drastic levels of transformation comparable to the agricultural or industrial revolutions. The notion has also been used much more loosely, with some implying that current AI systems are already having a transformative impact on society. This paper unpacks and analyses the notion of TAI, proposing a distinction between narrowly transformative AI (NTAI), TAI and radically transformative AI (RTAI), roughly corresponding to associated levels of societal change. We describe some relevant dimensions associated with each and discuss what kinds of advances in capabilities they might require. We further consider the relationship between TAI and RTAI and whether we should necessarily expect a period of TAI to precede the emergence of RTAI. This analysis is important as it can help guide discussions among AI policy researchers about how to allocate resources towards mitigating the most extreme impacts of AI and it can bring attention to negative TAI scenarios that are currently neglected.",2019,2022-01-30 04:50:26,2022-01-30 04:50:26,2020-11-14 00:55:29,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s1]  ACC: 6  arXiv: 1912.00747,,/Users/jacquesthibodeau/Zotero/storage/SD4BQGGF/Gruetzemacher and Whittlestone - 2020 - The Transformative Potential of Artificial Intelli.pdf; /Users/jacquesthibodeau/Zotero/storage/F5KGIKR2/1912.html,,MetaSafety; CFI; CSER; AmbiguosSafety; BERI,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7CSE8NXS,manuscript,2019,"Ovadya, Aviv; Whittlestone, Jess",Reducing malicious use of synthetic media research: Considerations and potential release practices for machine learning,,,,,http://arxiv.org/abs/1907.11274,"The aim of this paper is to facilitate nuanced discussion around research norms and practices to mitigate the harmful impacts of advances in machine learning (ML). We focus particularly on the use of ML to create ""synthetic media"" (e.g. to generate or manipulate audio, video, images, and text), and the question of what publication and release processes around such research might look like, though many of the considerations discussed will apply to ML research more broadly. We are not arguing for any specific approach on when or how research should be distributed, but instead try to lay out some useful tools, analogies, and options for thinking about these issues. We begin with some background on the idea that ML research might be misused in harmful ways, and why advances in synthetic media, in particular, are raising concerns. We then outline in more detail some of the different paths to harm from ML research, before reviewing research risk mitigation strategies in other fields and identifying components that seem most worth emulating in the ML and synthetic media research communities. Next, we outline some important dimensions of disagreement on these issues which risk polarizing conversations. Finally, we conclude with recommendations, suggesting that the machine learning community might benefit from: working with subject matter experts to increase understanding of the risk landscape and possible mitigation strategies; building a community and norms around understanding the impacts of ML research, e.g. through regular workshops at major conferences; and establishing institutions and systems to support release practices that would otherwise be onerous and error-prone.",2019-07-28,2022-01-30 04:50:25,2022-01-30 04:50:25,2019-12-16 22:39:38,,,,,,,Reducing malicious use of synthetic media research,,,,,,,,,,,,arXiv.org,,ZSCC: 0000012  arXiv: 1907.11274,,/Users/jacquesthibodeau/Zotero/storage/9JS2GGPU/Ovadya and Whittlestone - 2019 - Reducing malicious use of synthetic media research.pdf; /Users/jacquesthibodeau/Zotero/storage/DX2WMWTT/Ovadya and Whittlestone - 2019 - Reducing malicious use of synthetic media research.pdf; /Users/jacquesthibodeau/Zotero/storage/R2ETWS25/1907.html; /Users/jacquesthibodeau/Zotero/storage/ADSVSN46/1907.html,,MetaSafety; CFI; CSER; AmbiguosSafety,Computer Science - Machine Learning; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZAFKPQNH,manuscript,2018,"Martínez-Plumed, Fernando; Avin, Shahar; Brundage, Miles; Dafoe, Allan; hÉigeartaigh, Sean Ó; Hernández-Orallo, José",Accounting for the neglected dimensions of AI progress,,,,,https://arxiv.org/abs/1806.00610,,2018,2022-01-30 04:50:23,2022-01-30 04:50:23,,,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: NoCitationData[s4]  ACC: 19,,/Users/jacquesthibodeau/Zotero/storage/RDSFDZK3/Martínez-Plumed et al. - 2018 - Accounting for the neglected dimensions of ai prog.pdf; /Users/jacquesthibodeau/Zotero/storage/XMI73HC7/1806.html,,MetaSafety; CFI; CSER; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3ND6DSMU,manuscript,2018,"Manheim, David",Oversight of Unsafe Systems via Dynamic Safety Envelopes,,,,,https://arxiv.org/abs/1811.09246v1,"This paper reviews the reasons that Human-in-the-Loop is both critical for preventing widely-understood failure modes for machine learning, and not a practical solution. Following this, we review two current heuristic methods for addressing this. The first is provable safety envelopes, which are possible only when the dynamics of the system are fully known, but can be useful safety guarantees when optimal behavior is based on machine learning with poorly-understood safety characteristics. The second is the simpler circuit breaker model, which can forestall or prevent catastrophic outcomes by stopping the system, without any specific model of the system. This paper proposes using heuristic, dynamic safety envelopes, which are a plausible halfway point between these approaches that allows human oversight without some of the more difficult problems faced by Human-in-the-Loop systems. Finally, the paper concludes with how this approach can be used for governance of systems where otherwise unsafe systems are deployed.",2018-11-22,2022-01-30 04:50:07,2022-01-30 04:50:07,2020-12-12 02:12:35,,,,,,,,,,,,,,en,,,,,arxiv.org,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/GR2JUKSK/Manheim - 2018 - Oversight of Unsafe Systems via Dynamic Safety Env.pdf; /Users/jacquesthibodeau/Zotero/storage/UQH5ZDBA/1811.html,,TechSafety; BERI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BBXGS4Q8,manuscript,2019,"Maltinsky, Baeo; Gallagher, Jack; Taylor, Jessica",Feasibility of Training an AGI using Deep RL: A Very Rough Estimate,,,,,"http://mediangroup.org/docs/Feasibility%20of%20Training%20an%20AGI%20using%20Deep%20Reinforcement%20Learning,%20A%20Very%20Rough%20Estimate.pdf",,2019,2022-01-30 04:50:07,2022-01-30 04:50:07,2020-12-21,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: NoCitationData[s2]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/32AHTZBN/Maltinsky et al. - Feasibility of Training an AGI using Deep RL A Ve.pdf,,TechSafety; BERI; Median-group,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
W6GX3WIP,manuscript,,"McKenzie, Colleen; Hidysmith, J Bryce",AI Insights Dataset Analysis,,,,,http://mediangroup.org/docs/insights-analysis.pdf,,unknown,2022-01-30 04:50:07,2022-01-30 04:50:07,2020-12-21,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: NoCitationData[s2]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/AHG7R97Z/McKenzie and Hidysmith - AI Insights Dataset Analysis.pdf,,TechSafety; BERI; Median-group,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KKT6G9GE,manuscript,,"Rade, Luca",A Framework for the Safety of Agent-Environment Systems,,,,,,"Ensuring the safety of an autonomous artiﬁcial agent in a complex environment represents a formidable problem across multiple domains. However, there is little interaction between these domains, and no unifying theoretical framework forcing the delineation of assumptions. I propose such a framework in terms of agent-environment systems, in which an agent and environment co-evolve according to a modiﬁed statespace nonlinear system. The agent gathers limited information from the environment and itself to perform an action, operating implicitly on the basis of a coarse-grained model of the systems dynamics. To ensure the systems safety, it is minimally necessary ﬁrst to translate the set of undesirable states from human terms into an adequately precise deﬁnition within the system; then to identify a set of universal markers necessary to the system being in a pre-state to an undesirable state, which transfer with ﬁdelity from the systems state to the agents information; and ﬁnally to have a set of actions by the agent for each pre-state which keep the system out of the set of undesirable states, with the exception of agent-independent dynamics. Incomplete information, information distortion, and coarse-grained models make this a particularly difﬁcult challenge. I conclude by proposing three threads of a research agenda: reducing the possibility space of safe agents by demonstrating the failure of certain methods and identifying problems with particular agent-environment system classes; developing and verifying techniques which address those problems; and matching real systems to agentenvironment systems.",unknown,2022-01-30 04:50:06,2022-01-30 04:50:06,,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: NoCitationData[s3]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/Q5GH7SZQ/Mapping existing AI safety.pdf; /Users/jacquesthibodeau/Zotero/storage/CHU7DKNZ/Rade - A Framework for the Safety of Agent-Environment Sy.pdf,,TechSafety; BERI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A8VGP6QH,manuscript,2019,"Hidysmith, J Bryce",A Descending Veil of Maya,,,,,,,2019,2022-01-30 04:50:06,2022-01-30 04:50:06,,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: NoCitationData[s3]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/EWG32KDF/Hidysmith - A Descending Veil of Maya.pdf,,TechSafety; BERI; Median-group,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WI8THJZV,manuscript,2020,"Tětek, Jakub; Sklenka, Marek; Gavenčiak, Tomáš",Performance of Bounded-Rational Agents With the Ability to Self-Modify,,,,,http://arxiv.org/abs/2011.06275,"Self-modification of agents embedded in complex environments is hard to avoid, whether it happens via direct means (e.g. own code modification) or indirectly (e.g. influencing the operator, exploiting bugs or the environment). While it has been argued that intelligent agents have an incentive to avoid modifying their utility function so that their future instances will work towards the same goals, it is not clear whether this also applies in non-dualistic scenarios, where the agent is embedded in the environment. The problem of self-modification safety is raised by Bostrom in Superintelligence (2014) in the context of safe AGI deployment. In contrast to Everitt et al. (2016), who formally show that providing an option to self-modify is harmless for perfectly rational agents, we show that for agents with bounded rationality, self-modification may cause exponential deterioration in performance and gradual misalignment of a previously aligned agent. We investigate how the size of this effect depends on the type and magnitude of imperfections in the agent's rationality (1-4 below). We also discuss model assumptions and the wider problem and framing space. Specifically, we introduce several types of a bounded-rational agent, which either (1) doesn't always choose the optimal action, (2) is not perfectly aligned with human values, (3) has an innacurate model of the environment, or (4) uses the wrong temporal discounting factor. We show that while in the cases (2)-(4) the misalignment caused by the agent's imperfection does not worsen over time, with (1) the misalignment may grow exponentially.",2020-11-12,2022-01-30 04:49:30,2022-01-30 04:49:30,2020-11-21 18:15:00,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000001  arXiv: 2011.06275,,/Users/jacquesthibodeau/Zotero/storage/ZG9PUMJQ/Tětek et al. - 2020 - Performance of Bounded-Rational Agents With the Ab.pdf,,TechSafety; AI-Safety-Camp; AISRP2019,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
H8VNSUZZ,manuscript,2020,"Gruetzemacher, Ross; Dorner, Florian; Bernaola-Alvarez, Niko; Giattino, Charlie; Manheim, David",Forecasting AI Progress: A Research Agenda,,,,,http://arxiv.org/abs/2008.01848,"Forecasting AI progress is essential to reducing uncertainty in order to appropriately plan for research efforts on AI safety and AI governance. While this is generally considered to be an important topic, little work has been conducted on it and there is no published document that gives and objective overview of the field. Moreover, the field is very diverse and there is no published consensus regarding its direction. This paper describes the development of a research agenda for forecasting AI progress which utilized the Delphi technique to elicit and aggregate experts' opinions on what questions and methods to prioritize. The results of the Delphi are presented; the remainder of the paper follow the structure of these results, briefly reviewing relevant literature and suggesting future work for each topic. Experts indicated that a wide variety of methods should be considered for forecasting AI progress. Moreover, experts identified salient questions that were both general and completely unique to the problem of forecasting AI progress. Some of the highest priority topics include the validation of (partially unresolved) forecasts, how to make forecasting action-guiding and the quality of different performance metrics. While statistical methods seem more promising, there is also recognition that supplementing judgmental techniques can be quite beneficial.",2020-08-04,2022-01-30 04:49:30,2022-01-30 04:49:30,2020-08-24 20:25:42,,,,,,,Forecasting AI Progress,,,,,,,,,,,,arXiv.org,,ZSCC: 0000000[s0]  arXiv: 2008.01848,,/Users/jacquesthibodeau/Zotero/storage/C7CAJQ97/Gruetzemacher et al. - 2020 - Forecasting AI Progress A Research Agenda.pdf; /Users/jacquesthibodeau/Zotero/storage/5B7HP3TB/2008.html,,MetaSafety; AI-Safety-Camp; AISRP2019,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
U2DT5V9B,manuscript,2020,"Kovarik, Vojta",AI Services: Introduction v1.3,,,,,https://docs.google.com/document/d/1SYgvWBe1ruDl9dQnxmjll-8COUHPycGOlLvTI68xtLA/edit?pli=1&usp=embed_facebook,"This document aims to serve as an introduction for researchers who want to study the long-term impact of AI through the lens of AI services. It introduces basic concepts related to these systems and gives initial observations to enhance their initial study. It points to several relevant research fields that could be leveraged to study AI services, mentions a number of problems that seem specific to this setting, and makes suggestions for future work.",2020-03-31,2022-01-30 04:49:30,2022-01-30 04:49:30,2020-08-14 19:44:36,,,,,,,AI Services,,,,,,,en,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/QFJBG8U2/edit.html,,TechSafety; AI-Safety-Camp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SCB4T6S5,manuscript,2018,"Armstrong, Stuart; O'Rorke, Xavier",Good and safe uses of AI Oracles,,,,,http://arxiv.org/abs/1711.05541,"It is possible that powerful and potentially dangerous artificial intelligence (AI) might be developed in the future. An Oracle is a design which aims to restrain the impact of a potentially dangerous AI by restricting the agent to no actions besides answering questions. Unfortunately, most Oracles will be motivated to gain more control over the world by manipulating users through the content of their answers, and Oracles of potentially high intelligence might be very successful at this \citep{DBLP:journals/corr/AlfonsecaCACAR16}. In this paper we present two designs for Oracles which, even under pessimistic assumptions, will not manipulate their users into releasing them and yet will still be incentivised to provide their users with helpful answers. The first design is the counterfactual Oracle -- which choses its answer as if it expected nobody to ever read it. The second design is the low-bandwidth Oracle -- which is limited by the quantity of information it can transmit.",2018-06-05,2022-01-30 04:53:17,2022-01-30 04:53:17,2020-11-22 04:11:52,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s2]  ACC: 20  arXiv: 1711.05541,,/Users/jacquesthibodeau/Zotero/storage/J86CWCCN/Armstrong and O'Rorke - 2018 - Good and safe uses of AI Oracles.pdf; /Users/jacquesthibodeau/Zotero/storage/U7F3VQ4J/Armstrong and O'Rorke - 2018 - Good and safe uses of AI Oracles.pdf; /Users/jacquesthibodeau/Zotero/storage/JX48I6WQ/1711.html; /Users/jacquesthibodeau/Zotero/storage/RDZSWUWX/1711.html,,TechSafety; FHI,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
59AKR2E2,manuscript,2021,"Cohen, Michael K.; Hutter, Marcus; Nanda, Neel",Fully General Online Imitation Learning,,,,,http://arxiv.org/abs/2102.08686,"In imitation learning, imitators and demonstrators are policies for picking actions given past interactions with the environment. If we run an imitator, we probably want events to unfold similarly to the way they would have if the demonstrator had been acting the whole time. No existing work provides formal guidance in how this might be accomplished, instead restricting focus to environments that restart, making learning unusually easy, and conveniently limiting the significance of any mistake. We address a fully general setting, in which the (stochastic) environment and demonstrator never reset, not even for training purposes. Our new conservative Bayesian imitation learner underestimates the probabilities of each available action, and queries for more data with the remaining probability. Our main result: if an event would have been unlikely had the demonstrator acted the whole time, that event's likelihood can be bounded above when running the (initially totally ignorant) imitator instead. Meanwhile, queries to the demonstrator rapidly diminish in frequency.",2021-02-17,2022-01-30 04:53:10,2022-01-30 04:53:10,2021-10-31 19:13:33,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000000  arXiv: 2102.08686,,/Users/jacquesthibodeau/Zotero/storage/BDP2UPND/Cohen et al. - 2021 - Fully General Online Imitation Learning.pdf; /Users/jacquesthibodeau/Zotero/storage/8T8JCEB2/2102.html,,TechSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; I.2.0; I.2.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DI833FP9,manuscript,2016,"Leike, Jan",Exploration Potential,,,,,https://arxiv.org/abs/1609.04994v3,"We introduce exploration potential, a quantity that measures how much a reinforcement learning agent has explored its environment class. In contrast to information gain, exploration potential takes the problem's reward structure into account. This leads to an exploration criterion that is both necessary and sufficient for asymptotic optimality (learning to act optimally across the entire environment class). Our experiments in multi-armed bandits use exploration potential to illustrate how different algorithms make the tradeoff between exploration and exploitation.",2016-09-16,2022-01-30 04:53:10,2022-01-30 04:53:10,2019-12-19 01:45:54,,,,,,,,,,,,,,en,,,,,arxiv.org,,ZSCC: 0000002,,/Users/jacquesthibodeau/Zotero/storage/Q33MXUNJ/Leike - 2016 - Exploration Potential.pdf; /Users/jacquesthibodeau/Zotero/storage/IK85485C/1609.html; /Users/jacquesthibodeau/Zotero/storage/V7QHF9K7/Leike - 2016 - Exploration Potential.pdf; /Users/jacquesthibodeau/Zotero/storage/IM6V7HDQ/1609.html,,TechSafety; FHI,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ST9XJA8B,manuscript,2018,"Armstrong, Stuart","Counterfactual equivalence for POMDPs, and underlying deterministic environments",,,,,https://arxiv.org/abs/1801.03737,,2018,2022-01-30 04:53:09,2022-01-30 04:53:09,,,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000001,,"/Users/jacquesthibodeau/Zotero/storage/GJBHW4QW/Armstrong - 2018 - Counterfactual equivalence for POMDPs, and underly.pdf; /Users/jacquesthibodeau/Zotero/storage/5D5PX64R/1801.html",,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NIZKQPB2,manuscript,2018,"Sandberg, Anders; Drexler, Eric; Ord, Toby",Dissolving the Fermi Paradox,,,,,https://arxiv.org/abs/1806.02404,,2018,2022-01-30 04:53:09,2022-01-30 04:53:09,,,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000034,,/Users/jacquesthibodeau/Zotero/storage/QXFHXM8D/Sandberg et al. - 2018 - Dissolving the Fermi Paradox.pdf; /Users/jacquesthibodeau/Zotero/storage/7WUZBHUW/1806.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
W3EIIGEB,manuscript,2020,"O’Keefe, Cullen",Antitrust-Compliant AI Industry Self-Regulation,,,,,https://cullenokeefe.com/blog/antitrust-compliant-ai-industry-self-regulation,"The touchstone of antitrust compliance is competition. To be legally permissible, any industrial restraint on trade must have sufficient countervailing procompetitive justifications. Usually, anticompetitive horizontal agreements like boycotts (including a refusal to produce certain products) are per se illegal.",2020-07-07,2022-01-30 04:53:08,2022-01-30 04:53:08,2020-08-28,,15,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: NoCitationData[s2]  ACC: 1,,/Users/jacquesthibodeau/Zotero/storage/ZBGCJCWU/O’Keefe - Antitrust-Compliant AI Industry Self-Regulation.pdf,,MetaSafety; FHI; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XMQEGVQE,manuscript,2018,"Schulze, Sebastian; Evans, Owain",Active reinforcement learning with monte-carlo tree search,,,,,https://arxiv.org/abs/1803.04926,,2018,2022-01-30 04:53:08,2022-01-30 04:53:08,,,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000007,,/Users/jacquesthibodeau/Zotero/storage/VRI3QKNQ/Schulze and Evans - 2018 - Active Reinforcement Learning with Monte-Carlo Tre.pdf; /Users/jacquesthibodeau/Zotero/storage/9BXHK4PU/1803.html; /Users/jacquesthibodeau/Zotero/storage/J9AC5ZVX/Schulze and Evans - 2018 - Active reinforcement learning with monte-carlo tre.pdf; /Users/jacquesthibodeau/Zotero/storage/H347X5TR/1803.html,,TechSafety; FHI,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6IRHXH7N,manuscript,2019,"Everitt, Tom; Ortega, Pedro A.; Barnes, Elizabeth; Legg, Shane",Understanding Agent Incentives using Causal Influence Diagrams. Part I: Single Action Settings,,,,,http://arxiv.org/abs/1902.09980,"Agents are systems that optimize an objective function in an environment. Together, the goal and the environment induce secondary objectives, incentives. Modeling the agent-environment interaction using causal influence diagrams, we can answer two fundamental questions about an agent's incentives directly from the graph: (1) which nodes can the agent have an incentivize to observe, and (2) which nodes can the agent have an incentivize to control? The answers tell us which information and influence points need extra protection. For example, we may want a classifier for job applications to not use the ethnicity of the candidate, and a reinforcement learning agent not to take direct control of its reward mechanism. Different algorithms and training paradigms can lead to different causal influence diagrams, so our method can be used to identify algorithms with problematic incentives and help in designing algorithms with better incentives.",2019-09-06,2022-01-30 04:52:49,2022-01-30 04:52:49,2019-12-16 20:27:00,,,,,,,Understanding Agent Incentives using Causal Influence Diagrams. Part I,,,,,,,,,,,,arXiv.org,,ZSCC: 0000020  arXiv: 1902.09980,,/Users/jacquesthibodeau/Zotero/storage/FJWTC444/Everitt et al. - 2019 - Understanding Agent Incentives using Causal Influe.pdf; /Users/jacquesthibodeau/Zotero/storage/KKZ6SCGC/Everitt et al. - 2019 - Understanding Agent Incentives using Causal Influe.pdf; /Users/jacquesthibodeau/Zotero/storage/AX23IEDU/1902.html; /Users/jacquesthibodeau/Zotero/storage/H934KB2V/1902.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; I.2.6; I.2.8,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
27BEKE99,manuscript,2018,"Dvijotham, Krishnamurthy; Gowal, Sven; Stanforth, Robert; Arandjelovic, Relja; O'Donoghue, Brendan; Uesato, Jonathan; Kohli, Pushmeet",Training verified learners with learned verifiers,,,,,http://arxiv.org/abs/1805.10265,"This paper proposes a new algorithmic framework, predictor-verifier training, to train neural networks that are verifiable, i.e., networks that provably satisfy some desired input-output properties. The key idea is to simultaneously train two networks: a predictor network that performs the task at hand,e.g., predicting labels given inputs, and a verifier network that computes a bound on how well the predictor satisfies the properties being verified. Both networks can be trained simultaneously to optimize a weighted combination of the standard data-fitting loss and a term that bounds the maximum violation of the property. Experiments show that not only is the predictor-verifier architecture able to train networks to achieve state of the art verified robustness to adversarial examples with much shorter training times (outperforming previous algorithms on small datasets like MNIST and SVHN), but it can also be scaled to produce the first known (to the best of our knowledge) verifiably robust networks for CIFAR-10.",2018-05-29,2022-01-30 04:52:49,2022-01-30 04:52:49,2019-12-16 20:32:21,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000115  arXiv: 1805.10265,,/Users/jacquesthibodeau/Zotero/storage/7QDXM7DA/Dvijotham et al. - 2018 - Training verified learners with learned verifiers.pdf; /Users/jacquesthibodeau/Zotero/storage/V79K38GB/1805.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UNV94I7V,manuscript,2018,"Martic, Miljan; Leike, Jan; Trask, Andrew; Hessel, Matteo; Legg, Shane; Kohli, Pushmeet",Scaling shared model governance via model splitting,,,,,http://arxiv.org/abs/1812.05979,"Currently the only techniques for sharing governance of a deep learning model are homomorphic encryption and secure multiparty computation. Unfortunately, neither of these techniques is applicable to the training of large neural networks due to their large computational and communication overheads. As a scalable technique for shared model governance, we propose splitting deep learning model between multiple parties. This paper empirically investigates the security guarantee of this technique, which is introduced as the problem of model completion: Given the entire training data set or an environment simulator, and a subset of the parameters of a trained deep learning model, how much training is required to recover the model's original performance? We define a metric for evaluating the hardness of the model completion problem and study it empirically in both supervised learning on ImageNet and reinforcement learning on Atari and DeepMind~Lab. Our experiments show that (1) the model completion problem is harder in reinforcement learning than in supervised learning because of the unavailability of the trained agent's trajectories, and (2) its hardness depends not primarily on the number of parameters of the missing part, but more so on their type and location. Our results suggest that model splitting might be a feasible technique for shared model governance in some settings where training is very expensive.",2018-12-14,2022-01-30 04:52:48,2022-01-30 04:52:48,2019-12-16 20:33:08,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000002  arXiv: 1812.05979,,/Users/jacquesthibodeau/Zotero/storage/TXQ2A3M4/Martic et al. - 2018 - Scaling shared model governance via model splittin.pdf; /Users/jacquesthibodeau/Zotero/storage/9R7NEPEF/1812.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Computer Science - Cryptography and Security,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QBPVTXNN,manuscript,2018,"Leike, Jan; Krueger, David; Everitt, Tom; Martic, Miljan; Maini, Vishal; Legg, Shane",Scalable agent alignment via reward modeling: a research direction,,,,,http://arxiv.org/abs/1811.07871,"One obstacle to applying reinforcement learning algorithms to real-world problems is the lack of suitable reward functions. Designing such reward functions is difficult in part because the user only has an implicit understanding of the task objective. This gives rise to the agent alignment problem: how do we create agents that behave in accordance with the user's intentions? We outline a high-level research direction to solve the agent alignment problem centered around reward modeling: learning a reward function from interaction with the user and optimizing the learned reward function with reinforcement learning. We discuss the key challenges we expect to face when scaling reward modeling to complex and general domains, concrete approaches to mitigate these challenges, and ways to establish trust in the resulting agents.",2018-11-19,2022-01-30 04:52:48,2022-01-30 04:52:48,2019-12-16 20:33:45,,,,,,,Scalable agent alignment via reward modeling,,,,,,,,,,,,arXiv.org,,ZSCC: 0000083  arXiv: 1811.07871,,/Users/jacquesthibodeau/Zotero/storage/USVJ3JSI/Leike et al. - 2018 - Scalable agent alignment via reward modeling a re.pdf; /Users/jacquesthibodeau/Zotero/storage/5M854I6Q/Leike et al. - 2018 - Scalable agent alignment via reward modeling a re.pdf; /Users/jacquesthibodeau/Zotero/storage/SSZXRUAI/1811.html; /Users/jacquesthibodeau/Zotero/storage/Z3J54XGK/1811.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XKZTJI95,manuscript,2018,"Uesato, Jonathan; Kumar, Ananya; Szepesvari, Csaba; Erez, Tom; Ruderman, Avraham; Anderson, Keith; Dvijotham, Krishmamurthy; Heess, Nicolas; Kohli, Pushmeet",Rigorous Agent Evaluation: An Adversarial Approach to Uncover Catastrophic Failures,,,,,http://arxiv.org/abs/1812.01647,"This paper addresses the problem of evaluating learning systems in safety critical domains such as autonomous driving, where failures can have catastrophic consequences. We focus on two problems: searching for scenarios when learned agents fail and assessing their probability of failure. The standard method for agent evaluation in reinforcement learning, Vanilla Monte Carlo, can miss failures entirely, leading to the deployment of unsafe agents. We demonstrate this is an issue for current agents, where even matching the compute used for training is sometimes insufficient for evaluation. To address this shortcoming, we draw upon the rare event probability estimation literature and propose an adversarial evaluation approach. Our approach focuses evaluation on adversarially chosen situations, while still providing unbiased estimates of failure probabilities. The key difficulty is in identifying these adversarial situations -- since failures are rare there is little signal to drive optimization. To solve this we propose a continuation approach that learns failure modes in related but less robust agents. Our approach also allows reuse of data already collected for training the agent. We demonstrate the efficacy of adversarial evaluation on two standard domains: humanoid control and simulated driving. Experimental results show that our methods can find catastrophic failures and estimate failures rates of agents multiple orders of magnitude faster than standard evaluation schemes, in minutes to hours rather than days.",2018-12-04,2022-01-30 04:52:48,2022-01-30 04:52:48,2019-12-16 20:26:47,,,,,,,Rigorous Agent Evaluation,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s7]  ACC: 46  J: 16 arXiv: 1812.01647,,/Users/jacquesthibodeau/Zotero/storage/V96F8BPW/Uesato et al. - 2018 - Rigorous Agent Evaluation An Adversarial Approach.pdf; /Users/jacquesthibodeau/Zotero/storage/XQFBG5UT/Uesato et al. - 2018 - Rigorous Agent Evaluation An Adversarial Approach.pdf; /Users/jacquesthibodeau/Zotero/storage/ZSCSD84M/1812.html; /Users/jacquesthibodeau/Zotero/storage/RVGFMRR9/1812.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Cryptography and Security,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FXQMIVSU,manuscript,2020,"Carey, Ryan; Langlois, Eric; Everitt, Tom; Legg, Shane",The Incentives that Shape Behaviour,,,,,http://arxiv.org/abs/2001.07118,"Which variables does an agent have an incentive to control with its decision, and which variables does it have an incentive to respond to? We formalise these incentives, and demonstrate unique graphical criteria for detecting them in any single decision causal influence diagram. To this end, we introduce structural causal influence models, a hybrid of the influence diagram and structural causal model frameworks. Finally, we illustrate how these incentives predict agent incentives in both fairness and AI safety applications.",2020-01-20,2022-01-30 04:52:48,2022-01-30 04:52:48,2020-08-18 21:24:59,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000009  arXiv: 2001.07118,,/Users/jacquesthibodeau/Zotero/storage/JBVM9X3R/Carey et al. - 2020 - The Incentives that Shape Behaviour.pdf; /Users/jacquesthibodeau/Zotero/storage/VE49AUW5/2001.html,,TechSafety; FHI; DeepMind,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; I.2.6; I.2.8,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7MAZDCTT,manuscript,2018,"Dalal, Gal; Dvijotham, Krishnamurthy; Vecerik, Matej; Hester, Todd; Paduraru, Cosmin; Tassa, Yuval",Safe Exploration in Continuous Action Spaces,,,,,http://arxiv.org/abs/1801.08757,"We address the problem of deploying a reinforcement learning (RL) agent on a physical system such as a datacenter cooling unit or robot, where critical constraints must never be violated. We show how to exploit the typically smooth dynamics of these systems and enable RL algorithms to never violate constraints during learning. Our technique is to directly add to the policy a safety layer that analytically solves an action correction formulation per each state. The novelty of obtaining an elegant closed-form solution is attained due to a linearized model, learned on past trajectories consisting of arbitrary actions. This is to mimic the real-world circumstances where data logs were generated with a behavior policy that is implausible to describe mathematically; such cases render the known safety-aware off-policy methods inapplicable. We demonstrate the efficacy of our approach on new representative physics-based environments, and prevail where reward shaping fails by maintaining zero constraint violations.",2018-01-26,2022-01-30 04:52:48,2022-01-30 04:52:48,2019-12-16 20:35:38,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000173  arXiv: 1801.08757,,/Users/jacquesthibodeau/Zotero/storage/HM9VIAT3/Dalal et al. - 2018 - Safe Exploration in Continuous Action Spaces.pdf; /Users/jacquesthibodeau/Zotero/storage/3FWRT7KM/1801.html,,TechSafety; DeepMind,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
U8Z3P3R3,manuscript,2020,"Kumar, Ramana; Uesato, Jonathan; Ngo, Richard; Everitt, Tom; Krakovna, Victoria; Legg, Shane",REALab: An Embedded Perspective on Tampering,,,,,http://arxiv.org/abs/2011.08820,"This paper describes REALab, a platform for embedded agency research in reinforcement learning (RL). REALab is designed to model the structure of tampering problems that may arise in real-world deployments of RL. Standard Markov Decision Process (MDP) formulations of RL and simulated environments mirroring the MDP structure assume secure access to feedback (e.g., rewards). This may be unrealistic in settings where agents are embedded and can corrupt the processes producing feedback (e.g., human supervisors, or an implemented reward function). We describe an alternative Corrupt Feedback MDP formulation and the REALab environment platform, which both avoid the secure feedback assumption. We hope the design of REALab provides a useful perspective on tampering problems, and that the platform may serve as a unit test for the presence of tampering incentives in RL agent designs.",2020-11-17,2022-01-30 04:52:47,2022-01-30 04:52:47,2020-12-12 15:36:25,,,,,,,REALab,,,,,,,,,,,,arXiv.org,,ZSCC: 0000003  arXiv: 2011.08820,,/Users/jacquesthibodeau/Zotero/storage/5549P5DE/Kumar et al. - 2020 - REALab An Embedded Perspective on Tampering.pdf; /Users/jacquesthibodeau/Zotero/storage/2SPGRMZ3/2011.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WTI4ZW58,manuscript,2018,"Perolat, Julien; Malinowski, Mateusz; Piot, Bilal; Pietquin, Olivier",Playing the Game of Universal Adversarial Perturbations,,,,,http://arxiv.org/abs/1809.07802,"We study the problem of learning classifiers robust to universal adversarial perturbations. While prior work approaches this problem via robust optimization, adversarial training, or input transformation, we instead phrase it as a two-player zero-sum game. In this new formulation, both players simultaneously play the same game, where one player chooses a classifier that minimizes a classification loss whilst the other player creates an adversarial perturbation that increases the same loss when applied to every sample in the training set. By observing that performing a classification (respectively creating adversarial samples) is the best response to the other player, we propose a novel extension of a game-theoretic algorithm, namely fictitious play, to the domain of training robust classifiers. Finally, we empirically show the robustness and versatility of our approach in two defence scenarios where universal attacks are performed on several image classification datasets -- CIFAR10, CIFAR100 and ImageNet.",2018-09-25,2022-01-30 04:52:47,2022-01-30 04:52:47,2019-12-16 20:36:59,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000016  arXiv: 1809.07802,,/Users/jacquesthibodeau/Zotero/storage/9Z9532E6/Perolat et al. - 2018 - Playing the Game of Universal Adversarial Perturba.pdf; /Users/jacquesthibodeau/Zotero/storage/3DZF8UJT/1809.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GD4JV3IM,manuscript,2016,"Dewey, Daniel; Russell, Stuart J; Tegmark, Max",A survey of research questions for robust and beneficial AI,,,,,https://futureoflife.org/data/documents/research_survey.pdf?x96845,,2016-01-25,2022-01-30 04:53:38,2022-01-30 04:53:38,2020-11-21 17:06:15,,,,,,,,,,,,,Future of Life Institute,,,,,,,,ZSCC: 0000002[s0],,/Users/jacquesthibodeau/Zotero/storage/ZZC972RC/research_survey.pdf,,TechSafety; FLI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4VTKAFVS,manuscript,2021,"Evans, Owain; Cotton-Barratt, Owen; Finnveden, Lukas; Bales, Adam; Balwit, Avital; Wills, Peter; Righetti, Luca; Saunders, William",Truthful AI: Developing and governing AI that does not lie,,,,,http://arxiv.org/abs/2110.06674,"In many contexts, lying -- the use of verbal falsehoods to deceive -- is harmful. While lying has traditionally been a human affair, AI systems that make sophisticated verbal statements are becoming increasingly prevalent. This raises the question of how we should limit the harm caused by AI ""lies"" (i.e. falsehoods that are actively selected for). Human truthfulness is governed by social norms and by laws (against defamation, perjury, and fraud). Differences between AI and humans present an opportunity to have more precise standards of truthfulness for AI, and to have these standards rise over time. This could provide significant benefits to public epistemics and the economy, and mitigate risks of worst-case AI futures. Establishing norms or laws of AI truthfulness will require significant work to: (1) identify clear truthfulness standards; (2) create institutions that can judge adherence to those standards; and (3) develop AI systems that are robustly truthful. Our initial proposals for these areas include: (1) a standard of avoiding ""negligent falsehoods"" (a generalisation of lies that is easier to assess); (2) institutions to evaluate AI systems before and after real-world deployment; and (3) explicitly training AI systems to be truthful via curated datasets and human interaction. A concerning possibility is that evaluation mechanisms for eventual truthfulness standards could be captured by political interests, leading to harmful censorship and propaganda. Avoiding this might take careful attention. And since the scale of AI speech acts might grow dramatically over the coming decades, early truthfulness standards might be particularly important because of the precedents they set.",2021-10-13,2022-01-30 04:53:37,2022-01-30 04:53:37,2021-11-18 23:51:54,,,,,,,Truthful AI,,,,,,,,,,,,arXiv.org,,ZSCC: 0000000  arXiv: 2110.06674,,/Users/jacquesthibodeau/Zotero/storage/N89FGRG3/Evans et al. - 2021 - Truthful AI Developing and governing AI that does.pdf; /Users/jacquesthibodeau/Zotero/storage/RGF8PQC9/2110.html,,TechSafety,Computer Science - Artificial Intelligence; Computer Science - Computers and Society; I.2.0; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BHNFUZ5M,manuscript,2017,"Sandberg, Anders; Armstrong, Stuart; Cirkovic, Milan M.",That is not dead which can eternal lie: the aestivation hypothesis for resolving Fermi's paradox,,,,,https://arxiv.org/abs/1705.03394v1,"If a civilization wants to maximize computation it appears rational to aestivate until the far future in order to exploit the low temperature environment: this can produce a $10^{30}$ multiplier of achievable computation. We hence suggest the ""aestivation hypothesis"": the reason we are not observing manifestations of alien civilizations is that they are currently (mostly) inactive, patiently waiting for future cosmic eras. This paper analyzes the assumptions going into the hypothesis and how physical law and observational evidence constrain the motivations of aliens compatible with the hypothesis.",2017-04-27,2022-01-30 04:53:36,2022-01-30 04:53:36,2019-12-19 01:36:54,,,,,,,That is not dead which can eternal lie,,,,,,,en,,,,,arxiv.org,,ZSCC: 0000025,,/Users/jacquesthibodeau/Zotero/storage/HIATFPH7/Sandberg et al. - 2017 - That is not dead which can eternal lie the aestiv.pdf; /Users/jacquesthibodeau/Zotero/storage/RUSE883X/1705.html; /Users/jacquesthibodeau/Zotero/storage/97694V8V/Sandberg et al. - 2017 - That is not dead which can eternal lie the aestiv.pdf; /Users/jacquesthibodeau/Zotero/storage/2WWEMZ7N/1705.html,,MetaSafety; FHI,Physics - Popular Physics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FM6JR8HH,manuscript,2019,"Hubinger, Evan; van Merwijk, Chris; Mikulik, Vladimir; Skalse, Joar; Garrabrant, Scott",Risks from Learned Optimization in Advanced Machine Learning Systems,,,,,http://arxiv.org/abs/1906.01820,"We analyze the type of learned optimization that occurs when a learned model (such as a neural network) is itself an optimizer - a situation we refer to as mesa-optimization, a neologism we introduce in this paper. We believe that the possibility of mesa-optimization raises two important questions for the safety and transparency of advanced machine learning systems. First, under what circumstances will learned models be optimizers, including when they should not be? Second, when a learned model is an optimizer, what will its objective be - how will it differ from the loss function it was trained under - and how can it be aligned? In this paper, we provide an in-depth analysis of these two primary questions and provide an overview of topics for future research.",2019-06-11,2022-01-30 04:53:35,2022-01-30 04:53:35,2019-12-16 02:27:32,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000016  arXiv: 1906.01820,,/Users/jacquesthibodeau/Zotero/storage/MURNKGU7/Hubinger et al. - 2019 - Risks from Learned Optimization in Advanced Machin.pdf; /Users/jacquesthibodeau/Zotero/storage/CFIC3DIX/Hubinger et al. - 2019 - Risks from Learned Optimization in Advanced Machin.pdf; /Users/jacquesthibodeau/Zotero/storage/GCV386SM/1906.html,,TechSafety; FHI; MIRI,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
N84Z6VKW,manuscript,2017,"Garfinkel, Ben; Brundage, Miles; Filan, Daniel; Flynn, Carrick; Luketina, Jelena; Page, Michael; Sandberg, Anders; Snyder-Beattie, Andrew; Tegmark, Max",On the Impossibility of Supersized Machines,,,,,https://arxiv.org/abs/1703.10987,,2017,2022-01-30 04:53:19,2022-01-30 04:53:19,,,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000003,,/Users/jacquesthibodeau/Zotero/storage/3F5AE7EV/Garfinkel et al. - 2017 - On the Impossibility of Supersized Machines.pdf; /Users/jacquesthibodeau/Zotero/storage/EIAR3DMW/1703.html; /Users/jacquesthibodeau/Zotero/storage/2QFF7UF5/Garfinkel et al. - 2017 - On the Impossibility of Supersized Machines.pdf; /Users/jacquesthibodeau/Zotero/storage/H9N3TUKD/1703.html; /Users/jacquesthibodeau/Zotero/storage/I9J3PC5N/1703.html,,TechSafety; FHI,Computer Science - Computers and Society; Physics - Popular Physics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SDJWMEQZ,manuscript,2015,"Armstrong, Stuart",Oﬀ-policy Monte Carlo agents with variable behaviour policies,,,,,https://www.fhi.ox.ac.uk/wp-content/uploads/monte_carlo_arXiv.pdf,"This paper looks at the convergence property of oﬀ-policy Monte Carlo agents with variable behaviour policies. It presents results about convergence and lack of convergence. Even if the agent generates every possible episode history inﬁnitely often, the algorithm can fail to converge on the correct Q-values. On the other hand, it can converge on the correct Q-values under certain conditions. For instance, if, during the n-th episode, the agent has an independent probability of 1/ log(n) of following the original policy at any given state, then it will converge on the right Q-values for that policy.",2015,2022-01-30 04:53:19,2022-01-30 04:53:19,,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000000  J: 0,,/Users/jacquesthibodeau/Zotero/storage/SDACGAB8/Armstrong - Oﬀ-policy Monte Carlo agents with variable behavio.pdf,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WIRXPN9E,manuscript,2017,"Armstrong, Stuart; Levinstein, Benjamin",Low impact artificial intelligences,,,,,https://arxiv.org/abs/1705.10720,,2017,2022-01-30 04:53:18,2022-01-30 04:53:18,,,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000029,,/Users/jacquesthibodeau/Zotero/storage/VT7URXJ6/Armstrong and Levinstein - 2017 - Low impact artificial intelligences.pdf; /Users/jacquesthibodeau/Zotero/storage/6VWVU36V/1705.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
F3ICPAQP,manuscript,2005,"Tegmark, Max; Bostrom, Nick",How unlikely is a doomsday catastrophe?,,,,,https://arxiv.org/abs/astro-ph/0512204v2,"Numerous Earth-destroying doomsday scenarios have recently been analyzed, including breakdown of a metastable vacuum state and planetary destruction triggered by a ""strangelet'' or microscopic black hole. We point out that many previous bounds on their frequency give a false sense of security: one cannot infer that such events are rare from the the fact that Earth has survived for so long, because observers are by definition in places lucky enough to have avoided destruction. We derive a new upper bound of one per 10^9 years (99.9% c.l.) on the exogenous terminal catastrophe rate that is free of such selection bias, using planetary age distributions and the relatively late formation time of Earth.",2005-12-08,2022-01-30 04:53:18,2022-01-30 04:53:18,2019-12-19 01:44:09,,,,,,,,,,,,,,en,,,,,arxiv.org,,ZSCC: 0000022,,/Users/jacquesthibodeau/Zotero/storage/92I96SAN/Tegmark and Bostrom - 2005 - How unlikely is a doomsday catastrophe.pdf; /Users/jacquesthibodeau/Zotero/storage/69V52QGG/0512204.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AZM4AF64,manuscript,2017,"Armstrong, Stuart; O'Rourke, Xavier",Indifference' methods for managing agent rewards,,,,,https://arxiv.org/abs/1712.06365,,2017,2022-01-30 04:53:18,2022-01-30 04:53:18,,,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000012,,/Users/jacquesthibodeau/Zotero/storage/KX5XHD8P/Armstrong and O'Rourke - 2018 - 'Indifference' methods for managing agent rewards.pdf; /Users/jacquesthibodeau/Zotero/storage/K98E6NZA/1712.html; /Users/jacquesthibodeau/Zotero/storage/GVC3FDWX/Armstrong and O'Rourke - 2017 - 'Indifference'methods for managing agent rewards.pdf; /Users/jacquesthibodeau/Zotero/storage/3R2JSKSJ/1712.html,,TechSafety; FHI,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
S55X7DHC,manuscript,2016,"Critch, Andrew",Parametric Bounded Löb's Theorem and Robust Cooperation of Bounded Agents,,,,,http://arxiv.org/abs/1602.04184,"Löb's theorem and Gödel's theorems make predictions about the behavior of systems capable of self-reference with unbounded computational resources with which to write and evaluate proofs. However, in the real world, systems capable of self-reference will have limited memory and processing speed, so in this paper we introduce an effective version of L\""ob's theorem which is applicable given such bounded resources. These results have powerful implications for the game theory of bounded agents who are able to write proofs about themselves and one another, including the capacity to out-perform classical Nash equilibria and correlated equilibria, attaining mutually cooperative program equilibrium in the Prisoner's Dilemma. Previous cooperative program equilibria studied by Tennenholtz (2004) and Fortnow (2009) have depended on tests for program equality, a fragile condition, whereas ""L\""obian"" cooperation is much more robust and agnostic of the opponent's implementation.",2016-08-24,2022-01-30 04:50:56,2022-01-30 04:50:56,2019-12-16 02:30:38,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000005[s0]  arXiv: 1602.04184,,/Users/jacquesthibodeau/Zotero/storage/W9BVMS95/Critch - 2016 - Parametric Bounded Lob's Theorem and Robust Coop.pdf; /Users/jacquesthibodeau/Zotero/storage/UVHR7XHR/Critch - 2016 - Parametric Bounded Lob's Theorem and Robust Coop.pdf; /Users/jacquesthibodeau/Zotero/storage/XWNIBX84/1602.html; /Users/jacquesthibodeau/Zotero/storage/43T9ZUX6/1602.html; /Users/jacquesthibodeau/Zotero/storage/ZXV2JJQK/1602.html,,CHAI; TechSafety; MIRI,Computer Science - Computer Science and Game Theory; Computer Science - Logic in Computer Science,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AD9HNBFJ,manuscript,2017,"Hadﬁeld, Gillian K; Hadﬁeld-Menell, Dylan",Pervasive Spurious Normativity,,,,,,This paper proposes a mathematical model for a simpliﬁed version of the game deﬁned in Hadﬁeld and Weingast [2012] which proposes that legal order can be described as an equilibrium in thirdparty decentralized enforcement coordinated by a centralized classiﬁcation institution. We explore the attractiveness of joining a new group (which is assumed to have settled on an enforcement equilibrium already) where groups differ in terms of the frequency of interactions in which norm violation is possible (normative interactions) and thus punishment is called for. We show that groups in which normative interactions are frequent but involve relatively unimportant rules may achieve higher value for participants.,2017,2022-01-30 04:50:56,2022-01-30 04:50:56,,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000000[s0],,/Users/jacquesthibodeau/Zotero/storage/JAS5344V/Hadﬁeld and Hadﬁeld-Menell - Pervasive Spurious Normativity.pdf,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
46HGF7X4,manuscript,2020,"Turner, Alexander Matt; Smith, Logan; Shah, Rohin; Tadepalli, Prasad",Optimal Farsighted Agents Tend to Seek Power,,,,,http://arxiv.org/abs/1912.01683,"Some researchers have speculated that capable reinforcement learning (RL) agents pursuing misspecified objectives are often incentivized to seek resources and power in pursuit of those objectives. An agent seeking power is incentivized to behave in undesirable ways, including rationally preventing deactivation and correction. Others have voiced skepticism: humans seem idiosyncratic in their urges to power, which need not be present in the agents we design. We formalize a notion of power within the context of finite Markov decision processes (MDPs). With respect to a neutral class of reward function distributions, our results suggest that farsighted optimal policies tend to seek power over the environment.",2020-06-05,2022-01-30 04:50:55,2022-01-30 04:50:55,2020-11-21 17:26:57,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000001  arXiv: 1912.01683,,/Users/jacquesthibodeau/Zotero/storage/P5BE9NS6/Turner et al. - 2020 - Optimal Farsighted Agents Tend to Seek Power.pdf; /Users/jacquesthibodeau/Zotero/storage/4HCESMWM/1912.html,,CHAI; TechSafety,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A2T8FD8D,manuscript,2020,"Ndousse, Kamal; Eck, Douglas; Levine, Sergey; Jaques, Natasha",Multi-agent Social Reinforcement Learning Improves Generalization,,,,,http://arxiv.org/abs/2010.00581,"Social learning is a key component of human and animal intelligence. By taking cues from the behavior of experts in their environment, social learners can acquire sophisticated behavior and rapidly adapt to new circumstances. This paper investigates whether independent reinforcement learning (RL) agents in a multi-agent environment can use social learning to improve their performance using cues from other agents. We find that in most circumstances, vanilla model-free RL agents do not use social learning, even in environments in which individual exploration is expensive. We analyze the reasons for this deficiency, and show that by introducing a model-based auxiliary loss we are able to train agents to lever-age cues from experts to solve hard exploration tasks. The generalized social learning policy learned by these agents allows them to not only outperform the experts with which they trained, but also achieve better zero-shot transfer performance than solo learners when deployed to novel environments with experts. In contrast, agents that have not learned to rely on social learning generalize poorly and do not succeed in the transfer task. Further,we find that by mixing multi-agent and solo training, we can obtain agents that use social learning to out-perform agents trained alone, even when experts are not avail-able. This demonstrates that social learning has helped improve agents' representation of the task itself. Our results indicate that social learning can enable RL agents to not only improve performance on the task at hand, but improve generalization to novel environments.",2020-10-01,2022-01-30 04:50:55,2022-01-30 04:50:55,2020-11-14 00:37:58,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000002  arXiv: 2010.00581,,/Users/jacquesthibodeau/Zotero/storage/RX2T5UXT/Ndousse et al. - 2020 - Multi-agent Social Reinforcement Learning Improves.pdf; /Users/jacquesthibodeau/Zotero/storage/UJZ4W3NC/2010.html,,CHAI; TechSafety; Open-AI; AmbiguosSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7EWBZBCW,manuscript,2018,"Tucker, Aaron; Gleave, Adam; Russell, Stuart",Inverse reinforcement learning for video games,,,,,http://arxiv.org/abs/1810.10593,"Deep reinforcement learning achieves superhuman performance in a range of video game environments, but requires that a designer manually specify a reward function. It is often easier to provide demonstrations of a target behavior than to design a reward function describing that behavior. Inverse reinforcement learning (IRL) algorithms can infer a reward from demonstrations in low-dimensional continuous control environments, but there has been little work on applying IRL to high-dimensional video games. In our CNN-AIRL baseline, we modify the state-of-the-art adversarial IRL (AIRL) algorithm to use CNNs for the generator and discriminator. To stabilize training, we normalize the reward and increase the size of the discriminator training dataset. We additionally learn a low-dimensional state representation using a novel autoencoder architecture tuned for video game environments. This embedding is used as input to the reward network, improving the sample efﬁciency of expert demonstrations. Our method achieves high-level performance on the simple Catcher video game, substantially outperforming the CNN-AIRL baseline. We also score points on the Enduro Atari racing game, but do not match expert performance, highlighting the need for further work.",2018-10-24,2022-01-30 04:50:53,2022-01-30 04:50:53,2019-12-18 02:19:50,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000025  arXiv: 1810.10593,,/Users/jacquesthibodeau/Zotero/storage/WPQAXM4I/Tucker et al. - 2018 - Inverse reinforcement learning for video games.pdf; /Users/jacquesthibodeau/Zotero/storage/65P44U7Q/1810.html; /Users/jacquesthibodeau/Zotero/storage/BZ6IQPH2/Tucker et al. - 2018 - Inverse reinforcement learning for video games.pdf,,CHAI; TechSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; I.2.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IIUG993E,manuscript,2021,"Rashidinejad, Paria; Zhu, Banghua; Ma, Cong; Jiao, Jiantao; Russell, Stuart",Bridging Offline Reinforcement Learning and Imitation Learning: A Tale of Pessimism,,,,,http://arxiv.org/abs/2103.12021,"Offline (or batch) reinforcement learning (RL) algorithms seek to learn an optimal policy from a fixed dataset without active data collection. Based on the composition of the offline dataset, two main categories of methods are used: imitation learning which is suitable for expert datasets and vanilla offline RL which often requires uniform coverage datasets. From a practical standpoint, datasets often deviate from these two extremes and the exact data composition is usually unknown a priori. To bridge this gap, we present a new offline RL framework that smoothly interpolates between the two extremes of data composition, hence unifying imitation learning and vanilla offline RL. The new framework is centered around a weak version of the concentrability coefficient that measures the deviation from the behavior policy to the expert policy alone. Under this new framework, we further investigate the question on algorithm design: can one develop an algorithm that achieves a minimax optimal rate and also adapts to unknown data composition? To address this question, we consider a lower confidence bound (LCB) algorithm developed based on pessimism in the face of uncertainty in offline RL. We study finite-sample properties of LCB as well as information-theoretic limits in multi-armed bandits, contextual bandits, and Markov decision processes (MDPs). Our analysis reveals surprising facts about optimality rates. In particular, in all three settings, LCB achieves a faster rate of $1/N$ for nearly-expert datasets compared to the usual rate of $1/\sqrt{N}$ in offline RL, where $N$ is the number of samples in the batch dataset. In the case of contextual bandits with at least two contexts, we prove that LCB is adaptively optimal for the entire data composition range, achieving a smooth transition from imitation learning to offline RL. We further show that LCB is almost adaptively optimal in MDPs.",2021-03-22,2022-01-30 04:50:43,2022-01-30 04:50:43,2021-10-30 21:30:30,,,,,,,Bridging Offline Reinforcement Learning and Imitation Learning,,,,,,,,,,,,arXiv.org,,ZSCC: 0000016  arXiv: 2103.12021,,/Users/jacquesthibodeau/Zotero/storage/AUSU3VZ6/Rashidinejad et al. - 2021 - Bridging Offline Reinforcement Learning and Imitat.pdf; /Users/jacquesthibodeau/Zotero/storage/U5A563JV/2103.html,,TechSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Mathematics - Statistics Theory; Mathematics - Optimization and Control,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CJ245SU4,manuscript,2021,"Filan, Daniel; Casper, Stephen; Hod, Shlomi; Wild, Cody; Critch, Andrew; Russell, Stuart",Clusterability in Neural Networks,,,,,http://arxiv.org/abs/2103.03386,"The learned weights of a neural network have often been considered devoid of scrutable internal structure. In this paper, however, we look for structure in the form of clusterability: how well a network can be divided into groups of neurons with strong internal connectivity but weak external connectivity. We find that a trained neural network is typically more clusterable than randomly initialized networks, and often clusterable relative to random networks with the same distribution of weights. We also exhibit novel methods to promote clusterability in neural network training, and find that in multi-layer perceptrons they lead to more clusterable networks with little reduction in accuracy. Understanding and controlling the clusterability of neural networks will hopefully render their inner workings more interpretable to engineers by facilitating partitioning into meaningful clusters.",2021-03-04,2022-01-30 04:50:43,2022-01-30 04:50:43,2021-10-30 22:44:22,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000005  arXiv: 2103.03386,,/Users/jacquesthibodeau/Zotero/storage/2PQM9XHE/Filan et al. - 2021 - Clusterability in Neural Networks.pdf,,TechSafety,Computer Science - Neural and Evolutionary Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ME3AQUAM,manuscript,2018,"Mindermann, Sören; Shah, Rohin; Gleave, Adam; Hadfield-Menell, Dylan",Active Inverse Reward Design,,,,,https://arxiv.org/abs/1809.03060,"Reward design, the problem of selecting an appropriate reward function for an AI system, is both critically important, as it encodes the task the system should perform, and challenging, as it requires reasoning about and understanding the agent’s environment in detail. AI practitioners often iterate on the reward function for their systems in a trial-and-error process to get their desired behavior. Inverse reward design (IRD) is a preference inference method that infers a true reward function from an observed, possibly misspeciﬁed, proxy reward function. This allows the system to determine when it should trust its observed reward function and respond appropriately. This has been shown to avoid problems in reward design such as negative side-effects (omitting a seemingly irrelevant but important aspect of the task) and reward hacking (learning to exploit unanticipated loopholes). In this paper, we actively select the set of proxy reward functions available to the designer. This improves the quality of inference and simpliﬁes the associated reward design problem. We present two types of queries: discrete queries, where the system designer chooses from a discrete set of reward functions, and feature queries, where the system queries the designer for weights on a small set of features. We evaluate this approach with experiments in a personal shopping assistant domain and a 2D navigation domain. We ﬁnd that our approach leads to reduced regret at test time compared with vanilla IRD. Our results indicate that actively selecting the set of available reward functions is a promising direction to improve the efﬁciency and effectiveness of reward design.",2018,2022-01-30 04:50:42,2022-01-30 04:50:42,,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000015,,/Users/jacquesthibodeau/Zotero/storage/S59BK5N6/Mindermann et al. - 2019 - Active Inverse Reward Design.pdf; /Users/jacquesthibodeau/Zotero/storage/9KM9HA7F/1809.html; /Users/jacquesthibodeau/Zotero/storage/MTNGDAC9/Mindermann et al. - Active Inverse Reward Design.pdf,,CHAI; TechSafety,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
68WZKQQ4,manuscript,2018,"Ortega, Pedro A.; Legg, Shane",Modeling Friends and Foes,,,,,http://arxiv.org/abs/1807.00196,"How can one detect friendly and adversarial behavior from raw data? Detecting whether an environment is a friend, a foe, or anything in between, remains a poorly understood yet desirable ability for safe and robust agents. This paper proposes a definition of these environmental ""attitudes"" based on an characterization of the environment's ability to react to the agent's private strategy. We define an objective function for a one-shot game that allows deriving the environment's probability distribution under friendly and adversarial assumptions alongside the agent's optimal strategy. Furthermore, we present an algorithm to compute these equilibrium strategies, and show experimentally that both friendly and adversarial environments possess non-trivial optimal strategies.",2018-06-30,2022-01-30 04:52:39,2022-01-30 04:52:39,2019-12-16 20:35:04,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000004  J: 2 arXiv: 1807.00196,,/Users/jacquesthibodeau/Zotero/storage/VWQC9GCA/Ortega and Legg - 2018 - Modeling Friends and Foes.pdf; /Users/jacquesthibodeau/Zotero/storage/IUKTABAW/1807.html,,TechSafety; DeepMind,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3GKZHK9F,manuscript,2019,"Chow, Yinlam; Nachum, Ofir; Faust, Aleksandra; Duenez-Guzman, Edgar; Ghavamzadeh, Mohammad",Lyapunov-based Safe Policy Optimization for Continuous Control,,,,,http://arxiv.org/abs/1901.10031,"We study continuous action reinforcement learning problems in which it is crucial that the agent interacts with the environment only through safe policies, i.e.,~policies that do not take the agent to undesirable situations. We formulate these problems as constrained Markov decision processes (CMDPs) and present safe policy optimization algorithms that are based on a Lyapunov approach to solve them. Our algorithms can use any standard policy gradient (PG) method, such as deep deterministic policy gradient (DDPG) or proximal policy optimization (PPO), to train a neural network policy, while guaranteeing near-constraint satisfaction for every policy update by projecting either the policy parameter or the action onto the set of feasible solutions induced by the state-dependent linearized Lyapunov constraints. Compared to the existing constrained PG algorithms, ours are more data efficient as they are able to utilize both on-policy and off-policy data. Moreover, our action-projection algorithm often leads to less conservative policy updates and allows for natural integration into an end-to-end PG training pipeline. We evaluate our algorithms and compare them with the state-of-the-art baselines on several simulated (MuJoCo) tasks, as well as a real-world indoor robot navigation problem, demonstrating their effectiveness in terms of balancing performance and constraint satisfaction. Videos of the experiments can be found in the following link: https://drive.google.com/file/d/1pzuzFqWIE710bE2U6DmS59AfRzqK2Kek/view?usp=sharing.",2019-02-11,2022-01-30 04:52:39,2022-01-30 04:52:39,2019-12-16 20:32:58,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000083  arXiv: 1901.10031,,/Users/jacquesthibodeau/Zotero/storage/IN5MJ7VZ/Chow et al. - 2019 - Lyapunov-based Safe Policy Optimization for Contin.pdf; /Users/jacquesthibodeau/Zotero/storage/85W7HCZP/1901.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JBZW9WNV,manuscript,2020,"Krueger, David; Maharaj, Tegan; Leike, Jan",Hidden Incentives for Auto-Induced Distributional Shift,,,,,http://arxiv.org/abs/2009.09153,"Decisions made by machine learning systems have increasing influence on the world, yet it is common for machine learning algorithms to assume that no such influence exists. An example is the use of the i.i.d. assumption in content recommendation. In fact, the (choice of) content displayed can change users' perceptions and preferences, or even drive them away, causing a shift in the distribution of users. We introduce the term auto-induced distributional shift (ADS) to describe the phenomenon of an algorithm causing a change in the distribution of its own inputs. Our goal is to ensure that machine learning systems do not leverage ADS to increase performance when doing so could be undesirable. We demonstrate that changes to the learning algorithm, such as the introduction of meta-learning, can cause hidden incentives for auto-induced distributional shift (HI-ADS) to be revealed. To address this issue, we introduce `unit tests' and a mitigation strategy for HI-ADS, as well as a toy environment for modelling real-world issues with HI-ADS in content recommendation, where we demonstrate that strong meta-learners achieve gains in performance via ADS. We show meta-learning and Q-learning both sometimes fail unit tests, but pass when using our mitigation strategy.",2020-09-18,2022-01-30 04:52:38,2022-01-30 04:52:38,2020-11-21 17:32:28,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000007  arXiv: 2009.09153,,/Users/jacquesthibodeau/Zotero/storage/C4IC67EE/Krueger et al. - 2020 - Hidden Incentives for Auto-Induced Distributional .pdf; /Users/jacquesthibodeau/Zotero/storage/ZHEWPCUB/2009.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TQWJG7NP,manuscript,2020,"Hill, Felix; Mokra, Sona; Wong, Nathaniel; Harley, Tim",Human Instruction-Following with Deep Reinforcement Learning via Transfer-Learning from Text,,,,,http://arxiv.org/abs/2005.09382,"Recent work has described neural-network-based agents that are trained with reinforcement learning (RL) to execute language-like commands in simulated worlds, as a step towards an intelligent agent or robot that can be instructed by human users. However, the optimisation of multi-goal motor policies via deep RL from scratch requires many episodes of experience. Consequently, instructionfollowing with deep RL typically involves language generated from templates (by an environment simulator), which does not reﬂect the varied or ambiguous expressions of real users. Here, we propose a conceptually simple method for training instruction-following agents with deep RL that are robust to natural human instructions. By applying our method with a state-of-the-art pre-trained text-based language model (BERT), on tasks requiring agents to identify and position everyday objects relative to other objects in a naturalistic 3D simulated room, we demonstrate substantially-above-chance zero-shot transfer from synthetic template commands to natural instructions given by humans. Our approach is a general recipe for training any deep RL-based system to interface with human users, and bridges the gap between two research directions of notable recent success: agent-centric motor behavior and text-based representation learning.",2020-05-19,2022-01-30 04:52:38,2022-01-30 04:52:38,2020-08-31 18:19:41,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000019  arXiv: 2005.09382,,/Users/jacquesthibodeau/Zotero/storage/MSQMATUV/Hill et al. - 2020 - Human Instruction-Following with Deep Reinforcemen.pdf,,TechSafety; DeepMind,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6E69ZJCN,manuscript,2019,"Fort, Stanislav; Hu, Huiyi; Lakshminarayanan, Balaji",Deep Ensembles: A Loss Landscape Perspective,,,,,http://arxiv.org/abs/1912.02757,"Deep ensembles have been empirically shown to be a promising approach for improving accuracy, uncertainty and out-of-distribution robustness of deep learning models. While deep ensembles were theoretically motivated by the bootstrap, non-bootstrap ensembles trained with just random initialization also perform well in practice, which suggests that there could be other explanations for why deep ensembles work well. Bayesian neural networks, which learn distributions over the parameters of the network, are theoretically well-motivated by Bayesian principles, but do not perform as well as deep ensembles in practice, particularly under dataset shift. One possible explanation for this gap between theory and practice is that popular scalable approximate Bayesian methods tend to focus on a single mode, whereas deep ensembles tend to explore diverse modes in function space. We investigate this hypothesis by building on recent work on understanding the loss landscape of neural networks and adding our own exploration to measure the similarity of functions in the space of predictions. Our results show that random initializations explore entirely different modes, while functions along an optimization trajectory or sampled from the subspace thereof cluster within a single mode predictions-wise, while often deviating significantly in the weight space. We demonstrate that while low-loss connectors between modes exist, they are not connected in the space of predictions. Developing the concept of the diversity--accuracy plane, we show that the decorrelation power of random initializations is unmatched by popular subspace sampling methods.",2019-12-05,2022-01-30 04:52:38,2022-01-30 04:52:38,2019-12-16 20:30:39,,,,,,,Deep Ensembles,,,,,,,,,,,,arXiv.org,,ZSCC: 0000155  arXiv: 1912.02757,,/Users/jacquesthibodeau/Zotero/storage/6GHKJWB9/Fort et al. - 2019 - Deep Ensembles A Loss Landscape Perspective.pdf; /Users/jacquesthibodeau/Zotero/storage/DU5ECF4A/1912.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RNWXKCU9,manuscript,2020,"Uesato, Jonathan; Kumar, Ramana; Krakovna, Victoria; Everitt, Tom; Ngo, Richard; Legg, Shane",Avoiding Tampering Incentives in Deep RL via Decoupled Approval,,,,,http://arxiv.org/abs/2011.08827,"How can we design agents that pursue a given objective when all feedback mechanisms are influenceable by the agent? Standard RL algorithms assume a secure reward function, and can thus perform poorly in settings where agents can tamper with the reward-generating mechanism. We present a principled solution to the problem of learning from influenceable feedback, which combines approval with a decoupled feedback collection procedure. For a natural class of corruption functions, decoupled approval algorithms have aligned incentives both at convergence and for their local updates. Empirically, they also scale to complex 3D environments where tampering is possible.",2020-11-17,2022-01-30 04:52:37,2022-01-30 04:52:37,2020-12-12 15:36:22,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000001  arXiv: 2011.08827,,/Users/jacquesthibodeau/Zotero/storage/D9UAFGVG/Uesato et al. - 2020 - Avoiding Tampering Incentives in Deep RL via Decou.pdf; /Users/jacquesthibodeau/Zotero/storage/9PA82CZF/2011.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
26XP8265,manuscript,2020,"Ngo, Richard",AGI Safety From First Principles,,,,,,,2020,2022-01-30 04:52:37,2022-01-30 04:52:37,,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: NoCitationData[s0]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/J2MQFEZQ/Ngo - AGI Safety From First Principles.pdf,,TechSafety; DeepMind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4XFRI2SS,manuscript,2020,"Dulac-Arnold, Gabriel; Levine, Nir; Mankowitz, Daniel J.; Li, Jerry; Paduraru, Cosmin; Gowal, Sven; Hester, Todd",An empirical investigation of the challenges of real-world reinforcement learning,,,,,http://arxiv.org/abs/2003.11881,"Reinforcement learning (RL) has proven its worth in a series of artificial domains, and is beginning to show some successes in real-world scenarios. However, much of the research advances in RL are hard to leverage in real-world systems due to a series of assumptions that are rarely satisfied in practice. In this work, we identify and formalize a series of independent challenges that embody the difficulties that must be addressed for RL to be commonly deployed in real-world systems. For each challenge, we define it formally in the context of a Markov Decision Process, analyze the effects of the challenge on state-of-the-art learning algorithms, and present some existing attempts at tackling it. We believe that an approach that addresses our set of proposed challenges would be readily deployable in a large number of real world problems. Our proposed challenges are implemented in a suite of continuous control environments called realworldrl-suite which we propose an as an open-source benchmark.",2020-03-24,2022-01-30 04:52:37,2022-01-30 04:52:37,2020-08-18 21:24:33,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000047  arXiv: 2003.11881,,/Users/jacquesthibodeau/Zotero/storage/F4PISH98/Dulac-Arnold et al. - 2020 - An empirical investigation of the challenges of re.pdf; /Users/jacquesthibodeau/Zotero/storage/RIVBCVU2/2003.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BSH3KCJW,manuscript,2019,"Gowal, Sven; Uesato, Jonathan; Qin, Chongli; Huang, Po-Sen; Mann, Timothy; Kohli, Pushmeet",An Alternative Surrogate Loss for PGD-based Adversarial Testing,,,,,http://arxiv.org/abs/1910.09338,"Adversarial testing methods based on Projected Gradient Descent (PGD) are widely used for searching norm-bounded perturbations that cause the inputs of neural networks to be misclassified. This paper takes a deeper look at these methods and explains the effect of different hyperparameters (i.e., optimizer, step size and surrogate loss). We introduce the concept of MultiTargeted testing, which makes clever use of alternative surrogate losses, and explain when and how MultiTargeted is guaranteed to find optimal perturbations. Finally, we demonstrate that MultiTargeted outperforms more sophisticated methods and often requires less iterative steps than other variants of PGD found in the literature. Notably, MultiTargeted ranks first on MadryLab's white-box MNIST and CIFAR-10 leaderboards, reducing the accuracy of their MNIST model to 88.36% (with $\ell_\infty$ perturbations of $\epsilon = 0.3$) and the accuracy of their CIFAR-10 model to 44.03% (at $\epsilon = 8/255$). MultiTargeted also ranks first on the TRADES leaderboard reducing the accuracy of their CIFAR-10 model to 53.07% (with $\ell_\infty$ perturbations of $\epsilon = 0.031$).",2019-10-21,2022-01-30 04:52:37,2022-01-30 04:52:37,2019-12-16 20:31:05,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000036  arXiv: 1910.09338,,/Users/jacquesthibodeau/Zotero/storage/ACDBRTUC/Gowal et al. - 2019 - An Alternative Surrogate Loss for PGD-based Advers.pdf; /Users/jacquesthibodeau/Zotero/storage/K9GR5WQX/1910.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DJIA6CVN,manuscript,2021,"Kenton, Zachary; Everitt, Tom; Weidinger, Laura; Gabriel, Iason; Mikulik, Vladimir; Irving, Geoffrey",Alignment of Language Agents,,,,,http://arxiv.org/abs/2103.14659,"For artificial intelligence to be beneficial to humans the behaviour of AI agents needs to be aligned with what humans want. In this paper we discuss some behavioural issues for language agents, arising from accidental misspecification by the system designer. We highlight some ways that misspecification can occur and discuss some behavioural issues that could arise from misspecification, including deceptive or manipulative language, and review some approaches for avoiding these issues.",2021-03-26,2022-01-30 04:52:37,2022-01-30 04:52:37,2021-11-14 16:31:03,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000006  arXiv: 2103.14659,,/Users/jacquesthibodeau/Zotero/storage/74F9MU53/Kenton et al. - 2021 - Alignment of Language Agents.pdf; /Users/jacquesthibodeau/Zotero/storage/K3PCKCDE/2103.html,,TechSafety; AmbiguousSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AJ25JZ86,manuscript,2017,"Leike, Jan; Martic, Miljan; Krakovna, Victoria; Ortega, Pedro A.; Everitt, Tom; Lefrancq, Andrew; Orseau, Laurent; Legg, Shane",AI Safety Gridworlds,,,,,http://arxiv.org/abs/1711.09883,"We present a suite of reinforcement learning environments illustrating various safety properties of intelligent agents. These problems include safe interruptibility, avoiding side effects, absent supervisor, reward gaming, safe exploration, as well as robustness to self-modification, distributional shift, and adversaries. To measure compliance with the intended safe behavior, we equip each environment with a performance function that is hidden from the agent. This allows us to categorize AI safety problems into robustness and specification problems, depending on whether the performance function corresponds to the observed reward function. We evaluate A2C and Rainbow, two recent deep reinforcement learning agents, on our environments and show that they are not able to solve them satisfactorily.",2017-11-28,2022-01-30 04:52:37,2022-01-30 04:52:37,2019-12-16 20:35:43,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000215  arXiv: 1711.09883,,/Users/jacquesthibodeau/Zotero/storage/S9F534C5/Leike et al. - 2017 - AI Safety Gridworlds.pdf; /Users/jacquesthibodeau/Zotero/storage/2X4PWKEM/Leike et al. - 2017 - AI Safety Gridworlds.pdf; /Users/jacquesthibodeau/Zotero/storage/NF3EHNV5/1711.html; /Users/jacquesthibodeau/Zotero/storage/7587G9PA/1711.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SRQCVSJ8,manuscript,2018,"Orseau, Laurent; McGill, Simon McGregor; Legg, Shane",Agents and Devices: A Relative Definition of Agency,,,,,http://arxiv.org/abs/1805.12387,"According to Dennett, the same system may be described using a `physical' (mechanical) explanatory stance, or using an `intentional' (belief- and goal-based) explanatory stance. Humans tend to find the physical stance more helpful for certain systems, such as planets orbiting a star, and the intentional stance for others, such as living animals. We define a formal counterpart of physical and intentional stances within computational theory: a description of a system as either a device, or an agent, with the key difference being that `devices' are directly described in terms of an input-output mapping, while `agents' are described in terms of the function they optimise. Bayes' rule can then be applied to calculate the subjective probability of a system being a device or an agent, based only on its behaviour. We illustrate this using the trajectories of an object in a toy grid-world domain.",2018-05-31,2022-01-30 04:52:36,2022-01-30 04:52:36,2020-11-14 00:33:25,,,,,,,Agents and Devices,,,,,,,,,,,,arXiv.org,,ZSCC: 0000003  arXiv: 1805.12387,,/Users/jacquesthibodeau/Zotero/storage/J8BWQUT7/Orseau et al. - 2018 - Agents and Devices A Relative Definition of Agenc.pdf; /Users/jacquesthibodeau/Zotero/storage/DVSN4A5S/1805.html,,TechSafety; DeepMind,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GM2U5SSZ,manuscript,2020,"Clifton, Jesse; Riché, Maxime",Towards Cooperation in Learning Games,,,,,,"Suppose that several actors are going to deploy learning agents to act on their behalf. What principles should guide these actors in designing their agents, given that they may have competing goals? An appealing solution concept in this setting is welfareoptimal learning equilibrium. This means that the learning agents should constitute a Nash equilibrium whose payoff proﬁle is optimal according to some measure of total welfare (welfare function). In this work, we construct a class of learning algorithms in this spirit called learning tit-for-tat (L-TFT). L-TFT algorithms maximize a welfare function according to a speciﬁed optimization schedule, and punish their counterpart when they detect that they are deviating from this plan. Because the policies of other agents are not in general fully observed, agents must infer whether their counterpart is following a cooperative learning algorithm. This requires us to develop new techniques for making inferences about counterpart learning algorithms. In two sequential social dilemmas, our L-TFT algorithms successfully cooperate in self-play while effectively avoiding exploitation by and punishing defecting learning algorithms.",2020,2022-01-30 04:51:37,2022-01-30 04:51:37,,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000000[s0],,/Users/jacquesthibodeau/Zotero/storage/Q7RTK23R/Clifton and Riché - Towards Cooperation in Learning Games.pdf,,CLR; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PT6IB3US,manuscript,2016,"Gloor, Lukas","Suffering-focused AI safety: Why ""fail-safe'"" measures might be our top intervention",,,,,,"AI-safety eﬀorts focused on suﬀering reduction should place particular emphasis on avoiding risks of astronomical disvalue. Among the cases where uncontrolled AI destroys humanity, outcomes might still diﬀer enormously in the amounts of suﬀering produced. Rather than concentrating all our eﬀorts on a speciﬁc future we would like to bring about, we should identify futures we least want to bring about and work on ways to steer AI trajectories around these. In particular, a “fail-safe”1 approach to AI safety is especially promising because avoiding very bad outcomes might be much easier than making sure we get everything right. This is also a neglected cause despite there being a broad consensus among diﬀerent moral views that avoiding the creation of vast amounts of suﬀering in our future is an ethical priority.",2016,2022-01-30 04:51:37,2022-01-30 04:51:37,,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000005,,/Users/jacquesthibodeau/Zotero/storage/AIHAKRFD/Gloor - Suffering-focused AI safety Why ``fail-safe'' mea.pdf,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
79K88D35,manuscript,2019,"Kosoy, Vanessa",Forecasting using incomplete models,,,,,http://arxiv.org/abs/1705.04630,"We consider the task of forecasting an infinite sequence of future observations based on some number of past observations, where the probability measure generating the observations is ""suspected"" to satisfy one or more of a set of incomplete models, i.e. convex sets in the space of probability measures. This setting is in some sense intermediate between the realizable setting where the probability measure comes from some known set of probability measures (which can be addressed using e.g. Bayesian inference) and the unrealizable setting where the probability measure is completely arbitrary. We demonstrate a method of forecasting which guarantees that, whenever the true probability measure satisfies an incomplete model in a given countable set, the forecast converges to the same incomplete model in the (appropriately normalized) Kantorovich-Rubinstein metric. This is analogous to merging of opinions for Bayesian inference, except that convergence in the Kantorovich-Rubinstein metric is weaker than convergence in total variation.",2019-05-16,2022-01-30 04:56:48,2022-01-30 04:56:48,2019-12-16 02:29:04,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s6]  ACC: 1  J: 1 arXiv: 1705.04630,,/Users/jacquesthibodeau/Zotero/storage/I3DCSTP3/Kosoy - 2019 - Forecasting using incomplete models.pdf; /Users/jacquesthibodeau/Zotero/storage/MEKS9723/1705.html,,TechSafety; MIRI,"Computer Science - Machine Learning; I.2.6; G.3; 68Q32, 62M10, 62G08",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4CGXIREQ,manuscript,2015,"Hibbard, Bill",Ethical Artificial Intelligence,,,,,http://arxiv.org/abs/1411.1373,"This book-length article combines several peer reviewed papers and new material to analyze the issues of ethical artificial intelligence (AI). The behavior of future AI systems can be described by mathematical equations, which are adapted to analyze possible unintended AI behaviors and ways that AI designs can avoid them. This article makes the case for utility-maximizing agents and for avoiding infinite sets in agent definitions. It shows how to avoid agent self-delusion using model-based utility functions and how to avoid agents that corrupt their reward generators (sometimes called ""perverse instantiation"") using utility functions that evaluate outcomes at one point in time from the perspective of humans at a different point in time. It argues that agents can avoid unintended instrumental actions (sometimes called ""basic AI drives"" or ""instrumental goals"") by accurately learning human values. This article defines a self-modeling agent framework and shows how it can avoid problems of resource limits, being predicted by other agents, and inconsistency between the agent's utility function and its definition (one version of this problem is sometimes called ""motivated value selection""). This article also discusses how future AI will differ from current AI, the politics of AI, and the ultimate use of AI to help understand the nature of the universe and our place in it.",2015-11-17,2022-01-30 04:56:48,2022-01-30 04:56:48,2020-11-22 02:29:19,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s2]  ACC: 33  arXiv: 1411.1373,,/Users/jacquesthibodeau/Zotero/storage/DZPJDUCP/Hibbard - 2015 - Ethical Artificial Intelligence.pdf; /Users/jacquesthibodeau/Zotero/storage/7MF8SPGQ/1411.html,,TechSafety; MIRI,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TXH2RDI8,manuscript,2004,"Yudkowsky, Eliezer",Coherent Extrapolated Volition,,,,,,,2004,2022-01-30 04:56:47,2022-01-30 04:56:47,,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000120,,/Users/jacquesthibodeau/Zotero/storage/2CZG225N/Yudkowsky - Coherent Extrapolated Volition.pdf,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
659RCK7X,manuscript,2019,"Manheim, David; Garrabrant, Scott",Categorizing Variants of Goodhart's Law,,,,,http://arxiv.org/abs/1803.04585,"There are several distinct failure modes for overoptimization of systems on the basis of metrics. This occurs when a metric which can be used to improve a system is used to an extent that further optimization is ineffective or harmful, and is sometimes termed Goodhart's Law. This class of failure is often poorly understood, partly because terminology for discussing them is ambiguous, and partly because discussion using this ambiguous terminology ignores distinctions between different failure modes of this general type. This paper expands on an earlier discussion by Garrabrant, which notes there are ""(at least) four different mechanisms"" that relate to Goodhart's Law. This paper is intended to explore these mechanisms further, and specify more clearly how they occur. This discussion should be helpful in better understanding these types of failures in economic regulation, in public policy, in machine learning, and in Artificial Intelligence alignment. The importance of Goodhart effects depends on the amount of power directed towards optimizing the proxy, and so the increased optimization power offered by artificial intelligence makes it especially critical for that field.",2019-02-24,2022-01-30 04:56:47,2022-01-30 04:56:47,2019-12-16 02:27:58,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s6]  ACC: 36  J: 23 arXiv: 1803.04585,,/Users/jacquesthibodeau/Zotero/storage/34365ZKG/Manheim and Garrabrant - 2019 - Categorizing Variants of Goodhart's Law.pdf; /Users/jacquesthibodeau/Zotero/storage/XSBBKWCC/Manheim and Garrabrant - 2019 - Categorizing Variants of Goodhart's Law.pdf; /Users/jacquesthibodeau/Zotero/storage/T45ISW4V/1803.html; /Users/jacquesthibodeau/Zotero/storage/JRUQVZ6J/1803.html,,TechSafety; MIRI,Statistics - Machine Learning; Computer Science - Artificial Intelligence; 91E45; Quantitative Finance - General Finance,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PGK9NF8P,manuscript,2015,"LaVictoire, Patrick",An Introduction to Löb’s Theorem in MIRI Research,,,,,,,2015,2022-01-30 04:56:47,2022-01-30 04:56:47,,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: NoCitationData[s1]  ACC: 5,,/Users/jacquesthibodeau/Zotero/storage/STHEKPTX/LaVictoire - An Introduction to Lo¨b’s Theorem in MIRI Research.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RRPKMSTJ,manuscript,2021,"Garrabrant, Scott; Herrmann, Daniel A.; Lopez-Wild, Josiah",Cartesian Frames,,,,,http://arxiv.org/abs/2109.10996,"We introduce a novel framework, the theory of Cartesian frames (CF), that gives powerful tools for manipulating sets of acts. The CF framework takes as its most fundamental building block that an agent can freely choose from a set of available actions. The framework uses the mathematics of Chu spaces to develop a calculus of those sets of actions, how those actions change at various levels of description, and how different agents' actions can combine when agents work in concert. We discuss how this framework might provide an illuminating perspective on issues in decision theory and formal epistemology.",2021-09-22,2022-01-30 04:56:47,2022-01-30 04:56:47,2021-10-31 22:35:31,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000000  arXiv: 2109.10996,,/Users/jacquesthibodeau/Zotero/storage/SUBM35KQ/Garrabrant et al. - 2021 - Cartesian Frames.pdf; /Users/jacquesthibodeau/Zotero/storage/S52XF4ST/2109.html,,TechSafety,Mathematics - Category Theory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BZ2TFKZZ,manuscript,2016,"Garrabrant, Scott; Soares, Nate; Taylor, Jessica",Asymptotic Convergence in Online Learning with Unbounded Delays,,,,,http://arxiv.org/abs/1604.05280,"We study the problem of predicting the results of computations that are too expensive to run, via the observation of the results of smaller computations. We model this as an online learning problem with delayed feedback, where the length of the delay is unbounded, which we study mainly in a stochastic setting. We show that in this setting, consistency is not possible in general, and that optimal forecasters might not have average regret going to zero. However, it is still possible to give algorithms that converge asymptotically to Bayes-optimal predictions, by evaluating forecasters on specific sparse independent subsequences of their predictions. We give an algorithm that does this, which converges asymptotically on good behavior, and give very weak bounds on how long it takes to converge. We then relate our results back to the problem of predicting large computations in a deterministic setting.",2016-09-07,2022-01-30 04:56:47,2022-01-30 04:56:47,2019-12-16 02:30:59,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000011  arXiv: 1604.05280,,/Users/jacquesthibodeau/Zotero/storage/FSZFUVAS/Garrabrant et al. - 2016 - Asymptotic Convergence in Online Learning with Unb.pdf; /Users/jacquesthibodeau/Zotero/storage/5GD4TDT9/Garrabrant et al. - 2016 - Asymptotic Convergence in Online Learning with Unb.pdf; /Users/jacquesthibodeau/Zotero/storage/XWQ622KH/1604.html; /Users/jacquesthibodeau/Zotero/storage/5WDRANB4/1604.html,,TechSafety; MIRI,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Mathematics - Probability,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WU22E4BX,manuscript,,"Hoffman, Ben",The Professional's Dilemma,,,,,http://mediangroup.org/docs/the_professionals_dilemma.pdf,,unknown,2022-01-30 04:55:38,2022-01-30 04:55:38,2020-11-21 19:50:01,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/3GQXQPUN/the_professionals_dilemma.pdf,,MetaSafety; Median-group,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
53WPDXKG,manuscript,,"Perry, Miya",Toward A Working Theory of Mind,,,,,http://mediangroup.org/docs/toward_a_working_theory_of_mind.pdf,,unknown,2022-01-30 04:55:38,2022-01-30 04:55:38,2020-11-21 19:49:56,,,,,,,,,,,,,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/2SZEMX5T/toward_a_working_theory_of_mind.pdf,,TechSafety; Median-group,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8PSVNP3E,manuscript,2018,"Hwang, Tim",Computational Power and the Social Impact of Artificial Intelligence,,,,,http://arxiv.org/abs/1803.08971,"Machine learning is a computational process. To that end, it is inextricably tied to computational power - the tangible material of chips and semiconductors that the algorithms of machine intelligence operate on. Most obviously, computational power and computing architectures shape the speed of training and inference in machine learning, and therefore influence the rate of progress in the technology. But, these relationships are more nuanced than that: hardware shapes the methods used by researchers and engineers in the design and development of machine learning models. Characteristics such as the power consumption of chips also define where and how machine learning can be used in the real world. Despite this, many analyses of the social impact of the current wave of progress in AI have not substantively brought the dimension of hardware into their accounts. While a common trope in both the popular press and scholarly literature is to highlight the massive increase in computational power that has enabled the recent breakthroughs in machine learning, the analysis frequently goes no further than this observation around magnitude. This paper aims to dig more deeply into the relationship between computational power and the development of machine learning. Specifically, it examines how changes in computing architectures, machine learning methodologies, and supply chains might influence the future of AI. In doing so, it seeks to trace a set of specific relationships between this underlying hardware layer and the broader social impacts and risks around AI.",2018-03-23,2022-01-30 04:59:44,2022-01-30 04:59:44,2020-11-14 00:34:42,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000040  arXiv: 1803.08971,,/Users/jacquesthibodeau/Zotero/storage/C9KNXFPA/Hwang - 2018 - Computational Power and the Social Impact of Artif.pdf; /Users/jacquesthibodeau/Zotero/storage/6H9CEWGV/1803.html,,MetaSafety; Other-org,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AMJT8QFM,manuscript,2020,"Chatterjee, Satrajit",Coherent Gradients: An Approach to Understanding Generalization in Gradient Descent-based Optimization,,,,,http://arxiv.org/abs/2002.10657,"An open question in the Deep Learning community is why neural networks trained with Gradient Descent generalize well on real datasets even though they are capable of fitting random data. We propose an approach to answering this question based on a hypothesis about the dynamics of gradient descent that we call Coherent Gradients: Gradients from similar examples are similar and so the overall gradient is stronger in certain directions where these reinforce each other. Thus changes to the network parameters during training are biased towards those that (locally) simultaneously benefit many examples when such similarity exists. We support this hypothesis with heuristic arguments and perturbative experiments and outline how this can explain several common empirical observations about Deep Learning. Furthermore, our analysis is not just descriptive, but prescriptive. It suggests a natural modification to gradient descent that can greatly reduce overfitting.",2020-02-24,2022-01-30 04:59:44,2022-01-30 04:59:44,2020-09-05 18:39:31,,,,,,,Coherent Gradients,,,,,,,,,,,,arXiv.org,,ZSCC: 0000018  arXiv: 2002.10657,,/Users/jacquesthibodeau/Zotero/storage/J2MAP4DT/Chatterjee - 2020 - Coherent Gradients An Approach to Understanding G.pdf; /Users/jacquesthibodeau/Zotero/storage/FKT247Z3/2002.html,,TechSafety; AmbiguosSafety; Other-org,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UQRWRPSQ,manuscript,2018,"Trazzi, Michaël; Yampolskiy, Roman V.",Building Safer AGI by introducing Artificial Stupidity,,,,,http://arxiv.org/abs/1808.03644,"Artificial Intelligence (AI) achieved super-human performance in a broad variety of domains. We say that an AI is made Artificially Stupid on a task when some limitations are deliberately introduced to match a human's ability to do the task. An Artificial General Intelligence (AGI) can be made safer by limiting its computing power and memory, or by introducing Artificial Stupidity on certain tasks. We survey human intellectual limits and give recommendations for which limits to implement in order to build a safe AGI.",2018-08-10,2022-01-30 04:59:43,2022-01-30 04:59:43,2020-11-14 00:55:32,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000026  arXiv: 1808.03644,,/Users/jacquesthibodeau/Zotero/storage/KH2RT4UD/Trazzi and Yampolskiy - 2018 - Building Safer AGI by introducing Artificial Stupi.pdf; /Users/jacquesthibodeau/Zotero/storage/2AIX67AX/1808.html,,TechSafety; Other-org,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QWU6BGHU,manuscript,2020,"Saisubramanian, Sandhya; Zilberstein, Shlomo; Kamar, Ece",Avoiding Negative Side Effects due to Incomplete Knowledge of AI Systems,,,,,http://arxiv.org/abs/2008.12146,"Autonomous agents acting in the real-world often operate based on models that ignore certain aspects of the environment. The incompleteness of any given model---handcrafted or machine acquired---is inevitable due to practical limitations of any modeling technique for complex real-world settings. Due to the limited fidelity of its model, an agent's actions may have unexpected, undesirable consequences during execution. Learning to recognize and avoid such negative side effects of the agent's actions is critical to improving the safety and reliability of autonomous systems. This emerging research topic is attracting increased attention due to the increased deployment of AI systems and their broad societal impacts. This article provides a comprehensive overview of different forms of negative side effects and the recent research efforts to address them. We identify key characteristics of negative side effects, highlight the challenges in avoiding negative side effects, and discuss recently developed approaches, contrasting their benefits and limitations. We conclude with a discussion of open questions and suggestions for future research directions.",2020-08-28,2022-01-30 04:59:36,2022-01-30 04:59:36,2020-11-14 00:57:51,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000006  arXiv: 2008.12146,,/Users/jacquesthibodeau/Zotero/storage/F2EV268Z/Saisubramanian et al. - 2020 - Avoiding Negative Side Effects due to Incomplete K.pdf; /Users/jacquesthibodeau/Zotero/storage/BHMQUM67/2008.html,,TechSafety; Other-org,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
38TP8QCZ,manuscript,2016,"Yampolskiy, Roman V.; Spellchecker, M. S.",Artificial Intelligence Safety and Cybersecurity: a Timeline of AI Failures,,,,,http://arxiv.org/abs/1610.07997,"In this work, we present and analyze reported failures of artificially intelligent systems and extrapolate our analysis to future AIs. We suggest that both the frequency and the seriousness of future AI failures will steadily increase. AI Safety can be improved based on ideas developed by cybersecurity experts. For narrow AIs safety failures are at the same, moderate, level of criticality as in cybersecurity, however for general AI, failures have a fundamentally different impact. A single failure of a superintelligent system may cause a catastrophic event without a chance for recovery. The goal of cybersecurity is to reduce the number of successful attacks on the system; the goal of AI Safety is to make sure zero attacks succeed in bypassing the safety mechanisms. Unfortunately, such a level of performance is unachievable. Every security system will eventually fail; there is no such thing as a 100% secure system.",2016-10-25,2022-01-30 04:59:36,2022-01-30 04:59:36,2020-12-13 20:19:29,,,,,,,Artificial Intelligence Safety and Cybersecurity,,,,,,,,,,,,arXiv.org,,ZSCC: 0000084  arXiv: 1610.07997,,/Users/jacquesthibodeau/Zotero/storage/XBP68MBT/Yampolskiy and Spellchecker - 2016 - Artificial Intelligence Safety and Cybersecurity .pdf; /Users/jacquesthibodeau/Zotero/storage/GC9TXXSM/1610.html,,MetaSafety; Other-org,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CNJWTPTN,manuscript,2016,"Ziesche, Soenke; Yampolskiy, Roman V.",Artificial Fun: Mapping Minds to the Space of Fun,,,,,http://arxiv.org/abs/1606.07092,"Yampolskiy and others have shown that the space of possible minds is vast, actually infinite (Yampolskiy, 2015). A question of interest is 'Which activities can minds perform during their lifetime?' This question is very broad, thus in this article restricted to 'Which non-boring activities can minds perform?' The space of potential non-boring activities has been called by Yudkowsky 'fun space' (Yudkowsky, 2009). This paper aims to discuss the relation between various types of minds and the part of the fun space, which is accessible for them.",2016-06-22,2022-01-30 04:59:35,2022-01-30 04:59:35,2020-12-13 20:23:34,,,,,,,Artificial Fun,,,,,,,,,,,,arXiv.org,,ZSCC: 0000002  arXiv: 1606.07092,,/Users/jacquesthibodeau/Zotero/storage/X6ZMRXFI/Ziesche and Yampolskiy - 2016 - Artificial Fun Mapping Minds to the Space of Fun.pdf; /Users/jacquesthibodeau/Zotero/storage/AVP4AFV8/1606.html,,TechSafety; AmbiguosSafety; Other-org,Computer Science - Other Computer Science,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
V5B2XBHN,manuscript,2016,"Torres, Phil",Agential Risks: A Comprehensive Introduction,,,,,,"The greatest existential threats to humanity stem from increasingly powerful advanced technologies. Yet the “risk potential” of such tools can only be realized when coupled with a suitable agent who, through error or terror, could use the tool to bring about an existential catastrophe. While the existential risk literature has provided many accounts of how advanced technologies might be misused and abused to cause unprecedented harm, no scholar has yet explored the other half of the agent-tool coupling, namely the agent. This paper aims to correct this failure by offering a comprehensive overview of what we could call “agential riskology.” Only by studying the unique properties of different agential risk types can one acquire an accurate picture of the existential danger before us.",2016,2022-01-30 04:59:33,2022-01-30 04:59:33,,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000022,,/Users/jacquesthibodeau/Zotero/storage/22B4EZFK/Torres - Agential Risks A Comprehensive Introduction.pdf,,MetaSafety; AmbiguosSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
35ISE4PP,manuscript,2020,"Hoang, Lê Nguyên",A Roadmap for Robust End-to-End Alignment,,,,,http://arxiv.org/abs/1809.01036,"This paper discussed the {\it robust alignment} problem, that is, the problem of aligning the goals of algorithms with human preferences. It presented a general roadmap to tackle this issue. Interestingly, this roadmap identifies 5 critical steps, as well as many relevant aspects of these 5 steps. In other words, we have presented a large number of hopefully more tractable subproblems that readers are highly encouraged to tackle. Hopefully, this combination allows to better highlight the most pressing problems, how every expertise can be best used to, and how combining the solutions to subproblems might add up to solve robust alignment.",2020-02-25,2022-01-30 04:59:33,2022-01-30 04:59:33,2020-11-14 00:52:55,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s1]  ACC: 0  arXiv: 1809.01036,,/Users/jacquesthibodeau/Zotero/storage/KS6F4Q7M/Hoang - 2020 - A Roadmap for Robust End-to-End Alignment.pdf; /Users/jacquesthibodeau/Zotero/storage/HVT2WEJJ/1809.html,,TechSafety; Other-org,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QQ7I47Z4,manuscript,2018,"Irving, Geoffrey; Christiano, Paul; Amodei, Dario",AI safety via debate,,,,,http://arxiv.org/abs/1805.00899,"To make AI systems broadly useful for challenging real-world tasks, we need them to learn complex human goals and preferences. One approach to specifying complex goals asks humans to judge during training which agent behaviors are safe and useful, but this approach can fail if the task is too complicated for a human to directly judge. To help address this concern, we propose training agents via self play on a zero sum debate game. Given a question or proposed action, two agents take turns making short statements up to a limit, then a human judges which of the agents gave the most true, useful information. In an analogy to complexity theory, debate with optimal play can answer any question in PSPACE given polynomial time judges (direct judging answers only NP questions). In practice, whether debate works involves empirical questions about humans and the tasks we want AIs to perform, plus theoretical questions about the meaning of AI alignment. We report results on an initial MNIST experiment where agents compete to convince a sparse classifier, boosting the classifier's accuracy from 59.4% to 88.9% given 6 pixels and from 48.2% to 85.2% given 4 pixels. Finally, we discuss theoretical and practical aspects of the debate model, focusing on potential weaknesses as the model scales up, and we propose future human and computer experiments to test these properties.",2018-10-22,2022-01-30 04:58:18,2022-01-30 04:58:18,2019-12-16 20:07:38,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000044  arXiv: 1805.00899,,/Users/jacquesthibodeau/Zotero/storage/Q8PQWCV7/Irving et al. - 2018 - AI safety via debate.pdf; /Users/jacquesthibodeau/Zotero/storage/QKP3ZSWZ/Irving et al. - 2018 - AI safety via debate.pdf; /Users/jacquesthibodeau/Zotero/storage/26MTE6EQ/1805.html; /Users/jacquesthibodeau/Zotero/storage/38FDWSJQ/1805.html,,TechSafety; Open-AI,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FS5KIAHS,manuscript,2016,"Amodei, Dario; Olah, Chris; Steinhardt, Jacob; Christiano, Paul; Schulman, John; Mané, Dan",Concrete Problems in AI Safety,,,,,http://arxiv.org/abs/1606.06565,"Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function (""avoiding side effects"" and ""avoiding reward hacking""), an objective function that is too expensive to evaluate frequently (""scalable supervision""), or undesirable behavior during the learning process (""safe exploration"" and ""distributional shift""). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.",2016-07-25,2022-01-30 04:57:26,2022-01-30 04:57:26,2019-12-16 20:16:07,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0001335  arXiv: 1606.06565,,/Users/jacquesthibodeau/Zotero/storage/SB8ZRSGM/Amodei et al. - 2016 - Concrete Problems in AI Safety.pdf; /Users/jacquesthibodeau/Zotero/storage/3RX9H74D/Amodei et al. - 2016 - Concrete Problems in AI Safety.pdf; /Users/jacquesthibodeau/Zotero/storage/UAW8RNPB/1606.html; /Users/jacquesthibodeau/Zotero/storage/9T9F6RZW/1606.html,,TechSafety; Open-AI,Computer Science - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3BS9AFFF,manuscript,2019,"Ray, Alex; Achiam, Joshua; Amodei, Dario",Benchmarking Safe Exploration in Deep Reinforcement Learning,,,,,https://arxiv.org/abs/2007.01223,"Reinforcement learning (RL) agents need to explore their environments in order to learn optimal policies by trial and error. In many environments, safety is a critical concern and certain errors are unacceptable: for example, robotics systems that interact with humans should never cause injury to the humans while exploring. While it is currently typical to train RL agents mostly or entirely in simulation, where safety concerns are minimal, we anticipate that challenges in simulating the complexities of the real world (such as human-AI interactions) will cause a shift towards training RL agents directly in the real world, where safety concerns are paramount. Consequently we take the position that safe exploration should be viewed as a critical focus area for RL research, and in this work we make three contributions to advance the study of safe exploration. First, building on a wide range of prior work on safe reinforcement learning, we propose to standardize constrained RL as the main formalism for safe exploration. Second, we present the Safety Gym benchmark suite, a new slate of high-dimensional continuous control environments for measuring research progress on constrained RL. Finally, we benchmark several constrained deep RL algorithms on Safety Gym environments to establish baselines that future work can build on.",2019,2022-01-30 04:57:26,2022-01-30 04:57:26,,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000073,,/Users/jacquesthibodeau/Zotero/storage/KEPD9RAS/Ray et al. - Benchmarking Safe Exploration in Deep Reinforcemen.pdf,,TechSafety; Open-AI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IJ4UFE3M,manuscript,2021,"Chen, Mark; Tworek, Jerry; Jun, Heewoo; Yuan, Qiming; Pinto, Henrique Ponde de Oliveira; Kaplan, Jared; Edwards, Harri; Burda, Yuri; Joseph, Nicholas; Brockman, Greg; Ray, Alex; Puri, Raul; Krueger, Gretchen; Petrov, Michael; Khlaaf, Heidy; Sastry, Girish; Mishkin, Pamela; Chan, Brooke; Gray, Scott; Ryder, Nick; Pavlov, Mikhail; Power, Alethea; Kaiser, Lukasz; Bavarian, Mohammad; Winter, Clemens; Tillet, Philippe; Such, Felipe Petroski; Cummings, Dave; Plappert, Matthias; Chantzis, Fotios; Barnes, Elizabeth; Herbert-Voss, Ariel; Guss, William Hebgen; Nichol, Alex; Paino, Alex; Tezak, Nikolas; Tang, Jie; Babuschkin, Igor; Balaji, Suchir; Jain, Shantanu; Saunders, William; Hesse, Christopher; Carr, Andrew N.; Leike, Jan; Achiam, Josh; Misra, Vedant; Morikawa, Evan; Radford, Alec; Knight, Matthew; Brundage, Miles; Murati, Mira; Mayer, Katie; Welinder, Peter; McGrew, Bob; Amodei, Dario; McCandlish, Sam; Sutskever, Ilya; Zaremba, Wojciech",Evaluating Large Language Models Trained on Code,,,,,http://arxiv.org/abs/2107.03374,"We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.",2021-07-14,2022-01-30 04:57:26,2022-01-30 04:57:26,2021-10-31 22:37:39,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s1]  ACC: 36  arXiv: 2107.03374,,/Users/jacquesthibodeau/Zotero/storage/P7CPC96K/Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf; /Users/jacquesthibodeau/Zotero/storage/DDJ2TIQ7/2107.html,,MetaSafety,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QMZS3XGJ,manuscript,2019,"Ziegler, Daniel M.; Stiennon, Nisan; Wu, Jeffrey; Brown, Tom B.; Radford, Alec; Amodei, Dario; Christiano, Paul; Irving, Geoffrey",Fine-tuning language models from human preferences,,,,,,,2019,2022-01-30 04:57:26,2022-01-30 04:57:26,,,,,,,,,,,,,,,,,,,,Google Scholar,,ZSCC: 0000091,,/Users/jacquesthibodeau/Zotero/storage/G39X234W/Ziegler et al. - 2019 - Fine-tuning language models from human preferences.pdf; /Users/jacquesthibodeau/Zotero/storage/G5ISVZVZ/1909.html,,TechSafety; Open-AI; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FZ3JURGJ,manuscript,2020,"Mishra, Saurabh; Clark, Jack; Perrault, C. Raymond",Measurement in AI Policy: Opportunities and Challenges,,,,,http://arxiv.org/abs/2009.09071,"As artificial intelligence increasingly influences our world, it becomes crucial to assess its technical progress and societal impact. This paper surveys problems and opportunities in the measurement of AI systems and their impact, based on a workshop held at Stanford University in the fall of 2019. We identify six summary challenges inherent to measuring the progress and impact of AI, and summarize over 40 presentations and associated discussions from the workshop. We hope this can inspire research agendas in this crucial area.",2020-09-10,2022-01-30 04:57:25,2022-01-30 04:57:25,2020-11-14 00:54:58,,,,,,,Measurement in AI Policy,,,,,,,,,,,,arXiv.org,,ZSCC: 0000008  arXiv: 2009.09071,,/Users/jacquesthibodeau/Zotero/storage/RKJ6AFAG/Mishra et al. - 2020 - Measurement in AI Policy Opportunities and Challe.pdf; /Users/jacquesthibodeau/Zotero/storage/GSWE2PCX/2009.html,,MetaSafety; Open-AI; AmbiguosSafety,Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JPVIIMSV,manuscript,2021,"Wu, Jeff; Ouyang, Long; Ziegler, Daniel M.; Stiennon, Nisan; Lowe, Ryan; Leike, Jan; Christiano, Paul",Recursively Summarizing Books with Human Feedback,,,,,http://arxiv.org/abs/2109.10862,"A major challenge for scaling machine learning is training models to perform tasks that are very difficult or time-consuming for humans to evaluate. We present progress on this problem on the task of abstractive summarization of entire fiction novels. Our method combines learning from human feedback with recursive task decomposition: we use models trained on smaller parts of the task to assist humans in giving feedback on the broader task. We collect a large volume of demonstrations and comparisons from human labelers, and fine-tune GPT-3 using behavioral cloning and reward modeling to do summarization recursively. At inference time, the model first summarizes small sections of the book and then recursively summarizes these summaries to produce a summary of the entire book. Our human labelers are able to supervise and evaluate the models quickly, despite not having read the entire books themselves. Our resulting model generates sensible summaries of entire books, even matching the quality of human-written summaries in a few cases ($\sim5\%$ of books). We achieve state-of-the-art results on the recent BookSum dataset for book-length summarization. A zero-shot question-answering model using these summaries achieves state-of-the-art results on the challenging NarrativeQA benchmark for answering questions about books and movie scripts. We release datasets of samples from our model.",2021-09-27,2022-01-30 04:57:25,2022-01-30 04:57:25,2021-10-31 22:37:22,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000004  arXiv: 2109.10862,,/Users/jacquesthibodeau/Zotero/storage/XN8J3757/Wu et al. - 2021 - Recursively Summarizing Books with Human Feedback.pdf; /Users/jacquesthibodeau/Zotero/storage/53895JGP/2109.html,,TechSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VGRU4IVI,manuscript,2021,"Hernandez, Danny; Kaplan, Jared; Henighan, Tom; McCandlish, Sam",Scaling Laws for Transfer,,,,,http://arxiv.org/abs/2102.01293,"We study empirical scaling laws for transfer learning between distributions in an unsupervised, fine-tuning setting. When we train increasingly large neural networks from-scratch on a fixed-size dataset, they eventually become data-limited and stop improving in performance (cross-entropy loss). When we do the same for models pre-trained on a large language dataset, the slope in performance gains is merely reduced rather than going to zero. We calculate the effective data ""transferred"" from pre-training by determining how much data a transformer of the same size would have required to achieve the same loss when training from scratch. In other words, we focus on units of data while holding everything else fixed. We find that the effective data transferred is described well in the low data regime by a power-law of parameter count and fine-tuning dataset size. We believe the exponents in these power-laws correspond to measures of the generality of a model and proximity of distributions (in a directed rather than symmetric sense). We find that pre-training effectively multiplies the fine-tuning dataset size. Transfer, like overall performance, scales predictably in terms of parameters, data, and compute.",2021-02-01,2022-01-30 04:57:25,2022-01-30 04:57:25,2021-11-13 22:37:55,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000019  arXiv: 2102.01293,,/Users/jacquesthibodeau/Zotero/storage/E5B34BKD/Hernandez et al. - 2021 - Scaling Laws for Transfer.pdf; /Users/jacquesthibodeau/Zotero/storage/GXQAZ3BT/2102.html,,MetaSafety,Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
46ZCX4VI,manuscript,2018,"Christiano, Paul; Shlegeris, Buck; Amodei, Dario",Supervising strong learners by amplifying weak experts,,,,,http://arxiv.org/abs/1810.08575,"Many real world learning tasks involve complex or hard-to-specify objectives, and using an easier-to-specify proxy can lead to poor performance or misaligned behavior. One solution is to have humans provide a training signal by demonstrating or judging performance, but this approach fails if the task is too complicated for a human to directly evaluate. We propose Iterated Amplification, an alternative training strategy which progressively builds up a training signal for difficult problems by combining solutions to easier subproblems. Iterated Amplification is closely related to Expert Iteration (Anthony et al., 2017; Silver et al., 2017), except that it uses no external reward function. We present results in algorithmic environments, showing that Iterated Amplification can efficiently learn complex behaviors.",2018-10-19,2022-01-30 04:57:25,2022-01-30 04:57:25,2019-12-16 20:06:57,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000028  arXiv: 1810.08575,,/Users/jacquesthibodeau/Zotero/storage/K9A8KU7E/Christiano et al. - 2018 - Supervising strong learners by amplifying weak exp.pdf; /Users/jacquesthibodeau/Zotero/storage/5X96ZJNE/Christiano et al. - 2018 - Supervising strong learners by amplifying weak exp.pdf; /Users/jacquesthibodeau/Zotero/storage/JBIIC37H/1810.html; /Users/jacquesthibodeau/Zotero/storage/HUISI8Z7/1810.html,,TechSafety; Open-AI,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
V9BGAUCZ,manuscript,2019,"Askell, Amanda; Brundage, Miles; Hadfield, Gillian",The Role of Cooperation in Responsible AI Development,,,,,http://arxiv.org/abs/1907.04534,"In this paper, we argue that competitive pressures could incentivize AI companies to underinvest in ensuring their systems are safe, secure, and have a positive social impact. Ensuring that AI systems are developed responsibly may therefore require preventing and solving collective action problems between companies. We note that there are several key factors that improve the prospects for cooperation in collective action problems. We use this to identify strategies to improve the prospects for industry cooperation on the responsible development of AI.",2019-07-10,2022-01-30 04:57:24,2022-01-30 04:57:24,2019-12-16 20:04:39,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000025  arXiv: 1907.04534,,/Users/jacquesthibodeau/Zotero/storage/QQFIEMRW/Askell et al. - 2019 - The Role of Cooperation in Responsible AI Developm.pdf; /Users/jacquesthibodeau/Zotero/storage/9N5T94H5/1907.html,,MetaSafety; Open-AI,Computer Science - Artificial Intelligence; Computer Science - Computers and Society; K.1; K.4.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AGTN9FXU,manuscript,2019,"Kang, Daniel; Sun, Yi; Brown, Tom; Hendrycks, Dan; Steinhardt, Jacob",Transfer of Adversarial Robustness Between Perturbation Types,,,,,http://arxiv.org/abs/1905.01034,"We study the transfer of adversarial robustness of deep neural networks between different perturbation types. While most work on adversarial examples has focused on $L_\infty$ and $L_2$-bounded perturbations, these do not capture all types of perturbations available to an adversary. The present work evaluates 32 attacks of 5 different types against models adversarially trained on a 100-class subset of ImageNet. Our empirical results suggest that evaluating on a wide range of perturbation sizes is necessary to understand whether adversarial robustness transfers between perturbation types. We further demonstrate that robustness against one perturbation type may not always imply and may sometimes hurt robustness against other perturbation types. In light of these results, we recommend evaluation of adversarial defenses take place on a diverse range of perturbation types and sizes.",2019-05-03,2022-01-30 04:57:24,2022-01-30 04:57:24,2019-12-16 20:04:45,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000020  arXiv: 1905.01034,,/Users/jacquesthibodeau/Zotero/storage/78UQ6FFU/Kang et al. - 2019 - Transfer of Adversarial Robustness Between Perturb.pdf; /Users/jacquesthibodeau/Zotero/storage/R9H28U6H/1905.html,,TechSafety; Open-AI,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HS4BKV9Q,manuscript,2021,"Lin, Stephanie; Hilton, Jacob; Evans, Owain",TruthfulQA: Measuring How Models Mimic Human Falsehoods,,,,,http://arxiv.org/abs/2109.07958,"We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. For example, the 6B-parameter GPT-J model was 17% less truthful than its 125M-parameter counterpart. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.",2021-09-08,2022-01-30 04:57:24,2022-01-30 04:57:24,2021-11-18 23:35:58,,,,,,,TruthfulQA,,,,,,,,,,,,arXiv.org,,ZSCC: 0000005  arXiv: 2109.07958,,/Users/jacquesthibodeau/Zotero/storage/G39ZUQDN/Lin et al. - 2021 - TruthfulQA Measuring How Models Mimic Human False.pdf; /Users/jacquesthibodeau/Zotero/storage/GP7663XV/2109.html,,TechSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
M7RCGS4P,manuscript,2021,"Tamkin, Alex; Brundage, Miles; Clark, Jack; Ganguli, Deep","Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models",,,,,http://arxiv.org/abs/2102.02503,"On October 14th, 2020, researchers from OpenAI, the Stanford Institute for Human-Centered Artificial Intelligence, and other universities convened to discuss open research questions surrounding GPT-3, the largest publicly-disclosed dense language model at the time. The meeting took place under Chatham House Rules. Discussants came from a variety of research backgrounds including computer science, linguistics, philosophy, political science, communications, cyber policy, and more. Broadly, the discussion centered around two main questions: 1) What are the technical capabilities and limitations of large language models? 2) What are the societal effects of widespread use of large language models? Here, we provide a detailed summary of the discussion organized by the two themes above.",2021-02-04,2022-01-30 04:57:24,2022-01-30 04:57:24,2021-10-31 22:40:49,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000008  arXiv: 2102.02503,,"/Users/jacquesthibodeau/Zotero/storage/CUGQGQE9/Tamkin et al. - 2021 - Understanding the Capabilities, Limitations, and S.pdf",,MetaSafety,Computer Science - Machine Learning; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BJJSTE9N,manuscript,2021,"Hendrycks, Dan; Carlini, Nicholas; Schulman, John; Steinhardt, Jacob",Unsolved Problems in ML Safety,,,,,http://arxiv.org/abs/2109.13916,"Machine learning (ML) systems are rapidly increasing in size, are acquiring new capabilities, and are increasingly deployed in high-stakes settings. As with other powerful technologies, safety for ML should be a leading research priority. In response to emerging safety challenges in ML, such as those introduced by recent large-scale models, we provide a new roadmap for ML Safety and refine the technical problems that the field needs to address. We present four problems ready for research, namely withstanding hazards (""Robustness""), identifying hazards (""Monitoring""), steering ML systems (""Alignment""), and reducing hazards in deployment (""External Safety""). Throughout, we clarify each problem's motivation and provide concrete research directions.",2021-10-30,2022-01-30 04:57:23,2022-01-30 04:57:23,2021-11-18 23:47:25,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000004  arXiv: 2109.13916,,/Users/jacquesthibodeau/Zotero/storage/QZGMBZ4T/Hendrycks et al. - 2021 - Unsolved Problems in ML Safety.pdf; /Users/jacquesthibodeau/Zotero/storage/Z3BW94QM/2109.html,,TechSafety; AmbiguousSafety,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8IB8KRA5,manuscript,2021,"Garrabrant, Scott",Temporal Inference with Finite Factored Sets,,,,,http://arxiv.org/abs/2109.11513,"We propose a new approach to temporal inference, inspired by the Pearlian causal inference paradigm - though quite different from Pearl's approach formally. Rather than using directed acyclic graphs, we make use of factored sets, which are sets expressed as Cartesian products. We show that finite factored sets are powerful tools for inferring temporal relations. We introduce an analog of d-separation for factored sets, conditional orthogonality, and we demonstrate that this notion is equivalent to conditional independence in all probability distributions on a finite factored set.",2021-09-23,2022-01-30 04:56:59,2022-01-30 04:56:59,2021-10-31 22:35:10,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000000  arXiv: 2109.11513,,/Users/jacquesthibodeau/Zotero/storage/PIE5EQ45/Garrabrant - 2021 - Temporal Inference with Finite Factored Sets.pdf; /Users/jacquesthibodeau/Zotero/storage/4QRAPMBD/2109.html,,TechSafety,Computer Science - Artificial Intelligence; Mathematics - Probability; Mathematics - Combinatorics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A5HGSXIU,manuscript,2013,"Yudkowsky, Eliezer; Herreshoff, Marcello","Tiling Agents for Self-Modifying AI, and the Löbian Obstacle",,,,,https://intelligence.org/files/TilingAgentsDraft.pdf,,2013-10-07,2022-01-30 04:56:59,2022-01-30 04:56:59,2020-11-21 17:18:36,,,,,,,,,,,,,,,,,,,,,ZSCC: 0000028,,/Users/jacquesthibodeau/Zotero/storage/24VG56W8/TilingAgentsDraft.pdf,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MU9A79QN,manuscript,2019,"Kosoy, Vanessa; Appel, Alexander",Optimal Polynomial-Time Estimators: A Bayesian Notion of Approximation Algorithm,,,,,http://arxiv.org/abs/1608.04112,"We introduce a new concept of approximation applicable to decision problems and functions, inspired by Bayesian probability. From the perspective of a Bayesian reasoner with limited computational resources, the answer to a problem that cannot be solved exactly is uncertain and therefore should be described by a random variable. It thus should make sense to talk about the expected value of this random variable, an idea we formalize in the language of average-case complexity theory by introducing the concept of ""optimal polynomial-time estimators."" We prove some existence theorems and completeness results, and show that optimal polynomial-time estimators exhibit many parallels with ""classical"" probability theory.",2019-06-04,2022-01-30 04:56:57,2022-01-30 04:56:57,2019-12-16 02:31:07,,,,,,,Optimal Polynomial-Time Estimators,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s7]  ACC: 1  J: 1  arXiv: 1608.04112,,/Users/jacquesthibodeau/Zotero/storage/WBCNT5WT/Kosoy and Appel - 2019 - Optimal Polynomial-Time Estimators A Bayesian Not.pdf; /Users/jacquesthibodeau/Zotero/storage/8GRDS5M5/Kosoy and Appel - 2019 - Optimal Polynomial-Time Estimators A Bayesian Not.pdf; /Users/jacquesthibodeau/Zotero/storage/UQN6CZNA/1608.html; /Users/jacquesthibodeau/Zotero/storage/BWHW5UG4/1608.html,,TechSafety; MIRI,Computer Science - Computational Complexity,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6W43B8N7,manuscript,2011,"de Blanc, Peter",Ontological Crises in Artificial Agents' Value Systems,,,,,http://arxiv.org/abs/1105.3821,"Decision-theoretic agents predict and evaluate the results of their actions using a model, or ontology, of their environment. An agent's goal, or utility function, may also be specified in terms of the states of, or entities within, its ontology. If the agent may upgrade or replace its ontology, it faces a crisis: the agent's original goal may not be well-defined with respect to its new ontology. This crisis must be resolved before the agent can make plans towards achieving its goals. We discuss in this paper which sorts of agents will undergo ontological crises and why we may want to create such agents. We present some concrete examples, and argue that a well-defined procedure for resolving ontological crises is needed. We point to some possible approaches to solving this problem, and evaluate these methods on our examples.",2011-05-19,2022-01-30 04:56:57,2022-01-30 04:56:57,2021-01-23 20:43:20,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000020  arXiv: 1105.3821,,/Users/jacquesthibodeau/Zotero/storage/D876G6M2/de Blanc - 2011 - Ontological Crises in Artificial Agents' Value Sys.pdf,,,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
I5TDG7QC,manuscript,2016,"Garrabrant, Scott; Fallenstein, Benya; Demski, Abram; Soares, Nate",Inductive Coherence,,,,,http://arxiv.org/abs/1604.05288,"While probability theory is normally applied to external environments, there has been some recent interest in probabilistic modeling of the outputs of computations that are too expensive to run. Since mathematical logic is a powerful tool for reasoning about computer programs, we consider this problem from the perspective of integrating probability and logic. Recent work on assigning probabilities to mathematical statements has used the concept of coherent distributions, which satisfy logical constraints such as the probability of a sentence and its negation summing to one. Although there are algorithms which converge to a coherent probability distribution in the limit, this yields only weak guarantees about finite approximations of these distributions. In our setting, this is a significant limitation: Coherent distributions assign probability one to all statements provable in a specific logical theory, such as Peano Arithmetic, which can prove what the output of any terminating computation is; thus, a coherent distribution must assign probability one to the output of any terminating computation. To model uncertainty about computations, we propose to work with approximations to coherent distributions. We introduce inductive coherence, a strengthening of coherence that provides appropriate constraints on finite approximations, and propose an algorithm which satisfies this criterion.",2016-10-07,2022-01-30 04:56:57,2022-01-30 04:56:57,2019-12-16 02:30:52,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000003  arXiv: 1604.05288,,/Users/jacquesthibodeau/Zotero/storage/QSFXVV9V/Garrabrant et al. - 2016 - Inductive Coherence.pdf; /Users/jacquesthibodeau/Zotero/storage/BZ229SP3/Garrabrant et al. - 2016 - Inductive Coherence.pdf; /Users/jacquesthibodeau/Zotero/storage/KBHB4XEP/1604.html; /Users/jacquesthibodeau/Zotero/storage/BKI2SSHN/1604.html,,TechSafety; MIRI,Computer Science - Machine Learning; Computer Science - Artificial Intelligence; Mathematics - Probability,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Z23F8ZT9,manuscript,2017,"Garrabrant, Scott; Benson-Tilsen, Tsvi; Critch, Andrew; Soares, Nate; Taylor, Jessica",Logical Induction,,,,,http://arxiv.org/abs/1609.03543,"We present a computable algorithm that assigns probabilities to every logical statement in a given formal language, and refines those probabilities over time. For instance, if the language is Peano arithmetic, it assigns probabilities to all arithmetical statements, including claims about the twin prime conjecture, the outputs of long-running computations, and its own probabilities. We show that our algorithm, an instance of what we call a logical inductor, satisfies a number of intuitive desiderata, including: (1) it learns to predict patterns of truth and falsehood in logical statements, often long before having the resources to evaluate the statements, so long as the patterns can be written down in polynomial time; (2) it learns to use appropriate statistical summaries to predict sequences of statements whose truth values appear pseudorandom; and (3) it learns to have accurate beliefs about its own current beliefs, in a manner that avoids the standard paradoxes of self-reference. For example, if a given computer program only ever produces outputs in a certain range, a logical inductor learns this fact in a timely manner; and if late digits in the decimal expansion of $\pi$ are difficult to predict, then a logical inductor learns to assign $\approx 10\%$ probability to ""the $n$th digit of $\pi$ is a 7"" for large $n$. Logical inductors also learn to trust their future beliefs more than their current beliefs, and their beliefs are coherent in the limit (whenever $\phi \implies \psi$, $\mathbb{P}_\infty(\phi) \le \mathbb{P}_\infty(\psi)$, and so on); and logical inductors strictly dominate the universal semimeasure in the limit. These properties and many others all follow from a single logical induction criterion, which is motivated by a series of stock trading analogies. Roughly speaking, each logical sentence $\phi$ is associated with a stock that is worth \$1 per share if [...]",2017-12-12,2022-01-30 04:56:57,2022-01-30 04:56:57,2019-12-16 02:30:43,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s6]  ACC: 32  J: 23 arXiv: 1609.03543,,/Users/jacquesthibodeau/Zotero/storage/EHRSJNQS/Garrabrant et al. - 2017 - Logical Induction.pdf; /Users/jacquesthibodeau/Zotero/storage/QX7KU5KC/1609.html,,TechSafety; MIRI,Computer Science - Artificial Intelligence; Computer Science - Logic in Computer Science; Mathematics - Probability; Mathematics - Logic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QPEP6IKI,manuscript,2019,"Demski, Abram; Garrabrant, Scott",Embedded Agency,,,,,http://arxiv.org/abs/1902.09469,"Traditional models of rational action treat the agent as though it is cleanly separated from its environment, and can act on that environment from the outside. Such agents have a known functional relationship with their environment, can model their environment in every detail, and do not need to reason about themselves or their internal parts. We provide an informal survey of obstacles to formalizing good reasoning for agents embedded in their environment. Such agents must optimize an environment that is not of type ``function''; they must rely on models that fit within the modeled environment; and they must reason about themselves as just another physical system, made of parts that can be modified and that can work at cross purposes.",2019-02-25,2022-01-30 04:56:48,2022-01-30 04:56:48,2019-12-16 02:27:50,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000015  arXiv: 1902.09469,,/Users/jacquesthibodeau/Zotero/storage/F7TPMBEQ/Demski and Garrabrant - 2019 - Embedded Agency.pdf; /Users/jacquesthibodeau/Zotero/storage/T2XX98RN/1902.html,,TechSafety; MIRI,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3WXMKFV5,manuscript,2018,"Yudkowsky, Eliezer; Soares, Nate",Functional Decision Theory: A New Theory of Instrumental Rationality,,,,,http://arxiv.org/abs/1710.05060,"This paper describes and motivates a new decision theory known as functional decision theory (FDT), as distinct from causal decision theory and evidential decision theory. Functional decision theorists hold that the normative principle for action is to treat one's decision as the output of a fixed mathematical function that answers the question, ""Which output of this very function would yield the best outcome?"" Adhering to this principle delivers a number of benefits, including the ability to maximize wealth in an array of traditional decision-theoretic and game-theoretic problems where CDT and EDT perform poorly. Using one simple and coherent decision rule, functional decision theorists (for example) achieve more utility than CDT on Newcomb's problem, more utility than EDT on the smoking lesion problem, and more utility than both in Parfit's hitchhiker problem. In this paper, we define FDT, explore its prescriptions in a number of different decision problems, compare it to CDT and EDT, and give philosophical justifications for FDT as a normative theory of decision-making.",2018-05-22,2022-01-30 04:56:48,2022-01-30 04:56:48,2019-12-18 04:17:20,,,,,,,Functional Decision Theory,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s3]  ACC: 19  J: 11  arXiv: 1710.05060,,/Users/jacquesthibodeau/Zotero/storage/KQXR2IHP/Yudkowsky and Soares - 2018 - Functional Decision Theory A New Theory of Instru.pdf; /Users/jacquesthibodeau/Zotero/storage/RNCQ7RDX/Yudkowsky and Soares - 2018 - Functional Decision Theory A New Theory of Instru.pdf; /Users/jacquesthibodeau/Zotero/storage/J5XQEFMS/1710.html; /Users/jacquesthibodeau/Zotero/storage/K2U4NXWI/1710.html; /Users/jacquesthibodeau/Zotero/storage/VVPZQTSI/1710.html,,TechSafety; MIRI,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4H9VQJKN,manuscript,2019,"Evans, Owain; Saunders, William; Stuhlmüller, Andreas",Machine Learning Projects for Iterated Distillation and Ampliﬁcation,,,,,,"Iterated Distillation and Ampliﬁcation (IDA) is a framework for training ML models. IDA is related to existing frameworks like imitation learning and reinforcement learning, but it aims to solve tasks for which humans cannot construct a suitable reward function or solve directly.",2019,2022-01-30 05:00:28,2022-01-30 05:00:28,,,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: NoCitationData[s2]  ACC: 1  J: 0,,/Users/jacquesthibodeau/Zotero/storage/V6BVPGKN/Evans et al. - Machine Learning Projects for Iterated Distillatio.pdf,,TechSafety; Ought,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
I38TP5D8,manuscript,2019,"Roy, Mati",AI Safety Open Problems,,,,,https://docs.google.com/document/d/1J2fOOF-NYiPC0-J3ZGEfE0OhA-QcOInhlvWjr1fAsS0/edit?usp=embed_facebook,Created: 2018-11-08 | Updated: 2019-11-2 | Suggestions: please make suggestions directly in this Doc | List maintainer: Mati Roy (contact@matiroy.com)  AI Safety Open Problems Technical AGI safety research outside AI: https://forum.effectivealtruism.org/posts/2e9NDGiXt8PjjbTMC/technical-agi-safet...,2019,2022-01-30 05:00:28,2022-01-30 05:00:28,2019-12-16 19:57:56,,,,,,,,,,,,,,en,,Google Docs,,,,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/73B2JNDW/edit.html,,TechSafety; Ought,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IP5NBM2T,manuscript,2017,"Eysenbach, Benjamin; Gu, Shixiang; Ibarz, Julian; Levine, Sergey",Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning,,,,,http://arxiv.org/abs/1711.06782,"Deep reinforcement learning algorithms can learn complex behavioral skills, but real-world application of these methods requires a large amount of experience to be collected by the agent. In practical settings, such as robotics, this involves repeatedly attempting a task, resetting the environment between each attempt. However, not all tasks are easily or automatically reversible. In practice, this learning process requires extensive human intervention. In this work, we propose an autonomous method for safe and efficient reinforcement learning that simultaneously learns a forward and reset policy, with the reset policy resetting the environment for a subsequent attempt. By learning a value function for the reset policy, we can automatically determine when the forward policy is about to enter a non-reversible state, providing for uncertainty-aware safety aborts. Our experiments illustrate that proper use of the reset policy can greatly reduce the number of manual resets required to learn a task, can reduce the number of unsafe actions that lead to non-reversible states, and can automatically induce a curriculum.",2017-11-17,2022-01-30 04:59:59,2022-01-30 04:59:59,2020-11-21 17:26:04,,,,,,,Leave no Trace,,,,,,,,,,,,arXiv.org,,ZSCC: 0000081  arXiv: 1711.06782,,/Users/jacquesthibodeau/Zotero/storage/UIPW3M67/Eysenbach et al. - 2017 - Leave no Trace Learning to Reset for Safe and Aut.pdf; /Users/jacquesthibodeau/Zotero/storage/I3RPW323/1711.html,,TechSafety; Other-org,Computer Science - Machine Learning; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQ3QRMVD,manuscript,2019,"Sevilla, Jaime; Moreno, Pablo",Implications of Quantum Computing for Artificial Intelligence alignment research,,,,,http://arxiv.org/abs/1908.07613,"We explain some key features of quantum computing via three heuristics and apply them to argue that a deep understanding of quantum computing is unlikely to be helpful to address current bottlenecks in Artificial Intelligence Alignment. Our argument relies on the claims that Quantum Computing leads to compute overhang instead of algorithmic overhang, and that the difficulties associated with the measurement of quantum states do not invalidate any major assumptions of current Artificial Intelligence Alignment research agendas. We also discuss tripwiring, adversarial blinding, informed oversight and side effects as possible exceptions.",2019-08-24,2022-01-30 04:59:58,2022-01-30 04:59:58,2019-12-16 22:41:08,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000000  arXiv: 1908.07613,,/Users/jacquesthibodeau/Zotero/storage/5X947SXX/Sevilla and Moreno - 2019 - Implications of Quantum Computing for Artificial I.pdf; /Users/jacquesthibodeau/Zotero/storage/VTG2NVXB/Sevilla and Moreno - 2019 - Implications of Quantum Computing for Artificial I.pdf; /Users/jacquesthibodeau/Zotero/storage/AQSE5TCF/1908.html; /Users/jacquesthibodeau/Zotero/storage/VJGCCXHW/1908.html,,MetaSafety; Other-org,Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Computer Science - Emerging Technologies,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MBKBAF43,manuscript,2018,"Noothigattu, Ritesh; Bouneffouf, Djallel; Mattei, Nicholas; Chandra, Rachita; Madan, Piyush; Varshney, Kush; Campbell, Murray; Singh, Moninder; Rossi, Francesca",Interpretable Multi-Objective Reinforcement Learning through Policy Orchestration,,,,,http://arxiv.org/abs/1809.08343,"Autonomous cyber-physical agents and systems play an increasingly large role in our lives. To ensure that agents behave in ways aligned with the values of the societies in which they operate, we must develop techniques that allow these agents to not only maximize their reward in an environment, but also to learn and follow the implicit constraints of society. These constraints and norms can come from any number of sources including regulations, business process guidelines, laws, ethical principles, social norms, and moral values. We detail a novel approach that uses inverse reinforcement learning to learn a set of unspecified constraints from demonstrations of the task, and reinforcement learning to learn to maximize the environment rewards. More precisely, we assume that an agent can observe traces of behavior of members of the society but has no access to the explicit set of constraints that give rise to the observed behavior. Inverse reinforcement learning is used to learn such constraints, that are then combined with a possibly orthogonal value function through the use of a contextual bandit-based orchestrator that picks a contextually-appropriate choice between the two policies (constraint-based and environment reward-based) when taking actions. The contextual bandit orchestrator allows the agent to mix policies in novel ways, taking the best actions from either a reward maximizing or constrained policy. In addition, the orchestrator is transparent on which policy is being employed at each time step. We test our algorithms using a Pac-Man domain and show that the agent is able to learn to act optimally, act within the demonstrated constraints, and mix these two functions in complex ways.",2018-09-21,2022-01-30 04:59:58,2022-01-30 04:59:58,2020-12-13 23:27:45,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000027  arXiv: 1809.08343,,/Users/jacquesthibodeau/Zotero/storage/BZ3MDGSE/Noothigattu et al. - 2018 - Interpretable Multi-Objective Reinforcement Learni.pdf; /Users/jacquesthibodeau/Zotero/storage/CHJIW45A/1809.html,,TechSafety; Other-org,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PE2F3NVR,manuscript,2020,"Schneider, Johannes",Humans learn too: Better Human-AI Interaction using Optimized Human Inputs,,,,,http://arxiv.org/abs/2009.09266,"Humans rely more and more on systems with AI components. The AI community typically treats human inputs as a given and optimizes AI models only. This thinking is one-sided and it neglects the fact that humans can learn, too. In this work, human inputs are optimized for better interaction with an AI model while keeping the model fixed. The optimized inputs are accompanied by instructions on how to create them. They allow humans to save time and cut on errors, while keeping required changes to original inputs limited. We propose continuous and discrete optimization methods modifying samples in an iterative fashion. Our quantitative and qualitative evaluation including a human study on different hand-generated inputs shows that the generated proposals lead to lower error rates, require less effort to create and differ only modestly from the original samples.",2020-09-19,2022-01-30 04:59:57,2022-01-30 04:59:57,2020-11-14 00:52:26,,,,,,,Humans learn too,,,,,,,,,,,,arXiv.org,,ZSCC: 0000001  arXiv: 2009.09266,,/Users/jacquesthibodeau/Zotero/storage/XP5PX8VP/Schneider - 2020 - Humans learn too Better Human-AI Interaction usin.pdf; /Users/jacquesthibodeau/Zotero/storage/JXASWCQG/2009.html,,TechSafety; AmbiguosSafety; Other-org,Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Human-Computer Interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7EKAANH7,manuscript,2018,"Baek, Jongmin Jerome",How To Solve Moral Conundrums with Computability Theory,,,,,http://arxiv.org/abs/1805.08347,"Various moral conundrums plague population ethics: The Non-Identity Problem, The Procreation Asymmetry, The Repugnant Conclusion, and more. I argue that the aforementioned moral conundrums have a structure neatly accounted for, and solved by, some ideas in computability theory. I introduce a mathematical model based on computability theory and show how previous arguments pertaining to these conundrums fit into the model. This paper proceeds as follows. First, I do a very brief survey of the history of computability theory in moral philosophy. Second, I follow various papers, and show how their arguments fit into, or don't fit into, our model. Third, I discuss the implications of our model to the question why the human race should or should not continue to exist. Finally, I show that our model ineluctably leads us to a Confucian moral principle.",2018-05-21,2022-01-30 04:59:57,2022-01-30 04:59:57,2020-11-14 01:09:58,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000000  arXiv: 1805.08347,,/Users/jacquesthibodeau/Zotero/storage/CCQVF6AC/Baek - 2018 - How To Solve Moral Conundrums with Computability T.pdf; /Users/jacquesthibodeau/Zotero/storage/ZH342CDG/1805.html,,TechSafety; Other-org,Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Computer Science - Logic in Computer Science,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GGN2RAC5,manuscript,2017,"Babcock, James; Kramar, Janos; Yampolskiy, Roman V.",Guidelines for Artificial Intelligence Containment,,,,,http://arxiv.org/abs/1707.08476,"With almost daily improvements in capabilities of artificial intelligence it is more important than ever to develop safety software for use by the AI research community. Building on our previous work on AI Containment Problem we propose a number of guidelines which should help AI safety researchers to develop reliable sandboxing software for intelligent programs of all levels. Such safety container software will make it possible to study and analyze intelligent artificial agent while maintaining certain level of safety against information leakage, social engineering attacks and cyberattacks from within the container.",2017-07-24,2022-01-30 04:59:56,2022-01-30 04:59:56,2020-11-21 17:49:51,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000029[s0]  arXiv: 1707.08476,,/Users/jacquesthibodeau/Zotero/storage/I9CB87G3/Babcock et al. - 2017 - Guidelines for Artificial Intelligence Containment.pdf; /Users/jacquesthibodeau/Zotero/storage/59PWEW8F/1707.html,,TechSafety; Other-org,Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BSHKC8QE,manuscript,2019,"Rupprecht, Christian; Ibrahim, Cyril; Pal, Christopher J.",Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents,,,,,http://arxiv.org/abs/1904.01318,"As deep reinforcement learning driven by visual perception becomes more widely used there is a growing need to better understand and probe the learned agents. Understanding the decision making process and its relationship to visual inputs can be very valuable to identify problems in learned behavior. However, this topic has been relatively under-explored in the research community. In this work we present a method for synthesizing visual inputs of interest for a trained agent. Such inputs or states could be situations in which specific actions are necessary. Further, critical states in which a very high or a very low reward can be achieved are often interesting to understand the situational awareness of the system as they can correspond to risky states. To this end, we learn a generative model over the state space of the environment and use its latent space to optimize a target function for the state of interest. In our experiments we show that this method can generate insights for a variety of environments and reinforcement learning methods. We explore results in the standard Atari benchmark games as well as in an autonomous driving simulator. Based on the efficiency with which we have been able to identify behavioural weaknesses with this technique, we believe this general approach could serve as an important tool for AI safety applications.",2019-04-02,2022-01-30 04:59:48,2022-01-30 04:59:48,2020-09-05 17:02:57,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000009  arXiv: 1904.01318,,/Users/jacquesthibodeau/Zotero/storage/KZ3E7C6S/Rupprecht et al. - 2019 - Finding and Visualizing Weaknesses of Deep Reinfor.pdf; /Users/jacquesthibodeau/Zotero/storage/PJSBJRET/1904.html,,TechSafety; AmbiguosSafety; Other-org,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4TCFB2D9,manuscript,2019,"Gruetzemacher, Ross; Paradice, David; Lee, Kang Bok",Forecasting Transformative AI: An Expert Survey,,,,,http://arxiv.org/abs/1901.08579,"Transformative AI technologies have the potential to reshape critical aspects of society in the near future. However, in order to properly prepare policy initiatives for the arrival of such technologies accurate forecasts and timelines are necessary. A survey was administered to attendees of three AI conferences during the summer of 2018 (ICML, IJCAI and the HLAI conference). The survey included questions for estimating AI capabilities over the next decade, questions for forecasting five scenarios of transformative AI and questions concerning the impact of computational resources in AI research. Respondents indicated a median of 21.5% of human tasks (i.e., all tasks that humans are currently paid to do) can be feasibly automated now, and that this figure would rise to 40% in 5 years and 60% in 10 years. Median forecasts indicated a 50% probability of AI systems being capable of automating 90% of current human tasks in 25 years and 99% of current human tasks in 50 years. The conference of attendance was found to have a statistically significant impact on all forecasts, with attendees of HLAI providing more optimistic timelines with less uncertainty. These findings suggest that AI experts expect major advances in AI technology to continue over the next decade to a degree that will likely have profound transformative impacts on society.",2019-07-16,2022-01-30 04:59:48,2022-01-30 04:59:48,2020-11-14 00:34:52,,,,,,,Forecasting Transformative AI,,,,,,,,,,,,arXiv.org,,ZSCC: 0000008  arXiv: 1901.08579,,/Users/jacquesthibodeau/Zotero/storage/94FP5N9E/Gruetzemacher et al. - 2019 - Forecasting Transformative AI An Expert Survey.pdf; /Users/jacquesthibodeau/Zotero/storage/N9IVWXEE/1901.html,,MetaSafety; Other-org,Computer Science - Artificial Intelligence; Computer Science - Computers and Society,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6SSB5PK6,manuscript,2017,"Menda, Kunal; Driggs-Campbell, Katherine; Kochenderfer, Mykel J.",DropoutDAgger: A Bayesian Approach to Safe Imitation Learning,,,,,http://arxiv.org/abs/1709.06166,"While imitation learning is becoming common practice in robotics, this approach often suffers from data mismatch and compounding errors. DAgger is an iterative algorithm that addresses these issues by continually aggregating training data from both the expert and novice policies, but does not consider the impact of safety. We present a probabilistic extension to DAgger, which uses the distribution over actions provided by the novice policy, for a given observation. Our method, which we call DropoutDAgger, uses dropout to train the novice as a Bayesian neural network that provides insight to its confidence. Using the distribution over the novice's actions, we estimate a probabilistic measure of safety with respect to the expert action, tuned to balance exploration and exploitation. The utility of this approach is evaluated on the MuJoCo HalfCheetah and in a simple driving experiment, demonstrating improved performance and safety compared to other DAgger variants and classic imitation learning.",2017-09-18,2022-01-30 04:59:46,2022-01-30 04:59:46,2020-12-13 20:55:29,,,,,,,DropoutDAgger,,,,,,,,,,,,arXiv.org,,ZSCC: 0000013  arXiv: 1709.06166,,/Users/jacquesthibodeau/Zotero/storage/IGX8RKK6/Menda et al. - 2017 - DropoutDAgger A Bayesian Approach to Safe Imitati.pdf; /Users/jacquesthibodeau/Zotero/storage/KWHIGHR6/1709.html,,TechSafety; AmbiguosSafety; Other-org,Computer Science - Artificial Intelligence; Computer Science - Robotics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5E9BGMY8,manuscript,2019,"Quint, Eleanor; Xu, Dong; Flint, Samuel; Scott, Stephen; Dwyer, Matthew",Formal Language Constraints for Markov Decision Processes,,,,,https://arxiv.org/abs/1910.01074v3,"In order to satisfy safety conditions, an agent may be constrained from acting freely. A safe controller can be designed a priori if an environment is well understood, but not when learning is employed. In particular, reinforcement learned (RL) controllers require exploration, which can be hazardous in safety critical situations. We study the benefits of giving structure to the constraints of a constrained Markov decision process by specifying them in formal languages as a step towards using safety methods from software engineering and controller synthesis. We instantiate these constraints as finite automata to efficiently recognise constraint violations. Constraint states are then used to augment the underlying MDP state and to learn a dense cost function, easing the problem of quickly learning joint MDP/constraint dynamics. We empirically evaluate the effect of these methods on training a variety of RL algorithms over several constraints specified in Safety Gym, MuJoCo, and Atari environments.",2019-10-02,2020-11-14 01:21,2020-12-20 21:10,2020-11-14 01:21,,,,,,,,,,,,,,en,,,,,arxiv.org,,ZSCC: NoCitationData[s0]  ACC: 3,,/Users/angelica/Zotero/storage/WQXN48YU/1910.html; /Users/angelica/Zotero/storage/WPALV4PP/Quint et al. - 2019 - Formal Language Constraints for Markov Decision Pr.pdf,,Other-org; NotSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Within the framework of RL, the authors propose using constraints defined by DFAs (deterministic finite automata) in order to eliminate safety failures, or to prevent agents from exploring clearly ineffective policies (which would accelerate learning). Constraints can be defined on any auxiliary information that can be computed from the ""base"" MDP. A constraint could either restrict the action space, forcing the agent to take an action that doesn't violate the constraint, which they term ""hard"" constraints; or a constraint could impose a penalty on the agent, thus acting as a form of reward shaping, which they term a ""soft"" constraint. They consider two constraints: one that prevents the agent from ""dithering"" (going left, then right, then left, then right), and one that prevents the agent from ""overactuating"" (going in the same direction four times in a row). They evaluate their approach with these constraints on Atari games and Mujoco environments, and show that they lead to increased reward and decreased constraint violations."
ATXQ5FB3,manuscript,2020,"Laskin, Michael; Lee, Kimin; Stooke, Adam; Pinto, Lerrel; Abbeel, Pieter; Srinivas, Aravind",Reinforcement Learning with Augmented Data,,,,,http://arxiv.org/abs/2004.14990,"Learning from visual observations is a fundamental yet challenging problem in Reinforcement Learning (RL). Although algorithmic advances combined with convolutional neural networks have proved to be a recipe for success, current methods are still lacking on two fronts: (a) data-efﬁciency of learning and (b) generalization to new environments. To this end, we present Reinforcement Learning with Augmented Data (RAD), a simple plug-and-play module that can enhance most RL algorithms. We perform the ﬁrst extensive study of general data augmentations for RL on both pixel-based and state-based inputs, and introduce two new data augmentations - random translate and random amplitude scale. We show that augmentations such as random translate, crop, color jitter, patch cutout, random convolutions, and amplitude scale can enable simple RL algorithms to outperform complex state-of-the-art methods across common benchmarks. RAD sets a new state-of-the-art in terms of data-efﬁciency and ﬁnal performance on the DeepMind Control Suite benchmark for pixel-based control as well as OpenAI Gym benchmark for state-based control. We further demonstrate that RAD signiﬁcantly improves test-time generalization over existing methods on several OpenAI ProcGen benchmarks. Our RAD module and training code are available at https://www.github.com/MishaLaskin/rad.",2020-06-23,2020-08-31 19:00,2020-12-20 22:09,2020-08-31 19:00,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000012  ACC: 12  arXiv: 2004.14990,,/Users/angelica/Zotero/storage/GRHL8ZH8/Laskin et al. - 2020 - Reinforcement Learning with Augmented Data.pdf,,NotSafety; CHAI-Berkeley,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"While CURL (summarized above) applies contrastive learning in order to ensure the network is invariant to specific data augmentations, we can try something even simpler: what if we just run a regular RL algorithm on augmented observations (e.g. observations that have been randomly cropped)? The authors term this approach RAD (RL with Augmented Data), and find that this actually _outperforms_ CURL, despite not using the contrastive learning objective. The authors speculate that CURL is handicapped by using the contrastive loss as an auxiliary objective, and so its representations are forced to be good both for the true task and for the contrastive prediction task, whereas RAD only trains on the true task."
FMNZVCJ6,manuscript,2018,"Achiam, Joshua; Edwards, Harrison; Amodei, Dario; Abbeel, Pieter",Variational Option Discovery Algorithms,,,,,http://arxiv.org/abs/1807.10299,"We explore methods for option discovery based on variational inference and make two algorithmic contributions. First: we highlight a tight connection between variational option discovery methods and variational autoencoders, and introduce Variational Autoencoding Learning of Options by Reinforcement (VALOR), a new method derived from the connection. In VALOR, the policy encodes contexts from a noise distribution into trajectories, and the decoder recovers the contexts from the complete trajectories. Second: we propose a curriculum learning approach where the number of contexts seen by the agent increases whenever the agent's performance is strong enough (as measured by the decoder) on the current set of contexts. We show that this simple trick stabilizes training for VALOR and prior variational option discovery methods, allowing a single agent to learn many more modes of behavior than it could with a fixed context distribution. Finally, we investigate other topics related to variational option discovery, including fundamental limitations of the general approach and the applicability of learned options to downstream tasks.",2018-07-26,2019-12-16 20:07,2020-12-20 20:22,2019-12-16 20:07,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000034  arXiv: 1807.10299,,/Users/angelica/Zotero/storage/FZBGKVVS/1807.html; /Users/angelica/Zotero/storage/ABRLR2QA/1807.html; /Users/angelica/Zotero/storage/XCAD5YFA/Achiam et al. - 2018 - Variational Option Discovery Algorithms.pdf,,NotSafety; CHAI-Berkeley; AmbiguosSafety; Open-AI,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We can hope to do hierarchical reinforcement learning by first discovering several useful simple policies (or ""options"") by just acting in the environment without any reward function, and then using these options as primitive actions in a higher level policy that learns to do some task (using a reward function). How could we learn the options without a reward function though? Intuitively, we would like to learn behaviors that are different from each other. One way to frame this would be to think of this as an encoder-decoder problem. Suppose we want to learn K options. Then, we can give the encoder a number in the range [1, K], have it ""encode"" the number into a trajectory τ (that is, our encoder is a policy), and then have a decoder take τ and recover the original number. We train the encoder/policy and decoder jointly, optimizing them to successfully recover the original number (called a _context_). Intuitively, the encoder/policy wants to have very different behaviors for each option, so that it easy for decoder to figure out the context from the trajectory τ. However, a simple solution would be for the encoder/policy to just take a particular series of actions for each context and then stop, and the decoder learns an exact mapping from final states to contexts. To avoid this, we can decrease the capacity of the decoder (i.e. don't give it too many layers), and we also optimize for the _entropy_ of the encoder/policy, which encourages the encoder/policy to be more stochastic, and so it is more likely to learn overall behaviors that can still have some stochasticity, while still allowing the decoder to decode them. It turns out that this optimization problem has a one-to-one correspondence with variational autoencoders, motivating the name ""variational option discovery"". To stabilize training, they start with a small K, and increase K whenever the decoder becomes powerful enough. They evaluate in Gym environments, a simulated robotic hand, and a new ""Toddler"" environment. They find that the scheme works well (in terms of maximizing the objective) in all environments, but that the learned behaviors no longer look natural in the Toddler environment (which is the most complex). They also show that the learned policies can be used for hierarchical RL in the AntMaze problem.

This is very similar to the recent [Diversity Is All You Need](https://arxiv.org/abs/1802.06070). DIAYN aims to decode the context from _every state_ along a trajectory, which incentivizes it to find behaviors of the form ""go to a goal state"", whereas VALOR (this work) decodes the context from the entire trajectory (without actions, which would make the decoder's job too easy), which allows it to learn behaviors with motion, such as ""go around in a circle""."
72F4X2ZL,manuscript,2020,"Arenz, Oleg; Neumann, Gerhard",Non-Adversarial Imitation Learning and its Connections to Adversarial Methods,,,,,http://arxiv.org/abs/2008.03525,"Many modern methods for imitation learning and inverse reinforcement learning, such as GAIL or AIRL, are based on an adversarial formulation. These methods apply GANs to match the expert's distribution over states and actions with the implicit state-action distribution induced by the agent's policy. However, by framing imitation learning as a saddle point problem, adversarial methods can suffer from unstable optimization, and convergence can only be shown for small policy updates. We address these problems by proposing a framework for non-adversarial imitation learning. The resulting algorithms are similar to their adversarial counterparts and, thus, provide insights for adversarial imitation learning methods. Most notably, we show that AIRL is an instance of our non-adversarial formulation, which enables us to greatly simplify its derivations and obtain stronger convergence guarantees. We also show that our non-adversarial formulation can be used to derive novel algorithms by presenting a method for offline imitation learning that is inspired by the recent ValueDice algorithm, but does not rely on small policy updates for convergence. In our simulated robot experiments, our offline method for non-adversarial imitation learning seems to perform best when using many updates for policy and discriminator at each iteration and outperforms behavioral cloning and ValueDice.",2020-08-08,2020-11-14 00:50,2020-12-20 22:04,2020-11-14 00:50,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000000  arXiv: 2008.03525,,/Users/angelica/Zotero/storage/7NCUQLDG/2008.html; /Users/angelica/Zotero/storage/SVD29Q5A/Arenz and Neumann - 2020 - Non-Adversarial Imitation Learning and its Connect.pdf,,Other-org; NotSafety,Computer Science - Robotics; Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Information Theory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Viewing imitation learning as a distribution matching problem has become more popular in recent years (see <@Value-Dice@>(@Imitation Learning via Off-Policy Distribution Matching@) / <@I2L@>(@State-only Imitation with Transition Dynamics Mismatch@)). However, the authors in this paper argue that such methods are unstable due to their formulation as saddle-point problems which means they have weak convergence guarantees due to the assumption that the policy is slowly updated. In this paper, the authors reformulate <@Adversarial IRL@>(@Learning Robust Rewards with Adversarial Inverse Reinforcement Learning@) as a non-adversarial problem allowing for much stronger convergence guarantees to be proved. In particular, the authors derive a lower-bound on the discrimination reward which allows for larger policy updates and then introduce a method to iteratively tighten this bound. They also build on prior work for value-dice and derive a soft actor-critic algorithm (ONAIL) that they evaluate on a variety of control tasks."
978YC5F5,manuscript,2019,"Gleave, Adam; Dennis, Michael; Kant, Neel; Wild, Cody; Levine, Sergey; Russell, Stuart",Adversarial Policies: Attacking Deep Reinforcement Learning,,,,,http://arxiv.org/abs/1905.10615,"Deep reinforcement learning (RL) policies are known to be vulnerable to adversarial perturbations to their observations, similar to adversarial examples for classiﬁers. However, an attacker is not usually able to directly modify another agent’s observations. This might lead one to wonder: is it possible to attack an RL agent simply by choosing an adversarial policy acting in a multi-agent environment so as to create natural observations that are adversarial? We demonstrate the existence of adversarial policies in zero-sum games between simulated humanoid robots with proprioceptive observations, against state-of-the-art victims trained via self-play to be robust to opponents. The adversarial policies reliably win against the victims but generate seemingly random and uncoordinated behavior. We ﬁnd that these policies are more successful in high-dimensional environments, and induce substantially different activations in the victim policy network than when the victim plays against a normal opponent. Videos are available at https://adversarialpolicies.github.io/.",2019-05-25,2019-12-18 01:48,2020-12-20 20:58,2019-07-11 18:47,,,,,,,Adversarial Policies,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000023  arXiv: 1905.10615,,,,BERI; TechSafety; CHAI,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning; I.2.6; Computer Science - Cryptography and Security,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This work demonstrates the existence of _adversarial policies_ of behaviour in high-dimensional, two-player zero-sum games. Specifically, they show that adversarially-trained agents (""Adv""), who can only affect a victim's observations of their (Adv's) states, can act in ways that confuse the victim into behaving suboptimally.

An adversarial policy is trained by reinforcement learning in a single-player paradigm where the victim is a black-box fixed policy that was previously trained via self-play to be robust to adversarial attacks. As a result, the adversarial policies learn to push the observations of the victim outside the training distribution, causing the victim to behave poorly. The adversarial policies do not actually behave intelligently, such as blocking or tackling the victim, but instead do unusual things like spasming in a manner that appears random to humans, curling into a ball or kneeling.

Further experiments showed that if the victim's observations of the adversary were removed, then the adversary was unable to learn such an adversarial policy. In addition, the victim's network activations were very different when playing against an adversarial policy relative to playing against a random or lifeless opponent. By comparing two similar games where the key difference was the number of adversary dimensions being observed, they showed that such policies were easier to learn in higher-dimensional games."
JYPN7CP2,manuscript,2019,"Szlam, Arthur; Gray, Jonathan; Srinet, Kavya; Jernite, Yacine; Joulin, Armand; Synnaeve, Gabriel; Kiela, Douwe; Yu, Haonan; Chen, Zhuoyuan; Goyal, Siddharth; Guo, Demi; Rothermel, Danielle; Zitnick, C. Lawrence; Weston, Jason",Why Build an Assistant in Minecraft?,,,,,http://arxiv.org/abs/1907.09273,"In this document we describe a rationale for a research program aimed at building an open ""assistant"" in the game Minecraft, in order to make progress on the problems of natural language understanding and learning from dialogue.",2019-07-25,2019-12-16 22:39,2020-12-21 18:33,2019-12-16 22:39,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000003  arXiv: 1907.09273,,/Users/angelica/Zotero/storage/SEM85CKP/1907.html; /Users/angelica/Zotero/storage/WBGHQ65D/Szlam et al. - 2019 - Why Build an Assistant in Minecraft.pdf,,Other-org; NotSafety,Computer Science - Artificial Intelligence; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This position paper proposes a new challenge for AI research: building a bot that can provide assistance in [Minecraft](https://www.minecraft.net/en-us/) (creative mode). A [companion paper](https://research.fb.com/wp-content/uploads/2019/07/CraftAssist-A-Framework-for-Dialogue-enabled-Interactive-Agents-v3.pdf) presents an initial setup for such an agent.

The main goal here is to advance natural language understanding, intent inference and instruction following. As a result, there is no formal specification like a reward function -- in their own words, ""the ultimate goal of the bot is to be a useful and fun assistant in a wide variety of tasks specified and evaluated by human players"". They chose Minecraft in particular partly because it has a very rich space of _tasks_, even though the _execution_ of any given task is relatively straightforward. They script many low level policies to automate this execution in order to make learning easier (for example, they have policies to navigate to a location or to build specified structures) and focus the learning challenge on figuring out what the user wants.

The current version of the bot takes dialogue from the user and uses a neural model to parse it into an _action dictionary_ that unambiguously specifies what the agent should do -- I think this neural model is the main thing to be learned. There are a bunch of details on how the rest of the modules work as well. They have also released three datasets: a semantic parsing dataset that associates instructions with action dictionaries, a house dataset that has trajectories where a human builds a house, and a semantic segmentation dataset that labels various parts of houses."
BE6W2FT5,manuscript,2020,"Milli, Smitha; Belli, Luca; Hardt, Moritz",From Optimizing Engagement to Measuring Value,,,,,http://arxiv.org/abs/2008.12623,"Most recommendation engines today are based on predicting user engagement, e.g. predicting whether a user will click on an item or not. However, there is potentially a large gap between engagement signals and a desired notion of ""value"" that is worth optimizing for. We use the framework of measurement theory to (a) confront the designer with a normative question about what the designer values, (b) provide a general latent variable model approach that can be used to operationalize the target construct and directly optimize for it, and (c) guide the designer in evaluating and revising their operationalization. We implement our approach on the Twitter platform on millions of users. In line with established approaches to assessing the validity of measurements, we perform a qualitative evaluation of how well our model captures a desired notion of ""value"".",2020-08-20,2020-11-14 00:59,2020-12-20 21:11,2020-11-14 00:59,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000000  arXiv: 2008.12623,,/Users/angelica/Zotero/storage/VPK8L455/2008.html; /Users/angelica/Zotero/storage/VGJCBARX/Milli et al. - 2020 - From Optimizing Engagement to Measuring Value.pdf,,NotSafety; CHAI-Berkeley,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Social and Information Networks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper takes a stab at creating a better objective for existing recommender systems than engagement, in a way that could be applied at existing companies like Twitter. The basic approach is to treat the variable to be optimized (user value) as a latent variable, and use probabilistic inference to infer how likely it is that a particular recommendation was valuable.

Usually a major challenge with such an approach is specifying the _observation model_: how the observed data is caused by the latent variable. In the case of Twitter, this would require you to answer questions like, “if the user does not value a tweet, how likely is a user to hit the like button anyway?” This is a hard question to answer, since perhaps users like tweets in order to stop conversations, or because they are addicting at the moment but are not actually valuable, etc.

One simple heuristic is to take two datasets where we know one dataset has more valuable recommendations than the other. Differences in user behavior between these datasets can then be assumed to be correlations with value. The authors provide a quantitative method for inferring the observation model from such datasets, which I won’t go into here since it is primarily a heuristic baseline. One obvious problem is that if the “better” dataset was produced by optimizing (say) clicks, then the clicks may have increased for reasons other than improved value, but this heuristic approach will attribute the entire increase to improved value.

How can we do better? The key insight of this paper is that if you have a bunch of historical data, then you can get a lot of mileage by identifying an _anchor_: a type of feedback that when given provides unequivocal evidence of the latent value. On Twitter, this is taken to be the “See Less Often” (SLO) button: if this is clicked, then we know with effective certainty that this was not valuable, regardless of any other actions the user took. The connection between value and other behaviors such as liking a tweet can then be inferred by looking at the connection between those behaviors and the anchor, which we can estimate from historical data.

Formally, the authors assume access to a graph describing the relationships between the various possible behaviors (almost all of which have the latent value V as a parent). One of these is identified as the anchor node A, for which P(V = 1 | A = 1) is assumed to be known and independent of all other variables. However, P(V = 1 | A = 0) is not independent of other variables: intuitively, if the SLO button is _not_ clicked, then we need to fall back to looking at other variables to estimate value.

The authors then show that under some reasonable assumptions on the anchor variable, if you have a dataset of historical data to estimate P(A, B) (where B consists of all the other tracked behaviors), then instead of specifying observation models P(B | V) for all behaviors, you only need to specify observation models for the parents of A, that is P(parents(A) | V). Everything else is uniquely determined, allowing us to calculate our final objective P(V | A, B). (There are algorithmic details on how to do this efficiently; see the paper for details.) In this case, they use the heuristic method outlined above to estimate P(parents(A) | V).

They unfortunately don’t have a great way to evaluate their method: they clearly can’t evaluate it by seeing if it leads to higher clicks, since the whole point was to move away from clicks as an optimization target. (I assume a user study on Twitter was infeasible.) Their primary form of evaluation is to run the model and report the learned probabilities, and show that they seem reasonable, whereas those output by a Naive Bayes model do not."
,manuscript,2018,"Gleave, Adam; Habryka, Oliver",Multi-task Maximum Entropy Inverse Reinforcement Learning,,,,,http://arxiv.org/abs/1805.08882,"Multi-task Inverse Reinforcement Learning (IRL) is the problem of inferring multiple reward functions from expert demonstrations. Prior work, built on Bayesian IRL, is unable to scale to complex environments due to computational constraints. This paper contributes a formulation of multi-task IRL in the more computationally efficient Maximum Causal Entropy (MCE) IRL framework. Experiments show our approach can perform one-shot imitation learning in a gridworld environment that single-task IRL algorithms need hundreds of demonstrations to solve. We outline preliminary work using meta-learning to extend our method to the function approximator setting of modern MCE IRL algorithms. Evaluating on multi-task variants of common simulated robotics benchmarks, we discover serious limitations of these IRL algorithms, and conclude with suggestions for further work.",2018-07-15,2019-12-18 01:12,2020-12-20 21:22,2019-12-18 01:12,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000010  arXiv: 1805.08882,,/Users/angelica/Zotero/storage/BSIUB8HB/Gleave and Habryka - 2018 - Multi-task Maximum Entropy Inverse Reinforcement L.pdf,,TechSafety; CHAI,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning; I.2.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper tackles multi-task inverse reinforcement learning, where there are multiple related tasks which have similar rewards, and the goal is to infer the reward for a new task after having already seen demonstrations for previous tasks. In the tabular setting, where we can enumerate all of the states and actions, they adapt Maximum Causal Entropy IRL to the multi-task setting. To make any progress, it is necessary to have some model of how the tasks are related -- in this work, they assume that the reward weights are close to each other, and so they penalize the L2 distance from the reward weights to the mean weights across tasks. Experiments show that this approach can learn a new task reward in 1-2 trajectories, while learning from scratch takes 50+ trajectories. They then consider how to generalize to continuous control environments such as MountainCar. They propose an algorithm that applies <@Reptile@>(@Reptile: A Scalable Meta-Learning Algorithm@) to <@adversarial IRL@>(@Learning Robust Rewards with Adversarial Inverse Reinforcement Learning@), with the goal of learning a good initialization of the reward neural net which can quickly move to the correct reward function given data from a new task. This works well when the policy is unimodal (eg. for MountainCar, a policy that always goes left or always goes right), but not when the policy is multimodal (eg. you have to go either left or right depending on the color of the flag). Experiments suggest that this is because adversarial IRL does not do well with a multimodal policy. Meta-AIRL would require AIRL to produce good results on multiple environments -- if even one of the environments has a bad policy, there's a garbage input to meta-AIRL, which then tanks its performance."
,manuscript,2020,"Marcus, Gary",The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence,,,,,http://arxiv.org/abs/2002.06177,"Recent research in artificial intelligence and machine learning has largely emphasized general-purpose learning and ever-larger training sets and more and more compute. In contrast, I propose a hybrid, knowledge-driven, reasoning-based approach, centered around cognitive models, that could provide the substrate for a richer, more robust AI than is currently possible.",2020-02-19,2020-09-05 19:12,2020-12-20 22:15,2020-09-05 19:12,,,,,,,The Next Decade in AI,,,,,,,,,,,,arXiv.org,,ZSCC: 0000016  arXiv: 2002.06177,,/Users/angelica/Zotero/storage/FN3CVXUM/Marcus - 2020 - The Next Decade in AI Four Steps Towards Robust A.pdf; /Users/angelica/Zotero/storage/7D6YDGI3/2002.html,,Other-org; TechSafety,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; I.2.6; I.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper suggests a few directions which would allow us to build more _robust_ AI systems with better ""understanding"" of the world: specifically, it highlights **symbol manipulation, encoded knowledge, reasoning, and cognitive models** as areas of research for the next decade.

See also [Import AI #187](https://jack-clark.net/2020/03/02/import-ai-187-real-world-robot-tests-at-cvpr-all-hail-the-molecule-transformer-the-four-traits-needed-for-smarter-ai-systems/) and [Matthew Barnett's summary](https://www.lesswrong.com/posts/CeJs4rPgPtJPNqLMt/gary-marcus-four-steps-towards-robust-artificial)."
,manuscript,2020,"Guan, Lin; Verma, Mudit; Kambhampati, Subbarao",Explanation Augmented Feedback in Human-in-the-Loop Reinforcement Learning,,,,,http://arxiv.org/abs/2006.14804,"Human-in-the-loop Reinforcement Learning (HRL) aims to integrate human guidance with Reinforcement Learning (RL) algorithms to improve sample efficiency and performance. The usual human guidance in HRL is binary evaluative ""good"" or ""bad"" signal for queried states and actions. However, this suffers from the problems of weak supervision and poor efficiency in leveraging human feedback. To address this, we present EXPAND (Explanation Augmented Feedback) which allows for explanatory information to be given as saliency maps from the human in addition to the binary feedback. EXPAND employs a state perturbation approach based on the state salient information to augment the feedback, reducing the number of human feedback signals required. We choose two domains to evaluate this approach, Taxi and Atari-Pong. We demonstrate the effectiveness of our method on three metrics, environment sample efficiency, human feedback sample efficiency, and agent gaze. We show that our method outperforms our baselines. Finally, we present an ablation study to confirm our hypothesis that augmenting binary feedback with state salient information gives a boost in performance.",2020-07-16,2020-08-28 17:26,2020-12-21 18:08,2020-08-28 17:26,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000000  arXiv: 2006.14804,,/Users/angelica/Zotero/storage/BVC7424T/Guan et al. - 2020 - Explanation Augmented Feedback in Human-in-the-Loo.pdf; /Users/angelica/Zotero/storage/EJ73U4WX/2006.html,,Other-org; TechSafety; AmbiguosSafety,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper starts from a similar position as the highlighted paper: that we can improve on algorithms by having humans provide different kinds of feedback that help with learning. They ask humans to provide “explanations” to improve sample efficiency in deep RL, which in this case means asking a human to segment parts of the image observation that are important (similar to a saliency map). They use this to define auxiliary losses that incentivize the agent to be invariant to augmentations of the irrelevant parts of the image. Their empirical evaluation shows improvements in sample efficiency relative to simple good/bad evaluative feedback."
,manuscript,2019,"Edwards, Ashley D.; Isbell, Charles L.",Perceptual Values from Observation,,,,,https://arxiv.org/abs/1905.07861v1,"Imitation by observation is an approach for learning from expert demonstrations that lack action information, such as videos. Recent approaches to this problem can be placed into two broad categories: training dynamics models that aim to predict the actions taken between states, and learning rewards or features for computing them for Reinforcement Learning (RL). In this paper, we introduce a novel approach that learns values, rather than rewards, directly from observations. We show that by using values, we can significantly speed up RL by removing the need to bootstrap action-values, as compared to sparse-reward specifications.",2019-05-20,2020-11-14 01:26,2020-12-20 22:06,2020-11-14 01:26,,,,,,,,,,,,,,en,,,,,arxiv.org,,ZSCC: 0000002,,/Users/angelica/Zotero/storage/MPAIM7HG/Edwards and Isbell - 2019 - Perceptual Values from Observation.pdf; /Users/angelica/Zotero/storage/ESJBB7B7/1905.html,,Other-org; NotSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper proposes a technique for learning from raw expert-trajectory observations by assuming that the last state in the trajectory is the state where the goal was achieved, and that other states have value in proportion to how close they are to a terminal state in demonstration trajectories. They use this as a grounding to train models predicting value and action-value, and then use these estimated values to determine actions. "
,manuscript,2020,"Pruthi, Garima; Liu, Frederick; Sundararajan, Mukund; Kale, Satyen",Estimating Training Data Influence by Tracking Gradient Descent,,,,,http://arxiv.org/abs/2002.08484,"We introduce a method called TrackIn that computes the influence of a training example on a prediction made by the model, by tracking how the loss on the test point changes during the training process whenever the training example of interest was utilized. We provide a scalable implementation of TrackIn via a combination of a few key ideas: (a) a first-order approximation to the exact computation, (b) using random projections to speed up the computation of the first-order approximation for large models, (c) using saved checkpoints of standard training procedures, and (d) cherry-picking layers of a deep neural network. An experimental evaluation shows that TrackIn is more effective in identifying mislabelled training examples than other related methods such as influence functions and representer points. We also discuss insights from applying the method on vision, regression and natural language tasks.",2020-07-13,2020-09-05 17:05,2020-12-21 18:25,2020-09-05 17:05,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000002  arXiv: 2002.08484,,/Users/angelica/Zotero/storage/3TYCLPFP/Pruthi et al. - 2020 - Estimating Training Data Influence by Tracking Gra.pdf; /Users/angelica/Zotero/storage/8XN5TFHS/2002.html,,Other-org; TechSafety; AmbiguosSafety,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper presents the TrackIn method for tracking the influence of training datapoints on the loss on a test datapoint. The purpose of the method is to discover influential training points for decisions made on the testing set. This is defined (loosely) for a training point **x** and test point **z** as the total change in loss on **z** caused by training on **x**. They present several approximations and methods for calculating this quantity efficiently, *allowing them to scale their method to ResNet 50 models trained on ImageNet*

The standard method of evaluation for these kinds of methods is finding mislabelled examples in the training dataset. Mislabelled examples are likely to have a strong positive influence on their own loss (strong as they're outliers, and positive as they'll reduce their own loss). Sorting the training dataset in decreasing order of this self-influence, we should hence expect to see more mislabelled examples at the beginning of the list. We can measure what proportion of mislabelled examples is present in each different initial segments of the list. The authors perform this experiment on CiFAR, first training a model to convergence, and then mislabelling 10% of the training set as the next highest predicted class, and then retraining a new model on which TrackIn is run. *When compared to the two previous methods from the literature (Influence Functions and Representer Points), TrackIn recovers more than 80% of the mislabelled data in the first 20% of the ranking, whereas the other methods recover less than 50% at the same point. For all segments TrackIn does significantly better.*

They demonstrate the method on a variety of domains, including NLP tasks and vision tasks. The influential examples found seem reasonable, but there's no quantification of these results."
,manuscript,2020,"Rivera, Corban G.; Lyons, Olivia; Summitt, Arielle; Fatima, Ayman; Pak, Ji; Shao, William; Chalmers, Robert; Englander, Aryeh; Staley, Edward W.; Wang, I.-Jeng; Llorens, Ashley J.",TanksWorld: A Multi-Agent Environment for AI Safety Research,,,,,http://arxiv.org/abs/2002.11174,"The ability to create artificial intelligence (AI) capable of performing complex tasks is rapidly outpacing our ability to ensure the safe and assured operation of AI-enabled systems. Fortunately, a landscape of AI safety research is emerging in response to this asymmetry and yet there is a long way to go. In particular, recent simulation environments created to illustrate AI safety risks are relatively simple or narrowly-focused on a particular issue. Hence, we see a critical need for AI safety research environments that abstract essential aspects of complex real-world applications. In this work, we introduce the AI safety TanksWorld as an environment for AI safety research with three essential aspects: competing performance objectives, human-machine teaming, and multi-agent competition. The AI safety TanksWorld aims to accelerate the advancement of safe multi-agent decision-making algorithms by providing a software framework to support competitions with both system performance and safety objectives. As a work in progress, this paper introduces our research objectives and learning environment with reference code and baseline performance metrics to follow in a future work.",2020-02-25,2020-09-05 18:48,2020-12-21 18:27,2020-09-05 18:48,,,,,,,TanksWorld,,,,,,,,,,,,arXiv.org,,ZSCC: 0000000  arXiv: 2002.11174,,/Users/angelica/Zotero/storage/ATW7PHTZ/Rivera et al. - 2020 - TanksWorld A Multi-Agent Environment for AI Safet.pdf; /Users/angelica/Zotero/storage/CXJSRG7S/2002.html,,Other-org; TechSafety,Computer Science - Artificial Intelligence; Computer Science - Multiagent Systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper presents TanksWorld, a simulation environment that attempts to illustrate three important aspects of real-world AI safety challenges: competing performance objectives, human-machine learning, and multi-agent competition. TanksWorld consists of two teams of N vs. N tanks. Tanks move and shoot while navigating in a closed arena with obstacles. Tanks are rewarded for killing opponent tanks and penalized for killing neutral and allied tanks according to a specified reward function. Each tank is controlled by either its own AI or a special policy meant to mimic a 'human' teammate. Each individual tank can only see a small portion of its environment, and must communicate with other teammates to gain more information. The following parameters can be varied to emphasize different research challenges:
- The communication range between tanks -- meant to represent environmental uncertainty.
- The number of neutral tanks and obstacles -- meant to represent the extent to which tanks must care about 'safety', i.e. avoid collateral damage.
- The control policies of teammates -- meant to represent the variability of human-machine teams."
,manuscript,2020,"Lynch, Corey; Sermanet, Pierre",Grounding Language in Play,,,,,http://arxiv.org/abs/2005.07648,"Natural language is perhaps the most versatile and intuitive way for humans to communicate tasks to a robot. Prior work on Learning from Play (LfP) [Lynch et al, 2019] provides a simple approach for learning a wide variety of robotic behaviors from general sensors. However, each task must be specified with a goal image---something that is not practical in open-world environments. In this work we present a simple and scalable way to condition policies on human language instead. We extend LfP by pairing short robot experiences from play with relevant human language after-the-fact. To make this efficient, we introduce multicontext imitation, which allows us to train a single agent to follow image or language goals, then use just language conditioning at test time. This reduces the cost of language pairing to less than 1% of collected robot experience, with the majority of control still learned via self-supervised imitation. At test time, a single agent trained in this manner can perform many different robotic manipulation skills in a row in a 3D environment, directly from images, and specified only with natural language (e.g. ""open the drawer...now pick up the block...now press the green button...""). Finally, we introduce a simple technique that transfers knowledge from large unlabeled text corpora to robotic learning. We find that transfer significantly improves downstream robotic manipulation. It also allows our agent to follow thousands of novel instructions at test time in zero shot, in 16 different languages. See videos of our experiments at language-play.github.io",2020-05-15,2020-08-31 18:20,2020-12-21 18:19,2020-08-31 18:20,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000001  arXiv: 2005.07648,,/Users/angelica/Zotero/storage/W35Q9F4C/Lynch and Sermanet - 2020 - Grounding Language in Play.pdf; /Users/angelica/Zotero/storage/FNXJPVLS/2005.html,,Other-org; NotSafety; AmbiguosSafety,Computer Science - Artificial Intelligence; Computer Science - Robotics; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper presents a new approach to learning to follow natural language human instruction in a robotics setting. It builds on similar ideas to <@Learning Latent Plans from Play@>, in that it uses unsupervised ""play"" data (trajectories of humans playing on the robot with no goal in mind).

The paper combines several ideas to enable training a policy which can follow natural language instructions with only limited human annotations.
* In *Hindsight Instruction Pairing*, human annotators watch small trajectories from the play data, and label them with the instruction which is being completed in the clip. This instruction can take any form, and means we don't need to choose the instructions and ask humans to perform specific tasks.
* *Multicontext Imitation Learning* is a method designed to allow goal-conditioned policies to be learned with multiple different types of goals. For example, we can have lots of example trajectories where the goal is an end state image (as these can be generated automatically without humans), and just a small amount of example trajectories where the goal is a natural language instruction (gathered using *Hindsight Instruction Pairing*). The approach is to learn a goal embedding network for each type of goal specification, and a single shared policy which takes the goal embedding as input.

Combining these two methods enables them to train a policy and embedding networks end to end using imitation learning from a large dataset of (trajectory, image goal) pairs and a small dataset of (trajectory, natural language goal) pairs. The policy can follow very long sequences of natural language instructions in a fairly complex grasping environment with a variety of buttons and objects. Their method performs better than the Learning from Play (LfP) method, even though LfP uses a goal image as the goal conditioning, instead of a natural language instruction.

Further, they propose that instead of learning the goal embedding for the natural language instructions, they use a pretrained large language model to produce the embeddings. This improves the performance of their method over learning the embedding from scratch, which the authors claim is the first example of the knowledge in large language models being transferred and improving performance in a robotics domain. This model also performs well when they create purposefully out of distribution natural language instructions (i.e. with weird synonyms, or google-translated from a different language)."
,manuscript,2020,"Adiwardana, Daniel; Luong, Minh-Thang; So, David R.; Hall, Jamie; Fiedel, Noah; Thoppilan, Romal; Yang, Zi; Kulshreshtha, Apoorv; Nemade, Gaurav; Lu, Yifeng; Le, Quoc V.",Towards a Human-like Open-Domain Chatbot,,,,,http://arxiv.org/abs/2001.09977,"We present Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token. We also propose a human evaluation metric called Sensibleness and Specificity Average (SSA), which captures key elements of a human-like multi-turn conversation. Our experiments show strong correlation between perplexity and SSA. The fact that the best perplexity end-to-end trained Meena scores high on SSA (72% on multi-turn evaluation) suggests that a human-level SSA of 86% is potentially within reach if we can better optimize perplexity. Additionally, the full version of Meena (with a filtering mechanism and tuned decoding) scores 79% SSA, 23% higher in absolute SSA than the existing chatbots we evaluated.",2020-02-27,2020-09-07 18:19,2020-12-21 17:53,2020-09-07 18:19,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000057  arXiv: 2001.09977,,/Users/angelica/Zotero/storage/WDD2CM87/Adiwardana et al. - 2020 - Towards a Human-like Open-Domain Chatbot.pdf; /Users/angelica/Zotero/storage/BHZLQRPI/2001.html,,Other-org; TechSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Neural and Evolutionary Computing; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper presents a chatbot called Meena that reaches near human-level performance for measures of human likeness. The authors mined social media to find 341 GB of public domain conversations, and trained an [evolved transformer](https://arxiv.org/abs/1901.11117) on those conversations. To test its performance, they devised a metric they call Sensibility and Specificity (SSA) which measures how much sense the chatbot's responses make in context, as well as whether they were specific. SSA was tightly correlated with perplexity and a subjective measure of human likeness, suggesting that optimizing for perplexity will translate to greater conversational ability. Meena substantially improved on the state of the art, including both hand-crafted bots like [Mitsuku](https://en.wikipedia.org/wiki/Mitsuku) and the neural model [DialoGPT](https://arxiv.org/abs/1911.00536), though it still falls short of human performance. You can read some conversation transcrips [here](https://github.com/google-research/google-research/blob/master/meena/meena.txt); many of the responses from Meena are very human-like.

See also [Import AI #183](https://jack-clark.net/2020/02/03/import-ai-183-curve-fitting-conversation-with-meena-gans-show-us-our-climate-change-future-and-what-compute-data-arbitrage-means/)"
,manuscript,2020,"Henighan, Tom; Kaplan, Jared; Katz, Mor; Chen, Mark; Hesse, Christopher; Jackson, Jacob; Jun, Heewoo; Brown, Tom B.; Dhariwal, Prafulla; Gray, Scott; Hallacy, Chris; Mann, Benjamin; Radford, Alec; Ramesh, Aditya; Ryder, Nick; Ziegler, Daniel M.; Schulman, John; Amodei, Dario; McCandlish, Sam",Scaling Laws for Autoregressive Generative Modeling,,,,,http://arxiv.org/abs/2010.14701,"We identify empirical scaling laws for the cross-entropy loss in four domains: generative image modeling, video modeling, multimodal image$\leftrightarrow$text models, and mathematical problem solving. In all cases autoregressive Transformers smoothly improve in performance as model size and compute budgets increase, following a power-law plus constant scaling law. The optimal model size also depends on the compute budget through a power-law, with exponents that are nearly universal across all data domains. The cross-entropy loss has an information theoretic interpretation as $S($True$) + D_{\mathrm{KL}}($True$||$Model$)$, and the empirical scaling laws suggest a prediction for both the true data distribution's entropy and the KL divergence between the true and model distributions. With this interpretation, billion-parameter Transformers are nearly perfect models of the YFCC100M image distribution downsampled to an $8\times 8$ resolution, and we can forecast the model size needed to achieve any given reducible loss (ie $D_{\mathrm{KL}}$) in nats/image for other resolutions. We find a number of additional scaling laws in specific domains: (a) we identify a scaling relation for the mutual information between captions and images in multimodal models, and show how to answer the question ""Is a picture worth a thousand words?""; (b) in the case of mathematical problem solving, we identify scaling laws for model performance when extrapolating beyond the training distribution; (c) we finetune generative image models for ImageNet classification and find smooth scaling of the classification loss and error rate, even as the generative loss levels off. Taken together, these results strengthen the case that scaling laws have important implications for neural network performance, including on downstream tasks.",2020-11-05,2020-12-19 04:02,2020-12-20 22:11,2020-12-19 04:02,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s0]  ACC: 0  arXiv: 2010.14701,,/Users/angelica/Zotero/storage/9YC4HDGP/2010.html; /Users/angelica/Zotero/storage/T6MMZPI9/Henighan et al. - 2020 - Scaling Laws for Autoregressive Generative Modelin.pdf,,NotSafety; AmbiguosSafety; Open-AI,Computer Science - Machine Learning; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper looks at scaling laws for generative Transformer models of images (predicting pixels or parts of image encodings), videos (predicting frames of image encodings), multimodal image <-> text (predicting captions based on images or images based on captions), and mathematical problem solving (predicting answers to auto-generated questions about algebra, arithmetic, calculus, comparisons, integer properties, measurement, polynomials, and probability). The authors find that:

- Cross-entropy loss as a function of compute follows a power law + constant in all these data modalities (just as it does <@in language@>(@Scaling Laws for Neural Language Models@)). Information theoretically, this can be interpreted as scaling a 'reducible loss' which estimates the KL divergence between the true and model distributions, and an 'irreducible loss' which estimates the entropy of the true data distribution.
- Performance on ImageNet classification fine-tuned from their generative image model also follows such a power law, whereas ImageNet classification trained *from scratch* actually gets worse with sufficiently large model sizes. Interestingly, this classification power law continues even past model sizes where the generative cross-entropy loss starts bending as a result of irreducible loss. The authors conclude that approaching the irreducible loss for some dataset does not necessarily indicate diminishing returns for representation quality or semantic content.
- Optimal model size as a function of compute follows a power law with an exponent very close to ~0.7 for all data modalities they've studied so far. This implies that in the current compute regime, as compute budgets grow, it's best to devote a majority of compute towards making models bigger and a minority towards training on more data.
- Larger models perform better on extrapolating to math problems more difficult than those seen in training, but only insofar as they do better on the training distribution (no benefits to 'strong generalization').
- Larger models are able to take advantage of more multimodal information, but the scaling is extremely slow-- a 1-billion-parameter model uses 10% of the information in a caption to define an image, while using 20% of the information would require a 3-trillion-parameter model.

As in the <@language models paper@>(@Scaling Laws for Neural Language Models@), extrapolating the steep power laws found for optimally-used compute seems to eventually paradoxically result in loss lower than the bound given by shallower power laws for optimally-used training data. The authors offer a potential hypothesis for resolving this inconsistency-- in the regime of less compute and smaller model sizes, increasing model size effectively increases the amount of information you extract from each data point you train on, resulting in the steepness of the current compute law. As compute increases past a certain point, however, the amount of information extracted per data point approaches the maximum amount possible, so the curve switches to a shallower regime and marginal compute should be used increasingly on dataset increases rather than model size increases. If this hypothesis is true, we should eventually expect the scaling laws for compute to bend towards laws set by dataset size, and perhaps should think they will ultimately be set by trends for overfitting (see [this post](https://www.alignmentforum.org/posts/diutNaWF669WgEt3v/the-scaling-inconsistency-openai-s-new-insight) for another explanation of this)."
,manuscript,2018,"Arora, Saurabh; Doshi, Prashant; Banerjee, Bikramjit",A Framework and Method for Online Inverse Reinforcement Learning,,,,,http://arxiv.org/abs/1805.07871,"Inverse reinforcement learning (IRL) is the problem of learning the preferences of an agent from the observations of its behavior on a task. While this problem has been well investigated, the related problem of {\em online} IRL---where the observations are incrementally accrued, yet the demands of the application often prohibit a full rerun of an IRL method---has received relatively less attention. We introduce the first formal framework for online IRL, called incremental IRL (I2RL), and a new method that advances maximum entropy IRL with hidden variables, to this setting. Our formal analysis shows that the new method has a monotonically improving performance with more demonstration data, as well as probabilistically bounded error, both under full and partial observability. Experiments in a simulated robotic application of penetrating a continuous patrol under occlusion shows the relatively improved performance and speed up of the new method and validates the utility of online IRL.",2018-05-20,2020-11-14 00:53,2020-12-20 20:55,2020-11-14 00:52,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000001  arXiv: 1805.07871,,/Users/angelica/Zotero/storage/ZGN9GCXC/1805.html; /Users/angelica/Zotero/storage/3UEZDY95/Arora et al. - 2018 - A Framework and Method for Online Inverse Reinforc.pdf,,Other-org; NotSafety,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper introduces Incremental Inverse Reinforcement Learning (I2RL), where the agent continually gets new demonstrations from an expert, and has to update the estimate of the reward function in real time. The running example is a robot that has to navigate to a goal location without being seen by two guards that are patrolling. The robot needs to infer the rewards of the two guards in order to predict what they will do and plan around them. Since the guards are sometimes out of sight, we get demonstrations _with occlusion_, that is, some of the states in the demonstrations are hidden.

In the batch setting, this is solved with Latent Maximum Entropy IRL. To deal with occluded states Z, we define a probability distribution Pr(Z | Y, theta), where Y is the visible states and theta is the reward weights. Then, you can use expectation maximization to find theta -- in the expectation step, you compute feature expectations of the demonstrations (taking an expectation over hidden states Z), and in the maximization step, you compute the reward weights using the feature expectations as in standard maximum entropy IRL. The authors show how to extend this algorithm to the incremental setting where you only keep the reward weights, the feature expectations, and the number of past demonstrations as statistics. They show some convergence guarantees and evaluate on their running example of a robot that must evade guards."
,manuscript,2020,"Lepikhin, Dmitry; Lee, HyoukJoong; Xu, Yuanzhong; Chen, Dehao; Firat, Orhan; Huang, Yanping; Krikun, Maxim; Shazeer, Noam; Chen, Zhifeng",GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding,,,,,http://arxiv.org/abs/2006.16668,"Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.",2020-06-30,2020-08-28 18:05,2020-12-21 18:17,2020-08-28 18:05,,,,,,,GShard,,,,,,,,,,,,arXiv.org,,ZSCC: 0000005  arXiv: 2006.16668,,/Users/angelica/Zotero/storage/ZM6Z55RI/Lepikhin et al. - 2020 - GShard Scaling Giant Models with Conditional Comp.pdf; /Users/angelica/Zotero/storage/4Q8XLAAU/2006.html,,Other-org; NotSafety; AmbiguosSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper introduces GShard, a module that makes it easy to write parallel computation patterns with minimal changes to existing model code. GShard automatically does a lot of the work of splitting computations across machines, enabling the easy creation of much larger models than before.

The authors use GShard to train a 600 billion parameter multilingual Transformer translation model that's wide, rather than deep (36 layers). They use a ""mixture of experts"" model where some of the individual feed-forward networks in the Transformer are replaced with a set of feed-forward networks-- each one an ""expert"" in some part of the translation. The experts are distributed across different machines, and the function for sending inputs to experts is learned, with each input being sent to the top two most relevant experts. Since each expert only has to process a fraction of all the inputs, the amount of computation needed is dramatically less than if every input were fed through a single, larger network. This decrease in needed computation comes with a decrease in the amount of weight sharing done by the network.

The paper compares the 600 billion parameter model's performance to several other smaller models as well as a 96-layer deep model with only 2.3 billion parameters. For the wide networks, the authors find that in general, larger models do better, but that at some point the larger model starts doing worse for very ""low-resource"" languages-- languages that don't have much training data available. The authors argue that this is because the low-resource languages benefit from ""positive language transfer"", an effect where weights encode knowledge learned from training on other languages that can then be applied to the low-resource ones. As you increase the number of experts in the wide model past a certain point, the amount of training that each expert does decreases, so there's less positive language transfer to low-resource languages within each expert.

They also find that deeper networks are more sample efficient, reaching better test error with the same amount of training examples, but are less computationally efficient (given current constraints). The 600 billion parameter, 36-layer model takes 22.4 TPU core years and 4 days to train, reaching a score on the BLEU benchmark of 44.3. The 2.3 billion parameter, 96-layer model takes 235 TPU core years and 42 days to train, reaching a score on the BLEU benchmark of 36.9."
,manuscript,2018,"Clavera, Ignasi; Rothfuss, Jonas; Schulman, John; Fujita, Yasuhiro; Asfour, Tamim; Abbeel, Pieter",Model-Based Reinforcement Learning via Meta-Policy Optimization,,,,,http://arxiv.org/abs/1809.05214,"Model-based reinforcement learning approaches carry the promise of being data efficient. However, due to challenges in learning dynamics models that sufficiently match the real-world dynamics, they struggle to achieve the same asymptotic performance as model-free methods. We propose Model-Based Meta-Policy-Optimization (MB-MPO), an approach that foregoes the strong reliance on accurate learned dynamics models. Using an ensemble of learned dynamic models, MB-MPO meta-learns a policy that can quickly adapt to any model in the ensemble with one policy gradient step. This steers the meta-policy towards internalizing consistent dynamics predictions among the ensemble while shifting the burden of behaving optimally w.r.t. the model discrepancies towards the adaptation step. Our experiments show that MB-MPO is more robust to model imperfections than previous model-based approaches. Finally, we demonstrate that our approach is able to match the asymptotic performance of model-free methods while requiring significantly less experience.",2018-09-13,2019-12-18 02:53,2020-12-20 21:21,2019-12-18 02:53,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000070  arXiv: 1809.05214,,/Users/angelica/Zotero/storage/WZ7NJUR3/1809.html; /Users/angelica/Zotero/storage/LZR8VRWA/Clavera et al. - 2018 - Model-Based Reinforcement Learning via Meta-Policy.pdf,,NotSafety; CHAI-Berkeley,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper introduces a new approach to model-based RL, called Model-Based Meta-Policy-Optimisation (MB-MPO), which doesn't require the dynamics models to be as accurate. It does so by learning an ensemble of dynamics models each trained on different subsets of the data, and then using meta-learning (specifically MAML) to find a policy which adapts well to any of these models within one step of gradient descent. This approach is a form of regularisation of policy learning, and achieves much greater sample efficiency without compromising performance: MB-MPO does just as well as top model-free algorithms in various Mujoco continuous-control environments, while requiring between 10 and 100 times fewer samples. Experiments suggest that it does so by having higher plasticity in regions with high dynamics model uncertainty. See also [Import AI](https://jack-clark.net/2018/09/25/import-ai-113-why-satellitesai-gives-us-a-global-eye-industry-pays-academia-to-say-sorry-for-strip-mining-it-and-kindred-researchers-seek-robot-standardization/)."
,manuscript,2017,"Doshi-Velez, Finale; Kim, Been",Towards A Rigorous Science of Interpretable Machine Learning,,,,,http://arxiv.org/abs/1702.08608,"As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.",2017-03-02,2020-08-31 17:55,2020-12-21 18:04,2020-08-31 17:55,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000836  arXiv: 1702.08608,,/Users/angelica/Zotero/storage/JT94RUDJ/Doshi-Velez and Kim - 2017 - Towards A Rigorous Science of Interpretable Machin.pdf; /Users/angelica/Zotero/storage/2N27VUEW/1702.html,,Other-org; TechSafety; AmbiguosSafety,Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper from 2017 discusses the field of interpretability research, and how it can be made more rigorous and well-defined. The authors first highlight the problem of defining interpretability in the first place - they don't have a resolution to this problem, but suggest that we can think of interpretability in terms of what it's used for. They claim that interpretability is used for confirming other important desiderata in ML systems, which stem from an incompleteness in the problem formalization. For example, if we want a system to be unbiased but aren't able to formally specify this in the reward function, or the reward we're optimising for is only a proxy of the true reward, then we could use interpretability to inspect our model and see whether it's reasoning how we want it to.

The authors next move on to discussing how we can evaluate interpretability methods, providing a taxonomy of different evaluation methods: Application-grounded is when the method is evaluated in the context it will actually be used in, by real humans (i.e. doctors getting explanations for AI diagnoses); Human-grounded is about conducting simpler human-subject experiments (who are perhaps not domain experts) using possibly simpler tasks than what the intended purpose of the method is; Functionally-grounded is where no humans are involved in the experiments, and instead some formal notion of interpretability is measured for the method to evaluate its quality. Each of these evaluation methods can be used in different circumstances, depending on the method and the context it will be used in.

Finally, the authors propose a data-driven approach to understanding the factors which are important in interpretability. They propose to try and create a dataset of applications of machine learning models to tasks, and then analyse this dataset to find important factors. They list some possible task- and method- related factors, and then conclude with recommendations to researchers doing interpretability."
,manuscript,2020,"Scheller, Christian; Schraner, Yanick; Vogel, Manfred",Sample Efficient Reinforcement Learning through Learning from Demonstrations in Minecraft,,,,,http://arxiv.org/abs/2003.06066,"Sample inefficiency of deep reinforcement learning methods is a major obstacle for their use in real-world applications. In this work, we show how human demonstrations can improve final performance of agents on the Minecraft minigame ObtainDiamond with only 8M frames of environment interaction. We propose a training procedure where policy networks are first trained on human data and later fine-tuned by reinforcement learning. Using a policy exploitation mechanism, experience replay and an additional loss against catastrophic forgetting, our best agent was able to achieve a mean score of 48. Our proposed solution placed 3rd in the NeurIPS MineRL Competition for Sample-Efficient Reinforcement Learning.",2020-03-12,2020-09-05 18:02,2020-12-21 18:42,2020-09-05 18:02,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000004  arXiv: 2003.06066,,/Users/angelica/Zotero/storage/XR6TWEI8/Scheller et al. - 2020 - Sample Efficient Reinforcement Learning through Le.pdf; /Users/angelica/Zotero/storage/HX7N352C/2003.html,,Other-org; NotSafety; AmbiguosSafety,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper explains the technique used by the 3rd place team in the MineRL competition (summarized above). They used behavior cloning to train their neural net on human demonstrations, and then used reinforcement learning (specifically, IMPALA) with experience replay and advantage clipping to improve. There are more details about their architecture and design choices in the paper."
,manuscript,2020,"Kaplan, Jared; McCandlish, Sam; Henighan, Tom; Brown, Tom B.; Chess, Benjamin; Child, Rewon; Gray, Scott; Radford, Alec; Wu, Jeffrey; Amodei, Dario",Scaling Laws for Neural Language Models,,,,,http://arxiv.org/abs/2001.08361,"We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.",2020-01-22,2020-08-21 20:19,2020-12-20 22:11,2020-08-21 20:19,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000014  arXiv: 2001.08361,,/Users/angelica/Zotero/storage/MGNNQMVW/Kaplan et al. - 2020 - Scaling Laws for Neural Language Models.pdf; /Users/angelica/Zotero/storage/GLKNCSKK/2001.html,,NotSafety; Open-AI,Computer Science - Machine Learning; Statistics - Machine Learning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper empirically measures the effect of scaling model complexity, data, and computation on the cross entropy loss for neural language models. A few results that I would highlight are:

_Performance depends strongly on scale, weakly on model shape:_ Loss depends more strongly on the number of parameters, the size of the dataset, and the amount of compute used for training than on architecture hyperparameters.

_Smooth power laws:_ All three of these show power-law relationships that don’t flatten out even at the highest performance they reached.

_Sample efficiency:_ Larger models are more efficient than small models in both compute and data. For maximum computation efficiency, it is better to train large models and stop before convergence.

There are lots of other interesting conclusions in the paper not included here; section 1.1 provides a very nice one page summary of these conclusions, which I'd recommend you read for more information."
,manuscript,2019,"Eckersley, Peter",Impossibility and Uncertainty Theorems in AI Value Alignment (or why your AGI should not have a utility function),,,,,http://arxiv.org/abs/1901.00064,"Utility functions or their equivalents (value functions, objective functions, loss functions, reward functions, preference orderings) are a central tool in most current machine learning systems. These mechanisms for defining goals and guiding optimization run into practical and conceptual difficulty when there are independent, multi-dimensional objectives that need to be pursued simultaneously and cannot be reduced to each other. Ethicists have proved several impossibility theorems that stem from this origin; those results appear to show that there is no way of formally specifying what it means for an outcome to be good for a population without violating strong human ethical intuitions (in such cases, the objective function is a social welfare function). We argue that this is a practical problem for any machine learning system (such as medical decision support systems or autonomous weapons) or rigidly rule-based bureaucracy that will make high stakes decisions about human lives: such systems should not use objective functions in the strict mathematical sense. We explore the alternative of using uncertain objectives, represented for instance as partially ordered preferences, or as probability distributions over total orders. We show that previously known impossibility theorems can be transformed into uncertainty theorems in both of those settings, and prove lower bounds on how much uncertainty is implied by the impossibility results. We close by proposing two conjectures about the relationship between uncertainty in objectives and severe unintended consequences from AI systems.",2019-03-04,2020-11-14 00:58,2020-12-21 18:04,2020-11-14 00:58,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s0]  ACC: 16  arXiv: 1901.00064,,/Users/angelica/Zotero/storage/22Y3KWDY/1901.html; /Users/angelica/Zotero/storage/BMB5Y843/Eckersley - 2019 - Impossibility and Uncertainty Theorems in AI Value.pdf,,Other-org; TechSafety,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper discusses some impossibility theorems related to the Repugnant conclusion in population ethics (i.e. theorems showing that no moral theory simultaneously satisfies certain sets of intuitively desirable properties). Peter argues that in the context of AI it's best to treat these theorems as uncertainty results, either by allowing incommensurate outcomes or by allowing probabilistic moral judgements. He hypothesises that ""the emergence of instrumental subgoals is deeply connected to moral certainty"", and so implementing uncertain objective functions is a path to making AI safer."
,manuscript,2020,"Chen, Xinlei; Fan, Haoqi; Girshick, Ross; He, Kaiming",Improved Baselines with Momentum Contrastive Learning,,,,,http://arxiv.org/abs/2003.04297,"Contrastive unsupervised learning has recently shown encouraging progress, e.g., in Momentum Contrast (MoCo) and SimCLR. In this note, we verify the effectiveness of two of SimCLR’s design improvements by implementing them in the MoCo framework. With simple modiﬁcations to MoCo—namely, using an MLP projection head and more data augmentation—we establish stronger baselines that outperform SimCLR and do not require large training batches. We hope this will make state-of-the-art unsupervised learning research more accessible. Code will be made public.",2020-03-09,2020-08-31 18:58,2020-12-21 18:01,2020-08-31 18:58,,,,,,,,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000042  ACC: 42  arXiv: 2003.04297,,/Users/angelica/Zotero/storage/AQN4AM47/Chen et al. - 2020 - Improved Baselines with Momentum Contrastive Learn.pdf,,Other-org; NotSafety; AmbiguosSafety,Computer Science - Computer Vision and Pattern Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper applies the insights from the SimCLR paper to the MoCo framework: it adds an extra hidden layer on top of the representations while training on the contrastive loss, and adds the blur data augmentation. This results in a new SOTA on self-supervised representation learning for images."
,manuscript,2020,"Kostrikov, Ilya; Yarats, Denis; Fergus, Rob",Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels,,,,,http://arxiv.org/abs/2004.13649,"We propose a simple data augmentation technique that can be applied to standard model-free reinforcement learning algorithms, enabling robust learning directly from pixels without the need for auxiliary losses or pre-training. The approach leverages input perturbations commonly used in computer vision tasks to transform input examples, as well as regularizing the value function and policy. Existing model-free approaches, such as Soft Actor-Critic (SAC) [22], are not able to train deep networks effectively from image pixels. However, the addition of our augmentation method dramatically improves SAC’s performance, enabling it to reach state-of-the-art performance on the DeepMind control suite, surpassing model-based [23, 38, 24] methods and recently proposed contrastive learning [50]. Our approach, which we dub DrQ: Data-regularized Q, can be combined with any model-free reinforcement learning algorithm. We further demonstrate this by applying it to DQN [43] and signiﬁcantly improve its data-efﬁciency on the Atari 100k [31] benchmark. An implementation can be found at https://sites. google.com/view/data-regularized-q.",2020-06-11,2020-08-31 19:00,2020-12-21 18:16,2020-08-31 19:00,,,,,,,Image Augmentation Is All You Need,,,,,,,en,,,,,arXiv.org,,ZSCC: 0000014  ACC: 14  arXiv: 2004.13649,,/Users/angelica/Zotero/storage/SY5MM8DU/Kostrikov et al. - 2020 - Image Augmentation Is All You Need Regularizing D.pdf,,Other-org; NotSafety; AmbiguosSafety,Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Computer Vision and Pattern Recognition; Electrical Engineering and Systems Science - Image and Video Processing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper applies data augmentation to Q-learning algorithms, again without a contrastive loss. Specifically, they suggest that the Q-values of states should be invariant to data augmentations (e.g. random translations, which is what they use), and so any time we need to estimate a Q-value, we can reduce the variance of this estimate by sampling multiple data augmentations of the state, and averaging the predicted Q-values for each of them. They apply this to Soft Actor-Critic (SAC) and find that it significantly improves results."
,manuscript,2018,"Pereira, Ramon Fraga; Meneguzzi, Felipe",Heuristic Approaches for Goal Recognition in Incomplete Domain Models,,,,,http://arxiv.org/abs/1804.05917,"Recent approaches to goal recognition have progressively relaxed the assumptions about the amount and correctness of domain knowledge and available observations, yielding accurate and efficient algorithms. These approaches, however, assume completeness and correctness of the domain theory against which their algorithms match observations: this is too strong for most real-world domains. In this paper, we develop goal recognition techniques that are capable of recognizing goals using \textit{incomplete} (and possibly incorrect) domain theories. We show the efficiency and accuracy of our approaches empirically against a large dataset of goal and plan recognition problems with incomplete domains.",2018-04-16,2020-11-14 00:45,2020-12-20 21:13,2020-11-14 00:45,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000001  arXiv: 1804.05917,,/Users/angelica/Zotero/storage/QSH9H5GP/1804.html; /Users/angelica/Zotero/storage/86ZFTB49/Pereira and Meneguzzi - 2018 - Heuristic Approaches for Goal Recognition in Incom.pdf,,Other-org; NotSafety,Computer Science - Artificial Intelligence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The planning community works on algorithms that can plan given a _symbolic_ definition of the environment, how actions affect the environment, and the goal state; analogous to reinforcement learning. The task of inverting the optimal behavior to infer the goal is called goal recognition or plan recognition (analogous to inverse reinforcement learning). This paper looks at goal recognition where the models of the world are incomplete, so that there are _possible_ preconditions and effects of actions. They extract potential _landmarks_ from the plan, which are things (facts or actions) that must happen in order to achieve the goal, and then suggest two heuristics for how to use the landmarks to rank among possible goals."
XRKE5R8P,report,2020,"Flournoy, Michèle A; Haines, Avril; Chefitz, Gabrielle",Building Trust Through Testing,,,,,https://cset.georgetown.edu/wp-content/uploads/Building-Trust-Through-Testing.pdf,,2020,2022-01-30 04:48:44,2022-01-30 04:48:44,,,,,,,,,,,,,WestExec Advisors,,,,,,,,,ZSCC: 0000001,,/Users/jacquesthibodeau/Zotero/storage/2SCQ6DTV/Flournoy et al. - 2020 - Building Trust Through Testing.pdf,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PSX2XFIQ,report,2020,"Fitzgerald, McKenna; Boddy, Aaron; Baum, Seth","2020 Survey of Artificial General Intelligence Projects for Ethics, Risk, and Policy",,,,,https://www.ssrn.com/abstract=3070741,,2020,2022-01-30 04:48:11,2022-01-30 04:48:11,2021-10-31 19:23:44,,,,,,,,,,,,Global Catastrophic Risk Institute,,en,,,,,DOI.org (Crossref),,ZSCC: 0000001,,/Users/jacquesthibodeau/Zotero/storage/DQQQHZAB/Baum - 2017 - A Survey of Artificial General Intelligence Projec.pdf,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9US7JVWW,report,2020,"Lohn, Andrew",Hacking AI,,,,,https://cset.georgetown.edu/publication/hacking-ai/,"Machine learning systems’ vulnerabilities are pervasive. Hackers and adversaries can easily exploit them. As such, managing the risks is too large a task for the technology community to handle alone. In this primer, Andrew Lohn writes that policymakers must understand the threats well enough to assess the dangers that the United States, its military and intelligence services, and its civilians face when they use machine learning.",2020-12,2022-01-30 04:47:49,2022-01-30 04:47:49,2021-10-31 18:59:26,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: 0000002,,/Users/jacquesthibodeau/Zotero/storage/5H3AC6WR/hacking-ai.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HUDHI962,report,2020,"Page, Michael; Aiken, Catherine; Murdick, Dewey",Future Indices,,,,,https://cset.georgetown.edu/publication/future-indices/,Foretell is CSET's crowd forecasting pilot project focused on technology and security policy. It connects historical and forecast data on near-term events with the big-picture questions that are most relevant to policymakers. This issue brief uses recent forecast data to illustrate Foretell’s methodology.,2020-10-19,2022-01-30 04:47:49,2022-01-30 04:47:49,2021-11-08 23:47:33,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/SEWFFFFP/future-indices.html,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
75Q2IHR5,report,2020,"Althaus, David; Baumann, Tobias",Reducing long-term risks from malevolent actors,,,,,https://longtermrisk.org/reducing-long-term-risks-from-malevolent-actors/,"Summary Dictators who exhibited highly narcissistic, psychopathic, or sadistic traits were involved in some of the greatest catastrophes in human history.  Malevolent individuals in positions of power could negatively affect humanity’s long-term trajectory by, for example, exacerbating international conflict or other broad risk factors. Malevolent humans with access to advanced technology—such as whole brain emulation […]",2020-07-07,2022-01-30 04:51:36,2022-01-30 04:51:36,2020-08-20 20:10:59,,,,,,,,,,,,Center on Long-Term Risk,,en-US,,,,,,,ZSCC: 0000003,,/Users/jacquesthibodeau/Zotero/storage/8TP4B2UC/Althaus and Baumann - 2020 - Reducing long-term risks from malevolent actors.pdf; /Users/jacquesthibodeau/Zotero/storage/7NBW623R/reducing-long-term-risks-from-malevolent-actors.html,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3JEMBWP9,report,2020,"Clifton, Jesse","Cooperation, Conflict, and Transformative Artificial Intelligence - A Research Agenda",,,,,https://longtermrisk.org/files/Cooperation-Conflict-and-Transformative-Artificial-Intelligence-A-Research-Agenda.pdf,,2020-03,2022-01-30 04:51:07,2022-01-30 04:51:07,2020-11-22 07:42:34,,,,,,,,,,,,Center on Long-Term Risk,,,,,,,,,ZSCC: 0000002,,/Users/jacquesthibodeau/Zotero/storage/8FM6BFXK/Cooperation-Conflict-and-Transformative-Artificial-Intelligence-A-Research-Agenda.pdf,,CLR; MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
M6UT95U9,report,2019,"Kemp, Luke; Cihon, Peter; Maas, Matthijs M; Belfield, Haydn; Ó hÉigeartaigh, Seán; Leung, Jade; Cremer, Zoe",UN High-level Panel on Digital Cooperation: A Proposal for International AI Governance,,,,,https://digitalcooperation.org/wp-content/uploads/2019/02/Luke_Kemp_Submission-to-the-UN-High-Level-Panel-on-Digital-Cooperation-2019-Kemp-et-al.pdf,,2019,2022-01-30 04:50:26,2022-01-30 04:50:26,2020-12-12,,,,,,,,,,,,Centre for the Study of Existential Risk and Leverhulme Centre for the Future of Intelligence,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: 3,,/Users/jacquesthibodeau/Zotero/storage/CP3XZSMU/Kemp et al. - 2019 - UN High-level Panel on Digital Cooperation A Prop.pdf,,MetaSafety; CFI; CSER; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CG5RTG7G,report,2018,"Brundage, Miles; Avin, Shahar; Clark, Jack; Toner, Helen; Eckersley, Peter; Garfinkel, Ben; Dafoe, Allan; Scharre, Paul; Zeitzoff, Thomas; Filar, Bobby; Anderson, Hyrum; Roff, Heather; Allen, Gregory C.; Steinhardt, Jacob; Flynn, Carrick; hÉigeartaigh, Seán Ó; Beard, Simon; Belfield, Haydn; Farquhar, Sebastian; Lyle, Clare; Crootof, Rebecca; Evans, Owain; Page, Michael; Bryson, Joanna; Yampolskiy, Roman; Amodei, Dario","The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation",,,,,http://arxiv.org/abs/1802.07228,"This report surveys the landscape of potential security threats from malicious uses of AI, and proposes ways to better forecast, prevent, and mitigate these threats. After analyzing the ways in which AI may influence the threat landscape in the digital, physical, and political domains, we make four high-level recommendations for AI researchers and other stakeholders. We also suggest several promising areas for further research that could expand the portfolio of defenses, or make attacks less effective or harder to execute. Finally, we discuss, but do not conclusively resolve, the long-term equilibrium of attackers and defenders.",2018-02-20,2022-01-30 04:50:26,2022-01-30 04:50:26,2019-12-16 20:09:19,,,,,,,The Malicious Use of Artificial Intelligence,,,,,,,,,,,,arXiv.org,,ZSCC: NoCitationData[s6]  ACC: 461  J: 237 arXiv: 1802.07228,,/Users/jacquesthibodeau/Zotero/storage/TPDWWRCW/Brundage et al. - 2018 - The Malicious Use of Artificial Intelligence Fore.pdf; /Users/jacquesthibodeau/Zotero/storage/3WMW2XAM/1802.html; /Users/jacquesthibodeau/Zotero/storage/8DSHG3KJ/1802.html; /Users/jacquesthibodeau/Zotero/storage/VSTQKGMW/1802.html,,MetaSafety; CFI; CSER; FHI; Open-AI; BERI,Computer Science - Artificial Intelligence; Computer Science - Computers and Society; Computer Science - Cryptography and Security,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2GPMSVD7,report,2019,"Cihon, Peter",Standards for AI Governance: International Standards to Enable Global Coordination in AI Research & Development,,,,,,,2019,2022-01-30 04:50:08,2022-01-30 04:50:08,,,,,,,,Standards for AI Governance,,,,,Berkeley Existential Risk Initiative,,,,,,,Google Scholar,,ZSCC: 0000051,,/Users/jacquesthibodeau/Zotero/storage/GSWAIJG2/Cihon - 2019 - Standards for AI Governance International Standar.pdf,,MetaSafety; FHI; BERI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QHQHI65S,report,2019,"O’Keefe, Cullen; Candidate, J D",Stable Agreements in Turbulent Times: A Legal Toolkit for Constrained Temporal Decision Transmission,,,,,,,2019,2022-01-30 04:50:07,2022-01-30 04:50:07,,31,,,,,,,,,,,Berkeley Existential Risk Initiative,,en,,,,,Zotero,,ZSCC: NoCitationData[s2]  ACC: N/A,,/Users/jacquesthibodeau/Zotero/storage/6ZHFXKE4/O’Keefe and Candidate - Stable Agreements in Turbulent Times A Legal Tool.pdf,,MetaSafety; FHI; BERI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2IKXXPIH,report,2020,"Jayanti, Amritha; Avin, Shahar",It Takes a Village: The Shared Responsibility of 'Raising' an Autonomous Weapon,,,,,,"Expectations around future capabilities of lethal autonomous weapons systems (LAWS) have raised concerns for military risks, ethics, and accountability. The U.K.’s position, as presented among various international voices at the UN’s Convention on Certain Conventional Weapons (CCW) meetings, has attempted to address these concerns through a focused look at the weapons review process, humanmachine teaming or “meaningful human control” (see e.g. JCN1/18), and the ability of autonomous systems to adhere to the Rules of Engagement. Further, the U.K. has stated that the existing governance structures—both domestic and international—around weapons systems are sufficient in dealing with any concerns around the development, deployment, and accountability for emerging LAWS; there is no need for novel agreements on the control of these weapons systems. In an effort to better understand and test the U.K. position on LAWS, the Centre for the Study of Existential Risk has run a research project in which we interviewed experts in multiple relevant organisations, structured around a mock parliamentary inquiry of a hypothetical LAWS-related civilian death. The responses to this scenario have highlighted different, sometimes complementary and sometimes contradicting, conceptions of future systems, challenges, and accountability measures. They have provided rich ""on the ground” perspectives, while also highlighting key gaps that should be addressed by every military that is considering acquisition and deployment of autonomous and semi-autonomous weapon systems.",2020-11-10,2022-01-30 04:50:07,2022-01-30 04:50:07,,,,,,,,,,,,,Cornell University Press,,en,,,,,,,ZSCC: NoCitationData[s3]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/QDMITJNV/Carpenter - 2014 - Lost Causes Agenda Vetting in Global Issue Networ.pdf,,MetaSafety; CSER; AmbiguosSafety; BERI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UIHZHVCW,report,2020,"O’Keefe, Cullen",How Will National Security Considerations Affect Antitrust Decisions in AI? An Examination of Historical Precedents,,,,,,,2020-07-07,2022-01-30 04:50:07,2022-01-30 04:50:07,,39,,,,,,,,,,,Future of Humanity Institute,,en,,,,,Zotero,,ZSCC: NoCitationData[s2]  ACC: 2,,/Users/jacquesthibodeau/Zotero/storage/CNQBT7ZC/O’Keefe - How Will National Security Considerations Affect A.pdf,,MetaSafety; FHI; Open-AI; BERI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9RIUZJB7,report,2014,"Bostrom, Nick","Hail mary, value porosity, and utility diversification",,,,,,,2014,2022-01-30 04:53:17,2022-01-30 04:53:17,,,,,,,,,,,,,Future of Humanity Institute,,,,,,,Google Scholar,,ZSCC: 0000018,,"/Users/jacquesthibodeau/Zotero/storage/PIQNHXSA/Bostrom - 2014 - Hail mary, value porosity, and utility diversifica.pdf",,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
75IGTK3W,report,2008,"Sandberg, Anders; Bostrom, Nick",Global Catastrophic Risks Survey,,,,,,,2008,2022-01-30 04:53:17,2022-01-30 04:53:17,,,,,,,,,,,,,Future of Humanity Institute,Oxford University,,,,,,,,ZSCC: 0000056,,/Users/jacquesthibodeau/Zotero/storage/IWPM4BCR/gcr-report.pdf,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,2008-1,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PIF96FJT,report,2016,"Cotton-Barratt, Owen; Farquhar, Sebastian; Halstead, John; Schubert, Stefan; Snyder-Beattie, Andrew",Global Catastrophic Risks 2016,,,,,http://globalprioritiesproject.org/2016/04/global-catastrophic-risks-2016/,"Global catastrophes sometimes strike. In 1918 the Spanish Flu killed as many as one in twenty people. There have been even more devastating pandemics - the Black Death and the 6th century Plague of Justinian may have each killed nearer to one in every six people on this earth. More recently, the Cub",2016-04-28,2022-01-30 04:53:17,2022-01-30 04:53:17,2020-12-13 19:41:24,108,,,,,,,,,,,Global Challenges Foundation,,en-US,,,,,,,ZSCC: 0000034  Section: Policy research,,/Users/jacquesthibodeau/Zotero/storage/MFNIR97H/Cotton-Barratt et al. - 2016 - Global Catastrophic Risks 2016.pdf; /Users/jacquesthibodeau/Zotero/storage/R4KN9F3G/global-catastrophic-risks-2016.html,,MetaSafety; FHI; GPI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NQT3FZE8,report,2021,"Ord, Toby; Mercer, Angus; Dannreuther, Sophie",Future Proof,,,,,,,2021-06,2022-01-30 04:53:10,2022-01-30 04:53:10,,51,,,,,,,,,,,Centre for Long-Term Resilience,,en,,,,,Zotero,,ZSCC: NoCitationData[s1]  ACC: N/F,,"/Users/jacquesthibodeau/Zotero/storage/UV46GKTM/Dannreuther - Angus Mercer, Centre for Long-Term Resilience.pdf",,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C8MBI7MV,report,2016,"Sandberg, Anders",Energetics of the brain and AI,,,,,https://arxiv.org/abs/1602.04019,"Does the energy requirements for the human brain give energy constraints that give reason to  doubt the feasibility of artificial intelligence? This report will review some relevant estimates of  brain bioenergetics and analyze some of the methods of estimating brain emulation energy re- quirements. Turning to AI, there are reasons to believe the energy requirements for de novo AI  to  have  little  correlation  with  brain  (emulation)  energy  requirements  since  cost  could  depend  merely of the cost of processing higher-level representations rather than billions of neural fir- ings.  Unless  one  thinks  the  human  way  of  thinking  is  the  most  optimal  or  most  easily  imple- mentable  way  of  achieving  software  intelligence,  we  should  expect  de novo  AI  to  make  use  of  different, potentially very compressed and fast, processes.",2016,2022-01-30 04:53:09,2022-01-30 04:53:09,,,,,,,,,,,,,Sapience Project,,,,,,,Google Scholar,,ZSCC: 0000009,,/Users/jacquesthibodeau/Zotero/storage/E8476E6C/Sandberg - 2016 - Energetics of the brain and AI.pdf; /Users/jacquesthibodeau/Zotero/storage/6IMBPF7N/1602.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,STR 2016-2,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BD3PXT45,report,2019,"Zhang, Baobao; Dafoe, Allan",Artificial Intelligence: American Attitudes and Trends,,,,,https://www.ssrn.com/abstract=3312874,,2019,2022-01-30 04:53:08,2022-01-30 04:53:08,2019-12-16 22:39:34,,,,,,,Artificial Intelligence,,,,,Center for the Governance of AI and Future of Humanity Institute,,en,,,,,DOI.org (Crossref),,ZSCC: 0000127,,/Users/jacquesthibodeau/Zotero/storage/583N8IMW/papers.html; /Users/jacquesthibodeau/Zotero/storage/7BGC3MI8/papers.html; /Users/jacquesthibodeau/Zotero/storage/E5PBNQQ3/Zhang and Dafoe - 2019 - Artificial Intelligence American Attitudes and Tr.pdf,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MZJVM7EJ,report,2018,"Dafoe, Allan",AI governance: a research agenda,,,,,https://www.fhi.ox.ac.uk/wp-content/uploads/GovAI-Agenda.pdf,,2018,2022-01-30 04:53:08,2022-01-30 04:53:08,2020-12-21,,,,,,,AI governance,,,,,Future of Humanity Institute,,,,,,,Google Scholar,,ZSCC: 0000114,,/Users/jacquesthibodeau/Zotero/storage/9WM3BJIG/Dafoe - 2018 - AI governance a research agenda.pdf,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5FZ7A6KK,report,2020,"Calvin, Nathan; Leung, Jade",Who owns artificial intelligence? A preliminary analysis of corporate intellectual property strategies and why they matter.,,,,,https://www.fhi.ox.ac.uk/wp-content/uploads/GovAI-working-paper-Who-owns-AI-Apr2020.pdf,,2020-02,2022-01-30 04:53:45,2022-01-30 04:53:45,2020-09-05,23,,,,,,,,,,,Future of Humanity Institute,,,,,,,,,ZSCC: 0000003,,/Users/jacquesthibodeau/Zotero/storage/ZPVF27E5/Calvin and Leung - 2020 - Who owns artificial intelligence A preliminary an.pdf,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4NDFHXVI,report,2010,"Armstrong, Stuart",Utility Indifference,,,,,,"Consider an AI that follows its own motivations. We’re not entirely sure what its motivations are, but we would prefer that the AI cooperate with humanity; or, failing that, that we can destroy it before it defects. We’ll have someone sitting in a room, their finger on a detonator, ready at the slightest hint of defection. Unfortunately as has been noted ([3], [1]), this does not preclude the AI from misbehaving. It just means that the AI must act to take control of the explosives, the detonators or the human who will press the button. For a superlatively intelligence AI, this would represent merely a slight extra difficulty. But now imagine that the AI was somehow indifferent to the explosives going off or not (but that nothing else was changed). Then if ever the AI does decide to defect, it will most likely do so without taking control of the explosives, as that would be easier than otherwise. By “easier ” we mean that the chances of failure are less, since the plan is simpler – recall that under these assumptions, the AI counts getting blown up as an equal value to successfully defecting.",2010,2022-01-30 04:53:37,2022-01-30 04:53:37,,,,,,,,,,,,,Future of Humanity Institute,,,,,,,CiteSeer,,ZSCC: 0000025,,/Users/jacquesthibodeau/Zotero/storage/N5CJR7JC/2010-1_body.pdf; /Users/jacquesthibodeau/Zotero/storage/VAX4IF4Z/Armstrong - 2010 - Utility Indifference.pdf; /Users/jacquesthibodeau/Zotero/storage/JDJ3ES5A/summary.html; /Users/jacquesthibodeau/Zotero/storage/XNW3UESI/summary.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CUZWTKPN,report,2017,"Armstrong, Stuart; Weick, Mario; Sandberg, Anders; Snyder-Beattie, Andrew; Beckstead, Nick",The underwriter and the models-solo dances or pas-de-deux? What policy data can tell us about how underwriters use models,,,,,https://www.msamlin.com/content/dam/ms-amlin/corporate/our-world/Whitepapers/MS%20Amlin%20White%20Paper%20The%20underwriter%20and%20the%20models-%20solo%20dances%20or%20pas-de-deux.pdf.downloadasset.pdf,,2017,2022-01-30 04:53:36,2022-01-30 04:53:36,2020-12-19,,,,,,,The underwriter and the models-solo dances or pas-de-deux?,,,,,MS Amlin,,,,,,,Google Scholar,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/UP7CQVAJ/64873.html,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
W2XDGEWJ,report,2019,"Drexler, K Eric",Reframing Superintelligence: Comprehensive AI Services as General Intelligence,,,,,https://www.fhi.ox.ac.uk/reframing/,,2019,2022-01-30 04:53:20,2022-01-30 04:53:20,,210,,,,,,,,,,,Future of Humanity Institute,,en,,,,,Zotero,,ZSCC: 0000031,,/Users/jacquesthibodeau/Zotero/storage/TAPTRQJC/Drexler - Reframing Superintelligence.pdf,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Z7HXRRSA,report,2021,"Drexler, K. Eric",QNRs: Toward Language for Intelligent Machines,,,,,https://www.fhi.ox.ac.uk/wp-content/uploads/2021/08/QNRs_FHI-TR-2021-3.0.pdf,"Impoverished syntax and nondifferentiable vocabularies make natural language a poor medium for neural representation learning and appli- cations. Learned, quasilinguistic neural representations (QNRs) can upgrade words to embeddings and syntax to graphs to provide a more expressive and computationally tractable medium. Graph-structured, embedding-based quasilinguistic representations can support formal and informal reasoning, human and inter-agent communication, and the development of scalable quasilinguistic corpora with characteristics of both literatures and associative memory. To achieve human-like intellectual competence, machines must be fully literate, able not only to read and learn, but to write things worth retaining as contributions to collective knowledge. In support of this goal, QNR-based systems could translate and process natural language corpora to support the aggregation, refinement, integration, extension, and application of knowledge at scale. Incremental development of QNR- based models can build on current methods in neural machine learning, and as systems mature, could potentially complement or replace today’s opaque, error-prone “foundation models” with systems that are more capable, interpretable, and epistemically reliable. Potential applications and implications are broad.",2021,2022-01-30 04:53:20,2022-01-30 04:53:20,2021-10-31 19:09:57,,,,,,,,,,,,Future of Humanity Institute,,,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/S242XWXK/QNRs_FHI-TR-2021-3.0.pdf,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TX4W9496,report,2018,"Evans, Owain; Stuhlmüller, Andreas; Cundy, Chris; Carey, Ryan; Kenton, Zachary; McGrath, Thomas; Schreiber, Andrew",Predicting Human Deliberative Judgments with Machine Learning,,,,,,,2018,2022-01-30 04:53:19,2022-01-30 04:53:19,,,,,,,,,,,,,"Technical report, University of Oxford",,,,,,,Google Scholar,,ZSCC: 0000009,,/Users/jacquesthibodeau/Zotero/storage/FPMK4JWF/Evans et al. - 2018 - Predicting Human Deliberative Judgments with Machi.pdf,,TechSafety; FHI; Ought,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VBFREPX5,report,2014,"Sandberg, Anders",Monte Carlo model of brain emulation development,,,,,,,2014,2022-01-30 04:53:19,2022-01-30 04:53:19,,,,,,,,,,,,,"Working Paper 2014–1 (version 1.2), Future of Humanity Institute. http://www …",,,,,,,Google Scholar,,ZSCC: 0000004,,/Users/jacquesthibodeau/Zotero/storage/K8T8QJJ9/Sandberg - 2014 - Monte Carlo model of brain emulation development.pdf,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KWGIDUE8,report,2015,"Cotton-Barratt, Owen",How valuable is movement growth?,,,,,http://globalprioritiesproject.org/wp-content/uploads/2015/05/MovementGrowth.pdf,,2015,2022-01-30 04:53:18,2022-01-30 04:53:18,,,,,,,,,,,,,Centre for Effective Altruism,,,,,,,Google Scholar,,ZSCC: 0000002,,/Users/jacquesthibodeau/Zotero/storage/MW89CNJZ/Cotton-Barratt - 2015 - How valuable is movement growth.pdf,,MetaSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KJG4W372,report,2021,"Zaidi, Waqar; Dafoe, Allan",International Control of Powerful Technology: Lessons from the Baruch Plan for Nuclear Weapons,,,,,https://www.fhi.ox.ac.uk/wp-content/uploads/2021/03/International-Control-of-Powerful-Technology-Lessons-from-the-Baruch-Plan-Zaidi-Dafoe-2021.pdf,"The invention of atomic energy posed a novel global challenge: could the technology be controlled to avoid destructive uses and an existentially dangerous arms race while permitting the broad sharing of its benefits? From 1944 onwards, scientists, policymakers, and other t echnical specialists began to confront this challenge and explored policy options for dealing with the impact of nuclear technology. We focus on the years 1944 to 1951 and review this period for lessons for the governance of powerful technologies, and find the following: Radical schemes for international control can get broad support when confronted by existentially dangerous technologies, but this support can be tenuous and cynical. Secrecy is likely to play an important, and perhaps harmful, role. The public sphere may be an important source of influence, both in general and in particular in favor of cooperation, but also one that is manipulable and poorly informed. Technical experts may play a critical role, but need to be politically savvy. Overall, policymaking may look more like “muddling through” than clear-eyed grand strategy. Cooperation may be risky, and there may be many obstacles to success.",2021-03,2022-01-30 04:53:18,2022-01-30 04:53:18,2021-11-14 18:24:38,,,,,,,,,,,,,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/IX5D2GDZ/Cihon et al. - 2020 - Should Artificial Intelligence Governance be Centr.pdf,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6C2SDFXX,report,2020,"Critch, Andrew; Krueger, David",AI Research Considerations for Human Existential Safety (ARCHES),,,,,,"Framed in positive terms, this report examines how technical AI research might be steered in a manner that is more attentive to humanity’s long-term prospects for survival as a species. In negative terms, we ask what existential risks humanity might face from AI development in the next century, and by what principles contemporary technical research might be directed to address those risks.",2020-05-30,2022-01-30 04:50:42,2022-01-30 04:50:42,,131,,,,,,,,,,,Center for Human-Compatible AI,,en,,,,,Zotero,,ZSCC: 0000010,,/Users/jacquesthibodeau/Zotero/storage/ACREUKJD/Critch - AI Research Considerations for Human Existential S.pdf,,CHAI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VIV37QTM,report,2021,"Gehlhaus, Diana; Koslosky, Luke; Goode, Kayla; Perkins, Claire",U.S. AI Workforce: Policy Recommendations,,,,,https://cset.georgetown.edu/publication/u-s-ai-workforce-policy-recommendations/,"This policy brief addresses the need for a clearly defined artificial intelligence education and workforce policy by providing recommendations designed to grow, sustain, and diversify the U.S. AI workforce. The authors employ a comprehensive definition of the AI workforce—technical and nontechnical occupations—and provide data-driven policy goals. Their recommendations are designed to leverage opportunities within the U.S. education and training system while mitigating its challenges, and prioritize equity in access and opportunity to AI education and AI careers.",2021-10,2022-01-30 04:52:05,2022-01-30 04:52:05,2021-10-31 17:12:04,,,,,,,U.S. AI Workforce,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/NB3W8BMN/u-s-ai-workforce-policy-recommendations.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BQZXXA47,report,2021,"Buchanan, Ben; Lohn, Andrew; Musser, Micah; Sedova, Katerina","Truth, Lies, and Automation",,,,,https://cset.georgetown.edu/publication/truth-lies-and-automation/,Growing popular and industry interest in high-performing natural language generation models has led to concerns that such models could be used to generate automated disinformation at scale. This report examines the capabilities of GPT-3--a cutting-edge AI system that writes text--to analyze its potential misuse for disinformation. A model like GPT-3 may be able to help disinformation actors substantially reduce the work necessary to write disinformation while expanding its reach and potentially also its effectiveness.,2021-05,2022-01-30 04:52:05,2022-01-30 04:52:05,2021-10-31 17:46:29,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: 0000001,,/Users/jacquesthibodeau/Zotero/storage/FS8NAUIB/truth-lies-and-automation.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8JTKIW98,report,2021,"Konaev, Margarita; Huang, Tina; Chahal, Husanjot",Trusted Partners,,,,,https://cset.georgetown.edu/publication/trusted-partners/,"As the U.S. military integrates artificial intelligence into its systems and missions, there are outstanding questions about the role of trust in human-machine teams. This report examines the drivers and effects of such trust, assesses the risks from too much or too little trust in intelligent technologies, reviews efforts to build trustworthy AI systems, and offers future directions for research on trust relevant to the U.S. military.",2021-02,2022-01-30 04:52:05,2022-01-30 04:52:05,2021-10-31 18:56:18,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: 0000003,,/Users/jacquesthibodeau/Zotero/storage/U2JDP64H/trusted-partners.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
J3Q9XMGI,report,2020,"Imbrie, Andrew; Kania, Elsa; Laskai, Lorand",The Question of Comparative Advantage in Artificial Intelligence: Enduring Strengths and Emerging Challenges for the United States,,,,,https://cset.georgetown.edu/research/the-question-of-comparative-advantage-in-artificial-intelligence-enduring-strengths-and-emerging-challenges-for-the-united-states/,"How do we measure leadership in artificial intelligence, and where does the United States rank? What comparative advantages matter most? As nations embrace AI, answering these questions becomes increasingly critical.",2020-01,2022-01-30 04:52:04,2022-01-30 04:52:04,2020-08-18 21:21:07,,,,,,,The Question of Comparative Advantage in Artificial Intelligence,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: NoCitationData[s2]  ACC: 7,,/Users/jacquesthibodeau/Zotero/storage/VAHM3NKJ/Imbrie et al. - 2020 - The Question of Comparative Advantage in Artificia.pdf; /Users/jacquesthibodeau/Zotero/storage/GDRE4UKP/the-question-of-comparative-advantage-in-artificial-intelligence-enduring-strengths-and-emergin.html,,MetaSafety; CSET; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QBPC2N7S,report,2021,"Gehlhaus, Diana; Hodge, Ron; Koslosky, Luke; Goode, Kayla; Rotner, Jonathan",The DOD’s Hidden Artificial Intelligence Workforce,,,,,https://cset.georgetown.edu/publication/the-dods-hidden-artificial-intelligence-workforce/,"This policy brief, authored in collaboration with the MITRE Corporation, provides a new perspective on the U.S. Department of Defense’s struggle to recruit and retain artificial intelligence talent. The authors find that the DOD already has a cadre of AI and related experts, but that this talent remains hidden. Better leveraging this talent could go a long way in meeting the DOD’s AI objectives. The authors argue that this can be done through policies that more effectively identify AI talent and assignment opportunities, processes that incentivize experimentation and changes in career paths, and investing in the necessary technological infrastructure.",2021-09,2022-01-30 04:52:04,2022-01-30 04:52:04,2021-10-31 17:13:13,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: 0,,/Users/jacquesthibodeau/Zotero/storage/A4AS4MJI/the-dods-hidden-artificial-intelligence-workforce.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KU4UTF7B,report,2021,"Chahal, Husanjot; Toner, Helen; Rahkovsky, Ilya",Small Data’s Big AI Potential,,,,,https://cset.georgetown.edu/publication/small-datas-big-ai-potential/,"Conventional wisdom suggests that cutting-edge artificial intelligence is dependent on large volumes of data. An overemphasis on “big data” ignores the existence—and underestimates the potential—of several AI approaches that do not require massive labeled datasets. This issue brief is a primer on “small data” approaches to AI. It presents exploratory findings on the current and projected progress in scientific research across these approaches, which country leads, and the major sources of funding for this research.",2021-09,2022-01-30 04:52:04,2022-01-30 04:52:04,2021-10-31 17:18:18,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: 0,,/Users/jacquesthibodeau/Zotero/storage/I8S77599/small-datas-big-ai-potential.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SCHIPDXW,report,2020,"Hwang, Tim",Shaping the Terrain of AI Competition,,,,,https://cset.georgetown.edu/research/shaping-the-terrain-of-ai-competition/,How should democracies effectively compete against authoritarian regimes in the AI space? This report offers a “terrain strategy” for the United States to leverage the malleability of artificial intelligence to offset authoritarians' structural advantages in engineering and deploying AI.,2020-06,2022-01-30 04:52:04,2022-01-30 04:52:04,2020-08-18 21:12:30,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: NoCitationData[s2]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/E9B7EF92/CSET-Shaping-the-Terrain-of-AI-Competition.pdf; /Users/jacquesthibodeau/Zotero/storage/9FCEI8B5/shaping-the-terrain-of-ai-competition.html,,MetaSafety; CSET; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7CHZX93M,report,2021,"Cary, Dakota",Robot Hacking Games,,,,,https://cset.georgetown.edu/publication/robot-hacking-games/,"Software vulnerability discovery, patching, and exploitation—collectively known as the vulnerability lifecycle—is time consuming and labor intensive. Automating the process could significantly improve software security and offensive hacking. The Defense Advanced Research Projects Agency’s Cyber Grand Challenge supported teams of researchers from 2014 to 2016 that worked to create these tools. China took notice. In 2017, China hosted its first Robot Hacking Game, seeking to automate the software vulnerability lifecycle. Since then, China has hosted seven such competitions and the People’s Liberation Army has increased its role in hosting the games.",2021-09,2022-01-30 04:52:04,2022-01-30 04:52:04,2021-10-31 17:14:54,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/R33JGR9H/robot-hacking-games.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XQNJ4CAX,report,2021,"Lohn, Andrew",Poison in the Well,,,,,https://cset.georgetown.edu/publication/poison-in-the-well/,"Modern machine learning often relies on open-source datasets, pretrained models, and machine learning libraries from across the internet, but are those resources safe to use? Previously successful digital supply chain attacks against cyber infrastructure suggest the answer may be no. This report introduces policymakers to these emerging threats and provides recommendations for how to secure the machine learning supply chain.",2021-06,2022-01-30 04:52:04,2022-01-30 04:52:04,2021-10-31 17:44:21,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: 0,,/Users/jacquesthibodeau/Zotero/storage/N3PM2RS4/poison-in-the-well.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
K4XIA668,report,2021,"Stanley-Lockman, Zoe",Military AI Cooperation Toolbox,,,,,https://cset.georgetown.edu/publication/military-ai-cooperation-toolbox/,"The Department of Defense can already begin applying its existing international science and technology agreements, global scientific networks, and role in multilateral institutions to stimulate digital defense cooperation. This issue brief frames this collection of options as a military AI cooperation toolbox, finding that the available tools offer valuable pathways to align policies, advance research, development, and testing, and to connect personnel–albeit in more structured ways in the Euro-Atlantic than in the Indo-Pacific.",2021-08,2022-01-30 04:52:04,2022-01-30 04:52:04,2021-10-31 17:41:07,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/ETNQVAV4/military-ai-cooperation-toolbox.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7FVGJKQD,report,2021,"Musser, Micah; Garriott, Ashton",Machine Learning and Cybersecurity,,,,,https://cset.georgetown.edu/publication/machine-learning-and-cybersecurity/,Cybersecurity operators have increasingly relied on machine learning to address a rising number of threats. But will machine learning give them a decisive advantage or just help them keep pace with attackers? This report explores the history of machine learning in cybersecurity and the potential it has for transforming cyber defense in the near future.,2021-06,2022-01-30 04:52:04,2022-01-30 04:52:04,2021-10-31 17:45:14,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/55E2MHZX/machine-learning-and-cybersecurity.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
64I4UBIX,report,2021,"Rudner, Tim G. J.; Toner, Helen",Key Concepts in AI Safety: Robustness and Adversarial Examples,,,,,https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-robustness-and-adversarial-examples/,"This paper is the second installment in a series on “AI safety,” an area of machine learning research that aims to identify causes of unintended behavior in machine learning systems and develop tools to ensure these systems work safely and reliably. The first paper in the series, “Key Concepts in AI Safety: An Overview,” described three categories of AI safety issues: problems of robustness, assurance, and specification. This paper introduces adversarial examples, a major challenge to robustness in modern machine learning systems.",2021-03,2022-01-30 04:52:04,2022-01-30 04:52:04,2021-10-31 18:52:59,,,,,,,Key Concepts in AI Safety,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/482VNDJV/key-concepts-in-ai-safety-robustness-and-adversarial-examples.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3S8DDMWK,report,2021,"Stanley-Lockman, Zoe",Responsible and Ethical Military AI,,,,,https://cset.georgetown.edu/publication/responsible-and-ethical-military-ai/,"Allies of the United States have begun to develop their own policy approaches to responsible military use of artificial intelligence. This issue brief looks at key allies with articulated, emerging, and nascent views on how to manage ethical risk in adopting military AI. The report compares their convergences and divergences, offering pathways for the United States, its allies, and multilateral institutions to develop common approaches to responsible AI implementation.",2021-08,2022-01-30 04:52:04,2022-01-30 04:52:04,2021-10-31 17:39:09,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/XSV67832/responsible-and-ethical-military-ai.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CI7QIJ8V,report,2021,"VerWey, John","No Permits, No Fabs: The Importance of Regulatory Reform for Semiconductor Manufacturing",,,,,https://cset.georgetown.edu/publication/no-permits-no-fabs/,"Congress has advanced legislation to appropriate $52 billion in funding for the CHIPS for America Act, which aims to increase semiconductor manufacturing and supply chain resilience in the United States. But more can be done to improve the resiliency of U.S. access to microelectronics beyond manufacturing incentives. This report outlines infrastructure investments and regulatory reforms that could make the United States a more attractive place to build new chipmaking capacity and ensure continued U.S. access to key inputs for semiconductor manufacturing.",2021-10,2022-01-30 04:52:04,2022-01-30 04:52:04,2021-10-31 17:08:33,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/V5B6V6X7/no-permits-no-fabs.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TBNSHQP2,report,2021,"Daniels, Matthew; Chang, Ben",National Power After AI,,,,,https://cset.georgetown.edu/publication/national-power-after-ai/,"AI technologies will likely alter great power competitions in foundational ways, changing both how nations create power and their motives for wielding it against one another. This paper is a first step toward thinking more expansively about AI & national power and seeking pragmatic insights for long-term U.S. competition with authoritarian governments.",2021-07,2022-01-30 04:52:04,2022-01-30 04:52:04,2021-10-31 17:42:40,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/TWR2J25P/national-power-after-ai.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VKQNA4AA,report,2021,"Luong, Ngor; Gelles, Rebecca; Flagg, Melissa",Mapping the AI Investment Activities of Top Global Defense Companies,,,,,https://cset.georgetown.edu/publication/mapping-the-ai-investment-activities-of-top-global-defense-companies/,"Militaries around the world have often relied on the largest global defense companies to acquire and integrate cutting-edge technologies. This issue brief examines the investment and mergers and acquisition activities in artificial intelligence of the top 50 global defense companies — a key, if limited, approach to accessing AI innovation in the commercial sector — and assesses investment trends of their corporate venture capital subsidiaries and offers a geographic breakdown of defense companies and their AI target companies.",2021-10,2022-01-30 04:52:04,2022-01-30 04:52:04,2021-10-31 17:10:48,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/CI8QZ3GJ/mapping-the-ai-investment-activities-of-top-global-defense-companies.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
I4V99XXR,report,2021,"Daniels, Matthew; Toney, Autumn; Flagg, Melissa; Yang, Charles",Machine Intelligence for Scientific Discovery and Engineering Invention,,,,,https://cset.georgetown.edu/publication/machine-intelligence-for-scientific-discovery-and-engineering-invention/,The advantages of nations depend in part on their access to new inventions—and modern applications of artificial intelligence can help accelerate the creation of new inventions in the years ahead. This data brief is a first step toward understanding how modern AI and machine learning have begun accelerating growth across a wide array of science and engineering disciplines in recent years.,2021-05,2022-01-30 04:52:04,2022-01-30 04:52:04,2021-10-31 17:48:21,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/VVP6JR7H/machine-intelligence-for-scientific-discovery-and-engineering-invention.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GZTJRH52,report,2021,"Rudner, Tim G. J.; Toner, Helen",Key Concepts in AI Safety: Interpretability in Machine Learning,,,,,https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-interpretability-in-machine-learning/,"This paper is the third installment in a series on “AI safety,” an area of machine learning research that aims to identify causes of unintended behavior in machine learning systems and develop tools to ensure these systems work safely and reliably. The first paper in the series, “Key Concepts in AI Safety: An Overview,” described three categories of AI safety issues: problems of robustness, assurance, and specification. This paper introduces interpretability as a means to enable assurance in modern machine learning systems.",2021-03,2022-01-30 04:52:04,2022-01-30 04:52:04,2021-10-31 18:52:14,,,,,,,Key Concepts in AI Safety,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/Z256SBHM/key-concepts-in-ai-safety-interpretability-in-machine-learning.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VH792VSZ,report,2021,"Rudner, Tim G. J.; Toner, Helen",Key Concepts in AI Safety: An Overview,,,,,https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-an-overview/,"This paper is the first installment in a series on “AI safety,” an area of machine learning research that aims to identify causes of unintended behavior in machine learning systems and develop tools to ensure these systems work safely and reliably. In it, the authors introduce three categories of AI safety issues: problems of robustness, assurance, and specification. Other papers in this series elaborate on these and further key concepts.",2021-03,2022-01-30 04:52:04,2022-01-30 04:52:04,2021-10-31 18:53:33,,,,,,,Key Concepts in AI Safety,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: 0000001,,/Users/jacquesthibodeau/Zotero/storage/KDBM6KD4/key-concepts-in-ai-safety-an-overview.html,,TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
36HRFWME,report,2021,"Goode, Kayla; Kim, Heeu Millie",Indonesia’s AI Promise in Perspective,,,,,https://cset.georgetown.edu/publication/indonesias-ai-promise-in-perspective/,"The United States and China are keeping an eye on Indonesia’s artificial intelligence potential given the country’s innovation-driven national strategy and flourishing AI industry. China views Indonesia as an anchor for its economic, digital, and political inroads in Southeast Asia and has invested aggressively in new partnerships. The United States, with robust political and economic relations rooted in shared democratic ideals, has an opportunity to leverage its comparative advantages and tap into Indonesia’s AI potential through high-level agreements.",2021-08,2022-01-30 04:52:04,2022-01-30 04:52:04,2021-10-31 17:39:58,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: 0,,/Users/jacquesthibodeau/Zotero/storage/9P8XTSFN/indonesias-ai-promise-in-perspective.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EGF6E8W5,report,2021,"Baker, Jamie",Ethics and Artificial Intelligence,,,,,https://cset.georgetown.edu/publication/ethics-and-artificial-intelligence/,"The law plays a vital role in how artificial intelligence can be developed and used in ethical ways. But the law is not enough when it contains gaps due to lack of a federal nexus, interest, or the political will to legislate. And law may be too much if it imposes regulatory rigidity and burdens when flexibility and innovation are required. Sound ethical codes and principles concerning AI can help fill legal gaps. In this paper, CSET Distinguished Fellow James E. Baker offers a primer on the limits and promise of three mechanisms to help shape a regulatory regime that maximizes the benefits of AI and minimizes its potential harms.",2021-04,2022-01-30 04:52:03,2022-01-30 04:52:03,2021-10-31 18:49:21,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/WZE4UHJQ/ethics-and-artificial-intelligence.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R79CMVHN,report,2021,"Imbrie, Andrew; Gelles, Rebecca; Dunham, James; Aiken, Catherine",Contending Frames: Evaluating Rhetorical Dynamics in AI,,,,,https://cset.georgetown.edu/publication/contending-frames/,"The narrative of an artificial intelligence “arms race” among the great powers has become shorthand to describe evolving dynamics in the field. Narratives about AI matter because they reflect and shape public perceptions of the technology. In this issue brief, the second in a series examining rhetorical frames in AI, the authors compare four narrative frames that are prominent in public discourse: AI Competition, Killer Robots, Economic Gold Rush and World Without Work.",2021-05,2022-01-30 04:52:03,2022-01-30 04:52:03,2021-10-31 17:49:21,,,,,,,Contending Frames,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: 0,,/Users/jacquesthibodeau/Zotero/storage/3JV7AUFN/contending-frames.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VWID435G,report,2020,"Murdick, Dewey; Dunham, James; Melot, Jennifer",AI Definitions Affect Policymaking,,,,,https://cset.georgetown.edu/research/ai-definitions-affect-policymaking/,"The task of artificial intelligence policymaking is complex and challenging, made all the more difficult by such a rapidly evolving technology. In order to address the security and economic implications of AI, policymakers must be able to viably define, categorize and assess AI research and technology. In this issue brief, CSET puts forward a functional definition of AI, based on three core principles, that significantly outperforms methods developed over the last decade.",2020-06-02,2022-01-30 04:52:03,2022-01-30 04:52:03,2020-08-18 21:13:44,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: 0000003,,/Users/jacquesthibodeau/Zotero/storage/WD8HFI6J/Murdick et al. - 2020 - AI Definitions Affect Policymaking.pdf; /Users/jacquesthibodeau/Zotero/storage/QH8STACD/ai-definitions-affect-policymaking.html,,MetaSafety; CSET; AmbiguosSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8JRCH2RU,report,2021,"Hoffman, Wyatt",AI and the Future of Cyber Competition,,,,,https://cset.georgetown.edu/publication/ai-and-the-future-of-cyber-competition/,"As states turn to AI to gain an edge in cyber competition, it will change the cat-and-mouse game between cyber attackers and defenders. Embracing machine learning systems for cyber defense could drive more aggressive and destabilizing engagements between states. Wyatt Hoffman writes that cyber competition already has the ingredients needed for escalation to real-world violence, even if these ingredients have yet to come together in the right conditions.",2021-01,2022-01-30 04:52:03,2022-01-30 04:52:03,2021-10-31 18:58:12,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: 0000003,,/Users/jacquesthibodeau/Zotero/storage/NKNAVMTB/ai-and-the-future-of-cyber-competition.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CDKQ9KHV,report,2021,"Arnold, Zachary; Toner, Helen",AI Accidents: An Emerging Threat,,,,,https://cset.georgetown.edu/publication/ai-accidents-an-emerging-threat/,"As modern machine learning systems become more widely used, the potential costs of malfunctions grow. This policy brief describes how trends we already see today—both in newly deployed artificial intelligence systems and in older technologies—show how damaging the AI accidents of the future could be. It describes a wide range of hypothetical but realistic scenarios to illustrate the risks of AI accidents and offers concrete policy suggestions to reduce these risks.",2021-07,2022-01-30 04:52:03,2022-01-30 04:52:03,2021-10-31 17:43:15,,,,,,,AI Accidents,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: 0,,/Users/jacquesthibodeau/Zotero/storage/6PTVA9UX/ai-accidents-an-emerging-threat.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IQSW7FFQ,report,2021,"Konaev, Margarita; Imbrie, Andrew; Fedasiuk, Ryan; Weinstein, Emily; Sedova, Katerina; Dunham, James",Headline or Trend Line?,,,,,https://cset.georgetown.edu/publication/headline-or-trend-line/,"Chinese and Russian government officials are keen to publicize their countries’ strategic partnership in emerging technologies, particularly artificial intelligence. This report evaluates the scope of cooperation between China and Russia as well as relative trends over time in two key metrics of AI development: research publications and investment. The findings expose gaps between aspirations and reality, bringing greater accuracy and nuance to current assessments of Sino-Russian tech cooperation.",2021-08,2022-01-30 04:52:03,2022-01-30 04:52:03,2021-10-31 17:19:18,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/KZNE2FUE/headline-or-trend-line.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AGXWUS23,report,2021,"Fedasiuk, Ryan; Melot, Jennifer; Murphy, Ben",Harnessed Lightning,,,,,https://cset.georgetown.edu/publication/harnessed-lightning/,"This report examines nearly 350 artificial intelligence-related equipment contracts awarded by the People’s Liberation Army and state-owned defense enterprises in 2020 to assess how the Chinese military is adopting AI. The report identifies China’s key AI defense industry suppliers, highlights gaps in U.S. export control policies, and contextualizes the PLA’s AI investments within China’s broader strategy to compete militarily with the United States.",2021-10,2022-01-30 04:52:03,2022-01-30 04:52:03,2021-10-31 17:07:03,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/XXESBFB3/harnessed-lightning.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NFPE7BBS,report,2021,"Mittelsteadt, Matthew",AI Verification: Mechanisms to Ensure AI Arms Control Compliance,,,,,https://cset.georgetown.edu/publication/ai-verification/,"The rapid integration of artificial intelligence into military systems raises critical questions of ethics, design and safety. While many states and organizations have called for some form of “AI arms control,” few have discussed the technical details of verifying countries’ compliance with these regulations. This brief offers a starting point, defining the goals of “AI verification” and proposing several mechanisms to support arms inspections and continuous verification.",2021-02,2022-01-30 04:52:03,2022-01-30 04:52:03,2021-10-31 18:55:15,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: 0,,/Users/jacquesthibodeau/Zotero/storage/K3TV6PZX/ai-verification.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5HRJA33X,report,2021,"Peterson, Dahlia; Goode, Kayla; Gehlhaus, Diana",AI Education in China and the United States,,,,,https://cset.georgetown.edu/publication/ai-education-in-china-and-the-united-states/,"A globally competitive AI workforce hinges on the education, development, and sustainment of the best and brightest AI talent. This issue brief compares efforts to integrate AI education in China and the United States, and what advantages and disadvantages this entails. The authors consider key differences in system design and oversight, as well as strategic planning. They then explore implications for the U.S. national security community.",2021-09,2022-01-30 04:52:03,2022-01-30 04:52:03,2021-10-31 17:16:49,,,,,,,,,,,,Center for Security and Emerging Technology,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/TNTW4N5K/ai-education-in-china-and-the-united-states.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZW8JBQD3,report,2015,"Soares, Nate",Formalizing Two Problems of Realistic World-Models,,,,,https://intelligence.org/2015/01/22/new-report-formalizing-two-problems-realistic-world-models/,"An intelligent agent embedded within the real world must reason about an environment which is larger than the agent, and learn how to achieve goals in that environment. We discuss attempts to formalize two problems: one of induction, where an agent must use sensory data to infer a universe which embeds (and computes) the agent, and one of interaction, where an agent must learn to achieve complex goals in the universe. We review related problems formalized by Solomonoﬀ and Hutter, and explore challenges that arise when attempting to formalize analogous problems in a setting where the agent is embedded within the environment.",2015,2022-01-30 04:56:48,2022-01-30 04:56:48,,8,,,,,,,,,,,Machine Intelligence Research Institute,,en,,,,,Zotero,,ZSCC: 0000015[s0]  5 J: 15,,/Users/jacquesthibodeau/Zotero/storage/F254U9E2/Soares - Formalizing Two Problems of Realistic World-Models.pdf,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HGCDPKA8,report,2021,"Thorstad, David",The scope of longtermism,,,,,https://globalprioritiesinstitute.org/the-scope-of-longtermism-david-thorstad-global-priorities-institute-university-of-oxford/,"Longtermism holds roughly that in many decision situations, the best thing we can do is what is best for the long-term future. The scope question for longtermism asks: how large is the class of decision situations for which longtermism holds? Although longtermism was initially developed to describe the situation of...",2021-06-22,2022-01-30 04:55:29,2022-01-30 04:55:29,2021-10-31 22:29:16,,,,,,,,,,,,Global Priorities Institute,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: N/F,,/Users/jacquesthibodeau/Zotero/storage/KTVEVVB7/the-scope-of-longtermism-david-thorstad-global-priorities-institute-university-of-oxford.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NSMN5REW,report,2021,"Greaves, Hilary; MacAskill, William",The case for strong longtermism,,,,,https://globalprioritiesinstitute.org/hilary-greaves-william-macaskill-the-case-for-strong-longtermism-2/,"A striking fact about the history of civilisation is just how early we are in it. There are 5000 years of recorded history behind us, but how many years are still to come? If we merely last as long as the typical mammalian species, we still have over 200,000 years to go (Barnosky et al. 2011); there could be a further one billion years until the Earth is no longer habitable for humans (Wolf and Toon 2015); and trillions of years until the last conventional star formations (Adams and Laughlin 1999:34). Even on the most conservative of these timelines, we have progressed through a tiny fraction of history. If humanity’s saga were a novel, we would be on the very first page.",2021-06-14,2022-01-30 04:55:29,2022-01-30 04:55:29,2021-10-31 22:29:46,,,,,,,,,,,,Global Priorities Institute,,en-US,,,,,,,ZSCC: NoCitationData[s0]  ACC: 29,,/Users/jacquesthibodeau/Zotero/storage/HQW64F7C/hilary-greaves-william-macaskill-the-case-for-strong-longtermism-2.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RE4PBPVA,report,2021,"Riedener, Stefan",Existential risks from a Thomist Christian perspective,,,,,https://globalprioritiesinstitute.org/stefan-riedener-existential-risks-from-a-thomist-christian-perspective/,"Let’s say with Nick Bostrom that an ‘existential risk’ (or ‘x-risk’) is a risk that ‘threatens the premature extinction of Earth-originating intelligent life or the permanent and drastic destruction of its potential for desirable future development’ (2013, 15). There are a number of such risks: nuclear wars, developments in biotechnology or artificial intelligence, climate change, pandemics, supervolcanos, asteroids, and so on (see e.g. Bostrom and Ćirković 2008). ...",2021-01-04,2022-01-30 04:55:29,2022-01-30 04:55:29,2021-10-31 22:32:05,,,,,,,,,,,,Global Priorities Institute,,en-US,,,,,,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/TAM2MWDV/stefan-riedener-existential-risks-from-a-thomist-christian-perspective.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EPFEIRBC,report,2020,"Trammell, Phillip; Korinek, Anton",Economic growth under transformative AI,,,,,https://globalprioritiesinstitute.org/wp-content/uploads/Philip-Trammell-and-Anton-Korinek_Economic-Growth-under-Transformative-AI.pdf,,2020-10,2022-01-30 04:55:29,2022-01-30 04:55:29,2020-11-21 19:28:29,,,,,,,,,,,,Global Priorities Institute,,,,,,,,,ZSCC: 0000003,,/Users/jacquesthibodeau/Zotero/storage/6MF4ET6T/Philip-Trammell-and-Anton-Korinek_Economic-Growth-under-Transformative-AI.pdf,,MetaSafety; AmbiguosSafety; GPI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A9VMCT9R,report,2020,"Wilkinson, Hayden",In defence of fanaticism,,,,,https://globalprioritiesinstitute.org/wp-content/uploads/Hayden-Wilkinson_In-defence-of-fanaticism.pdf,,2020-09,2022-01-30 04:55:29,2022-01-30 04:55:29,2020-11-21 19:28:53,,,,,,,,,,,,Global Priorities Institute,,,,,,,,,ZSCC: 0000000[s0],,/Users/jacquesthibodeau/Zotero/storage/ZJ79B6HC/Hayden-Wilkinson_In-defence-of-fanaticism.pdf,,MetaSafety; AmbiguosSafety; GPI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EGHGP6FP,report,2021,"Thomas, Teruji",Doomsday and objective chance,,,,,https://globalprioritiesinstitute.org/doomsday-and-objective-chance-teruji-thomas/,"Lewis’s Principal Principle says that one should usually align one’s credences with the known chances. In this paper I develop a version of the Principal Principle that deals well with some exceptional cases related to the distinction between metaphysical and epistemic modal­ity. I explain how this principle gives a unified account of the Sleeping Beauty problem and chance-­based principles of anthropic reasoning. In doing so, I defuse the Doomsday Argument that the end of the world is likely to be nigh.",2021-07-13,2022-01-30 04:55:29,2022-01-30 04:55:29,2021-10-31 22:27:47,,,,,,,,,,,,Global Priorities Institute,,en-US,,,,,,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/QZH4S7N9/doomsday-and-objective-chance-teruji-thomas.html,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
W4ESBNR3,report,2020,"MacAskill, William",Are we living at the hinge of history,,,,,,,2020-09,2022-01-30 04:55:28,2022-01-30 04:55:28,,28,,,,,,,,,,,Global Priorities Institute,,en,,,,,Zotero,,ZSCC: 0000001,,/Users/jacquesthibodeau/Zotero/storage/T3DJ73RE/MacAskill - Are we living at the hinge of history.pdf,,MetaSafety; AmbiguosSafety; GPI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VN9N5P2C,report,2017,"Baum, Seth; Barrett, Anthony",Global Catastrophes: The Most Extreme Risks,,,,,https://papers.ssrn.com/abstract=3046668,"The most extreme risk are those that threaten the entirety of human civilization, known as global catastrophic risks. The very extreme nature of global catastrophes makes them both challenging to analyze and important to address. They are challenging to analyze because they are largely unprecedented and because they involve the entire global human system. They are important to address because they threaten everyone around the world and future generations. Global catastrophic risks also pose some deep dilemmas. One dilemma occurs when actions to reduce global catastrophic risk could harm society in other ways, as in the case of geoengineering to reduce catastrophic climate change risk. Another dilemma occurs when reducing one global catastrophic risk could increase another, as in the case of nuclear power reducing climate change risk while increasing risks from nuclear weapons. The complex, interrelated nature of global catastrophic risk suggests a research agenda in which the full space of risks are assessed in an integrated fashion in consideration of the deep dilemmas and other challenges they pose. Such an agenda can help identify the best ways to manage these most extreme risks and keep human civilization safe.",2017-10-02,2022-01-30 04:55:19,2022-01-30 04:55:19,2019-12-16 02:43:45,,,,,,,Global Catastrophes,,,,,Social Science Research Network,"Rochester, NY",en,,SSRN Scholarly Paper,,,papers.ssrn.com,,ZSCC: 0000010,,/Users/jacquesthibodeau/Zotero/storage/38X6ZDSW/papers.html,,MetaSafety; GCRI,risk; catastrophic risk; extreme risk; global catastrophic risk,,,,,,,,,,,,,,,,,,,ID 3046668,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WT78EJX5,report,2017,"Baum, Seth","A Survey of Artificial General Intelligence Projects for Ethics, Risk, and Policy",,,,,https://papers.ssrn.com/abstract=3070741,"Artificial general intelligence (AGI) is AI that can reason across a wide range of domains. It has long been considered the “grand dream” or “holy grail” of AI. It also poses major issues of ethics, risk, and policy due to its potential to transform society: if AGI is built, it could either help solve the world’s problems or cause major catastrophe, possibly even human extinction. This paper presents the first-ever survey of active AGI R&D projects in terms of ethics, risk, and policy. A thorough search identifies 45 projects of diverse sizes, nationalities, ethical goals, and other attributes. Most projects are either academic or corporate. The academic projects tend to express goals of advancing knowledge and are less likely to be active on AGI safety issues. The corporate projects tend to express goals of benefiting humanity and are more likely to be active on safety. Most projects are based in the US, and almost all are in either the US or a US ally, including all of the larger projects. This geographic concentration could simplify policymaking, though most projects publish open-source code, enabling contributions from anywhere in the world. These and other findings of the survey offer an empirical basis for the study of AGI R&D and a guide for policy and other action.",2017-11-12,2022-01-30 04:55:18,2022-01-30 04:55:18,2019-12-16 02:43:48,,,,,,,,,,,,Social Science Research Network,"Rochester, NY",en,,SSRN Scholarly Paper,,,papers.ssrn.com,,ZSCC: 0000059,,/Users/jacquesthibodeau/Zotero/storage/IQ679H9R/papers.html,,MetaSafety; GCRI,artificial intelligence; risk; ethics; policy,,,,,,,,,,,,,,,,,,,ID 3070741,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GKMKG435,report,2020,Jessica Cussins Newman,Decision Points in AI Governance,,,,,https://cltc.berkeley.edu/wp-content/uploads/2020/05/Decision_Points_AI_Governance.pdf,,2020,2022-01-30 04:59:45,2022-01-30 04:59:45,,58,,,,,,,,,,,Center for Long-Term Cybersecurity,,,,,,,,,ZSCC: 0000006,,,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9GV2AK67,report,2017,"Duettmann, Allison",Artificial General Intelligence: Timeframes & Policy White Paper,,,,,https://foresight.org/publications/AGI-Timeframes&PolicyWhitePaper.pdf,,2017,2022-01-30 04:59:36,2022-01-30 04:59:36,,26,,,,,,,,,,,Foresight Institute,,en,,,,,Zotero,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/7GZFE7Z4/Duettmann - Artificial General Intelligence Timeframes & Poli.pdf,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AWG743AN,report,2018,"Duettman, Allison; Afanasjeva, Olga; Armstrong, Stuart; Braley, Ryan; Cussins, Jessica; Ding, Jeffrey; Eckersley, Peter; Guan, Melody; Vance, Alyssa; Yampolskiy, Roman",Artificial General Intelligence: Coordination and Great Powers,,,,,https://fsone-bb4c.kxcdn.com/wp-content/uploads/2018/11/AGI-Coordination-Geat-Powers-Report.pdf,,2018,2022-01-30 04:59:35,2022-01-30 04:59:35,,,,,,,,,,,,,Foresight Institute,,,,,,,,,ZSCC: NoCitationData[s1]  ACC: 5,,/Users/jacquesthibodeau/Zotero/storage/7TTB38TG/Duettman et al. - 2018 - Artificial General Intelligence Coordination and .pdf,,MetaSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
T5UVAGFV,report,2020,"McGraw, Gary; Figueroa, Harold; Shepardson, Victor; Bonett, Richie",An Architectural Risk Analysis of Machine Learning Systems: Toward More Secure Machine Learning,,,,,https://www.garymcgraw.com/wp-content/uploads/2020/02/BIML-ARA.pdf,,2020-01-13,2022-01-30 04:59:35,2022-01-30 04:59:35,2020-09-07,42,,,,,,An Architectural Risk Analysis of Machine Learning Systems,,,,,Berryville Institute of Machine Learning,,,,,,,,,ZSCC: 0000003,,,,TechSafety; AmbiguosSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AHAITFCH,report,2021,"Horowitz, Michael; Scharre, Paul",AI and International Stability: Risks and Confidence-Building Measures,,,,,https://www.cnas.org/publications/reports/ai-and-international-stability-risks-and-confidence-building-measures,Exploring the potential use of confidence-building measures built around the shared interests that all countries have in preventing inadvertent war.,2021-01-12,2022-01-30 04:59:34,2022-01-30 04:59:34,2021-11-14 18:05:37,,,,,,,AI and International Stability,,,,,Center for a New American Security,,en,,,,,,,ZSCC: NoCitationData[s0]  ACC: 2,,/Users/jacquesthibodeau/Zotero/storage/SZ2WC6V5/Horowitz and Scharre - 2021 - AI and International Stability Risks and Confiden.pdf; /Users/jacquesthibodeau/Zotero/storage/VC38NJHI/ai-and-international-stability-risks-and-confidence-building-measures.html,,MetaSafety; AmbiguousSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MRQB3GAA,report,2015,"Fallenstein, Benja; Soares, Nate",Vingean Reﬂection: Reliable Reasoning for Self-Improving Agents,,,,,https://intelligence.org/files/VingeanReflection.pdf,"Today, human-level machine intelligence is in the domain of futurism, but there is every reason to expect that it will be developed eventually. Once artiﬁcial agents become able to improve themselves further, they may far surpass human intelligence, making it vitally important to ensure that the result of an “intelligence explosion” is aligned with human interests. In this paper, we discuss one aspect of this challenge: ensuring that the initial agent’s reasoning about its future versions is reliable, even if these future versions are far more intelligent than the current reasoner. We refer to reasoning of this sort as Vingean reﬂection.",2015,2022-01-30 04:57:32,2022-01-30 04:57:32,,,,,,,,,,,,,Machine Intelligence Research Institute,,en,,,,,Zotero,,ZSCC: 0000016  5 J: 15,,/Users/jacquesthibodeau/Zotero/storage/PIFEUWZX/Fallenstein and Soares - Vingean Reﬂection Reliable Reasoning for Self-Imp.pdf,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4J3EZPG6,report,2014,"Soares, Nate; Fallenstein, Benja",Questions of Reasoning Under Logical Uncertainty,,,,,https://intelligence.org/2015/01/09/new-report-questions-reasoning-logical-uncertainty/,"A logically uncertain reasoner would be able to reason as if they know both a programming language and a program, without knowing what the program outputs. Most practical reasoning involves some logical uncertainty, but no satisfactory theory of reasoning under logical uncertainty yet exists. A better theory of reasoning under logical uncertainty is needed in order to develop the tools necessary to construct highly reliable artiﬁcial reasoners. This paper introduces the topic, discusses a number of historical results, and describes a number of open problems.",2014,2022-01-30 04:56:58,2022-01-30 04:56:58,,8,,,,,,,,,,,Machine Intelligence Research Institute,,en,,,,,Zotero,,ZSCC: 0000018  5 J: 15,,/Users/jacquesthibodeau/Zotero/storage/UQ2E4GWJ/Soares and Fallenstein - Questions of Reasoning Under Logical Uncertainty.pdf,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
I9GK83X9,report,2013,"Yudkowsky, Eliezer",Intelligence Explosion Microeconomics,,,,,,"I. J. Good’s thesis of the “intelligence explosion” states that a suﬃciently advanced machine intelligence could build a smarter version of itself, which could in turn build an even smarter version, and that this process could continue to the point of vastly exceeding human intelligence. As Sandberg (2010) correctly notes, there have been several attempts to lay down return on investment formulas intended to represent sharp speedups in economic or technological growth, but very little attempt has been made to deal formally with Good’s intelligence explosion thesis as such.",2013,2022-01-30 04:56:57,2022-01-30 04:56:57,,96,,,,,,,,,,,Machine Intelligence Research Institute,,en,,,,,Zotero,,ZSCC: 0000054  4 J: 34,,/Users/jacquesthibodeau/Zotero/storage/EINNW7XJ/Yudkowsky - Intelligence Explosion Microeconomics.pdf,,TechSafety; MIRI,,,,,,,,,,,,,,,,,,,,2013-1,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DQSC7JWK,report,2019,"Kumar, Ram Shankar Siva; Brien, David O; Albert, Kendra; Viljöen, Salomé; Snover, Jeffrey",Failure Modes in Machine Learning - Security documentation,,,,,https://docs.microsoft.com/en-us/security/failure-modes-in-machine-learning,"In the last two years, more than 200 papers have been written on how machine learning (ML) systems can fail because of adversarial attacks on the algorithms and data; this number balloons if we were to incorporate papers covering non-adversarial failure modes. The spate of papers has made it difficult for ML practitioners, let alone engineers, lawyers, and policymakers, to keep up with the attacks against and defenses of ML systems. However, as these systems become more pervasive, the need to understand how they fail, whether by the hand of an adversary or due to the inherent design of a system, will only become more pressing. In order to equip software developers, security incident responders, lawyers, and policy makers with a common vernacular to talk about this problem, we developed a framework to classify failures into ""Intentional failures"" where the failure is caused by an active adversary attempting to subvert the system to attain her goals; and ""Unintentional failures"" where the failure is because an ML system produces an inherently unsafe outcome. After developing the initial version of the taxonomy last year, we worked with security and ML teams across Microsoft, 23 external partners, standards organization, and governments to understand how stakeholders would use our framework. Throughout the paper, we attempt to highlight how machine learning failure modes are meaningfully different from traditional software failures from a technology and policy perspective.",2019,2022-01-30 04:59:48,2022-01-30 04:59:48,2019-12-16 22:40:25,,,,,,,,,,,,"Microsoft Corporation, Berkman Klein Center for Internet and Society at Harvard University",,en-us,,,,,,,ZSCC: NoCitationData[s9]  ACC: 16,,/Users/jacquesthibodeau/Zotero/storage/93TJR5IH/failure-modes-in-machine-learning.html,,TechSafety; AmbiguosSafety; Other-org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
I97QI9BH,report,2021,"Gates, Vael; Callaway, Frederick; Ho, Mark K.; Griffiths, Tom",A rational model of people's inferences about others' preferences based on response times,,,,,https://psyarxiv.com/25zfx/,"There's a difference between someone instantaneously saying ""Yes!"" when you ask them on a date compared to ""...yes."" Psychologists and economists have long studied how people can infer preferences from others' choices. However, these models have tended to focus on what people choose and not how long it takes them to make a choice. We present a rational model for inferring preferences from response times, using a Drift Diffusion Model to characterize how preferences influence response time and Bayesian inference to invert this relationship. We test our model's predictions for three experimental questions. Matching model predictions, participants inferred that a decision-maker preferred a chosen item more if the decision-maker spent longer deliberating (Experiment 1), participants predicted a decision-maker's choice in a novel comparison based on inferring the decision-maker's relative preferences from previous response times and choices (Experiment 2), and participants could incorporate information about a decision-maker's mental state of cautious or careless (Experiments 3, 4A, and 4B).",2021-03-15,2022-03-09 23:00:25,2022-03-11 01:37:16,2022-03-09 23:00:25,,,,,,,,,,,,PsyArXiv,,en-us,,,,,OSF Preprints,,DOI: 10.31234/osf.io/25zfx type: article,,/Users/jacquesthibodeau/Zotero/storage/FTBJT79R/Gates et al. - 2021 - A rational model of people's inferences about othe.pdf; /Users/jacquesthibodeau/Zotero/storage/DURF8E83/Gates et al. - 2021 - A rational model of people's inferences about othe.pdf,,,Cognitive Psychology; Computational Modeling; drift diffusion model; Experimental Design and Sample Surveys; inference; Judgment and Decision Making; Quantitative Methods; Reasoning; Social and Behavioral Sciences; social cognition; theory of mind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WUAWT6MC,report,2021,"Maas, Matthijs M.",Aligning AI Regulation to Sociotechnical Change,,,,,https://papers.ssrn.com/abstract=3871635,"How do we regulate a changing technology, with changing uses, in a changing world? This chapter argues that while existing (inter)national AI governance approaches are important, they are often siloed. Technology-centric approaches focus on individual AI applications; law-centric approaches emphasize AI’s effects on pre-existing legal fields or doctrines. This chapter argues that to foster a more systematic, functional and effective AI regulatory ecosystem, policy actors should instead complement these approaches with a regulatory perspective that emphasizes how, when, and why AI applications enable patterns of ‘sociotechnical change’. Drawing on theories from the emerging field of ‘TechLaw’, it explores how this perspective can provide informed, more nuanced, and actionable perspectives on AI regulation. A focus on sociotechnical change can help analyze when and why AI applications actually do create a meaningful rationale for new regulation — and how they are consequently best approached as targets for regulatory intervention, considering not just the technology, but also six distinct ‘problem logics’ that appear around AI issues across domains. The chapter concludes by briefly reviewing concrete institutional and regulatory actions that can draw on this approach in order to improve the regulatory triage, tailoring, timing & responsiveness, and design of AI policy.",2021-06-16,2022-03-10 20:27:54,2022-03-10 20:27:54,2022-03-10 20:27:54,,,,,,,,,,,,Social Science Research Network,"Rochester, NY",en,,SSRN Scholarly Paper,,,papers.ssrn.com,,DOI: 10.2139/ssrn.3871635,,/Users/jacquesthibodeau/Zotero/storage/89RKKSJR/Maas - 2021 - Aligning AI Regulation to Sociotechnical Change.pdf,,,AI; Artificial Intelligence; Problem Logics; Regulation; Regulatory Rationale; Regulatory Target; Sociotechnical Change; Techlaw,,,,,,,,,,,,,,,,,,,ID 3871635,,,,,,,,,,,,,,,,,,,,,,,,,,,,
B58AD7HY,report,2021,"Liu, Hin-Yan; Maas, Matthijs M.",Solving for X?' Towards a Problem-Finding Framework to Ground Long-Term Governance Strategies for Artificial Intelligence,,,,,https://papers.ssrn.com/abstract=3761623,"Change is hardly a new feature in human affairs. Yet something has begun to change in change. In the face of a range of emerging, complex, and interconnected global challenges, society’s collective governance efforts may need to be put on a different footing. Many of these challenges derive from emerging technological developments – take Artificial Intelligence (AI), the focus of much contemporary governance scholarship and efforts. AI governance strategies have predominantly oriented themselves towards clear, discrete clusters of pre-defined problems. We argue that such ‘problem-solving’ approaches may be necessary, but are also insufficient in the face of many of the ‘wicked problems’ created or driven by AI. Accordingly, we propose in this paper a complementary framework for grounding long-term governance strategies for complex emerging issues such as AI into a ‘problem-finding’ orientation. We first provide a rationale by sketching the range of policy problems created by AI, and providing five reasons why problem-solving governance approaches to these challenges fail or fall short. We conversely argue that that creative, ‘problem-finding’ research into these governance challenges is not only warranted scientifically, but will also be critical in the formulation of governance strategies that are effective, meaningful, and resilient over the long-term. We accordingly illustrate the relation between- and the complementarity of problem-solving and problem-finding research, by articulating a framework that distinguishes between four distinct ‘levels’ of governance: problem-solving research generally approaches AI (governance) issues from a perspective of (Level 0) ‘business-as-usual’ or as (Level 1) ‘governance puzzle-solving’. In contrast, problem-finding approaches emphasize (Level 2) ‘governance Disruptor-Finding’; or (Level 3) ‘Charting Macrostrategic Trajectories’. We apply this theoretical framework to contemporary governance debates around AI throughout our analysis to elaborate upon and to better illustrate our framework. We conclude with reflections on nuances, implications, and shortcomings of this long-term governance framework, offering a range of observations on intra-level failure modes, between-level complementarities, within-level path dependencies, and the categorical boundary conditions of governability (‘Governance Goldilocks Zone’). We suggest that this framework can help underpin more holistic approaches for long-term strategy-making across diverse policy domains and contexts, and help cross the bridge between concrete policies on local solutions, and longer-term considerations of path-dependent societal trajectories to avert, or joint visions towards which global communities can or should be rallied.",2021-01-07,2022-03-10 20:36:06,2022-03-10 20:36:06,2022-03-10 20:36:06,,,,,,,Solving for X?,,,,,Social Science Research Network,"Rochester, NY",en,,SSRN Scholarly Paper,,,papers.ssrn.com,,DOI: 10.2139/ssrn.3761623,,/Users/jacquesthibodeau/Zotero/storage/BKR6VUWD/Liu and Maas - 2021 - 'Solving for X' Towards a Problem-Finding Framewo.pdf,,,AI; Artificial Intelligence; Futures; Governance; Long-term; Macrostrategy; Problem-Finding; Problem-Solving,,,,,,,,,,,,,,,,,,,ID 3761623,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,report,2019,"Drexler, K Eric",Reframing Superintelligence: Comprehensive AI Services as General Intelligence,,,,,,,2019,2019-12-16 02:15,2020-12-19 23:32,,210,,,,,,,,,,,Future of Humanity Institute,,en,,,,,Zotero,,ZSCC: 0000009,,/Users/angelica/Zotero/storage/PEXGT3JJ/Drexler - Reframing Superintelligence.pdf,,FHI; TechSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This is a huge document; rather than summarize it all in this newsletter, I wrote up my summary in [this post](https://www.alignmentforum.org/posts/x3fNwSe5aWZb5yXEG/reframing-superintelligence-comprehensive-ai-services). For this newsletter, I've copied over the description of the model, but left out all of the implications and critiques.

The core idea is to look at the pathway by which we will develop general intelligence, rather than assuming that at some point we will get a superintelligent AGI agent. To predict how AI will progress in the future, we can look at how AI progresses currently -- through research and development (R&D) processes. AI researchers consider a problem, define a search space, formulate an objective, and use an optimization technique in order to obtain an AI system, called a service, that performs the task.

A service is an AI system that delivers bounded results for some task using bounded resources in bounded time. Superintelligent language translation would count as a service, even though it requires a very detailed understanding of the world, including engineering, history, science, etc. Episodic RL agents also count as services.

While each of the AI R&D subtasks is currently performed by a human, as AI progresses we should expect that we will automate these tasks as well. At that point, we will have automated R&D, leading to recursive technological improvement. This is not recursive self-improvement, because the improvement comes from R&D services creating improvements in basic AI building blocks, and those improvements feed back into the R&D services. All of this should happen before we get any powerful AGI agents that can do arbitrary general reasoning."
W8F6VI9I,thesis,2020,"Shah, Rohin Monish",Extracting and Using Preference Information from the State of the World,,,,,https://www.proquest.com/openview/da8bf63ef343781a5bb552122be1bd6a/1?pq-origsite=gscholar&cbl=18750&diss=y,"Typically when learning about what people want and don’t want, we look to human action as evidence: what reward they specify, how they perform a task, or what preferences they express can all provide useful information about what an agent should do. This is essential in order to build AI systems that do what we intend them to do. However, existing methods require a lot of expensive human feedback in order to learn even simple tasks. This dissertation argues that there is an additional source of information that is rather helpful: the state of the world. The key insight of this dissertation is that when a robot is deployed in an envi- ronment that humans have been acting in, the state of the environment is already optimized for what humans want, and is thus informative about human preferences. We formalize this setting by assuming that a human H has been acting in an environment for some time, and a robot R observes the final state produced. From this final state, R must infer as much as possible about H’s reward function. We analyze this problem formulation theoretically and show that it is particularly well suited to inferring aspects of the state that should not be changed – exactly the aspects of the reward that H is likely to forget to specify. We develop an algorithm using dynamic programming for tabular environments, analogously to value iteration, and demonstrate its behavior on several simple environments. To scale to high-dimensional environments, we use function approximators judiciously to allow the various parts of our algorithm to be trained without needing to enumerate all possible states. Of course, there is no point in learning about H’s reward function unless we use it to guide R’s decision-making. While we could have R simply optimize the inferred reward, this suffers from a “status quo bias”: the inferred reward is likely to strongly prefer the observed state, since by assumption it is already optimized for H’s preferences. To get R to make changes to 2 the environment, we will usually need to integrate the inferred reward with other sources of preference information. In order to support such reward combination, we use a model in which R must maximize an unknown reward function known only to H. Learning from the state of the world arises as an instrumentally useful behavior in such a setting, and can serve to form a prior belief over the reward function that can then be updated after further interaction with H",2020-12-17,2022-01-30 04:47:35,2022-01-30 04:47:35,,,24,,,,,,,,,,"University of California, Berkeley","Berkeley, CA",en,,,,,Zotero,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/S96M3KTK/Shah - Extracting and Using Preference Information from t.pdf,,UnsortedSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
H9CGA9V7,thesis,2021,"Maas, Matthijs M.","Artificial Intelligence Governance Under Change: Foundations, Facets, Frameworks",,,,,https://www.ssrn.com/abstract=3833395,"This dissertation explores how we may govern a changing technology, in a changing world, using governance systems that may themselves be left changed. Artificial Intelligence (AI) has made remarkable progress in the past decade, and is anticipated to become an increasingly disruptive, even transformative technology. AI can be functionally understood as a diverse portfolio of computational techniques to improve the accuracy, speed, or scale of machine decision-making, producing capabilities that can support, substitute for-, or improve upon human task performance. The resulting breadth of application makes AI promising—and challenging — in so many domains of life. In recent years diverse AI applications — from facial recognition to automated legal decision-making, and from computational propaganda to Lethal Autonomous Weapons Systems — have raised deep ethical, political, legal and security concerns. With growing public and policymaker attention has come a wave of governance initiatives and proposals. Nonetheless, global governance for AI remains relatively fragmented and incipient. At this cross-roads, this dissertation takes up the research question, “How should global governance for artificial intelligence account for change?” To answer this question, this dissertation draws together scholarship on technology regulation, (international) law, and global governance, in order to unpack three facets of ‘change’ that will prove critical to the global governance of AI. These three facets of change are examined through the conceptual lenses of Sociotechnical Change, Governance Disruption, and Regime Complexity. Sociotechnical Change (Chapter 4) explores how and why technological change in AI produces societal changes that create a rationale for regulatory intervention, and how we can productively characterize the appropriate targets for AI governance. Along with material features, I distinguish six problem logics which highlight different governance solutions and conditions. Governance Disruption (Chapter 5) addresses when, where and why certain AI capabilities might drive or demand change in the substance (Development), tools or processes (Displacement) or political scaffolding (Destruction) of global governance itself, and what are the implications for global regime complexity. Regime Complexity (Chapter 6) helps focus attention on how prospective AI regimes are shaped by underlying changes in the broader global governance architecture. It provides insight into the (1) origins or foundations of AI regimes; the (2) topology of the AI ‘regime complex’; its (3) evolution towards integration or fragmentation; (4) the functional consequences of these paths; and (5) strategies for managing the AI regime complex. Through these three lenses, this dissertation explores key considerations, insights and tradeoffs for AI governance (Chapter 7). It argues that AI governance needs to shift or adopt novel strategies — in conceptual approach, instrument choice, and instrument design — to ensure the efficacy of AI regimes in tracking AI’s sociotechnical impacts, their resilience to future AI-driven disruption to the tools, norms or broader conditions of governance, and their coherence. In this way, AI governance regimes may remain fit for change.",2021,2022-01-30 04:50:24,2022-01-30 04:50:24,2021-12-11 14:45:47,,,,,,,Artificial Intelligence Governance Under Change,,,,,University of Copenhagen,,en,,,,,DOI.org (Crossref),,ZSCC: 0000002,,/Users/jacquesthibodeau/Zotero/storage/2CR8NPGZ/Maas - 2021 - Artificial Intelligence Governance Under Change F.pdf,,MetaSafety,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HHIK9C74,thesis,2015,"Evans, Owain Rhys",Bayesian computational models for inferring preferences,,,,,,,2015,2022-01-30 04:53:08,2022-01-30 04:53:08,,,,,,,,,,,,,Massachusetts Institute of Technology,,,,PhD Thesis,,,Google Scholar,,ZSCC: 0000000,,/Users/jacquesthibodeau/Zotero/storage/ETI5EWEM/Evans - 2015 - Bayesian computational models for inferring prefer.pdf; /Users/jacquesthibodeau/Zotero/storage/S4QH28R6/101522.html,,TechSafety; FHI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
343FMLM8,webpage,,,A Tour of Emerging Cryptographic Technologies | GovAI,,,,,https://www.governance.ai/research-paper/a-tour-of-emerging-cryptographic-technologies,"Historically, progress in the field of cryptography has been enormously consequential. Over the past century, for instance, cryptographic discoveries have played a key role in a world war and made it possible to use the internet..",,2022-03-09 22:49:05,2022-03-09 22:49:05,2022-03-09 22:49:05,,,,,,,,,,,,,,en,,,,,,,,,/Users/jacquesthibodeau/Zotero/storage/INLAMK28/a-tour-of-emerging-cryptographic-technologies.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BAHHAZC4,webpage,,,"The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation (2018). Brundage and Avin et al.",,,,,https://docs.google.com/document/d/e/2PACX-1vQzbSybtXtYzORLqGhdRYXUqiFsaEOvftMSnhVgJ-jRh6plwkzzJXoQ-sKtej3HW_0pzWTFY7-1eoGf/pub,,,2022-03-09 23:37:56,2022-03-09 23:37:56,2022-03-09 23:37:56,,,,,,,,,,,,,,,,,,,,,,,/Users/jacquesthibodeau/Zotero/storage/IQ32QQR3/pub.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CXMQR5MA,webpage,,,Futureproof: Artificial Intelligence Chapter | GovAI,,,,,https://www.governance.ai/research-paper/futureproof-artificial-intelligence-chapter,"Out of the wreckage of the Second World War, the UK transformed itself. It rebuilt its shattered economy. It founded the NHS. It created national insurance. And it helped establish international institutions like the United...",,2022-03-09 23:52:36,2022-03-09 23:52:36,2022-03-09 23:52:36,,,,,,,Futureproof,,,,,,,en,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RQ9ISC6C,webpage,,,"AI Policy Levers: A Review of the U.S. Government’s Tools to Shape AI Research, Development, and Deployment | GovAI",,,,,https://www.governance.ai/research-paper/ai-policy-levers-a-review-of-the-u-s-governments-tools-to-shape-ai-research-development-and-deployment,"The U.S. government (USG) has taken increasing interest in the national security implications of artificial intelligence (AI). In this report, we ask: Given its national security concerns, how migh...",,2022-03-09 23:53:29,2022-03-09 23:53:29,2022-03-09 23:53:29,,,,,,,AI Policy Levers,,,,,,,en,,,,,,,,,/Users/jacquesthibodeau/Zotero/storage/JFEB4SSS/ai-policy-levers-a-review-of-the-u-s-governments-tools-to-shape-ai-research-development-and-dep.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FIJL5WTX,webpage,,,AI & Antitrust: Reconciling Tensions Between Competition Law and Cooperative AI Development | Yale Journal of Law & Technology,,,,,https://yjolt.org/ai-antitrust-reconciling-tensions-between-competition-law-and-cooperative-ai-development,,,2022-03-10 16:29:40,2022-03-10 16:29:40,2022-03-10 16:29:40,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QBEFB3C2,webpage,,,Highly accurate protein structure prediction with AlphaFold | Nature,,,,,https://www.nature.com/articles/s41586-021-03819-2,,,2022-03-10 20:43:39,2022-03-10 20:43:39,2022-03-10 20:43:39,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IXGXFBJD,webpage,,,The Immigration Preferences of Top AI Researchers: New Survey Evidence | GovAI,,,,,https://www.governance.ai/research-paper/the-immigration-preferences-of-top-ai-researchers-new-survey-evidence,"Artificial intelligence (AI) talent is global. AI researchers and engineers come from, and are in high demand, all over theworld. Countries and companies trying to recruit and retain AI talent thus...",,2022-03-10 20:54:14,2022-03-10 20:54:14,2022-03-10 20:54:14,,,,,,,The Immigration Preferences of Top AI Researchers,,,,,,,en,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XZ8LNYAX,webpage,2020,,Improving Verifiability in AI Development,OpenAI,,,,https://openai.com/blog/improving-verifiability/,"We’ve contributed to a multi-stakeholder report by 58 co-authors at 30 organizations, including the Centre for the Future of Intelligence, Mila, Schwartz Reisman Institute for Technology and Society, Center for Advanced Study in the Behavioral Sciences, and Center for Security and Emerging Technologies. This report describes 10 mechanisms to",2020-04-16,2022-03-10 21:12:56,2022-03-10 21:12:56,2022-03-10 21:12:56,,,,,,,,,,,,,,en,,,,,,,,,/Users/jacquesthibodeau/Zotero/storage/5WMR2334/improving-verifiability.html,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8V336TAD,webpage,2019,"Zhou, Hattie","Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask",Uber Engineering Blog,,,,https://eng.uber.com/deconstructing-lottery-tickets/,Uber builds upon the Lottery Ticket Hypothesis by proposing explanations behind these mechanisms and deriving a surprising by-product: the Supermask.,2019-05-06,2022-03-10 21:18:16,2022-03-10 21:18:16,2022-03-10 21:18:16,,,,,,,Deconstructing Lottery Tickets,,,,,,,en-US,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2JH5DTPD,webpage,2016,,Faulty Reward Functions in the Wild,OpenAI,,,,https://openai.com/blog/faulty-reward-functions/,"Reinforcement learning algorithms can break in surprising, counterintuitive ways. In this post we'll explore one failure mode, which is where you misspecify your reward function.",2016-12-22,2022-03-10 22:05:48,2022-03-10 22:05:48,2022-03-10 22:05:48,,,,,,,,,,,,,,en,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
F8KYURCP,webpage,2018,,Likelihood of discontinuous progress around the development of AGI,AI Impacts,,,,https://aiimpacts.org/likelihood-of-discontinuous-progress-around-the-development-of-agi/,"We aren’t convinced by any of the arguments we’ve seen to expect large discontinuity in AI progress above the extremely low base rate for all technologies. However this topic is controversial, and many thinkers on the topic disagree with us, so we consider this an open question. Details Definitions We say a technological discontinuity has...",2018-02-23,2022-03-10 22:07:13,2022-03-10 22:07:13,2022-03-10 22:07:13,,,,,,,,,,,,,,en-US,,,,,,,Section: Featured Articles,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VTR4ADB8,webpage,,"Seita, Daniel",Learning Preferences by Looking at the World,The Berkeley Artificial Intelligence Research Blog,,,,http://bair.berkeley.edu/blog/2019/02/11/learning_preferences/,The BAIR Blog,,2022-03-10 22:42:22,2022-03-10 22:42:22,2022-03-10 22:42:22,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CM7G7ACY,webpage,,"Seita, Daniel",Robots Learning to Move like Animals,The Berkeley Artificial Intelligence Research Blog,,,,http://bair.berkeley.edu/blog/2020/04/03/laikago/,The BAIR Blog,,2022-03-10 22:59:54,2022-03-10 22:59:54,2022-03-10 22:59:54,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UBN7XD3G,webpage,,"Zellers, Rowan",Defending Against Neural Fake News,,,,,https://rowanzellers.com/grover/,,,2022-03-10 23:44:08,2022-03-10 23:44:08,2022-03-10 23:44:08,,,,,,,,,,,,,,en,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UP6S5XPD,webpage,,"Slides, ADAM GAIER Google Brain DAVID HA Google Brain June 12 2019 Download PDF NeurIPS 2019",Weight Agnostic Neural Networks,Weight Agnostic Neural Networks,,,,https://weightagnostic.github.io/,Networks that can already (sort of) perform tasks with random weights.,,2022-03-10 23:47:39,2022-03-10 23:47:39,2022-03-10 23:47:39,,,,,,,,,,,,,,en,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,