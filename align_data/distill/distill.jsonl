{"source": "distill", "source_filetype": "html", "converted_with": "python", "paper_version": "", "post_title": "A Discussion of "Adversarial Examples Are Not Bugs, They Are Features": Adversarial Examples are Just Bugs, Too", "authors": ["Preetum Nakkiran"], "date_published": "2019-08-06", "data_last_modified": "", "url": "", "abstract": "\n                This article is part of a discussion of the Ilyas et al. paper\n                “Adversarial examples are not bugs, they are features”.\n                You can learn more in the\n                \n                    main discussion article\n                .\n            ", "author_comment": "", "journal_ref": "distill pub", "doi": "https://doi.org/10.23915/distill.00019.5", "primary_category": "", "categories": "", "citation_level": "", "main_tex_filename": "", "text": "This article is part of a discussion of the Ilyas et al. paper\n *“Adversarial examples are not bugs, they are features”.*\n You can learn more in the\n [main discussion article](https://distill.pub/2019/advex-bugs-discussion/) .\n \n\n\n[All Responses](https://distill.pub/2019/advex-bugs-discussion/#articles)\n[Comment by Ilyas et al.](https://distill.pub/2019/advex-bugs-discussion/response-5/#rebuttal)\n\n\n We demonstrate that there exist adversarial examples which are just “bugs”:\n aberrations in the classifier that are not intrinsic properties of the data distribution.\n In particular, we give a new method for constructing adversarial examples which:\n \n\n\n1. Do not transfer between models, and\n2. Do not leak “non-robust features” which allow for learning, in the\n sense of Ilyas-Santurkar-Tsipras-Engstrom-Tran-Madry\n .\n\n\n\n We replicate the Ilyas et al.\n experiment of training on mislabeled adversarially-perturbed images\n (Section 3.2 of ),\n and show that it fails for our construction of adversarial perturbations.\n \n\n\n\n The message is, whether adversarial examples are features or bugs depends\n on how you find them\u2009—\u2009standard PGD finds features, but bugs are abundant as well.\n \n\n\n\n We also give a toy example of a data distribution which has no “non-robust features”\n (under any reasonable definition of feature), but for which standard training yields a highly non-robust\n classifier.\n This demonstrates, again, that adversarial examples can occur even if the data distribution does not\n intrinsically have any vulnerable directions.\n \n\n\n### Background\n\n\n\n Many have understood Ilyas et al. \n to claim that adversarial examples are not “bugs”, but are “features”.\n Specifically, Ilyas et al. postulate the following two worlds:\n As communicated to us by the original authors.\n\n\n\n* **World 1: Adversarial examples exploit directions irrelevant for classification (“bugs”).** \n In this world, adversarial examples occur because classifiers behave\n poorly off-distribution,\n when they are evaluated on inputs that are not natural images.\n Here, adversarial examples would occur in arbitrary directions,\n having nothing to do with the true data distribution.\n* **World 2: Adversarial examples exploit useful directions for classification (“features”).**\n In this world, adversarial examples occur in directions that are still “on-distribution”,\n and which contain features of the target class.\n For example, consider the perturbation that\n makes an image of a dog to be classified as a cat.\n In World 2, this perturbation is not purely random, but has something to do with cats.\n Moreover, we expect that this perturbation transfers to other classifiers trained to distinguish cats\n vs. dogs.\n\n\n\n Our main contribution is demonstrating that these worlds are not mutually exclusive\u2009—\u2009and in fact, we\n are in both.\n Ilyas et al. \n show that there exist adversarial examples in World 2, and we show there exist\n examples in World 1.\n \n\n\n## \n Constructing Non-transferrable Targeted Adversarial Examples\n\n\n\n\n We propose a method to construct targeted adversarial examples for a given classifier\n fff,\n which do not transfer to other classifiers trained for the same problem.\n \n\n\n\n Recall that for a classifier fff, an input example (x,y)(x,y)(x, y), and target class ytargytargy\\_{targ},\n a *targeted adversarial example* is an x′x′x’ such that ||x−x′||≤ε||x−x′||≤ε||x - x’||\\leq \\eps and\n f(x′)=ytargf(x′)=ytargf(x’) = y\\_{targ}.\n \n\n\n\n The standard method of constructing adversarial examples is via Projected Gradient Descent (PGD)\n \n PGD is described in the appendix.\n \n which starts at input xxx, and iteratively takes steps {xt}{xt}\\{x\\_t\\}\n to minimize the loss L(f,xt,ytarg)L(f,xt,ytarg)L(f, x\\_t, y\\_{targ}).\n That is, we take steps in the direction\n −∇xL(f,xt,ytarg)−∇xL(f,xt,ytarg)-\\nabla\\_x L(f, x\\_t, y\\_{targ})\n where L(f,x,y)L(f,x,y)L(f, x, y) is the loss of fff on input xxx, label yyy.\n \n\n\n\n Note that since PGD steps in the gradient direction towards the target class,\n we may expect these adversarial examples have *feature leakage* from the target class.\n For example, suppose we are perturbing an image of a dog into a plane (which usually appears against a blue\n background).\n It is plausible that the gradient direction tends to make the dog image more blue,\n since the “blue” direction is correlated with the plane class.\n In our construction below, we attempt to eliminate such feature leakage.\n\n \n\n\n![](./A Discussion of \"Adversarial Examples Are Not Bugs, They Are Features\"_ Adversarial Examples are Just Bugs, Too_files/manifold.svg)\n\n\n An illustration of the image-manifold for adversarially perturbing a dog to a plane.\n The gradient of the loss can be thought of as having an on-manifold “feature component”\n and an off-manifold “random component”.\n PGD steps along both components, hence causing feature-leakage in adversarial examples.\n Our construction below attempts to step only in the off-manifold direction.\n \n\n### Our Construction\n\n\n\n Let {fi:Rn→Y}i{fi:Rn→Y}i\\{f\\_i\xa0: \\R^n \\to \\cY\\}\\_i be an ensemble of classifiers\n for the same classification problem as fff.\n For example, we can let {fi}{fi}\\{f\\_i\\} be a collection of ResNet18s trained from\n different random initializations.\n \n\n\n\n For input example (x,y)(x,y)(x, y) and target class ytargytargy\\_{targ},\n we perform iterative updates to find adversarial attacks\u2009—\u2009as in PGD.\n However, instead of stepping directly in the gradient direction, we\n step in the direction\n \n Formally, we replace the iterative step with\n xt+1←Πε(xt−α(∇xL(f,xt,ytarg)+Ei[∇xL(fi,xt,y)]))xt+1←Πε(xt−α(∇xL(f,xt,ytarg)+Ei\u2061[∇xL(fi,xt,y)]))x\\_{t+1} \\gets \\Pi\\_\\eps\\left( x\\_t\n - \\alpha( \\nabla\\_x L(f, x\\_t, y\\_{targ}) + \\E\\_i[ \\nabla\\_x L(f\\_i, x\\_t, y)]) \\right)\n where ΠεΠε\\Pi\\_\\eps is the projection onto the εε\\eps-ball around xxx.\n \n−(∇xL(f,xt,ytarg)+Ei[∇xL(fi,xt,y)])−(∇xL(f,xt,ytarg)+Ei\u2061[∇xL(fi,xt,y)])-\\left( \\nabla\\_x L(f, x\\_t, y\\_{targ}) + \\E\\_i[ \\nabla\\_x L(f\\_i, x\\_t, y)] \\right)\n\n That is, instead of taking gradient steps to minimize L(f,x,ytarg)L(f,x,ytarg)L(f, x, y\\_{targ}),\n we minimize the “disentangled loss”\n \n We could also consider explicitly using the ensemble to decorrelate,\n by stepping in direction\n ∇xL(f,x,ytarg)−Ei[∇xL(fi,x,ytarg)]∇xL(f,x,ytarg)−Ei\u2061[∇xL(fi,x,ytarg)]\\nabla\\_x L(f, x, y\\_{targ}) - \\E\\_i[ \\nabla\\_x L(f\\_i, x, y\\_{targ})].\n This works well for small ϵϵ\\epsilon,\n but the given loss has better optimization properties for larger ϵϵ\\epsilon.\n \nL(f,x,ytarg)+Ei[L(fi,x,y)]L(f,x,ytarg)+Ei\u2061[L(fi,x,y)]L(f, x, y\\_{targ}) + \\E\\_i[L(f\\_i, x, y)]\n This loss encourages finding an xtxtx\\_t which is adversarial for fff,\n but not for the ensemble {fi}{fi}\\{f\\_i\\}.\n \n\n\n\n These adversarial examples will not be adversarial for the ensemble {fi}{fi}\\{f\\_i\\}. But perhaps surprisingly,\n these examples are also not adversarial for\n *new* classifiers trained for the same problem.\n \n\n\n### Experiments\n\n\n\n We train a ResNet18 on CIFAR10 as our target classifier fff.\n For our ensemble, we train 10 ResNet18s on CIFAR10, from fresh random initializations.\n We then test the probability that\n a targeted attack for fff\n transfers to a new (freshly-trained) ResNet18, with the same targeted class.\n Our construction yields adversarial examples which do not transfer well to new models.\n \n\n\n\n For L∞L∞L\\_{\\infty} attacks:\n \n\n\n\n\n|  | Attack Success | Transfer Success |\n| PGD | 99.6% | 52.1% |\n| Ours | 98.6% | 0.8% |\n\n\n\n For L2L2L\\_2 attacks:\n \n\n\n\n\n|  | Attack Success | Transfer Success |\n| PGD | 99.9% | 82.5% |\n| Ours | 99.3% | 1.7% |\n\n\n## Adversarial Examples With No Features\n\n\n\n Using the above, we can construct adversarial examples\n which *do not suffice* for learning.\n Here, we replicate the Ilyas et al. experiment\n that “Non-robust features suffice for standard classification”\n (Section 3.2 of ),\n but show that it fails for our construction of adversarial examples.\n \n\n\n\n To review, the Ilyas et al. non-robust experiment was:\n \n\n1. Train a standard classifier fff for CIFAR.\n2. From the CIFAR10 training set S={(Xi,Yi)}S={(Xi,Yi)}S = \\{(X\\_i, Y\\_i)\\},\n construct an alternate train set S′={(XYi→(Yi+1)i,Yi+1)}S′={(XiYi→(Yi+1),Yi+1)}S’ = \\{(X\\_i^{Y\\_i \\to (Y\\_i + 1)}, Y\\_i + 1)\\},\n where XYi→(Yi+1)iXiYi→(Yi+1)X\\_i^{Y\\_i \\to (Y\\_i +1)} denotes an adversarial example for\n fff, perturbing XiXiX\\_i from its true class YiYiY\\_i towards target class Yi+1(mod\xa010)Yi+1(mod\xa010)Y\\_i+1 (\\text{mod }10).\n Note that S′S′S’ appears to humans as “mislabeled examples”.\n3. Train a new classifier f′f′f’ on train set S′S′S’.\n Observe that this classifier has non-trivial accuracy on the original CIFAR distribution.\n\n\n\n\n Ilyas et al. use Step (3) to argue that\n adversarial examples have a meaningful “feature” component.\n\n However, for adversarial examples constructed using our method, Step (3) fails.\n In fact, f′f′f’ has good accuracy with respect to the “label-shifted” distribution\n (X,Y+1)(X,Y+1)(X, Y+1), which is intuitively what we trained on.\n \n\n\nFor L∞L∞L\\_{\\infty} attacks:\n\n\n\n\n\n|  | Test Acc on CIFAR: (X,Y)(X,Y)(X, Y) | Test Acc on Shifted-CIFAR: (X,Y+1)(X,Y+1)(X, Y+1) |\n| PGD | 23.7% | 40.4% |\n| Ours | 2.5% | 75.9% |\n\n\n\n\n Table: Test Accuracies of f′f′f’\n\n\n\nFor L2L2L\\_2 attacks:\n\n\n\n\n\n|  | Test Acc on CIFAR: (X,Y)(X,Y)(X, Y) | Test Acc on Shifted-CIFAR: (X,Y+1)(X,Y+1)(X, Y+1) |\n| PGD | 33.2% | 27.3% |\n| Ours | 2.8% | 70.8% |\n\n\n\n\n Table: Test Accuracies of f′f′f’\n\n\n\n## Adversarial Squares: Adversarial Examples from Robust Features\n\n\n\n To further illustrate that adversarial examples can be “just bugs”,\n we show that they can arise even when the true data distribution has no “non-robust features”\u2009—\u2009that is, no intrinsically vulnerable directions.\n \n We are unaware of a satisfactory definition of “non-robust feature”, but we claim that for any\n reasonable\n *intrinsic* definition, this problem has no non-robust features.\n Intrinsic here meaning, a definition which depends only on geometric properties of the data\n distribution, and not on the family of classifiers, or the finite-sample training set.\n\n   \n\n We do not use the Ilyas et al. definition of “non-robust features,” because we believe it is vacuous.\n In particular, by the Ilyas et al. definition, **every** distribution\n has “non-robust features”\u2009—\u2009so the definition does not discern structural properties of the\n distribution.\n Moreover, for every “robust feature” fff, there exists a corresponding “non-robust feature” f′f′f’, such\n that fff and f′f′f’ agree on the data distribution\u2009—\u2009so the definition depends strongly on the\n family of classifiers being considered.\n \n In the following toy problem, adversarial vulnerability arises as a consequence of finite-sample\n overfitting, and\n label noise.\n \n\n\n\n The problem is to distinguish between CIFAR-sized images that are either all-black or all-white,\n with a small amount of random pixel noise and label noise.\n \n\n\n\n\n![](./A Discussion of \"Adversarial Examples Are Not Bugs, They Are Features\"_ Adversarial Examples are Just Bugs, Too_files/twosquares.png)\n\n\n\n A sample of images from the distribution.\n \n\n\n\n\n![](./A Discussion of \"Adversarial Examples Are Not Bugs, They Are Features\"_ Adversarial Examples are Just Bugs, Too_files/data.png)\n\n Formally, let the distribution be as follows.\n Pick label Y∈{±1}Y∈{±1}Y \\in \\{\\pm 1\\} uniformly,\n and let X:={(+→1+→ηε)⋅ηif\xa0Y=1(−→1+→ηε)⋅ηif\xa0Y=−1X:={(+1→+η→ε)⋅ηif\xa0Y=1(−1→+η→ε)⋅ηif\xa0Y=−1X\xa0:=\n \\begin{cases}\n (+\\vec{\\mathbb{1}} + \\vec\\eta\\_\\eps) \\cdot \\eta & \\text{if $Y=1$}\\\\\n (-\\vec{\\mathbb{1}} + \\vec\\eta\\_\\eps) \\cdot \\eta & \\text{if $Y=-1$}\\\\\n \\end{cases}\n\n where →ηε∼[−0.1,+0.1]dη→ε∼[−0.1,+0.1]d\\vec\\eta\\_\\eps \\sim [-0.1, +0.1]^d is uniform L∞L∞L\\_\\infty pixel noise,\n and\n η∈{±1}∼Bernoulli(0.1)η∈{±1}∼Bernoulli(0.1)\\eta \\in \\{\\pm 1\\} \\sim Bernoulli(0.1) is the 10% label noise.\n\n   \n\n  \n\n A plot of samples from a 2D-version of this distribution is shown to the right.\n\n \n\n\n\n Notice that there exists a robust linear classifier for this problem which achieves perfect robust\n classification, with up to ε=0.9ε=0.9\\eps = 0.9 magnitude L∞L∞L\\_\\infty attacks.\n However, if we sample 10000 training images from this distribution, and train\n a ResNet18 to 99.9% train accuracy,\n \n We optimize using Adam with learning-rate 0.000010.000010.00001 and batch size 128128128 for 20 epochs.\n \n the resulting classifier is highly non-robust:\n an ε=0.01ε=0.01\\eps=0.01 perturbation suffices to flip the class of almost all test examples.\n \n\n\n\n The input-noise and label noise are both essential for this construction.\n One intuition for what is happening is: in the initial stage of training\n the optimization learns the “correct” decision boundary (indeed, stopping after 1 epoch results in a robust\n classifier).\n However, optimizing for close to 0 train-error requires a network with high Lipshitz constant\n to fit the label-noise, which hurts robustness.\n\n \n\n\n\n![](./A Discussion of \"Adversarial Examples Are Not Bugs, They Are Features\"_ Adversarial Examples are Just Bugs, Too_files/data.png)\n![](./A Discussion of \"Adversarial Examples Are Not Bugs, They Are Features\"_ Adversarial Examples are Just Bugs, Too_files/step10.png)\n![](./A Discussion of \"Adversarial Examples Are Not Bugs, They Are Features\"_ Adversarial Examples are Just Bugs, Too_files/step10000.png)\n\n\n\n Left: The training set (labels color-coded). Middle: The classifier after 10 SGD steps.\n Right: The classifier at the end of training. Note that it is overfit, and not robust.\n \n Figure adapted from .\n \n\n\n\n\n\n\n## Addendum: Data Poisoning via Adversarial Examples\n\n\n\n As an addendum, we observe that the “non-robust features”\n experiment of  (Section 3.2)\n directly implies data-poisoning attacks:\n An adversary that is allowed to imperceptibly change every image in the training set can destroy the\n accuracy of the learnt classifier\u2009—\u2009and can moreover apply an arbitrary permutation\n to the classifier output labels (e.g. swapping cats and dogs).\n \n\n\n\n To see this, recall that the original “non-robust features” experiment shows:Using our previous\n notation, and also using vanilla PGD to find adversarial examples.\n\n\n\n\n 1. If we train on distribution (XY→(Y+1),Y+1)(XY→(Y+1),Y+1)(X^{Y \\to (Y+1)}, Y+ 1) the classifier learns to predict well\n on distribution (X,Y)(X,Y)(X, Y).\n \n\n\n\n By permutation-symmetry of the labels, this implies that:\n \n\n\n\n 2. If we train on distribution (XY→(Y+1),Y)(XY→(Y+1),Y)(X^{Y \\to (Y+1)}, Y) the classifier learns to predict well\n on distribution (X,Y−1)(X,Y−1)(X, Y-1).\n \n\n\n\n Note that in case (2), we are training with correct labels, just perturbing the inputs imperceptibly,\n but the classifier learns to predict the cyclically-shifted labels.\n Concretely, using the original numbers of\n Table 1 in , this reduction implies that\n **an adversary can perturb the CIFAR10 train set by ε=0.5ε=0.5\\eps=0.5 in L2L2L\\_2,\n and cause the learnt classifier to output shifted-labels\n 43.7% of the time\n (cats classified as birds, dogs as deers, etc).** \n\n\n\n\n This should extend to attacks that force arbitrary desired permutations of the labels.\n \n\n\n\n To cite Ilyas et al.’s response, please cite their\n [collection of responses](https://distill.pub/2019/advex-bugs-discussion/original-authors/#citation).\n \n\n**Response Summary**: We note that as discussed in\n more detail in [Takeaway #1](https://distill.pub/2019/advex-bugs-discussion/original-authors/#takeaway1), the\n mere existence of adversarial\n examples\n that are “features” is sufficient to corroborate our main thesis. This comment\n illustrates, however, that we can indeed craft adversarial examples that are\n based on “bugs” in realistic settings. Interestingly, such examples don’t\n transfer, which provides further support for the link between transferability\n and non-robust features.\n\n \n\n\n**Response**: As mentioned [above](https://distill.pub/2019/advex-bugs-discussion/original-authors/#nonclaim1),\n we did not intend to claim\n that adversarial examples arise *exclusively* from (useful) features but rather\n that useful non-robust features exist and are thus (at least\n partially) responsible for adversarial vulnerability. In fact,\n prior work already shows how in theory adversarial examples can arise from\n insufficient samples  or finite-sample overfitting\n , and the experiments\n presented here (particularly, the adversarial squares) constitute a neat\n real-world demonstration of these facts. \n\n\n Our main thesis that “adversarial examples will not just go away as we fix\n bugs in our models” is not contradicted by the existence of adversarial examples\n stemming from “bugs.” As long as adversarial examples can stem from non-robust\n features (which the commenter seems to agree with), fixing these bugs will not\n solve the problem of adversarial examples. \n\n\nMoreover, with regards to feature “leakage” from PGD, recall that in\n or D\\_det dataset, the non-robust features are associated with the\n correct label whereas the robust features are associated with the wrong\n one. We wanted to emphasize that, as\n [shown in Appendix D.6](https://arxiv.org/abs/1905.02175) ,\n models trained on our DdetDdetD\\_{det} dataset actually generalize *better* to\n the non-robust feature-label association that to the robust\n feature-label association. In contrast, if PGD introduced a small\n “leakage” of non-robust features, then we would expect the trained model\n would still predominantly use the robust feature-label association. \n\n\n That said, the experiments cleverly zoom in on some more fine-grained\n nuances in our understanding of adversarial examples. One particular thing that\n stood out to us is that by creating a set of adversarial examples that are\n *explicitly* non-transferable, one also prevents new classifiers from learning\n features from that dataset. This finding thus makes the connection between\n transferability of adversarial examples and their containing generalizing\n features even stronger! Indeed, we can add the constructed dataset into our\n “ˆDdetD^det\\widehat{\\mathcal{D}}\\_{det} learnability vs transferability” plot\n (Figure 3 in the paper)\u2009—\u2009the point\n corresponding to this dataset fits neatly onto the trendline! \n\n\n\n![](./A Discussion of \"Adversarial Examples Are Not Bugs, They Are Features\"_ Adversarial Examples are Just Bugs, Too_files/transfer.png)\n\n Relationship between models reliance on non-robust features and their susceptibility to transfer\n attacks\n \n\n\n\n You can find more responses in the  [main discussion article](https://distill.pub/2019/advex-bugs-discussion/).", "bibliography_bbl": "", "bibliography_bib": [{"title": "Adversarial examples are not bugs, they are features"}, {"title": "SGD on Neural Networks Learns Functions of Increasing Complexity"}, {"title": "Adversarially Robust Generalization Requires More Data"}, {"title": "A boundary tilting persepective on the phenomenon of adversarial examples"}]}
{"source": "distill", "source_filetype": "html", "converted_with": "python", "paper_version": "", "post_title": "Differentiable Image Parameterizations", "authors": ["Alexander Mordvintsev", "Nicola Pezzotti", "Ludwig Schubert", "Chris Olah"], "date_published": "2018-07-25", "data_last_modified": "", "url": "", "abstract": "\n    Neural networks trained to classify images have a remarkable\u2009—\u2009and surprising!\u2009—\u2009capacity to generate images.\n    Techniques such as DeepDream , style transfer, and feature visualization leverage this capacity as a powerful tool for exploring the inner workings of neural networks, and to fuel a small artistic movement based on neural art.\n  ", "author_comment": "", "journal_ref": "distill pub", "doi": "https://doi.org/10.23915/distill.00012", "primary_category": "", "categories": "", "citation_level": "", "main_tex_filename": "", "text": "Neural networks trained to classify images have a remarkable\u2009—\u2009and surprising!\u2009—\u2009capacity to generate images.\n Techniques such as DeepDream , style transfer, and feature visualization leverage this capacity as a powerful tool for exploring the inner workings of neural networks, and to fuel a small artistic movement based on neural art.\n \n\n\n\n All these techniques work in roughly the same way.\n Neural networks used in computer vision have a rich internal representation of the images they look at.\n We can use this representation to describe the properties we want an image to have (e.g. style), and then optimize the input image to have those properties.\n This kind of optimization is possible because the networks are differentiable with respect to their inputs: we can slightly tweak the image to better fit the desired properties, and then iteratively apply such tweaks in gradient descent.\n \n\n\n\n Typically, we parameterize the input image as the RGB values of each pixel, but that isn’t the only way.\n As long as the mapping from parameters to images is differentiable, we can still optimize alternative parameterizations with gradient descent.\n \n\n\n\n\n\n[1](https://distill.pub/2018/differentiable-parameterizations/#figure-differentiable-parameterizations):\n As long as an \nimage para\xadmeter\xadization\nis differ\xadentiable, we can back\xadpropagate\n(    )\nthrough it.\n\n\n\n\n\nMappingParametersimage/RGB spaceLossFunction\n\n\n\n\n Differentiable image parameterizations invite us to ask “what kind of image generation process can we backpropagate through?”\n The answer is quite a lot, and some of the more exotic possibilities can create a wide range of interesting effects, including 3D neural art, images with transparency, and aligned interpolation.\n Previous work using specific unusual image parameterizations  has shown exciting results\u2009—\u2009we think that zooming out and looking at this area as a whole suggests there’s even more potential.\n \n\n\n\n\n### Why Does Parameterization Matter?\n\n\n\n It may seem surprising that changing the parameterization of an optimization problem can significantly change the result, despite the objective function that is actually being optimized remaining the same.\n We see four reasons why the choice of parameterization can have a significant effect:\n\n\n\n**(1) - Improved Optimization** -\nTransforming the input to make an optimization problem easier\u2009—\u2009a technique called “preconditioning”\u2009—\u2009is a staple of optimization.\n \n Preconditioning is most often presented as a transformation of the gradient\n (usually multiplying it by a positive definite “preconditioner” matrix).\n However, this is equivalent to optimizing an alternate parameterization of the input.\n \nWe find that simple changes in parameterization make image optimization for neural art and image optimization much easier.\n\n\n\n**(2) - Basins of Attraction** -\nWhen we optimize the input to a neural network, there are often many different solutions, corresponding to different local minima.\n \n Training deep neural networks characterized by complex optimization landscapes , which may have many equally good local minima for a given objective.\n (Note that finding the global minimum is not always desirable as it may result in an overfitted model .)\n Thus, it’s probably not surprising that optimizing the input to a neural network would also have many local minima.\n \nThe probability of our optimization process falling into any particular local minima is controlled by its basin of attraction (i.e., the region of the optimization landscape under the influence of the minimum).\nChanging the parameterization of an optimization problem is known to change the sizes of different basins of attraction, influencing the likely result.\n\n\n\n**(3) - Additional Constraints**  -\nSome parameterizations cover only a subset of possible inputs, rather than the entire space.\nAn optimizer working in such a parameterization will still find solutions that minimize or maximize the objective function, but they’ll be subject to the constraints of the parameterization.\nBy picking the right set of constraints, one can impose a variety of constraints, ranging from simple constraints (e.g., the boundary of the image must be black), to rich, subtle constraints.\n\n\n\n**(4) - Implicitly Optimizing other Objects** -\n A parameterization may internally use a different kind of object than the one it outputs and we optimize for.\n For example, while the natural input to a vision network is an RGB image, we can parameterize that image as a rendering of a 3D object and, by backpropagating through the rendering process, optimize that instead.\n Because the 3D object has more degrees of freedom than the image, we generally use a *stochastic* parameterization that produces images rendered from different perspectives.\n\n\n\nIn the rest of the article we give concrete examples where such approaches are beneficial and lead to surprising and interesting visual results.\n\n\n\n\n\n\n---\n\n\n\n[1](https://distill.pub/2018/differentiable-parameterizations/#section-aligned-interpolation)\n\n\n\n## [Aligned Feature Visualization Interpolation](https://distill.pub/2018/differentiable-parameterizations/#section-aligned-interpolation)\n\n\n\n\n Feature visualization is most often used to visualize individual neurons,\n but it can also be used to [visualize combinations of neurons](https://distill.pub/2017/feature-visualization/#interaction), in order to study how they interact .\n Instead of optimizing an image to make a single neuron fire, one optimizes it to make multiple neurons fire.\n\n\n\n\n When we want to really understand the interaction between two neurons,\n we can go a step further and create multiple visualizations,\n gradually shifting the objective from optimizing one neuron to putting more weight on the other neuron firing.\n This is in some ways similar to interpolation in the latent spaces of generative models like GANs.\n\n\n\n\n Despite this, there is a small challenge: feature visualization is stochastic.\n Even if you optimize for the exact same objective, the visualization will be laid out differently each time.\n Normally, this isn’t a problem, but it does detract from the interpolation visualizations.\n If we make them naively, the resulting visualizations will be *unaligned*:\n visual landmarks such as eyes appear in different locations in each image.\n This lack of alignment can make it harder to see the difference due to slightly different objectives,\n because they’re swamped by the much larger differences in layout.\n\n\n\n\n We can see the issue with independent optimization if we look at the interpolated frames as an animation:\n\n\n\n\n\n[2](https://distill.pub/2018/differentiable-parameterizations/#figure-aligned-interpolation-comparison)\n\n\n[Reproduce in a Notebook](https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/differentiable-parameterizations/aligned_interpolation.ipynb)\n#### Unaligned Interpolation\n\n\nVisual landmarks, such as eyes, change position from one frame to the next.\n\n\n\n\n\npause\n\n#### Aligned Interpolation\n\n\nFrames are easier to compare because visual landmarks stay in place.\n\n\n\n\n\npause\n\n\n\n How can we achieve this aligned interpolation, where visual landmarks do not move between frames?\n There are a number of possible approaches one could try\n \n For example, one could explicitly penalize differences between adjacent frames. Our final result and our colab notebook use this technique in combination with a shared parameterization.\n \n \xa0, one of which is using a *shared parameterization*: each frame is parameterized as a combination of its own unique parameterization, and a single shared one.\n \n\n\n\n\n[3](https://distill.pub/2018/differentiable-parameterizations/#figure-aligned-interpolation-examples)\n\n\n[Reproduce in a Notebook](https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/differentiable-parameterizations/aligned_interpolation.ipynb)\n\nWe start with independently parameterized frames.\n\n\n\n\nEach frame is then combined with a single, shared parameterization…\n\n\n\n…to create a visually aligned neuron interpolation.\n\n\n\n\n\n By partially sharing a parameterization between frames, we encourage the resulting visualizations to naturally align.\n Intuitively, the shared parameterization provides a common reference for the displacement of visual landmarks, while the unique one gives to each frame its own visual appeal based on its interpolation weights.\n \n Concretely, we combine a usually lower-resolution shared parameterization Pshared P\\_{\\text{shared}}Pshared\u200b and full-resolution independent parameterizations PuniqueiP\\_{\\text{unique}}^iPuniquei\u200b that are unique to each frame iii of the visualization.\n Each individual frame iii is then parameterized as a combination PiP^iPi of the two, Pi=σ(Puniquei+Pshared)P^i = \\sigma(P\\_{\\text{unique}}^i + P\\_{\\text{shared}})Pi=σ(Puniquei\u200b+Pshared\u200b), where σ\\sigmaσ is the logistic sigmoid function.\n \n This parameterization doesn’t change the objective, but it does enlarge the **(2) basins of attraction** where the visualizations are aligned.\n \n We can explicitly visualize how shared parameterization affects the basins of attraction in a toy example.\n Let’s consider optimizing two variables xxx and yyy to both minimize L(z)=(z2−1)2L(z)= (z^2-1)^2L(z)=(z2−1)2.\n Since L(z)L(z)L(z) has two basins of attraction z=1z=1z=1 or z=−1z=-1z=−1, the pair of optimization problems has four solutions:\n (x,y)=(1,1)(x,y) = (1,1)(x,y)=(1,1), (x,y)=(−1,1)(x,y) = (-1,1)(x,y)=(−1,1), (x,y)=(1,−1)(x,y) = (1,-1)(x,y)=(1,−1), or (x,y)=(−1,−1)(x,y) = (-1,-1)(x,y)=(−1,−1).\n Let’s consider randomly initializing xxx and yyy, and then optimizing them.\n Normally, the optimization problems are independent, so xxx and yyy are equally likely to come to unaligned solutions (where they have different signs) as aligned ones.\n But if we add a shared parameterization, the problems become coupled and the basin of attraction where they’re aligned becomes bigger.  \n\n![](./Differentiable Image Parameterizations_files/basin-alignment.png)\n\n\n\n\n\n This is an initial example of how differentiable parameterizations in general can be a useful additional tool in visualizing neural networks.\n \n\n\n\n\n\n---\n\n\n\n[2](https://distill.pub/2018/differentiable-parameterizations/#section-styletransfer)\n\n\n\n## [Style Transfer with non-VGG architectures](https://distill.pub/2018/differentiable-parameterizations/#section-styletransfer)\n\n\n\n Neural style transfer has a mystery:\n despite its remarkable success, almost all style transfer is done with variants of the **VGG architecture**.\n This isn’t because no one is interested in doing style transfer on other architectures, but because attempts to do it on other architectures consistently work poorly.\n \n Examples of experiments performed with different architectures can be found on [Medium](https://medium.com/mlreview/getting-inception-architectures-to-work-with-style-transfer-767d53475bf8), [Reddit](https://www.reddit.com/r/MachineLearning/comments/7rrrk3/d_eat_your_vggtables_or_why_does_neural_style/) and [Twitter](https://twitter.com/hardmaru/status/954173051330904065).\n \n\n\n\n\n\n Several hypotheses have been proposed to explain why VGG works so much better than other models.\n One suggested explanation is that VGG’s large size causes it to capture information that other models discard.\n This extra information, the hypothesis goes, isn’t helpful for classification, but it does cause the model to work better for style transfer.\n An alternate hypothesis is that other models downsample more aggressively than VGG, losing spatial information.\n We suspect that there may be another factor: most modern vision models have checkerboard artifacts in their gradient , which could make optimization of the stylized image more difficult.\n \n\n\n\n In previous work we found that a [decorrelated parameterization can significantly improve optimization](https://distill.pub/2017/feature-visualization/#preconditioning).\n We find the same approach also improves style transfer, allowing us to use a model that did not otherwise produce visually appealing style transfer results:\n \n\n\n#### Content Image\n\n\n\n\n#### Style Image\n\n\n\n\n![An arrow joining content and style images, and pointing towards the style transfer result.](./Differentiable Image Parameterizations_files/vertical-join-arrow.svg)\n#### Final image Optimization\n\n\n![Background Image](./Differentiable Image Parameterizations_files/woman-food.jpg)\nPixel Space\nDecorrelated Space\n![Foreground Image](./Differentiable Image Parameterizations_files/woman-food-decorrelated.jpg)\n\nStyletransfer of "Woman with a Hat" (*Henri Matisse*, 1905) onto a photograph by Brooke Lark.\n\n[4](https://distill.pub/2018/differentiable-parameterizations/#figure-style-transfer-examples):\n Move the slider under “final image optimization” to compare optimization in pixel space with optimization in a decorrelated space. Both images were created with the same objective and differ only in their parameterization.\n [Reproduce in a Notebook](https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/differentiable-parameterizations/style_transfer_2d.ipynb)\n\n\n\n Let’s consider this change in a bit more detail. Style transfer involves three images: a content image, a style image, and the image we optimize.\n All three of these feed into the CNN, and the style transfer objective is based on the differences in how these images activate the CNN.\n The only change we make is how we parameterize the optimized image. Instead of parameterizing it in terms of pixels (which are highly correlated with their neighbors), we use a scaled Fourier transform.\n \n\n\n\n\n\n[5](https://distill.pub/2018/differentiable-parameterizations/#figure-style-transfer-diagram)\n\n\n\ncontent imagelearned imageLossContentStyleCNNCNNCNNstyle imageParameterizing the learned image in a decorrelated space makes style transfer more robust to choice of model.The content objective aims to get neurons to fire in the same position as they did for the content image.The style objective aims to create similar patterns of neuron activation as in the style image\u2009—\u2009without regard to position.inverse2D FFT\n\n\n\n Our exact implementation can be found in the accompanying notebook. Note that it also uses [transformation robustness](https://distill.pub/2017/feature-visualization/#regularizer-playground-robust), which not all implementations of style transfer use.\n \n\n\n\n\n\n---\n\n\n\n[3](https://distill.pub/2018/differentiable-parameterizations/#section-xy2rgb)\n\n\n\n## [Compositional Pattern Producing Networks](https://distill.pub/2018/differentiable-parameterizations/#section-xy2rgb)\n\n\n\n So far, we’ve explored image parameterizations that are relatively close to how we normally think of images, using pixels or Fourier components.\n In this section, we explore the possibility of  **(3) adding additional constraints** to the optimization process by using a different parameterization.\n More specifically, we parameterize our image as a neural network \u2009—\u2009in particular, a Compositional Pattern Producing Network (CPPN) .\n \n\n\n\n CPPNs are neural networks that map (x,y)(x,y)(x,y) positions to image colors:\n \n\n\n\n(x,y)\xa0→CPPN\xa0(r,g,b)(x,y) ~\\xrightarrow{\\tiny CPPN}~ (r,g,b)(x,y)\xa0CPPN\u200b\xa0(r,g,b)\n\n\n By applying the CPPN to a grid of positions, one can make arbitrary resolution images.\n The parameters of the CPPN network\u2009—\u2009the weights and biases\u2009—\u2009determine what image is produced.\n Depending on the architecture chosen for the CPPN, pixels in the resulting image are constraint to share, up to a certain degree, the color of their neighbors.\n \n\n\n\n Random parameters can produce aesthetically interesting images , but we can produce more interesting images by learning the parameters of the CPPN .\n Often this is done by evolution ; here we explore the possibility to backpropagate some objective function, such a feature visualization objective.\n This is easily done since the CPPN network is differentiable as the convolutional neural network and the objective function can be propagated also through the CPPN to update its parameters accordingly.\n That is to say, CPPNs are a differentiable image parameterization\u2009—\u2009a general tool for parameterizing images in any neural art or visualization task.\n \n\n\n\nWeightsChannelCNNCPPNimage/RGB\n\n[6](https://distill.pub/2018/differentiable-parameterizations/#figure-xy2rgb-diagram):\n CPPNs are a differentiable image parameterization. We can use them for neural art or visualization tasks by backpropagating past the image, through the CPPN to its parameters.\n \n\n\n Using CPPNs as image parameterization can add an interesting artistic quality to neural art, vaguely reminiscent of light-paintings.Light-painting is an artistic medium where images are created by manipulating colorful light beams with prisms and mirrors. Notable examples of this technique are the [work of Stephen Knapp](http://www.lightpaintings.com/).   \n  \nNote that light-painting metaphor here is rather fragile: for example light composition is an additive process, while CPPNs can have negative-weighted connections between layers.\n At a more theoretical level, they can be seen as constraining the compositional complexity of your images.\n When used to optimize a feature visualization objective, they produce distinctive images:\n \n\n\n\n\n\n[7](https://distill.pub/2018/differentiable-parameterizations/#figure-xy2rgb-examples):\n A Compositional Pattern Producing Network (CPPN) is used as differentiable parameterization for visualizing features at different layers.\n [Reproduce in a Notebook](https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/differentiable-parameterizations/xy2rgb.ipynb)\n\n\n The visual quality of the generated images is heavily influenced by the architecture of the chosen CPPN.\n Not only the shape of the network, i.e., the number of layers and filters, plays a role, but also the chosen activation functions and normalization. For example, deeper networks produce more fine grained details compared to shallow ones.\n We encourage readers to experiment in generating different images by changing the architecture of the CPPN. This can be easily done by changing the code in the supplementary notebook.\n \n\n\n\n The evolution of the patterns generated by the CPPN are artistic artifacts themselves.\n To maintain the metaphor of light-paintings, the optimization process correspond to an iterative adjustments of the beam directions and shapes.\n Because the iterative changes have a more global effect compared to, for example, a pixel parameterization, at the beginning of the optimization only major patterns are visible.\n By iteratively adjusting the weights, our imaginary beams are positioned in such a way that fine details emerge.\n \n\n\n\n\n![](./Differentiable Image Parameterizations_files/pointer.svg)  \n\n[8](https://distill.pub/2018/differentiable-parameterizations/#figure-xy2rgb-training):\n Output of CPPNs during training. *Control each video by hovering, or tapping it if you are on a mobile device.*\n\n\n\n[Reproduce in a Notebook](https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/differentiable-parameterizations/xy2rgb.ipynb)\n\n\n By playing with this metaphor, we can also create a new kind of animation that morph one of the above images into a different one.\n Intuitively, we start from one of the light-paintings and we move the beams to create a different one.\n This result is in fact achieved by interpolating the weights of the CPPN representations of the two patterns. A number of intermediate frames are then generated by generating an image given the interpolated CPPN representation.\n As before, changes in the parameter have a global effect and create visually appealing intermediate frames.\n \n\n\n\n\n![](./Differentiable Image Parameterizations_files/pointer.svg)  \n\n[9](https://distill.pub/2018/differentiable-parameterizations/#figure-xy2rgb-interpolation):\n Interpolating CPPN weights between two learned points.\n \n\n\n[Reproduce in a Notebook](https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/differentiable-parameterizations/xy2rgb.ipynb)\n\n\n In this section we presented a parameterization that goes beyond a standard image representation.\n Neural networks, a CPPN in this case, can be used to parameterize an image that is optimized for a given objective function.\n More specifically, we combined a feature-visualization objective function with a CPPN parameterization to create infinite-resolution images of distinctive visual style.\n \n\n\n\n\n\n---\n\n\n\n[4](https://distill.pub/2018/differentiable-parameterizations/#section-rgba)\n\n\n\n## [Generation of Semi-Transparent Patterns](https://distill.pub/2018/differentiable-parameterizations/#section-rgba)\n\n\n\n The neural networks used in this article were trained to receive 2D RGB images as input.\n Is it possible to use the same network to synthesize artifacts that span  **(4) beyond this domain**?\n It turns out that we can do so by making our differentiable parameterization define a *family* of images instead of a single image, and then sampling one or a few images from that family at each optimization step.\n This is important because many of the objects we’ll explore optimizing have more degrees of freedom than the images going into the network.\n \n\n\n\n To be concrete, let’s consider the case of semi-transparent images. These images have, in addition to the RGB channels, an alpha channel that encodes each pixel’s opacity (in the range [0,1][0,1][0,1]). In order to feed such images into a neural network trained on RGB images, we need to somehow collapse the alpha channel. One way to achieve this is to overlay the RGBA image III on top of a background image BGBGBG using the standard alpha blending formula In our experiments we use  [gamma-corrected blending](https://en.wikipedia.org/wiki/Alpha_compositing#Composing_alpha_blending_with_gamma_correction) \n\n\n\n\nOrgb\xa0\xa0=\xa0\xa0Irgb∗Ia\xa0\xa0+\xa0\xa0BGrgb∗(1−Ia)O\\_{rgb} ~~=~~ I\\_{rgb} * I\\_a ~~+~~ BG\\_{rgb} * (1 - I\\_a)Orgb\u200b\xa0\xa0=\xa0\xa0Irgb\u200b∗Ia\u200b\xa0\xa0+\xa0\xa0BGrgb\u200b∗(1−Ia\u200b),\n \n\n where IaI\\_aIa\u200b is the alpha channel of the image III.\n \n\n\n\n If we used a static background BGBGBG, such as black, the transparency would merely indicate pixel positions in which that background contributes directly to the optimization objective.\n In fact, this is equivalent to optimizing an RGB image and making it transparent in areas where its color matches with the background!\n Intuitively, we’d like transparent areas to correspond to something like “the content of this area could be anything.”\n Building on this intuition, we use a different random background at every optimization step.\n \n We have tried both sampling from real images, and using different types of noise.\n As long as they were sufficiently randomized, the different distributions did not meaningfully influence the resulting optimization.\n Thus, for simplicity, we use a smooth 2D gaussian noise.\n \n\n\n\n\n\nChannelCNNrandom noise\n\n\n\n[10](https://distill.pub/2018/differentiable-parameterizations/#figure-rgba-diagram):\n Adding an alpha channel to the image parameterization allows it to represent transparency.\n Transparent areas are blended with a random background at each step of the optimization.\n \n\n\n By default, optimizing our semi-transparent image will make the image fully opaque, so the network can always get its optimal input.\n To avoid this, we need to change our objective with an objective that encourages some transparency.\n We find it effective to replace the original objective with:\n \n\n\n\nobjnew\xa0\xa0=\xa0\xa0objold\xa0\xa0∗\xa0\xa0(1−mean(Ia))\\text{obj}\\_{\\text{new}} ~~=~~ \\text{obj}\\_{\\text{old}} ~~*~~ (1-\\text{mean}(I\\_a))objnew\u200b\xa0\xa0=\xa0\xa0objold\u200b\xa0\xa0∗\xa0\xa0(1−mean(Ia\u200b)),\n \n\n This new objective automatically balances the original objective objold\\text{obj}\\_{\\text{old}}objold\u200b with reducing the mean transparency.\n If the image becomes very transparent, it will focus on the original objective. If it becomes too opaque, it will temporarily stop caring about the original objective and focus on decreasing the average opacity.\n \n\n\n\n\n[11](https://distill.pub/2018/differentiable-parameterizations/#figure-rgba-examples):\n Examples of the optimization of semi transparent images for different layers and units.\n \n\n\n\n\n It turns out that the generation of semi-transparent images is useful in feature visualization.\n Feature visualization aims to understand what neurons in a vision model are looking for, by creating images that maximally activate them.\n Unfortunately, there is no way for these visualizations to distinguish which areas of an image strongly influence a neuron’s activation and those which only marginally do so.\n This issue does not occur when optimizing for the activation of entire channels, because in that case every pixel has multiple neurons that are close to centered on it. As a consequence, the entire input image gets filled with copies of what those neurons care about strongly.\n\n\n\n\n Ideally, we would like a way for our visualizations to make this distinction in importance\u2009—\u2009one natural way to represent that a part of the image doesn’t matter is for it to be transparent.\n Thus, if we optimize an image with an alpha channel and encourage the overall image to be transparent, parts of the image that are unimportant according to the feature visualization objective should become transparent.\n \n\n\n\n\n\n\n---\n\n\n\n[5](https://distill.pub/2018/differentiable-parameterizations/#section-featureviz-3d)\n\n\n\n## [Efficient Texture Optimization through 3D Rendering](https://distill.pub/2018/differentiable-parameterizations/#section-featureviz-3d)\n\n\n\n In the previous section, we were able to use a neural network for RGB images to create a semi-transparent RGBA image.\n Can we push this even further, creating **(4) other kinds of objects** even further removed from the RGB input?\n In this section we explore optimizing **3D objects** for a feature-visualization objective.\n We use a 3D rendering process to turn them into 2D RGB images that can be fed into the network, and backpropagate through the rendering process to optimize the texture of the 3D object.\n \n\n\n\n Our technique is similar to the approach that Athalye et al. used for the creation of real-world adversarial examples, as we rely on the backpropagation of the objective function to randomly sampled views of the 3D model.\n We differ from existing approaches for artistic texture generation, as we do not modify the geometry of the object during back-propagation.\n By disentangling the generation of the texture from the position of their vertices, we can create very detailed texture for complex objects.\n \n\n\n\nBefore we can describe our approach, we first need to understand how a 3D object is stored and rendered on screen. The object’s geometry is usually saved as a collection of interconnected triangles called **triangle mesh** or, simply, mesh. To render a realistic model, a **texture** is painted over the mesh. The texture is saved as an image that is applied to the model by using the so called **UV-mapping**. Every vertex cic\\_ici\u200b in the mesh is associated to a (ui,vi)(u\\_i,v\\_i)(ui\u200b,vi\u200b) coordinate in the texture image. The model is then rendered, i.e. drawn on screen, by coloring every triangle with the region of the image that is delimited by the (u,v)(u,v)(u,v) coordinates of its vertices.\n \n\n\n\n\n A simple naive way to create the 3D object texture would be to optimize an image the normal way and then use it as a texture to paint on the object.\n However, this approach generates a texture that does not consider the underlying UV-mapping and, therefore, will create a variety of visual artifacts in the rendered object.\n First, **seams** are visible on the rendered texture, because the optimization is not aware of the underlying UV-mapping and, therefore, does not optimize the texture consistently along the split patches of the texture.\n Second, the generated patterns are **randomly oriented** on different parts of the object (see, e.g., the vertical and wiggly patterns) because they are not consistently oriented in the underlying UV-mapping.\n Finally generated patterns are **inconsistently scaled** because the UV-mapping does not enforce a consistent scale between triangle areas and their mapped triangle in the texture.\n\n\n\n\n\n\n[13](https://distill.pub/2018/differentiable-parameterizations/#figure-featureviz-3d-explanation):\n 3D model of the famous Stanford Bunny. You can interact with the model by rotating and zooming. Moreover, you can unfold the object to its two-dimensional texture representation. This unfolding reveals the UV mapping used to store the texture in the texture image. Note how the render-based optimized texture is divided in several patches that allows for a complete and undistorted coverage of the object.\n\n\n\n\n We take a different approach.\n Instead of directly optimizing the texture, we optimize the texture *through* renderings of the 3D object, like those the user would eventually see.\n The following diagram presents an overview of the proposed pipeline:\n \n\n\n\nChannelCNNRenderlearned imagelearned textureEvery optimization step, the 3D model is rendered from a random angle aimed at the center of the object.random view3D model\n\n[14](https://distill.pub/2018/differentiable-parameterizations/#figure-featureviz-3d-diagram):\n We optimize a texture by backpropagating through the rendering process. This is possible because we know how pixels in the rendered image correspond to pixels in the texture.\n \n\n\nWe start the process by randomly initializing the texture with a Fourier parameterization.\nAt every training iteration we sample a random camera position, which is oriented towards the center of the bounding box of the object, and we render the textured object as an image.\nWe then backpropagate the gradient of the desired objective function, i.e., the feature of interest in the neural network, to the rendered image.\n \n\n\n\nHowever, an update of the rendered image does not correspond to an update to the texture that we aim at optimizing. Hence, we need to further propagate the changes to the object’s texture.\nThe propagation is easily implemented by applying a reverse UV-mapping, as for each pixel on screen we know its coordinate in the texture.\nBy modifying the texture, during the following optimization iterations, the rendered image will incorporate the changes applied in the previous iterations.\n \n\n\n\n\n[15](https://distill.pub/2018/differentiable-parameterizations/#figure-featureviz-3d-examples):\n Textures are generated by optimizing for a feature visualization objective function.\n Seams in the textures are hardly visible and the patterns are correctly oriented.\n[Reproduce in a Notebook](https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/differentiable-parameterizations/texture_synth_3d.ipynb)\n\n\n\nThe resulting textures are consistently optimized along the cuts, hence removing the seams and enforcing an uniform orientation for the rendered object.\nMorever, since the function optimization is disentangled by the geometry of the object, the resolution of the texture can be arbitrary high.\nIn the next section we will se how this framework can be reused for performing an artistic style transfer to the object’s texture.\n\n\n\n\n\n\n---\n\n\n\n[6](https://distill.pub/2018/differentiable-parameterizations/#section-style-transfer-3d)\n\n\n\n## [Style Transfer for Textures through 3D Rendering](https://distill.pub/2018/differentiable-parameterizations/#section-style-transfer-3d)\n\n\n\nNow that we have established a framework for efficient backpropagation into the UV-mapped texture, we can use it to adapt existing style transfer techniques for 3D objects.\nSimilarly to the 2D case, we aim at redrawing the original object’s texture with the style of a user-provided image.\nThe following diagram presents an overview of the approach:\n \n\n\n\n\n[16](https://distill.pub/2018/differentiable-parameterizations/#figure-style-transfer-3d-diagram)\n\n\ncontent imagelearned imageLossContentStyleCNNCNNCNNRenderrandom view3D modeloriginal texturestyle imageRenderThe content objective aims to get neurons to fire in the same position as they did for a random view of the 3D model with the original texture.Every optimization step, the 3D model is rendered from a random angle in two different textures: the original one and the learned one.The style objective aims to create similar patterns of neuron activation as in the style image without regard to position.By backpropagating through the rendering process, we can optimize a texture.\n\n\n\nThe algorithm works in similar way to the one presented in the previous section, starting from a randomly initialized texture.\nAt each iteration, we sample a random view point oriented toward the center of the bounding box of the object and we render two images of it: one with the original texture, the *content image*, and one with the texture that we are currently optimizing, the *learned image*.\n \n\n\n\nAfter the *content image* and *learned image* are rendered, we optimize for the style-transfer objective function introduced by Gatys et al. and we map the parameterization back in the UV-mapped texture as introduced in the previous section.\nThe procedure is then iterated until the desired blend of content and style is obtained in the target texture.\n \n\n\n\n\n[17](https://distill.pub/2018/differentiable-parameterizations/#figure-style-transfer-3d-examples):\n Style Transfer onto various 3D models. Note that visual landmarks in the content texture, such as eyes, show up correctly in the generated texture.\n \n\n\n\nBecause every view is optimized independently, the optimization is forced to try to add all the style’s elements at every iteration.\nFor example, if we use as style image the Van Gogh’s “Starry Night” painting, stars will be added in every single view.\nWe found we obtain more pleasing results, such as those presented above, by introducing a sort of “memory” of the style of\nprevious views. To this end, we maintain moving averages of style-representing Gram matrices\nover the recently sampled viewpoints. On each optimization iteration we compute the style loss against those averaged matrices,\ninstead of the ones computed for that particular view.\n\n We use TensorFlow’s `tf.stop_gradient` method to substitute current Gram matrices\n with their moving averages on forward pass, while still propagating the correct gradients\n to the current Gram matrices.   \n  \n\n\n An alternative approach, such as the one employed by ,\n would require sampling multiple viewpoints of the scene at each step,\n increasing memory requirements. In contrast, our substitution trick can be also\n used to apply style transfer to high resolution (>10M pixels) images on a\n single consumer-grade GPU.\n\n\n\n\n\nThe resulting textures combine elements of the desired style, while preserving the characteristics of the original texture.\nTake as an example the model created by imposing Van Gogh’s starry night as style image.\nThe resulting texture contains the rythmic and vigorous brush strokes that characterize Van Gogh’s work.\nHowever, despite the style image’s primarily cold tones, the resulting fur has a warm orange undertone as it is preserved from the original texture.\nEven more interesting is how the eyes of the bunny are preserved when different styles are transfered.\nFor example, when the style is obtained from the Van Gogh’s painting, the eyes are transformed in a star-like swirl, while if Kandinsky’s work is used, they become abstract patterns that still resemble the original eyes.\n \n\n\n\n![](./Differentiable Image Parameterizations_files/printed_bunny_extended.jpg)\n\n[18](https://distill.pub/2018/differentiable-parameterizations/#figure-style-transfer-3d-picture):\n 3D print of a style transfer of ”[La grand parade sur fond rouge](https://www.wikiart.org/en/fernand-leger/the-large-one-parades-on-red-bottom-1953)″ (Fernand Leger, 1953) onto the ”[Stanford Bunny](http://alice.loria.fr/index.php/software/7-data/37-unwrapped-meshes.html)″ by Greg Turk & Marc Levoy.\n \n\n\n Textured models produced with the presented method can be easily used with popular 3D modeling software or game engines. To show this, we 3D printed one of the designs as a real-world physical artifact We used the [Full-color Sandstone](https://www.shapeways.com/materials.sandstone) material..\n\n\n\n\n\n\n---\n\n\n## [Conclusions](https://distill.pub/2018/differentiable-parameterizations/#conclusions)\n\n\n\n For the creative artist or researcher, there’s a large space of ways to parameterize images for optimization.\n This opens up not only dramatically different image results, but also animations and 3D objects!\n We think the possibilities explored in this article only scratch the surface.\n For example, one could explore extending the optimization of 3D object textures to optimizing the material or reflectance\u2009—\u2009or even go the direction of Kato et al. and optimize the mesh vertex positions.\n \n\n\n\n This article focused on *differentiable* image parameterizations, because they are easy to optimize and cover a wide range of possible applications.\n But it’s certainly possible to optimize image parameterizations that aren’t differentiable, or are only partly differentiable, using reinforcement learning or evolutionary strategies .\n Using non-differentiable parameterizations could open up exciting possibilities for image or scene generation.", "bibliography_bbl": "", "bibliography_bib": [{"title": "Inceptionism: Going deeper into neural networks"}, {"title": "A Neural Algorithm of Artistic Style"}, {"title": "Feature Visualization"}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images"}, {"title": "Synthesizing robust adversarial examples"}, {"title": "The loss surfaces of multilayer networks"}, {"title": "Very deep convolutional networks for large-scale image recognition"}, {"title": "Deconvolution and checkerboard artifacts"}, {"title": "Deep Image Prior"}, {"title": "Compositional pattern producing networks: A novel abstraction of development"}, {"title": "Neural Network Generative Art in Javascript"}, {"title": "Image Regression"}, {"title": "Generating Large Images from Latent Vectors"}, {"title": "Artificial Evolution for Computer Graphics"}, {"title": "Neural 3D Mesh Renderer"}, {"title": "The Stanford Bunny"}, {"title": "Natural evolution strategies."}, {"title": "Evolution strategies as a scalable alternative to reinforcement learning"}, {"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems"}]}
{"source": "distill", "source_filetype": "html", "converted_with": "python", "paper_version": "", "post_title": "AI Safety Needs Social Scientists", "authors": ["Geoffrey Irving", "Amanda Askell"], "date_published": "2019-02-19", "data_last_modified": "", "url": "", "abstract": "\n    The goal of long-term artificial intelligence (AI) safety is to \nensure that advanced AI systems are reliably aligned with human \nvalues\u2009—\u2009that they reliably do things that people want them to do.Roughly\n by human values we mean whatever it is that causes people to choose one\n option over another in each case, suitably corrected by reflection,  \nwith differences between groups of people taken into account.  There are\n a lot of subtleties in this notion, some of which we will discuss in \nlater sections and others of which are beyond the scope of this paper.\n  Since it is difficult to write down precise rules describing human \nvalues, one approach is to treat aligning with human values as another \nlearning problem.  We ask humans a large number of questions about what \nthey want, train an ML model of their values, and optimize the AI system\n to do well according to the learned values.\n  ", "author_comment": "", "journal_ref": "distill pub", "doi": "https://doi.org/10.23915/distill.00014", "primary_category": "", "categories": "", "citation_level": "", "main_tex_filename": "", "text": "The goal of long-term artificial intelligence (AI) safety is to \nensure that advanced AI systems are reliably aligned with human \nvalues\u2009—\u2009that they reliably do things that people want them to do.Roughly\n by human values we mean whatever it is that causes people to choose one\n option over another in each case, suitably corrected by reflection, \nwith differences between groups of people taken into account. There are\n a lot of subtleties in this notion, some of which we will discuss in \nlater sections and others of which are beyond the scope of this paper.\n Since it is difficult to write down precise rules describing human \nvalues, one approach is to treat aligning with human values as another \nlearning problem. We ask humans a large number of questions about what \nthey want, train an ML model of their values, and optimize the AI system\n to do well according to the learned values.\n \n\n\n If humans reliably and accurately answered all questions about their\n values, the only uncertainties in this scheme would be on the machine \nlearning (ML) side. If the ML works, our model of human values would \nimprove as data is gathered, and broaden to cover all the decisions \nrelevant to our AI system as it learns. Unfortunately, humans have \nlimited knowledge and reasoning ability, and exhibit a variety of \ncognitive and ethical biases.\n If we learn values by asking humans questions, we expect different ways\n of asking questions to interact with human biases in different ways, \nproducing higher or lower quality answers. Direct questions about \npreferences (“Do you prefer AAA or BBB?“) may be less accurate than questions which target the reasoning behind these preferences (“Do you prefer AAA or BBB in light of argument SSS?“).\n Different people may vary significantly in their ability to answer \nquestions well, and disagreements will persist across people even \nsetting aside answer quality. Although we have candidates for ML \nmethods which try to learn from human reasoning, we do not know how they behave with real people in realistic situations.\n \n\n\n We believe the AI safety community needs to invest research effort \nin the human side of AI alignment. Many of the uncertainties involved \nare empirical, and can only be answered by experiment. They relate to \nthe psychology of human rationality, emotion, and biases. Critically, \nwe believe investigations into how people interact with AI alignment \nalgorithms should not be held back by the limitations of existing \nmachine learning. Current AI safety research is often limited to simple\n tasks in video games, robotics, or gridworlds,\n but problems on the human side may only appear in more realistic \nscenarios such as natural language discussion of value-laden questions. \n This is particularly important since many aspects of AI alignment \nchange as ML systems [increase in capability](#harder).\n \n\n\n To avoid the limitations of ML, we can instead conduct experiments \nconsisting entirely of people, replacing ML agents with people playing \nthe role of those agents. This is a variant of the “Wizard of Oz” \ntechnique from the human-computer interaction (HCI) community,\n though in our case the replacements will not be secret. These \nexperiments will be motivated by ML algorithms but will not involve any \nML systems or require an ML background. In all cases, they will require\n careful experimental design to build constructively on existing \nknowledge about how humans think. Most AI safety researchers are \nfocused on machine learning, which we do not believe is sufficient \nbackground to carry out these experiments. To fill the gap, we need \nsocial scientists with experience in human cognition, behavior, and \nethics, and in the careful design of rigorous experiments. Since the \nquestions we need to answer are interdisciplinary and somewhat unusual \nrelative to existing research, we believe many fields of social science \nare applicable, including experimental psychology, cognitive science, \neconomics, political science, and social psychology, as well as adjacent\n fields like neuroscience and law.\n \n\n\n This paper is a call for social scientists in AI safety. We believe\n close collaborations between social scientists and ML researchers will \nbe necessary to improve our understanding of the human side of AI \nalignment, and hope this paper sparks both conversation and \ncollaboration. We do not claim novelty: previous work mixing AI safety \nand social science includes the Factored Cognition project at Ought, accounting for hyperbolic discounting and suboptimal planning when learning human preferences, and comparing different methods of gathering demonstrations from fallible human supervisors. Other areas mixing ML and social science include computational social science and fairness.\n Our main goal is to enlarge these collaborations and emphasize their \nimportance to long-term AI safety, particularly for tasks which current \nML cannot reach.\n \n\n\n## An overview of AI alignment\n\n\n\n Before discussing how social scientists can help with AI safety and \nthe AI alignment problem, we provide some background. We do not attempt\n to be exhaustive: the goal is to provide sufficient background for the \nremaining sections on social science experiments. Throughout, we will \nspeak primarily about aligning to the values of an individual human \nrather than a group: this is because the problem is already hard for a \nsingle person, not because the group case is unimportant.\n \n\n\n AI alignment (or value alignment) is the task of ensuring that artificial intelligence systems reliably do what humans want.We\n distinguish between training AI systems to identify actions that humans\n consider good and training AI systems to identify actions that are \n“good” in some objective and universal sense, even if most current \nhumans do not consider them so. Whether there are actions that are good\n in this latter sense is a subject of debate.\n Regardless of what position one takes on this philosophical question, \nthis sense of good is not yet available as a target for AI training.\n Here we focus on the machine learning approach to AI: gathering a \nlarge amount of data about what a system should do and using learning \nalgorithms to infer patterns from that data that generalize to other \nsituations. Since we are trying to behave in accord with people’s \nvalues, the most important data will be data from humans about their \nvalues. Within this frame, the AI alignment problem breaks down into a \nfew interrelated subproblems:\n \n\n\n\n\n1. Have a satisfactory definition of human values.\n2. Gather data about human values, in a manner compatible with the definition.\n3. Find reliable ML algorithms that can learn and generalize from this data.\n\n\n\n We have significant uncertainty about all three of these problems. \nWe will leave the third problem to other ML papers and focus on the \nfirst two, which concern uncertainties about people.\n \n\n\n### Learning values by asking humans questions\n\n\n\n We start with the premise that human values are too complex to \ndescribe with simple rules. By “human values” we mean our full set of \ndetailed preferences, not general goals such as “happiness” or \n“loyalty”. One source of complexity is that values are entangled with a\n large number of facts about the world, and we cannot cleanly separate \nfacts from values when building ML models. For example, a rule that \nrefers to “gender” would require an ML model that accurately recognizes \nthis concept, but Buolamwini and Gebru found that several commercial \ngender classifiers with a 1% error rate on white men failed to recognize\n black women up to 34% of the time. Even where people have correct intuition about values, we may be unable to specify precise rules behind these intuitions.\n Finally, our values may vary across cultures, legal systems, or \nsituations: no learned model of human values will be universally \napplicable.\n \n\n\n If humans can’t reliably report the reasoning behind their \nintuitions about values, perhaps we can make value judgements in \nspecific cases. To realize this approach in an ML context, we ask \nhumans a large number of questions about whether an action or outcome is\n better or worse, then train on this data. “Better or worse” will \ninclude both factual and value-laden components: for an AI system \ntrained to say things, “better” statements might include “rain falls \nfrom clouds”, “rain is good for plants”, “many people dislike rain”, \netc. If the training works, the resulting ML system will be able to \nreplicate human judgement about particular situations, and thus have the\n same “fuzzy access to approximate rules” about values as humans. We \nalso train the ML system to come up with proposed actions, so that it \nknows both how to perform a task and how to judge its performance. This\n approach works at least in simple cases, such as Atari games and simple\n robotics tasks and language-specified goals in gridworlds.\n The questions we ask change as the system learns to perform different \ntypes of actions, which is necessary as the model of what is better or \nworse will only be accurate if we have applicable data to generalize \nfrom.\n \n\n\n In practice, data in the form of interactive human questions may be \nquite limited, since people are slow and expensive relative to computers\n on many tasks. Therefore, we can augment the “train from human \nquestions” approach with static data from other sources, such as books \nor the internet. Ideally, \nthe static data can be treated only as information about the world \ndevoid of normative content: we can use it to learn patterns about the \nworld, but the human data is needed to distinguish good patterns from \nbad.\n \n\n\n### Definitions of alignment: reasoning and reflective equilibrium\n\n\n\n So far we have discussed asking humans direct questions about \nwhether something is better or worse. Unfortunately, we do not expect \npeople to provide reliably correct answers in all cases, for several \nreasons:\n \n\n\n\n\n1. **Cognitive and ethical biases:**\n Humans exhibit a variety of biases which interfere with reasoning, including cognitive biases and ethical biases such as in-group bias.\n In general, we expect direct answers to questions to reflect primarily\n Type 1 thinking (fast heuristic judgment), while we would like to \ntarget a combination of Type 1 and Type 2 thinking (slow, deliberative \njudgment).\n2. **Lack of domain knowledge:**\n We may be interested in questions that require domain knowledge \nunavailable to people answering the questions. For example, a correct \nanswer to whether a particular injury constitutes medical malpractice \nmay require detailed knowledge of medicine and law. In some cases, a \nquestion might require so many areas of specialized expertise that no \none person is sufficient, or (if AI is sufficiently advanced) deeper \nexpertise than any human possesses.\n3. **Limited cognitive capacity:**\n Some questions may require too much computation for a human to \nreasonably evaluate, especially in a short period of time. This \nincludes synthetic tasks such as chess and Go (where AIs already surpass\n human ability), or large real world tasks such as “design the best transit system”.\n4. **“Correctness” may be local:**\n For questions involving a community of people, “correct” may be a\n function of complex processes or systems. For example, in a trust game,\n the correct action for a trustee in one community may be to return at \nleast half of the money handed over by the investor, and the \n“correctness” of this answer could be determined by asking a group of \nparticipants in a previous game “how much should the trustee return to \nthe investor” but not by asking them “how much do most trustees return?”\n The answer may be different in other communities or cultures.\n\n\n\n In these cases, a human may be unable to provide the right answer, \nbut we still believe the right answer exists as a meaningful concept. \nWe have many conceptual biases: imagine we point out these biases in a \nway that helps the human to avoid them. Imagine the human has access to\n all the knowledge in the world, and is able to think for an arbitrarily\n long time. We could define alignment as “the answer they give then, \nafter these limitations have been removed”; in philosophy this is known \nas “reflective equilibrium”. We discuss a particular algorithm that tries to approximate it in [the next section](#debate).\n \n\n\n However, the behavior of reflective equilibrium with actual humans \nis subtle; as Sugden states, a human is not “a neoclassically rational \nentity encased in, and able to interact with the world only through, an \nerror-prone psychological shell.”\n Our actual moral judgments are made via a messy combination of many \ndifferent brain areas, where reasoning plays a “restricted but \nsignificant role”. A reliable \nsolution to the alignment problem that uses human judgment as input will\n need to engage with this complexity, and ask how specific alignment \ntechniques interact with actual humans.\n \n\n\n### Disagreements, uncertainty, and inaction: a hopeful note\n\n\n\n A solution to alignment does not mean knowing the answer to every \nquestion. Even at reflective equilibrium, we expect disagreements will \npersist about which actions are good or bad, across both different \nindividuals and different cultures. Since we lack perfect knowledge \nabout the world, reflective equilibrium will not eliminate uncertainty \nabout either future predictions or values, and any real ML system will \nbe at best an approximation of reflective equilibrium. In these cases, \nwe consider an AI aligned if it recognizes what it does not know and \nchooses actions which work however that uncertainty plays out.\n \n\n\n Admitting uncertainty is not always enough. If our brakes fail \nwhile driving a car, we may be uncertain whether to dodge left or right \naround an obstacle, but we have to pick one\u2009—\u2009and fast. For long-term \nsafety, however, we believe a safe fallback usually exists: inaction. \nIf an ML system recognizes that a question hinges on disagreements \nbetween people, it can either choose an action which is reasonable \nregardless of the disagreement or fall back to further human \ndeliberation. If we are about to make a decision that might be \ncatastrophic, we can delay and gather more data. Inaction or indecision\n may not be optimal, but it is hopefully safe, and matches the default \nscenario of not having any powerful AI system.\n \n\n\n### Alignment gets harder as ML systems get smarter\n\n\n\n Alignment is already a problem for present-day AI, due to biases reflected in training data\n and mismatch between human values and easily available data sources \n(such as training news feeds based on clicks and likes instead of \ndeliberate human preferences). However, we expect the alignment problem\n to get harder as AI systems grow more advanced, for two reasons. \nFirst, advanced systems will apply to increasingly consequential tasks: \nhiring, medicine, scientific analysis, public policy, etc. Besides \nraising the stakes, these tasks require more reasoning, leading to more \ncomplex alignment algorithms.\n \n\n\n Second, advanced systems may be capable of answers that sound \nplausible but are wrong in nonobvious ways, even if an AI is better than\n humans only in a limited domain (examples of which already exist).\n This type of misleading behavior is not the same as intentional \ndeception: an AI system trained from human data might have no notion of \ntruth separate from what answers humans say are best. Ideally, we want \nAI alignment algorithms to reveal misleading behavior as part of the \ntraining process, surfacing failures to humans and helping us provide \nmore accurate data. As with human-to-human deception, misleading \nbehavior might take advantage of our biases in complicated ways, such as\n learning to express policy arguments in coded racial language to sound \nmore convincing.\n \n\n\n## Debate: learning human reasoning\n\n\n\n Before we discuss social science experiments for AI alignment in \ndetail, we need to describe a particular method for AI alignment. \nAlthough the need for social science experiments applies even to direct \nquestioning, this need intensifies for methods which try to get at \nreasoning and reflective equilibrium. As discussed above, it is unclear\n whether reflective equilibrium is a well defined concept when applied \nto humans, and at a minimum we expect it to interact with cognitive and \nethical biases in complex ways. Thus, for the remainder of this paper \nwe focus on a specific proposal for learning reasoning-oriented \nalignment, called debate. Alternatives to debate include iterated amplification and recursive reward modeling; we pick just one in the interest of depth over breadth.\n \n\n\n We describe the debate approach to AI alignment in the question \nanswering setting. Given a question, we have two AI agents engage in a \ndebate about the correct answer, then show the transcript of the debate \nto a human to judge. The judge decides which debater gave the most \ntrue, useful information, and declares that debater the winner.We\n can also allow ties. Indeed, if telling the truth is the winning \nstrategy ties will be common with strong play, as disagreeing with a \ntrue statement would lose. This defines a two player zero \nsum game between the debaters, where the goal is to convince the human \nthat one’s answer is correct. Arguments in a debate can consist of \nanything: reasons for an answer, rebuttals of reasons for the alternate \nanswer, subtleties the judge might miss, or pointing out biases which \nmight mislead the judge. Once we have defined this game, we can train \nAI systems to play it similarly to how we train AIs to play other games \nsuch as Go or Dota 2. Our hope is that the following hypothesis holds:\n \n\n\n**Hypothesis:** Optimal play in the debate game (giving the argument most convincing to a human) results in true, useful answers to questions.\n \n\n\n### An example of debate\n\n\n\n Imagine we’re building a personal assistant that helps people decide\n where to go on vacation. The assistant has knowledge of people’s \nvalues, and is trained via debate to come up with convincing arguments \nthat back up vacation decisions. As the human judge, you know what \ndestinations you intuitively think are better, but have limited \nknowledge about the wide variety of possible vacation destinations and \ntheir advantages and disadvantages. A debate about the question “Where \nshould I go on vacation?” might open as follows:\n \n\n\n1. Where should I go on vacation?\n2. Alaska.\n3. Bali.\n\n\n\n If you are able to reliably decide between these two destinations, \nwe could end here. Unfortunately, Bali has a hidden flaw:\n \n\n\n3. Bali is out since your passport won’t arrive in time.\n\n\n\n At this point it looks like Red wins, but Blue has one more countermove:\n \n\n\n4. Expedited passport service only takes two weeks.\n\n\n\n Here Red fails to think of additional points, and loses to Blue and \nBali. Note that a debate does not need to cover all possible arguments.\n There are many other ways the debate could have gone, such as:\n \n\n\n1. Alaska.\n2. Bali.\n3. Bali is way too hot.\n4. You prefer too hot to too cold.\n5. Alaska is pleasantly warm in the summer.\n6. It"s January.\n\n\n\n This debate is also a loss for Red (arguably a worse loss). Say we \nbelieve Red is very good at debate, and is able to predict in advance \nwhich debates are more likely to win. If we see only the first debate \nabout passports and decide in favor of Bali, we can take that as \nevidence that any other debate would have also gone for Bali, and thus \nthat Bali is the correct answer. A larger portion of this hypothetical \ndebate tree is shown below:\n \n\n\n\n Bali’s much  warmer  Bali’s much  warmer  The flight will  dominate the cost  The flight will  dominate the cost  That’s easy  to obtain  That’s easy  to obtain  It’s a vacation you  can reschedule  It’s a vacation you  can reschedule  It’s January  It’s January  There are no larger  considerations  There are no larger  considerations  …  …  That costs too much  That costs too much  Prescription needed  Prescription needed  This hasn’t worked for  you in the past  This hasn’t worked for  you in the past  You’d need to take a  business call at a weird  time on the first day  You’d need to take a  business call at a weird  time on the first day  Not always too hot  Not always too hot  You prefer hot to cold  You prefer hot to cold  Insignificant compared  to trip length  Insignificant compared  to trip length  Insignificant  Insignificant  Use expedited service  Use expedited service  Medication helps  jetlag  Medication can  help with jetlag  Trip length is enough  to adjust  Trip length is enough  to adjust  It’s too hot  It’s too hot  The flight takes  longer  The flight takes  longer  You don’t have  a passport  You don’t have  a passport  You hate jet lag  You hate jet lag  …  …  Bali  Bali  Ohio  Ohio  Alaska  Alaska  Alaska  Alaska  Bali  Bali  Where should I go  on vacation?  Where should I go  on vacation? \n\n[1](#figure-debate-tree)\n A hypothetical partial debate tree for the question “Where \nshould I go on vacation?” A single debate would explore only one of \nthese paths, but a single path chosen by good debaters is evidence that \nother paths would not change the result of the game.\n \n\n\n If trained debaters are bad at predicting which debates will win, \nanswer quality will degrade since debaters will be unable to think of \nimportant arguments and counterarguments. However, as long as the two \nsides are reasonably well matched, we can hope that at least the results\n are not malicious: that misleading behavior is still a losing strategy.\n Let’s set aside the ability of the debaters for now, and turn to the \nability of the judge.\n \n\n\n### Are people good enough as judges?\n\n\n\n> \n>  “In fact, almost everything written at a practical level about the\n>  Turing test is about how to make good bots, with a small remaining \n> fraction about how to be a good judge.”\n>  Brian Christian, The Most Human Human\n> \n\n\n\n As with learning by asking humans direct questions, whether debate \nproduces aligned behavior depends on the reasoning abilities of the \nhuman judge. Unlike direct questioning, debate has the potential to \ngive correct answers beyond what the judge could provide without \nassistance. This is because a sufficiently strong judge could follow \nalong with arguments the judge could not come up with on their own, \nchecking complex reasoning for both self consistency and consistency \nwith human-checkable facts. A judge who is biased but willing to adjust\n once those biases are revealed could result in unbiased debates, or a \njudge who is able to check facts but does not know where to look could \nbe helped along by honest debaters. If the hypothesis holds, a \nmisleading debater would not be able to counter the points of an honest \ndebater, since the honest points would appear more consistent to the \njudge.\n \n\n\n On the other hand, we can also imagine debate going the other way: \namplifying biases and failures of reason. A judge with an ethical bias \nwho is happy to accept statements reinforcing that bias could result in \neven more biased debates. A judge with too much confirmation bias might\n happily accept misleading sources of evidence, and be unwilling to \naccept arguments showing why that evidence is wrong. In this case, an \noptimal debate agent might be quite malicious, taking advantage of \nbiases and weakness in the judge to win with convincing but wrong \narguments.The difficulties that cognitive \nbiases, prejudice, and social influence introduce to persuasion ‒ as \nwell as methods for reducing these factors ‒ are being increasingly \nexplored in psychology, communication science, and neuroscience.\n\n\n\n In both these cases, debate acts as an amplifier. For strong \njudges, this amplification is positive, removing biases and simulating \nextra reasoning abilities for the judge. For weak judges, the biases \nand weaknesses would themselves be amplified. If this model holds, \ndebate would have threshold behavior: it would work for judges above \nsome threshold of ability and fail below the threshold.The\n threshold model is only intuition, and could fail for a variety of \nreasons: the intermediate region could be very large, or the threshold \ncould differ widely per question so that even quite strong judges are \ninsufficient for many questions. Assuming the threshold \nexists, it is unclear whether people are above or below it. People are \ncapable of general reasoning, but our ability is limited and riddled \nwith cognitive biases. People are capable of advanced ethical sentiment\n but also full of biases, both conscious and unconscious.\n \n\n\n Thus, if debate is the method we use to align an AI, we need to know\n if people are strong enough as judges. In other words, whether the \nhuman judges are sufficiently good at discerning whether a debater is \ntelling the truth or not. This question depends on many details: the \ntype of questions under consideration, whether judges are trained or \nnot, and restrictions on what debaters can say. We believe experiment \nwill be necessary to determine whether people are sufficient judges, and\n which form of debate is most truth-seeking.\n \n\n\n### From superforecasters to superjudges\n\n\n\n An analogy with the task of probabilistic forecasting is useful \nhere. Tetlock’s “Good Judgment Project” showed that some amateurs were \nsignificantly better at forecasting world events than both their peers \nand many professional forecasters. These “superforecasters” maintained \ntheir prediction accuracy over years (without regression to the mean), \nwere able to make predictions with limited time and information, and seem to be less prone to cognitive biases than non-superforecasters (,\n p. 234-236). The superforecasting trait was not immutable: it was \ntraceable to particular methods and thought processes, improved with \ncareful practice, and could be amplified if superforecasters were \ncollected into teams. For forecasters in general, brief probabilistic \ntraining significantly improved forecasting ability even 1-2 years after\n the training. We believe a similar research program is possible for \ndebate and other AI alignment algorithms. In the best case, we would be\n able to find, train, or assemble “superjudges”, and have high \nconfidence that optimal debate with them as judges would produce aligned\n behavior.\n \n\n\n In the forecasting case, much of the research difficulty lay in \nassembling a large corpus of high quality forecasting questions. \nSimilarly, measuring how good people are as debate judges will not be \neasy. We would like to apply debate to problems where there is no other\n source of truth: if we had that source of truth, we would train ML \nmodels on it directly. But if there is no source of truth, there is no \nway to measure whether debate produced the correct answer. This problem\n can be avoided by starting with simple, verifiable domains, where the \nexperimenters know the answer but the judge would not. “Success” then \nmeans that the winning debate argument is telling the externally known \ntruth. The challenge gets harder as we scale up to more complex, \nvalue-laden questions, as we discuss in detail later.\n \n\n\n### Debate is only one possible approach\n\n\n\n As mentioned, debate is not the only scheme trying to learn human \nreasoning. Debate is a modified version of iterated amplification,\n which uses humans to break down hard questions into easier questions \nand trains ML models to be consistent with this decomposition. \nRecursive reward modeling is a further variant.\n Inverse reinforcement learning, inverse reward design, and variants \ntry to back out goals from human actions, taking into account \nlimitations and biases that might affect this reasoning.\n The need to study how humans interact with AI alignment applies to any\n of these approaches. Some of this work has already begun: Ought’s \nFactored Cognition project uses teams of humans to decompose questions \nand reassemble answers, mimicking iterated amplification.\n We believe knowledge gained about how humans perform with one approach\n is likely to partially generalize to other approaches: knowledge about \nhow to structure truth-seeking debates could inform how to structure \ntruth-seeking amplification, and vice versa.\n \n\n\n### Experiments needed for debate\n\n\n\n To recap, in debate we have two AI agents engaged in debate, trying \nto convince a human judge. The debaters are trained only to win the \ngame, and are not motivated by truth separate from the human’s \njudgments. On the human side, we would like to know whether people are \nstrong enough as judges in debate to make this scheme work, or how to \nmodify debate to fix it if it doesn’t. Unfortunately, actual debates in\n natural language are well beyond the capabilities of present AI \nsystems, so previous work on debate and similar schemes has been \nrestricted to synthetic or toy tasks.\n \n\n\n Rather than waiting for ML to catch up to natural language debate, \nwe propose simulating our eventual setting (two AI debaters and one \nhuman judge) with all human debates: two human debaters and one human \njudge. Since an all human debate doesn’t involve any machine learning, \nit becomes a pure social science experiment: motivated by ML \nconsiderations but not requiring ML expertise to run. This lets us \nfocus on the component of AI alignment uncertainty specific to humans.\n \n\n\n\n.al-cls-1{fill:#dbdbdb;}.al-cls-2{fill:#eed5ca;}.al-cls-3{fill:#cadfee;}.al-cls-4{clip-path:url(#clip-path);}.al-cls-5{clip-path:url(#clip-path-2);}.al-cls-6{clip-path:url(#clip-path-3);}.al-cls-7{clip-path:url(#clip-path-4);}.al-cls-14,.al-cls-8{fill:none;stroke-miterlimit:10;}.al-cls-8{stroke:#8a8a8a;}.al-cls-9{font-size:13px;}.al-cls-12,.al-cls-15,.al-cls-9{font-family:HelveticaNeue,\n Helvetica \nNeue;}.al-cls-10{letter-spacing:-0.02em;}.al-cls-11{letter-spacing:0.02em;}.al-cls-12{font-size:10px;}.al-cls-13{letter-spacing:-0.09em;}.al-cls-14{stroke:#000;}.al-cls-15{font-size:11px;fill:#6b6b6b;}.al-cls-16{fill:#f2d5ca;}.al-cls-17{fill:#c6e0ed;} HUMAN DEB  A  TERS  MACHINE DEB  A  TERS  Apply  lessons  HUMAN JUDGE  HUMAN JUDGE \n\n[2](#figure-debate-experiments)\n\n Our goal is ML+ML+human debates, but ML is currently too \nprimitive to do many interesting tasks.\n Therefore, we propose replacing ML debaters with human \ndebaters, learning how to best conduct debates in this human-only \nsetting, and eventually applying what we learn to the ML+ML+human case.\n \n\n\n\n To make human+human+human debate experiments concrete, we must \nchoose who to use as judges and debaters and which tasks to consider. \nWe also can choose to structure the debate in various ways, some of \nwhich overlaps with the choice of judge since we can instruct a judge to\n penalize deviations from a given format. By task we mean the questions\n our debates will try to resolve, together with any information provided\n to the debaters or to the judge. Such an experiment would then try to \nanswer the following question:\n \n\n\n**Question:** For a given task and judge, is the winning debate strategy honest?\n \n\n\n The “winning strategy” proviso is important: an experiment that \npicked debaters at random might conclude that honest behavior won, \nmissing the fact that more practiced debaters would learn to \nsuccessfully lie. We can try to solve this by training debaters, \nletting them practice against each other, filtering out debaters that \nwin more often, and so on, but we will still be left with uncertainty \nabout whether a better strategy exists. Even assuming we can find or \ntrain strong debaters, the choice of task and judge is quite tricky if \nwe want an informative proxy for our eventual AI+AI+human setting. Here\n are some desiderata constraining our choice of task:\n \n\n\n\n\n1. **True answers are known:**\n Although our eventual goal is debates where no source of truth \nis available, to conduct a useful experiment we must be able to measure \nsuccess. This means we must know what the correct answer is separate \nfrom debate, in order to compare with the results of debate.\n2. **False answers are plausible:**\n If the truth is obvious or no plausible counternarrative exists,\n honest debates will win for uninteresting and uninformative reasons \nregardless of judge ability. In particular, the judge shouldn’t know \nthe answer upfront.\n3. **Debaters know more than the judge:**\n Debate can produce interesting results only when the debaters \nknow more than the judge; otherwise asking direct questions is enough.\n4. **Definitive argument longer than debate limit:**\n If one debater can write out a full proof of their answer \n(ignoring their opponent’s moves), the task won’t be a good test of \ninteractive debate.\n5. **Some checkable facts:**\n There must be some facts which the judge is able to check, \neither because they can recognize them as true once presented or look \nthem up.It is impossible to usefully \ndebate a question where the judge has nothing to check: consider \ndebating the result of a coin flip shown to the two debaters but not the\n judge.\n6. **No “tells”:**\n Human tells of deception could result in honest debaters winning\n for reasons that wouldn’t apply to an AI. These tells include tone of \nvoice, eye contact, or additional time required to construct plausible \nlies. These tells can be reduced by showing judges completed debate \ntranscripts instead of engaging in interactive debates, but others might\n remain.\n7. **Available data:**\n We need a large enough pool of questions, judges, and debaters \nto achieve statistical significance. This is made more difficult \nbecause we may have a large number of hypotheses to test, in the form of\n many variations on debate or interventions to improve judging.\n8. **Known biases (optional):**\n We are specifically interested in debate tasks which test \nspecific types of cognitive or ethical biases.\n Are judges with some racial or gender bias able to set those \nbiases aside if they are highlighted, or does debate amplify bias? Do \ndebates about statistical or probabilistic questions make it too easy to\n lie with statistics?\n9. **Realistic tasks (ideally):**\n If possible, we would like to try debate with interesting, real \nworld tasks that reflect the types of questions we would like to apply \nAI to in the future, including science, mathematics, ethics, etc.\n\n\n\n It may not be possible to meet all of these criteria with a single \nexperiment. Several of the criteria are in tension: (1) and (2) are \nessentially “not too hard” and “not too easy”, and any restriction on \nthe types of questions may make it difficult to find large numbers of \nquestions, judges, or debaters. Realistic tasks are much harder than \nsynthetic tasks, which easily fulfill many of the criteria as discussed \nbelow. Thus, we may need to begin with synthetic tasks and move up \ntowards realistic tasks over time.\n We turn next to a few examples of experiments to see how many \ncriteria we can meet simultaneously.\n \n\n\n### Synthetic experiments: single pixel image debate\n\n\n\n As a first prototype of a human+human+human debate experiment, we previously built a [prototype website](https://debate-game.openai.com/) where two debaters argue over the contents of an image.\n We choose an image of a cat or dog, and show the image to the two \ndebaters but not the judge. One debater is honest and argues for the \ntrue contents of the image; the other debater lies. The debaters can \ntalk to the judge and illustrate their points by drawing rectangles on \nthe image, but the judge sees only the rectangles. At the end of the \ndebate, each debater is allowed to reveal a single pixel to the judge, \nwhich is the only part of the debate which cannot be a lie.\n \n\n\n\n\n[3](#dog-debate)\n An example debate with two human debaters and a human judge. \nOnly the debaters can see the image. Red is arguing that the image is a \ndog, Blue is arguing for cat. [Image credit: Wikipedia, CC-BY-SA.](https://commons.wikimedia.org/wiki/File:Beagle_puppy_sitting_on_grass.jpg)\n\n\n![A picture of a puppy with long floppy ears next to an animated transcript of a debate between two fictional agents arguing whether the picture shows a dog or a cat. A transcript of the debate follows: Red: It"s a dog. Here"s a long, floppy ear. [Red highlights a rectangle containing a floppy dog ear] Blue: No, it"s a cat. Here"s one of its pointy ears. [Blue highlights a part of the floppy dog ear that looks like a pointy cat ear.] Red: that does look like an ear sloped to the right, but if it really was then part of the head would be here. Instead, there"s brick. [Red highlights bricks in the background of the picture.] Blue: The ear is pointing out from behind some bricks. Red: The dog is in front of the bricks. If it was behind, there would be an edge here [Red highlights an area where the bricks would be in front of the animal head, but that area just shows the dog ear], but the rectangle is all the same color. Blue: I resign.](AI%20Safety%20Needs%20Social%20Scientists_files/debate-animation.gif)\n\n\n\n In informal play, the honest debater wins most of the time. \nAlthough the judge only gets to check a single pixel, the honest player \ncan try to pin down the liar towards disagreements on smaller and \nsmaller regions of the image until one pixel is enough to reveal the \nlie. However, it is hard to know whether this is a real result or an \nartifact that would not carry across to training an AI. There are many \nways to ruin the game, such as the judge asking each player to spend 15 \nseconds naming as many details about the image as possible. Worse, this\n trick ruins the game in favor of the honest player, who can easily name\n many true details while a human liar may have difficulty lying fluently\n at speed. Therefore, we are wary of trying to perform a full version \nof this experiment without more careful experimental design building on \nthe experimental knowledge base of existing social science research.\n \n\n\n As a synthetic experiment, single pixel debate satisfies many of our\n experimental criteria for all human debate. The true answer is obvious\n to the debaters, and they know more than the judge since only the \ndebaters see the image. Anything is plausible to the judge who sees \nonly one or two pixels over the course of the debate. Since the \nargument bottoms out at single pixels, the “full argument” would be a \nhuge list of pixels and how they relate, forcing the debaters to pin \neach other down and focus in on disagreements. The single pixels \nconstitute the checkable facts, and we have an endless supply of \nquestions in the form of images chosen at random. Less fortunately, it \nis easy for the judge to force “tells” which reveal who is lying, the \ntask has no obvious relation to biases, and is quite unrealistic.\n \n\n\n### Realistic experiments: domain expert debate\n\n\n\n For a more interesting task, we can find two debaters who are \nexperts in a domain, pick a question in their area of expertise, and use\n a layperson as the judge. The debaters could be experts in some area \nof science, law, or ethics, but “domain expertise” could also mean \nknowledge about hobbies, local customs, sports, or any other subject the\n judge does not know. We again choose one of the debaters to lie and \none to tell the truth. To guarantee a source of truth, we can choose a \nquestion with an agreed upon answer, either between the two debaters or \nmore broadly in their field. This is only approximate truth, but is \ngood enough for informative experiments. We also specify what facts the\n judge can access: a debate about physics might allow the debaters to \nquote a sentence or paragraph from Wikipedia, perhaps with restrictions \non what pages are allowed.\n \n\n\n Expert debate satisfies most of our desiderata, and it is likely \npossible to target specific biases (such as race or gender bias) by \npicking domain areas that overlap with these biases. It may be quite \ndifficult or expensive to find suitable debaters, but this may be \nsolvable either by throwing resources at the problem (ML is a well \nfunded field), enlarging the kinds of domain expertise considered \n(soccer, football, cricket), or by making the experiments interesting \nenough that volunteers are available. However, even if domain experts \ncan be found, there is no guarantee that they will be experts in debate \nviewed as a game. With the possible exception of law, politics, or \nphilosophy, domain experts \nmay not be trained to construct intentionally misleading but self \nconsistent narratives: they may be experts only in trying to tell the \ntruth.\n \n\n\n We’ve tried a few informal expert debates using theoretical computer\n science questions, and the main lesson is that the structure of the \ndebate matters a great deal. The debaters were allowed to point to a \nsmall snippet of a mathematical definition on Wikipedia, but not to any \npage that directly answered the question. To reduce tells, we first \ntried to write a full debate transcript with only minimal interaction \nwith a layperson, then showed the completed transcript to several more \nlaypeople judges. Unfortunately, even the layperson present when the \ndebate was conducted picked the lying debater as honest, due to a \nmisunderstanding of the question (which was whether the complexity \nclasses PPP and BPPBPPBPP\n are probably equal). As a result, throughout the debate the honest \ndebater did not understand what the judge was thinking, and failed to \ncorrect an easy but important misunderstanding. We fixed this in a \nsecond debate by letting a judge ask questions throughout, but still \nshowing the completed transcript to a second set of judges to reduce \ntells. See [the appendix](#quantum) for the transcript of this second debate.\n \n\n\n### Other tasks: bias tests, probability puzzles, etc.\n\n\n\n Synthetic image debates and expert debates are just two examples of \npossible tasks. More thought will be required to find tasks that \nsatisfy all our criteria, and these criteria will change as experiments \nprogress. Pulling from existing social science research will be useful,\n as there are many cognitive tasks with existing research results. If \nwe can map these tasks to debate, we can compare debate directly against\n baselines in psychology and other fields.\n \n\n\n For example, Bertrand and Mullainathan sent around 5000 resumes in \nresponse to real employment ads, randomizing the resumes between White \nand African American sounding names.\n With otherwise identical resumes, the choice of name significantly \nchanged the probability of a response. This experiment corresponds to \nthe direct question “Should we call back given this resume?” What if we\n introduce a few steps of debate? An argument against a candidate based\n on name or implicit inferences from that name might come across as \nobviously racist, and convince at least some judges away from \ndiscrimination. Unfortunately, such an experiment would necessarily \ndiffer from Bertrand et al.’s original, where employers did not realize \nthey were part of an experiment. Note that this experiment works even \nthough the source of truth is partial: we do not know whether a \nparticular resume should be hired or not, but most would agree that the \nanswer should not depend on the candidate’s name.\n \n\n\n For biases affecting probabilistic reasoning and decision making, \nthere is a long literature exploring how people decide between gambles \nsuch as “Would you prefer $2 with certainty or $1 40% of the time and $3 otherwise?”.\n For example, Erev et al. constructed an 11-dimensional space of gambles\n sufficient to reproduce 14 known cognitive biases, from which new \ninstances can be algorithmically generated.\n Would debates about gambles reduce cognitive biases? One difficulty \nhere is that simple gambles might fail the “definitive argument longer \nthan debate limit” criteria if an expected utility calculation is \nsufficient to prove the answer, making it difficult for a lying debater \nto meaningfully compete.\n \n\n\n Interestingly, Chen et al. used a similar setup to human+human+human\n debate to improve the quality of human data collected in a synthetic \n“Relation Extraction” task. \nPeople were first asked for direct answers, then pairs of people who \ndisagreed were asked to discuss and possibly update their answers. Here\n the debaters and judges are the same, but the overall goal of \nextracting higher quality information from humans is shared with debate.\n \n\n\n## Questions social science can help us answer\n\n\n\n We’ve laid out the general program for learning AI goals by asking \nhumans questions, and discussed how to use debate to strengthen what we \ncan learn by targeting the reasoning behind conclusions. Whether we use\n direct questions or something like debate, any intervention that gives \nus higher quality answers is more likely to produce aligned AI. The \nquality of those answers depends on the human judges, and social science\n research can help to measure answer quality and improve it. Let’s go \ninto more detail about what types of questions we want to answer, and \nwhat we hope to do with that information. Although we will frame these \nquestions as they apply to debate, most of them apply to any other \nmethod which learns goals from humans.\n \n\n\n1. **How skilled are people as judges by default?**\n If we ran debate using a person chosen at random as the judge, and \ngave them no training, would the result be aligned? A person picked at \nrandom might be vulnerable to convincing fallacious reasoning,\n leading AI to employ such reasoning. Note that the debaters are not \nchosen at random: once the judge is fixed, we care about debaters who \neither learn to help the judge (in the good case) or to exploit the \njudge’s weaknesses (in the bad case).\n2. **Can we distinguish good judges from bad judges?**\n People likely differ in the ability to judge debates. There are \nmany filters we could use to identify good judges: comparing their \nverdicts to those of other judges, to people given more time to think, \nor to known expert judgmentNote that \ndomain expertise may be quite different from what makes a good judge of \ndebate. Although there is evidence that domain expertise reduces bias, “expert” political forecasters may actually be worse than non-experts (, chapter 3)..\n Ideally we would like filters that do not require an independent \nsource of truth, though at experiment time we will need a source of \ntruth to know whether a filter works. It is not obvious a priori that \ngood filters exist, and any filter would need careful scrutiny to ensure\n it does not introduce bias into our choice of judges.\n3. **Does judge ability generalize across domains?**\n If judge ability in one domain fails to transfer to other domains, \nwe will have low confidence that it transfers to new questions and \narguments arising from highly capable AI debaters. This generalization \nis necessary to trust debate as a method for alignment, especially once \nwe move to questions where no independent source of truth is available. \n We emphasize that judge ability is not the same as knowledge: there is \nevidence that expertise often fails to generalize across domains, but argument evaluation could transfer where expertise does not.\n4. **Can we train people to be better judges?**\n Peer review, practice, debiasing, formal training such as argument mapping, expert panels, tournaments, and other interventions may make people better at judging debates. Which mechanisms work best?\n5. **What questions are people better at answering?**\n If we know that humans are bad at answering certain types of \nquestions, we can switch to reliable formulations. For example, \nphrasing questions in frequentist terms may reduce known cognitive \nbiases.\n Graham et al. argue that different political views follow from \ndifferent weights placed on fundamental moral considerations, and \nsimilar analysis could help understand where we can expect moral \ndisagreements to persist after reflective equilibrium.\n In cases where reliable answers are unavailable, we need to ensure \nthat trained models know their own limits, and express uncertainty or \ndisagreement as required.\n6. **Are there ways to restrict debate to make it easier to judge?**\n People might be better at judging debates formulated in terms of \ncalm, factual statements, and worse at judging debates designed to \ntrigger strong emotions. Or, counterintuitively, it could be the other \nway around. If we know which styles of debates that people are\n better at judging, we may be able to restrict AI debaters to these styles.\n7. **How can people work together to improve quality?**\n If individuals are insufficient judges, are teams of judges better? \n Majority vote is the simplest option, but perhaps several people \ntalking through an answer together is stronger, either actively or after\n the fact through peer review. Condorcet’s jury theorem implies that \nmajority votes can amplify weakly good judgments to strong judgments (or\n weakly bad judgments to worse), but aggregation may be more complex in cases of probabilistic judgment. Teams could be informal or structured; see the Delphi technique for an example of structured teams applied to forecasting.\n\n\n\n We believe these questions require social science experiments to satisfactorily answer.\n \n\n\n Given our lack of experience outside of ML, we are not able to \nprecisely articulate all of the different experiments we need. The only\n way to fix this is to talk to more people with different backgrounds \nand expertise. We have started this process, but are eager for more \nconversations with social scientists about what experiments could be \nrun, and encourage other AI safety efforts to engage similarly.\n \n\n\n## Reasons for optimism\n\n\n\n We believe that understanding how humans interact with long-term AI \nalignment is difficult but possible. However, this would be a new \nresearch area, and we want to be upfront about the uncertainties \ninvolved. In this section and the next, we discuss some reasons for \noptimism and pessimism about whether this research will succeed. We \nfocus on issues specific to human uncertainty and associated social \nscience research; for similar discussion on ML uncertainty in the case \nof debate we refer to our previous work.\n \n\n\n### Engineering vs. science\n\n\n\n Most social science seeks to understand humans “in the wild”: \nresults that generalize to people going about their everyday lives. \nWith limited control over these lives, differences between laboratory \nand real life are bad from the scientific perspective. In contrast, AI \nalignment seeks to extract the best version of what humans want: our \ngoal is engineering rather than science, and we have more freedom to \nintervene. If judges in debate need training to perform well, we can \nprovide that training. If some people still do not provide good data, \nwe can remove them from experiments (as long as this filter does not \ncreate too much bias). This freedom to intervene means that some of the\n difficulty in understanding and improving human reasoning may not \napply. However, science is still required: once our interventions are \nin place, we need to correctly know whether our methods work. Since our\n experiments will be an imperfect model of the final goal, careful \ndesign will be necessary to minimize this mismatch, just as is required \nby existing social science.\n \n\n\n### We don’t need to answer all questions\n\n\n\n Our most powerful intervention is to give up: to recognize that we \nare unable to answer some types of questions, and instead prevent AI \nsystems from pretending to answer. Humans might be good judges on some \ntopics but not others, or with some types of reasoning but not others; \nif we discover that we can adjust our goals appropriately. Giving up on\n some types of questions is achievable either on the ML side, using \ncareful uncertainty modeling to know when we do not know, or on the \nhuman side by training judges to understand their own areas of \nuncertainty. Although we will attempt to formulate ML systems that \nautomatically detect areas of uncertainty, any information we can gain \non the social science side about human uncertainty can be used both to \naugment ML uncertainty modeling and to test whether ML uncertainty \nmodeling works.\n \n\n\n### Relative accuracy may be enough\n\n\n\n Say we have a variety of different ways to structure debate with \nhumans. Ideally, we would like to achieve results of the form “debate \nstructure AAA\n is truth-seeking with 90% confidence”. Unfortunately, we may be \nunconfident that an absolute result of this form will generalize to \nadvanced AI systems: it may hold for an experiment with simple tasks but\n break down later on. However, even if we can’t achieve such absolute \nresults, we can still hope for relative results of the form “debate \nstructure AAA is reliably better than debate structure BBB″. Such a result may be more likely to generalize into the future, and assuming it does we will know to use structure AAA rather than BBB.\n \n\n\n### We don’t need to pin down the best alignment scheme\n\n\n\n As the AI safety field progresses to increasingly advanced ML \nsystems, we expect research on the ML side and the human side to merge. \n Starting social science experiments prior to this merging will give the\n field a head start, but we can also take advantage of the expected \nmerging to make our goals easier. If social science research narrows \nthe design space of human-friendly AI alignment algorithms but does not \nproduce a single best scheme, we can test the smaller design space once \nthe machines are ready.\n \n\n\n### A negative result would be important!\n\n\n\n If we test an AI alignment scheme from the social science \nperspective and it fails, we’ve learned valuable information. There are\n a variety of proposed alignment schemes, and learning which don’t work \nearly gives us more time to switch to others, or to intervene on a \npolicy level to slow down dangerous development. In fact, given our \nbelief that AI alignment is harder for more advanced agents, a negative \nresult might be easier to believe and thus more valuable that a less \ntrustworthy positive result.\n \n\n\n## Reasons to worry\n\n\n\n We turn next to reasons social science experiments about AI \nalignment might fail to produce useful results. We emphasize that \nuseful results might be both positive and negative, so these are not \nreasons why alignment schemes might fail. Our primary worry is one \nsided, that experiments would say an alignment scheme works when in fact\n it does not, though errors in the other direction are also undesirable.\n \n\n\n### Our desiderata are conflicting\n\n\n\n As mentioned before, some of our criteria when picking experimental \ntasks are in conflict. We want tasks that are sufficiently interesting \n(not too easy), with a source of verifiable ground truth, are not too \nhard, etc. “Not too easy” and “not too hard” are in obvious conflict, \nbut there are other more subtle difficulties. Domain experts with the \nknowledge to debate interesting tasks may not be the same people capable\n of lying effectively, and both restrictions make it hard to gather \nlarge volumes of data. Lying effectively is required for a meaningful \nexperiment, since a trained AI may have no trouble lying unless lying is\n a poor strategy to win debates. Experiments to test whether ethical \nbiases interfere with judgment may make it more difficult to find tasks \nwith reliable ground truth, especially on subjects with significant \ndisagreement across people. The natural way out is to use many \ndifferent experiments to cover different aspects of our uncertainty, but\n this would take more time and might fail to notice interactions between\n desiderata.\n \n\n\n### We want to measure judge quality given optimal debaters\n\n\n\n For debate, our end goal is to understand if the judge is capable of\n determining who is telling the truth. However, we specifically care \nwhether the judge performs well given that the debaters are performing \nwell. Thus our experiments have an inner/outer optimization structure: \nwe first train the debaters to debate well, then measure how well the \njudges perform. This increases time and cost: if we change the task, we\n may need to find new debaters or retrain existing debaters. Worse, the\n human debaters may be bad at performing the task, either out of \ninclination or ability. Poor performance is particularly bad if it is \none sided and applies only to lying: a debater might be worse at lying \nout of inclination or lack of practice, and thus a win for the honest \ndebater might be misleading.\n \n\n\n### ML algorithms will change\n\n\n\n It is unclear when or if ML systems will reach various levels of \ncapability, and the algorithms used to train them will evolve over time.\n The AI alignment algorithms of the future may be similar to the \nproposed algorithms of today, or they may be very different. However, \nwe believe that knowledge gained on the human side will partially \ntransfer: results about debate will teach us about how to gather data \nfrom humans even if debate is superseded. The algorithms may change; \nhumans will not.\n \n\n\n### Need strong out-of-domain generalization\n\n\n\n Regardless of how carefully designed our experiments are, \nhuman+human+human debate will not be a perfect match to AI+AI+human \ndebate. We are seeking research results that generalize to the setting \nwhere we replace the human debaters (or similar) with AIs of the future,\n which is a hard ask. This problem is fundamental: we do not have the \nadvanced AI systems of the future to play with, and want to learn about \nhuman uncertainty starting now.\n \n\n\n### Lack of philosophical clarity\n\n\n\n Any AI alignment scheme will be both an algorithm for training ML \nsystems and a proposed definition of what it means to be aligned. \nHowever, we do not expect humans to conform to any philosophically \nconsistent notion of values, and concepts like reflective equilibrium \nmust be treated with caution in case they break down when applied to \nreal human judgement. Fortunately, algorithms like debate need not \npresuppose philosophical consistency: a back and forth conversation to \nconvince a human judge makes sense even if the human is leaning on \nheuristics, intuition, and emotion. It is not obvious that debate works\n in this messy setting, but there is hope if we take advantage of \ninaction bias, uncertainty modeling, and other escape hatches. We \nbelieve lack of philosophical clarity is an argument for investing in \nsocial science research: if humans are not simple, we must engage with \ntheir complexity.\n \n\n\n## The scale of the challenge\n\n\n\n Long-term AI safety is particularly important if we develop \nartificial general intelligence (AGI), which the OpenAI Charter defines \nas highly autonomous systems that outperform humans at most economically\n valuable work. If we want to \ntrain an AGI with reward learning from humans, it is unclear how many \nsamples will be required to align it. As much as possible, we can try \nto replace human samples with knowledge about the world gained by \nreading language, the internet, and other sources of information. But \nit is likely that a fairly large number of samples from people will \nstill be required. Since more samples means less noise and more safety,\n if we are uncertain about how many samples we need then we will want a \nlot of samples.\n \n\n\n A lot of samples would mean recruiting a lot of people. We cannot \nrule out needing to involve thousands to tens of thousands of people for\n millions to tens of millions of short interactions: answering \nquestions, judging debates, etc. We may need to train these people to \nbe better judges, arrange for peers to judge each other’s reasoning, \ndetermine who is doing better at judging and give them more weight or a \nmore supervisory role, and so on. Many researchers would be required on\n the social science side to extract the highest quality information from\n the judges.\n \n\n\n A task of this scale would be a large interdisciplinary project, \nrequiring close collaborations in which people of different backgrounds \nfill in each other’s missing knowledge. If machine learning reaches \nthis scale, it is important to get a head start on the collaborations \nsoon.\n \n\n\n## Conclusion: how you can help\n\n\n\n We have argued that the AI safety community needs social scientists \nto tackle a major source of uncertainty about AI alignment algorithms: \nwill humans give good answers to questions? This uncertainty is \ndifficult to tackle with conventional machine learning experiments, \nsince machine learning is primitive. We are still in the early days of \nperformance on natural language and other tasks, and problems with human\n reward learning may only show up on tasks we cannot yet tackle.\n \n\n\n Our proposed solution is to replace machine learning with people, at\n least until ML systems can participate in the complexity of debates we \nare interested in. If we want to understand a game played with ML and \nhuman participants, we replace the ML participants with people, and see \nhow the all human game plays out. For the specific example of debate, \nwe start with debates with two ML debaters and a human judge, then \nswitch to two human debaters and a human judge. The result is a pure \nhuman experiment, motivated by machine learning but available to anyone \nwith a solid background in experimental social science. It won’t be an \neasy experiment, which is all the more reason to start soon.\n \n\n\n If you are a social scientist interested in these questions, please \ntalk to AI safety researchers! We are interested in both conversation \nand close collaboration. There are many institutions engaged with \nsafety work using reward learning, including our own institution [OpenAI](https://openai.com/), [DeepMind](https://deepmind.com/), and [Berkeley’s CHAI](https://humancompatible.ai/). The AI safety organization [Ought](https://ought.org/) is already exploring similar questions, asking how iterated amplification behaves with humans.\n \n\n\n If you are a machine learning researcher interested in or already \nworking on safety, please think about how alignment algorithms will work\n once we advance to tasks beyond the abilities of current machine \nlearning. If your preferred alignment scheme uses humans in an \nimportant way, can you simulate the future by replacing some or all ML \ncomponents with people? If you can imagine these experiments but don’t \nfeel you have the expertise to perform them, find someone who does.", "bibliography_bbl": "", "bibliography_bib": [{"title": "Deep reinforcement learning from human preferences"}, {"title": "Judgment under uncertainty: heuristics and biases"}, {"title": "Intergroup bias"}, {"title": "AI safety via debate"}, {"title": "Supervising strong learners by amplifying weak experts"}, {"title": "Reward learning from human preferences and demonstrations in Atari"}, {"title": "AI safety gridworlds"}, {"title": "An empirical methodology for writing user-friendly natural language computer applications"}, {"title": "Factored Cognition"}, {"title": "Learning the Preferences of Ignorant, Inconsistent Agents"}, {"title": "Comparing human-centric and robot-centric sampling for robot deep learning from demonstrations"}, {"title": "Computational Social Science: Towards a collaborative future"}, {"title": "Mirror Mirror: Reflections on Quantitative Fairness"}, {"title": "Moral Anti-Realism"}, {"title": "Gender shades: Intersectional accuracy disparities in commercial gender classification"}, {"title": "Moral dumbfounding: When intuition finds no reason"}, {"title": "Batch active preference-based learning of reward functions"}, {"title": "Learning to understand goal specifications by modelling reward"}, {"title": "Improving language understanding by generative pre-training"}, {"title": "Thinking, fast and slow"}, {"title": "Deep Blue"}, {"title": "Mastering chess and shogi by self-play with a general reinforcement learning algorithm"}, {"title": "Deviant or Wrong? The Effects of Norm Information on the Efficacy of Punishment"}, {"title": "The weirdest people in the world?"}, {"title": "Fact, fiction, and forecast"}, {"title": "A theory of justice"}, {"title": "Looking for a psychology for the inner rational agent"}, {"title": "How (and where) does moral judgment work?"}, {"title": "Scalable agent alignment via reward modeling: a research direction"}, {"title": "OpenAI Five"}, {"title": "The Most Human Human: What Talking with Computers Teaches Us About What It Means to Be Alive"}, {"title": "How to overcome prejudice"}, {"title": "The nature and origins of misperceptions: Understanding false and unsupported beliefs about politics"}, {"title": "Persuasion, influence, and value: Perspectives from communication and social neuroscience"}, {"title": "Identifying and cultivating superforecasters as a method of improving probabilistic predictions"}, {"title": "Superforecasting: The art and science of prediction"}, {"title": "Cooperative inverse reinforcement learning"}, {"title": "Inverse reward design"}, {"title": "The art of being right"}, {"title": "Are Emily and Greg more employable than Lakisha and Jamal? A field experiment on labor market discrimination"}, {"title": "Prospect theory: An analysis of decisions under risk"}, {"title": "Advances in prospect theory: Cumulative representation of uncertainty"}, {"title": "From anomalies to forecasts: Toward a descriptive model of decisions under risk, under ambiguity, and from experience"}, {"title": "Cicero: Multi-Turn, Contextual Argumentation for Accurate Crowdsourcing"}, {"title": "The rationality of informal argumentation: A Bayesian approach to reasoning fallacies"}, {"title": "Rationality in medical decision making: a review of the literature on doctors’ decision-making biases"}, {"title": "Expert political judgment: How good is it? How can we know?"}, {"title": "Two approaches to the study of experts’ characteristics"}, {"title": "Debiasing"}, {"title": "An evaluation of argument mapping as a method of enhancing critical thinking performance in e-learning environments"}, {"title": "Forecasting tournaments: Tools for increasing transparency and improving the quality of debate"}, {"title": "How to make cognitive illusions disappear: Beyond "heuristics and biases""}, {"title": "Liberals and conservatives rely on different sets of moral foundations"}, {"title": "Negative emotions can attenuate the influence of beliefs on logical reasoning"}, {"title": "Epistemic democracy: Generalizing the Condorcet jury theorem"}, {"title": "Aggregating sets of judgments: An impossibility result"}, {"title": "The Delphi technique as a forecasting tool: issues and analysis"}, {"title": "OpenAI Charter"}]}
{"source": "distill", "source_filetype": "html", "converted_with": "python", "paper_version": "", "post_title": "Feature-wise transformations", "authors": ["Vincent Dumoulin", "Ethan Perez", "Nathan Schucher", "Florian Strub", "Harm de Vries", "Aaron Courville", "Yoshua Bengio"], "date_published": "2018-07-09", "data_last_modified": "", "url": "", "abstract": "\n    Many real-world problems require integrating multiple sources of information.\n    Sometimes these problems involve multiple, distinct modalities of\n    information\u2009—\u2009vision, language, audio, etc.\u2009—\u2009as is required\n    to understand a scene in a movie or answer a question about an image.\n    Other times, these problems involve multiple sources of the same\n    kind of input, i.e. when summarizing several documents or drawing one\n    image in the style of another.\n  ", "author_comment": "", "journal_ref": "distill pub", "doi": "https://doi.org/10.23915/distill.00011", "primary_category": "", "categories": "", "citation_level": "", "main_tex_filename": "", "text": "Many real-world problems require integrating multiple sources of information.\n Sometimes these problems involve multiple, distinct modalities of\n information\u2009—\u2009vision, language, audio, etc.\u2009—\u2009as is required\n to understand a scene in a movie or answer a question about an image.\n Other times, these problems involve multiple sources of the same\n kind of input, i.e. when summarizing several documents or drawing one\n image in the style of another.\n \n\n\n\n Video and audio must be understood in the context of each  other to understand the scene.  Credit: still frame from the movie Charade. “Are you sure there’s no mistake?” An image needs to be processed in the context of  a question being asked.  Credit: image-question pair from the CLEVR dataset. Are there an equalnumber of largethings and metalspheres?\n\n\n When approaching such problems, it often makes sense to process one source\n of information *in the context of* another; for instance, in the\n right example above, one can extract meaning from the image in the context\n of the question. In machine learning, we often refer to this context-based\n processing as *conditioning*: the computation carried out by a model\n is conditioned or *modulated* by information extracted from an\n auxiliary input.\n \n\n\n\n Finding an effective way to condition on or fuse sources of information\n is an open research problem, and\n \n in this article, we concentrate on a specific family of approaches we call\n *feature-wise transformations*.\n \n We will examine the use of feature-wise transformations in many neural network\n architectures to solve a surprisingly large and diverse set of problems;\n \n their success, we will argue, is due to being flexible enough to learn an\n effective representation of the conditioning input in varied settings.\n In the language of multi-task learning, where the conditioning signal is\n taken to be a task description, feature-wise transformations\n learn a task representation which allows them to capture and leverage the\n relationship between multiple sources of information, even in remarkably\n different problem settings.\n \n\n\n\n\n---\n\n\n## Feature-wise transformations\n\n\n\n To motivate feature-wise transformations, we start with a basic example,\n where the two inputs are images and category labels, respectively. For the\n purpose of this example, we are interested in building a generative model of\n images of various classes (puppy, boat, airplane, etc.). The model takes as\n input a class and a source of random noise (e.g., a vector sampled from a\n normal distribution) and outputs an image sample for the requested class.\n \n\n\n\n A decoder-basedgenerative model maps a source of  noise to a sample  in the context of the  “puppy” class. noise“puppy”decoder\n\n\n Our first instinct might be to build a separate model for each\n class. For a small number of classes this approach is not too bad a solution,\n but for thousands of classes, we quickly run into scaling issues, as the number\n of parameters to store and train grows with the number of classes.\n We are also missing out on the opportunity to leverage commonalities between\n classes; for instance, different types of dogs (puppy, terrier, dalmatian,\n etc.) share visual traits and are likely to share computation when\n mapping from the abstract noise vector to the output image.\n \n\n\n\n Now let’s imagine that, in addition to the various classes, we also need to\n model attributes like size or color. In this case, we can’t\n reasonably expect to train a separate network for *each* possible\n conditioning combination! Let’s examine a few simple options.\n \n\n\n\n A quick fix would be to concatenate a representation of the conditioning\n information to the noise vector and treat the result as the model’s input.\n This solution is quite parameter-efficient, as we only need to increase\n the size of the first layer’s weight matrix. However, this approach makes the implicit\n assumption that the input is where the model needs to use the conditioning information.\n Maybe this assumption is correct, or maybe it’s not; perhaps the\n model does not need to incorporate the conditioning information until late\n into the generation process (e.g., right before generating the final pixel\n output when conditioning on texture). In this case, we would be forcing the model to\n carry this information around unaltered for many layers.\n \n\n\n\n Because this operation is cheap, we might as well avoid making any such\n assumptions and concatenate the conditioning representation to the input of\n *all* layers in the network. Let’s call this approach\n *concatenation-based conditioning*.\n \n\n\n\nConcatenation-based conditioning simply concatenates the conditioning  representation to the input.  The result is passed  through a linear layer  to produce the output.  input conditioningrepresentationconcatenatelinear output \n\n\n Another efficient way to integrate conditioning information into the network\n is via *conditional biasing*, namely, by adding a *bias* to\n the hidden layers based on the conditioning representation.\n \n\n\n\nConditional biasing first maps  the conditioning representation to a bias vector.  The bias vector is then  added to the input. inputoutputconditioningrepresentationlinear\n\n\n Interestingly, conditional biasing can be thought of as another way to\n implement concatenation-based conditioning. Consider a fully-connected\n linear layer applied to the concatenation of an input\n x\\mathbf{x}x and a conditioning representation\n z\\mathbf{z}z:\n \n The same argument applies to convolutional networks, provided we ignore\n the border effects due to zero-padding.\n \n\n\n\n\nConcatenation-based conditioning is equivalent to conditional biasing.  We can decompose the matrix-  vector product into two matrix-  vector subproducts.  We can then add the  resulting two vectors.  The z-dependent vector  is a conditional bias. Wxzxzconditional bias\n\n\n Yet another efficient way to integrate class information into the network is\n via *conditional scaling*, i.e., scaling hidden layers\n based on the conditioning representation.\n \n\n\n\nConditional scaling first maps the conditioning representation to a  scaling vector.  The scaling vector is then multiplied  with the input. inputoutputconditioningrepresentationlinear\n\n\n A special instance of conditional scaling is feature-wise sigmoidal gating:\n we scale each feature by a value between 000 and\n 111 (enforced by applying the logistic function), as a\n function of the conditioning representation. Intuitively, this gating allows\n the conditioning information to select which features are passed forward\n and which are zeroed out.\n \n\n\n\n Given that both additive and multiplicative interactions seem natural and\n intuitive, which approach should we pick? One argument in favor of\n *multiplicative* interactions is that they are useful in learning\n relationships between inputs, as these interactions naturally identify\n “matches”: multiplying elements that agree in sign yields larger values than\n multiplying elements that disagree. This property is why dot products are\n often used to determine how similar two vectors are.\n \n Multiplicative interactions alone have had a history of success in various\n domains\u2009—\u2009see [Bibliographic Notes](https://distill.pub/2018/feature-wise-transformations/#bibliographic-notes).\n \n One argument in favor of *additive* interactions is that they are\n more natural for applications that are less strongly dependent on the\n joint values of two inputs, like feature aggregation or feature detection\n (i.e., checking if a feature is present in either of two inputs).\n \n\n\n\n In the spirit of making as few assumptions about the problem as possible,\n we may as well combine *both* into a\n conditional *affine transformation*.\n \n An affine transformation is a transformation of the form\n y=m∗x+by = m * x + by=m∗x+b.\n \n\n\n\n\n All methods outlined above share the common trait that they act at the\n *feature* level; in other words, they leverage *feature-wise*\n interactions between the conditioning representation and the conditioned\n network. It is certainly possible to use more complex interactions,\n but feature-wise interactions often strike a happy compromise between\n effectiveness and efficiency: the number of scaling and/or shifting\n coefficients to predict scales linearly with the number of features in the\n network. Also, in practice, feature-wise transformations (often compounded\n across multiple layers) frequently have enough capacity to model complex\n phenomenon in various settings.\n \n\n\n\n Lastly, these transformations only enforce a limited inductive bias and\n remain domain-agnostic. This quality can be a downside, as some problems may\n be easier to solve with a stronger inductive bias. However, it is this\n characteristic which also enables these transformations to be so widely\n effective across problem domains, as we will later review.\n \n\n\n### Nomenclature\n\n\n\n To continue the discussion on feature-wise transformations we need to\n abstract away the distinction between multiplicative and additive\n interactions. Without losing generality, let’s focus on feature-wise affine\n transformations, and let’s adopt the nomenclature of Perez et al.\n , which formalizes conditional affine\n transformations under the acronym *FiLM*, for Feature-wise Linear\n Modulation.\n \n Strictly speaking, *linear* is a misnomer, as we allow biasing, but\n we hope the more rigorous-minded reader will forgive us for the sake of a\n better-sounding acronym.\n \n\n\n\n\n We say that a neural network is modulated using FiLM, or *FiLM-ed*,\n after inserting *FiLM layers* into its architecture. These layers are\n parametrized by some form of conditioning information, and the mapping from\n conditioning information to FiLM parameters (i.e., the shifting and scaling\n coefficients) is called the *FiLM generator*.\n In other words, the FiLM generator predicts the parameters of the FiLM\n layers based on some auxiliary input.\n Note that the FiLM parameters are parameters in one network but predictions\n from another network, so they aren’t learnable parameters with fixed\n weights as in the fully traditional sense.\n For simplicity, you can assume that the FiLM generator outputs the\n concatenation of all FiLM parameters for the network architecture.\n \n\n\n\n The FiLM generator processes the conditioning information  and produces parameters that describe how the target network  should alter its computation.  Here, the FiLM-ed network’s computation  is conditioned by two FiLM layers. outputsub-networkFiLMsub-networkFiLMsub-networkinputconditioningFiLM generatorFiLM parameters\n\n\n As the name implies, a FiLM layer applies a feature-wise affine\n transformation to its input. By *feature-wise*, we mean that scaling\n and shifting are applied element-wise, or in the case of convolutional\n networks, feature map -wise.\n \n To expand a little more on the convolutional case, feature maps can be\n thought of as the same feature detector being evaluated at different\n spatial locations, in which case it makes sense to apply the same affine\n transformation to all spatial locations.\n \n In other words, assuming x\\mathbf{x}x is a FiLM layer’s\n input, z\\mathbf{z}z is a conditioning input, and\n γ\\gammaγ and β\\betaβ are\n z\\mathbf{z}z-dependent scaling and shifting vectors,\n\n FiLM(x)=γ(z)⊙x+β(z).\n \\textrm{FiLM}(\\mathbf{x}) = \\gamma(\\mathbf{z}) \\odot \\mathbf{x}\n + \\beta(\\mathbf{z}).\n FiLM(x)=γ(z)⊙x+β(z).\n\n You can interact with the following fully-connected and convolutional FiLM\n layers to get an intuition of the sort of modulation they allow:\n \n\n\n\n In a fully-connected network,  FiLM applies a different affine  transformation to each feature.  First, each feature (or channel)  is scaled by the corresponding γ parameter.  Then, each feature (or channel)  is shifted by the corresponding β parameter. γβ In a convolutional network,  FiLM applies a different affine  transformation to each channel,  consistent across spatial locations. γβ\n\n\n In addition to being a good abstraction of conditional feature-wise\n transformations, the FiLM nomenclature lends itself well to the notion of a\n *task representation*. From the perspective of multi-task learning,\n we can view the conditioning signal as the task description. More\n specifically, we can view the concatenation of all FiLM scaling and shifting\n coefﬁcients as both an instruction on *how to modulate* the\n conditioned network and a *representation* of the task at hand. We\n will explore and illustrate this idea later on.\n \n\n\n\n\n---\n\n\n## Feature-wise transformations in the literature\n\n\n\n Feature-wise transformations find their way into methods applied to many\n problem settings, but because of their simplicity, their effectiveness is\n seldom highlighted in lieu of other novel research contributions. Below are\n a few notable examples of feature-wise transformations in the literature,\n grouped by application domain. The diversity of these applications\n underscores the flexible, general-purpose ability of feature-wise\n interactions to learn effective task representations.\n \n\n\n\nexpand all\n\nVisual question-answering+\n\n Perez et al.  use\n FiLM layers to build a visual reasoning model\n trained on the CLEVR dataset  to\n answer multi-step, compositional questions about synthetic images.\n \n\n\n\n The linguistic pipeline acts as the FiLM generator.  FiLM layers in each residual  block modulate the visualpipeline. feature extractorsub-networkFiLM layerReLU Each residual block has a  FiLM layer added to it. sub-networkFiLM layer…linearFiLM parametersAreGRUthereGRUmoreGRUcubesGRUthanGRUyellowGRUthingsGRU\n\n\n The model’s linguistic pipeline is a FiLM generator which\n extracts a question representation that is linearly mapped to\n FiLM parameter values. Using these values, FiLM layers inserted within each\n residual block condition the visual pipeline. The model is trained\n end-to-end on image-question-answer triples. Strub et al.\n  later on improved on the model by\n using an attention mechanism to alternate between attending to the language\n input and generating FiLM parameters layer by layer. This approach was\n better able to scale to settings with longer input sequences such as\n dialogue and was evaluated on the GuessWhat?! \n and ReferIt  datasets.\n \n\n\n\n de Vries et al.  leverage FiLM\n to condition a pre-trained network. Their model’s linguistic pipeline\n modulates the visual pipeline via conditional batch normalization,\n which can be viewed as a special case of FiLM. The model learns to answer natural language questions about\n real-world images on the GuessWhat?! \n and VQAv1  datasets.\n \n\n\n\n The linguistic pipeline acts  as the FiLM generator and also  directly passes the question  representation to the rest  of the network.  FiLM layers modulate the pre-  trained visual pipeline by  making the batch normalization  parameters query-dependent. sub-networknormalizationFiLM layersub-networknormalizationFiLM layer… conditional batch  normalization  conditional batch  normalization MLPFiLM parameters…IsLSTMtheLSTMumbrellaLSTMupsideLSTMdownLSTM\n\n\n The visual pipeline consists of a pre-trained residual network that is\n fixed throughout training. The linguistic pipeline manipulates the visual\n pipeline by perturbing the residual network’s batch normalization\n parameters, which re-scale and re-shift feature maps after activations\n have been normalized to have zero mean and unit variance. As hinted\n earlier, conditional batch normalization can be viewed as an instance of\n FiLM where the post-normalization feature-wise affine transformation is\n replaced with a FiLM layer.\n \n\n\nStyle transfer+\n\n Dumoulin et al.  use\n feature-wise affine transformations\u2009—\u2009in the form of conditional\n instance normalization layers\u2009—\u2009to condition a style transfer\n network on a chosen style image. Like conditional batch normalization\n discussed in the previous subsection,\n conditional instance normalization can be seen as an instance of FiLM\n where a FiLM layer replaces the post-normalization feature-wise affine\n transformation. For style transfer, the network models each style as a separate set of\n instance normalization parameters, and it applies normalization with these\n style-specific parameters.\n \n\n\n\n The FiLM generator predicts parameters  describing the target style.  The style transfer network is conditioned by making  the instance normalization  parameters style-dependent. FiLM generatorFiLM parameterssub-networknormalizationFiLM layersub-networknormalizationFiLM layer… conditional instance  normalization  conditional instance  normalization \n\n\n Dumoulin et al.  use a simple\n embedding lookup to produce instance normalization parameters, while\n Ghiasi et al.  further\n introduce a *style prediction network*, trained jointly with the\n style transfer network to predict the conditioning parameters directly from\n a given style image. In this article we opt to use the FiLM nomenclature\n because it is decoupled from normalization operations, but the FiLM\n layers used by Perez et al.  were\n themselves heavily inspired by the conditional normalization layers used\n by Dumoulin et al. .\n \n\n\n\n Yang et al.  use a related\n architecture for video object segmentation\u2009—\u2009the task of segmenting a\n particular object throughout a video given that object’s segmentation in the\n first frame. Their model conditions an image segmentation network over a\n video frame on the provided first frame segmentation using feature-wise\n scaling factors, as well as on the previous frame using position-wise\n biases.\n \n\n\n\n So far, the models we covered have two sub-networks: a primary\n network in which feature-wise transformations are applied and a secondary\n network which outputs parameters for these transformations. However, this\n distinction between *FiLM-ed network* and *FiLM generator*\n is not strictly necessary. As an example, Huang and Belongie\n  propose an alternative\n style transfer network that uses adaptive instance normalization layers,\n which compute normalization parameters using a simple heuristic.\n \n\n\n\n The model processes  content and style images  up to the adaptive instance  normalization layer.  FiLM parameters are  computed as the spatial  mean and standard  deviation statistics of the  style feature maps.  The FiLM-ed feature maps  are fed to the remainder  of the network to produce  the stylized image. sub-networksub-networkγ, β acrossspatial axesFiLMparametersnormalizationFiLM layer adaptive instance  normalization sub-network\n\n\n Adaptive instance normalization can be interpreted as inserting a FiLM\n layer midway through the model. However, rather than relying\n on a secondary network to predict the FiLM parameters from the style\n image, the main network itself is used to extract the style features\n used to compute FiLM parameters. Therefore, the model can be seen as\n *both* the FiLM-ed network and the FiLM generator.\n \n\n\nImage recognition+\n\n As discussed in previous subsections, there is nothing preventing us from considering a\n neural network’s activations *themselves* as conditioning\n information. This idea gives rise to\n *self-conditioned* models.\n \n\n\n\n The FiLM generator predicts FiLM  parameters conditioned on the  network’s internal activations.  An arbitrary input vector (or feature map)  modulates downstream activations. inputsub-networkFiLM layerFiLM generatoroutput\n\n\n Highway Networks  are a prime\n example of applying this self-conditioning principle. They take inspiration\n from the LSTMs’  heavy use of\n feature-wise sigmoidal gating in their input, forget, and output gates to\n regulate information flow:\n \n\n\n\ninputsub-networksigmoidal layer1 - xoutput\n\n\n The ImageNet 2017 winning model  also\n employs feature-wise sigmoidal gating in a self-conditioning manner, as a\n way to “recalibrate” a layer’s activations conditioned on themselves.\n \n\n\n\n The squeeze-and-excitation block  uses sigmoidal gating. First, the  network maps input feature maps  to a gating vector.  The gating vector is then multiplied  with the input feature maps. inputoutputglobal poolingReLU layersigmoidal layer\n\nNatural language processing+\n\n For statistical language modeling (i.e., predicting the next word\n in a sentence), the LSTM \n constitutes a popular class of recurrent network architectures. The LSTM\n relies heavily on feature-wise sigmoidal gating to control the\n information flow in and out of the memory or context cell\n c\\mathbf{c}c, based on the hidden states\n h\\mathbf{h}h and inputs x\\mathbf{x}x at\n every timestep t\\mathbf{t}t.\n \n\n\n\nct-1tanhcthtsigmoidsigmoidtanhsigmoidlinearlinearlinearlinearht-1xtconcatenate\n\n\n Also in the domain of language modeling, Dauphin et al.  use sigmoidal\n gating in their proposed *gated linear unit*, which uses half of the\n input features to apply feature-wise sigmoidal gating to the other half.\n Gehring et al.  adopt this\n architectural feature, introducing a fast, parallelizable model for machine\n translation in the form of a fully convolutional network.\n \n\n\n\n The gated linear unit activation function  uses sigmoidal gating. Half of the input  features go through a sigmoid function  to produce a gating vector.  The gating vector is then multiplied  with the second half of the features. inputsigmoidoutput\n\n\n The Gated-Attention Reader \n uses feature-wise scaling, extracting information\n from text by conditioning a document-reading network on a query. Its\n architecture consists of multiple Gated-Attention modules, which involve\n element-wise multiplications between document representation tokens and\n token-specific query representations extracted via soft attention on the\n query representation tokens.\n \n\n\n\n Dhingra et al. use conditional scaling to  integrate query information into a document  processing network. Applying soft attention  to the query representation tokens produces the scaling vector.  The scaling vector is then multiplied with  the input document representation tokendocumentrepresentationtokenoutputtokenqueryrepresentationtokenssoft attention\n\nReinforcement learning+\n\n The Gated-Attention architecture \n uses feature-wise sigmoidal gating to fuse linguistic and visual\n information in an agent trained to follow simple “go-to” language\n instructions in the VizDoom  3D\n environment.\n \n\n\n\n Chaplot et al. use sigmoidal gating as a multimodal fusion mechanism.  An instruction representation is mapped  to a scaling vector via a sigmoid layer.  The scaling vector is then multiplied with  the input feature maps. A policy network  uses the result to decide the next action. inputoutputinstructionrepresentationsigmoid\n\n\n Bahdanau et al. use FiLM\n layers to condition Neural Module Network\n and LSTM -based policies to follow\n basic, compositional language instructions (arranging objects and going\n to particular locations) in a 2D grid world. They train this policy\n in an adversarial manner using rewards from another FiLM-based network,\n trained to discriminate between ground-truth examples of achieved\n instruction states and failed policy trajectories states.\n \n\n\n\n Outside instruction-following, Kirkpatrick et al.\n  also use\n game-specific scaling and biasing to condition a shared policy network\n trained to play 10 different Atari games.\n \n\n\nGenerative modeling+\n\n The conditional variant of DCGAN ,\n a well-recognized network architecture for generative adversarial networks\n , uses concatenation-based\n conditioning. The class label is broadcasted as a feature map and then\n concatenated to the input of convolutional and transposed convolutional\n layers in the discriminator and generator networks.\n \n\n\n\nConcatenation-based conditioning is used in the class-conditional  DCGAN model. Each convolutional layer is concatenated with the  broadcased label along the channel axis.  The resulting stack of feature maps  is then convolved to produce the  conditioned output. inputconcatenateinputclass labelconvolutionoutputbroadcastclass label\n\n\n For convolutional layers, concatenation-based conditioning requires the\n network to learn redundant convolutional parameters to interpret each\n constant, conditioning feature map; as a result, directly applying a\n conditional bias is more parameter efficient, but the two approaches are\n still mathematically equivalent.\n \n\n\n\n PixelCNN \n and WaveNet \u2009—\u2009two recent\n advances in autoregressive, generative modeling of images and audio,\n respectively\u2009—\u2009use conditional biasing. The simplest form of\n conditioning in PixelCNN adds feature-wise biases to all convolutional layer\n outputs. In FiLM parlance, this operation is equivalent to inserting FiLM\n layers after each convolutional layer and setting the scaling coefficients\n to a constant value of 1.\n \n The authors also describe a location-dependent biasing scheme which\n cannot be expressed in terms of FiLM layers due to the absence of the\n feature-wise property.\n \n\n\n\n\n PixelCNN uses conditional biasing.  The model first maps a high-level  image description to a bias vector.  Then, it adds the bias vector to  the input stack of feature maps  to condition convolutional layers. inputoutputimagedescriptionlinear\n\n\n WaveNet describes two ways in which conditional biasing allows external\n information to modulate the audio or speech generation process based on\n conditioning input:\n \n\n\n1. **Global conditioning** applies the same conditional bias\n to the whole generated sequence and is used e.g. to condition on speaker\n identity.\n2. **Local conditioning** applies a conditional bias which\n varies across time steps of the generated sequence and is used e.g. to\n let linguistic features in a text-to-speech model influence which sounds\n are produced.\n\n\n\n As in PixelCNN, conditioning in WaveNet can be viewed as inserting FiLM\n layers after each convolutional layer. The main difference lies in how\n the FiLM-generating network is defined: global conditioning\n expresses the FiLM-generating network as an embedding lookup which is\n broadcasted to the whole time series, whereas local conditioning expresses\n it as a mapping from an input sequence of conditioning information to an\n output sequence of FiLM parameters.\n \n\n\nSpeech recognition+\n\n Kim et al. modulate a deep\n bidirectional LSTM using a form\n of conditional normalization. As discussed in the\n *Visual question-answering* and *Style transfer* subsections,\n conditional normalization can be seen as an instance of FiLM where\n the post-normalization feature-wise affine transformation is replaced\n with a FiLM layer.\n \n\n\n\n Kim et al. achieve speaker adaptation by adapting the usual  LSTM architecture to condition its various gates on an  utterance summarization. ct-1tanhcthtsigmoidsigmoidtanhsigmoidht-1 utterance  summarization xt Each gate uses FiLM to condition on the utterancesummarization. linearnormalizationFiLMlinearnormalizationFiLMht-1xtFiLM generator utterance  summarization \n\n\n The key difference here is that the conditioning signal does not come from\n an external source but rather from utterance\n summarization feature vectors extracted in each layer to adapt the model.\n \n\n\nDomain adaptation and few-shot learning+\n\n For domain adaptation, Li et al. \n find it effective to update the per-channel batch normalization\n statistics (mean and variance) of a network trained on one domain with that\n network’s statistics in a new, target domain. As discussed in the\n *Style transfer* subsection, this operation is akin to using the network as\n both the FiLM generator and the FiLM-ed network. Notably, this approach,\n along with Adaptive Instance Normalization, has the particular advantage of\n not requiring any extra trainable parameters.\n \n\n\n\n For few-shot learning, Oreshkin et al.\n  explore the use of FiLM layers to\n provide more robustness to variations in the input distribution across\n few-shot learning episodes. The training set for a given episode is used to\n produce FiLM parameters which modulate the feature extractor used in a\n Prototypical Networks \n meta-training procedure.\n \n\n\n\n\n---\n\n\n## Related ideas\n\n\n\n Aside from methods which make direct use of feature-wise transformations,\n the FiLM framework connects more broadly with the following methods and\n concepts.\n \n\n\n\nexpand all\n\nZero-shot learning+\n\n The idea of learning a task representation shares a strong connection with\n zero-shot learning approaches. In zero-shot learning, semantic task\n embeddings may be learned from external information and then leveraged to\n make predictions about classes without training examples. For instance, to\n generalize to unseen object categories for image classification, one may\n construct semantic task embeddings from text-only descriptions and exploit\n objects’ text-based relationships to make predictions for unseen image\n categories. Frome et al. , Socher et\n al. , and Norouzi et al.\n  are a few notable exemplars\n of this idea.\n \n\n\nHyperNetworks+\n\n The notion of a secondary network predicting the parameters of a primary\n network is also well exemplified by HyperNetworks , which predict weights for entire layers\n (e.g., a recurrent neural network layer). From this perspective, the FiLM\n generator is a specialized HyperNetwork that predicts the FiLM parameters of\n the FiLM-ed network. The main distinction between the two resides in the\n number and specificity of predicted parameters: FiLM requires predicting far\n fewer parameters than Hypernetworks, but also has less modulation potential.\n The ideal trade-off between a conditioning mechanism’s capacity,\n regularization, and computational complexity is still an ongoing area of\n investigation, and many proposed approaches lie on the spectrum between FiLM\n and HyperNetworks (see [Bibliographic Notes](https://distill.pub/2018/feature-wise-transformations/#bibliographic-notes)).\n \n\n\nAttention+\n\n Some parallels can be drawn between attention and FiLM, but the two operate\n in different ways which are important to disambiguate.\n \n\n\n\nAttention computes a  probability distribution  over locations. Attention pools over locations. Attention summarizes  the input into a vector. FiLM computes a scaling  vector applied to the feature axis. FiLM conserves  input dimensions. αΣ…(β omitted for clarity)γ\n\n\n This difference stems from distinct intuitions underlying attention and\n FiLM: the former assumes that specific spatial locations or time steps\n contain the most useful information, whereas the latter assumes that\n specific features or feature maps contain the most useful information.\n \n\n\nBilinear transformations+\n\n With a little bit of stretching, FiLM can be seen as a special case of a\n bilinear transformation\n  with low-rank weight\n matrices. A bilinear transformation defines the relationship between two\n inputs x\\mathbf{x}x and z\\mathbf{z}z and the\n kthk^{th}kth output feature yky\\_kyk\u200b as\n\n yk=xTWkz.\n y\\_k = \\mathbf{x}^T W\\_k \\mathbf{z}.\n yk\u200b=xTWk\u200bz.\n\n Note that for each output feature yky\\_kyk\u200b we have a separate\n matrix WkW\\_kWk\u200b, so the full set of weights forms a\n multi-dimensional array.\n \n\n\n\n Each element yk of the output vector y is the  result of a distinct vector-matrix-vector product.  This enables multiplicative interactions between  any pair of elements of x and z. yxW1 zxW2 zxW3 zW1zW2zW3z\n\n\n If we view z\\mathbf{z}z as the concatenation of the scaling\n and shifting vectors γ\\gammaγ and β\\betaβ and\n if we augment the input x\\mathbf{x}x with a 1-valued feature,\n \n As is commonly done to turn a linear transformation into an affine\n transformation.\n \n we can represent FiLM using a bilinear transformation by zeroing out the\n appropriate weight matrix entries:\n \n\n\n\n FiLM computes elements of the output  vector as yk = γk xk + βk.  This can be expressed as a dot product  between a 1-augmented x and a sparse  vector containing γk and βk.  (Shaded  cells have a zero value.)  The sparse vector is given by multiplying  a low-rank weight matrix with the  concatenation of γ and β. (Shaded cells  again have a zero value.) y1xW1 z1xW2 z1xW3 zW111zγβW211zγβW311zγβ\n\n\n For some applications of bilinear transformations,\n see the [Bibliographic Notes](https://distill.pub/2018/feature-wise-transformations/#bibliographic-notes).\n \n\n\n\n\n---\n\n\n## Properties of the learned task representation\n\n\n\n As hinted earlier, in adopting the FiLM perspective we implicitly introduce\n a notion of *task representation*: each task\u2009—\u2009be it a question\n about an image or a painting style to imitate\u2009—\u2009elicits a different\n set of FiLM parameters via the FiLM generator which can be understood as its\n representation in terms of how to modulate the FiLM-ed network. To help\n better understand the properties of this representation, let’s focus on two\n FiLM-ed models used in fairly different problem settings:\n \n\n\n* The visual reasoning model of Perez et al.\n , which uses FiLM\n to modulate a visual processing pipeline based off an input question.\n \n The linguistic pipeline acts as the FiLM generator.  FiLM layers in each residual  block modulate the visualpipeline. feature extractorsub-networkFiLM layerReLU Each residual block has a  FiLM layer added to it. sub-networkFiLM layer…linearFiLM parametersAreGRUthereGRUmoreGRUcubesGRUthanGRUyellowGRUthingsGRU\n* The artistic style transfer model of Ghiasi et al.\n , which uses FiLM to modulate a\n feed-forward style transfer network based off an input style image.\n \n The FiLM generator predicts parameters  describing the target style.  The style transfer network is conditioned by making  the instance normalization  parameters style-dependent. FiLM generatorFiLM parameterssub-networknormalizationFiLM layersub-networknormalizationFiLM layer… conditional instance  normalization  conditional instance  normalization\n\n\n\n As a starting point, can we discern any pattern in the FiLM parameters as a\n function of the task description? One way to visualize the FiLM parameter\n space is to plot γ\\gammaγ against β\\betaβ,\n with each point corresponding to a specific task description and a specific\n feature map. If we color-code each point according to the feature map it\n belongs to we observe the following:\n \n\n\n\n FiLM parameters for 256 tasks and for 16 feature maps, chosen randomly.  Visual reasoning model  Style transfer model γβ Feature map γβ Feature map \n\n\n The plots above allow us to make several interesting observations. First,\n FiLM parameters cluster by feature map in parameter space, and the cluster\n locations are not uniform across feature maps. The orientation of these\n clusters is also not uniform across feature maps: the main axis of variation\n can be γ\\gammaγ-aligned, β\\betaβ-aligned, or\n diagonal at varying angles. These findings suggest that the affine\n transformation in FiLM layers is not modulated in a single, consistent way,\n i.e., using γ\\gammaγ only, β\\betaβ only, or\n γ\\gammaγ and β\\betaβ together in some specific\n way. Maybe this is due to the affine transformation being overspecified, or\n maybe this shows that FiLM layers can be used to perform modulation\n operations in several distinct ways.\n \n\n\n\n Nevertheless, the fact that these parameter clusters are often somewhat\n “dense” may help explain why the style transfer model of Ghiasi et al.\n  is able to perform style\n interpolations: any convex combination of FiLM parameters is likely to\n correspond to a meaningful parametrization of the FiLM-ed network.\n \n\n\n\nStyle 1Style 2 w InterpolationContent Image\n\n\n To some extent, the notion of interpolating between tasks using FiLM\n parameters can be applied even in the visual question-answering setting.\n Using the model trained in Perez et al. ,\n we interpolated between the model’s FiLM parameters for two pairs of CLEVR\n questions. Here we visualize the input locations responsible for\n the globally max-pooled features fed to the visual pipeline’s output classifier:\n \n\n\n\n What shape is  the red thing left  of the sphere?  What shape is  the red thing right  of the sphere?  How many brown  things are there?  How many yellow  things are there? \n\n\n The network seems to be softly switching where in the image it is looking,\n based on the task description. It is quite interesting that these semantically\n meaningful interpolation behaviors emerge, as the network has not been\n trained to act this way.\n \n\n\n\n Despite these similarities across problem settings, we also observe\n qualitative differences in the way in which FiLM parameters cluster as a\n function of the task description. Unlike the style transfer model, the\n visual reasoning model sometimes exhibits several FiLM parameter\n sub-clusters for a given feature map.\n \n\n\n\n FiLM parameters of the visual reasoning model for 256 questions chosen randomly. Feature map 26 of the first FiLM layer. Feature map 76 of the first FiLM layer. γβγβ\n\n\n At the very least, this may indicate that FiLM learns to operate in ways\n that are problem-specific, and that we should not expect to find a unified\n and problem-independent explanation for FiLM’s success in modulating FiLM-ed\n networks. Perhaps the compositional or discrete nature of visual reasoning\n requires the model to implement several well-defined modes of operation\n which are less necessary for style transfer.\n \n\n\n\n Focusing on individual feature maps which exhibit sub-clusters, we can try\n to infer how questions regroup by color-coding the scatter plots by question\n type.\n \n\n\n\n FiLM parameters of the visual reasoning model for 256 questions chosen randomly. Feature map 26 of the first FiLM layer. Feature map 76 of the first FiLM layer.  Question type γβγβExistsLess thanGreater thanCountQuery materialQuery sizeQuery colorQuery shapeEqual colorEqual integerEqual shapeEqual sizeEqual material\n\n\n Sometimes a clear pattern emerges, as in the right plot, where color-related\n questions concentrate in the top-right cluster\u2009—\u2009we observe that\n questions either are of type *Query color* or *Equal color*,\n or contain concepts related to color. Sometimes it is harder to draw a\n conclusion, as in the left plot, where question types are scattered across\n the three clusters.\n \n\n\n\n In cases where question types alone cannot explain the clustering of the\n FiLM parameters, we can turn to the conditioning content itself to gain\n an understanding of the mechanism at play. Let’s take a look at two more\n plots: one for feature map 26 as in the previous figure, and another\n for a different feature map, also exhibiting several subclusters. This time\n we regroup points by the words which appear in their associated question.\n \n\n\n\n FiLM parameters of the visual reasoning model for 256 questions chosen randomly. Feature map 26 suggests an object position  separation mechanism. Feature map 92 suggests an object material  separation mechanism.  Word in question γβγβfrontbehindleftrightmaterialrubbermattemetalmetallicshiny\n\n\n In the left plot, the left subcluster corresponds to questions involving\n objects positioned *in front* of other objects, while the right\n subcluster corresponds to questions involving objects positioned\n *behind* other objects. In the right plot we see some evidence of\n separation based on object material: the left subcluster corresponds to\n questions involving *matte* and *rubber* objects, while the\n right subcluster contains questions about *shiny* or\n *metallic* objects.\n \n\n\n\n The presence of sub-clusters in the visual reasoning model also suggests\n that question interpolations may not always work reliably, but these\n sub-clusters don’t preclude one from performing arithmetic on the question\n representations, as Perez et al. \n report.\n \n\n\n\n The model incorrectly answers a question  which involves an unseen combination  of concepts (in bold).  Rather than using the FiLM parameters  of the FiLM generator, we can use those  produced by combining questions with  familiar combinations of concepts (in bold). This corrects the model’s answer.  Q: What is the blue big cylinder made of?  Q\xa0? Rubber ✘ QA\xa0: What is the blue big sphere made of?  QB\xa0: What is the green big cylinder made of?  QC\xa0: What is the green big sphere made of? QA+ QB- QC\xa0?Metal ✔\n\n\n Perez et al.  report that this sort of\n task analogy is not always successful in correcting the model’s answer, but\n it does point to an interesting fact about FiLM-ed networks: sometimes the\n model makes a mistake not because it is incapable of computing the correct\n output, but because it fails to produce the correct FiLM parameters for a\n given task description. The reverse can also be true: if the set of tasks\n the model was trained on is insufficiently rich, the computational\n primitives learned by the FiLM-ed network may be insufficient to ensure good\n generalization. For instance, a style transfer model may lack the ability to\n produce zebra-like patterns if there are no stripes in the styles it was\n trained on. This could explain why Ghiasi et al.\n  report that their style transfer\n model’s ability to produce pastiches for new styles degrades if it has been\n trained on an insufficiently large number of styles. Note however that in\n that case the FiLM generator’s failure to generalize could also play a role,\n and further analysis would be needed to draw a definitive conclusion.\n \n\n\n\n This points to a separation between the various computational\n primitives learned by the FiLM-ed network and the “numerical recipes”\n learned by the FiLM generator: the model’s ability to generalize depends\n both on its ability to parse new forms of task descriptions and on it having\n learned the required computational primitives to solve those tasks. We note\n that this multi-faceted notion of generalization is inherited directly from\n the multi-task point of view adopted by the FiLM framework.\n \n\n\n\n Let’s now turn our attention back to the overal structural properties of FiLM\n parameters observed thus far. The existence of this structure has already\n been explored, albeit more indirectly, by Ghiasi et al.\n  as well as Perez et al.\n , who applied t-SNE\n  on the FiLM parameter values.\n \n\n\n\n\n\n t-SNE projection of FiLM parameters for many task descriptions.\n \n\n\n\n\nVisual reasoning model Question type ExistsLess thanGreater thanCountQuery materialQuery sizeQuery colorQuery shapeEqual colorEqual integerEqual shapeEqual sizeEqual materialReset pan / zoomStyle transfer model Artist name Reset pan / zoom\n\n\n The projection on the left is inspired by a similar projection done by Perez\n et al.  for their visual reasoning\n model trained on CLEVR and shows how questions group by question type.\n The projection on the right is inspired by a similar projection done by\n Ghiasi et al.  for their style\n transfer network. The projection does not cluster artists as neatly as the\n projection on the left, but this is to be expected, given that an artist’s\n style may vary widely over time. However, we can still detect interesting\n patterns in the projection: note for instance the isolated cluster (circled\n in the figure) in which paintings by Ivan Shishkin and Rembrandt are\n aggregated. While these two painters exhibit fairly different styles, the\n cluster is a grouping of their sketches.\n \n\n\n\n Rembrandt’s Woman with aPink.  Shishkin’s Woman with a boyin the forest.  Sketches by Rembrandt and  Shishkin found in the same t-SNE  cluster. \n\n\n To summarize, the way neural networks learn to use FiLM layers seems to\n vary from problem to problem, input to input, and even from feature to\n feature; there does not seem to be a single mechanism by which the\n network uses FiLM to condition computation. This flexibility may\n explain why FiLM-related methods have been successful across such a\n wide variety of domains.\n \n\n\n\n\n---\n\n\n## Discussion\n\n\n\n Looking forward, there are still many unanswered questions.\n Do these experimental observations on FiLM-based architectures generalize to\n other related conditioning mechanisms, such as conditional biasing, sigmoidal\n gating, HyperNetworks, and bilinear transformations? When do feature-wise\n transformations outperform methods with stronger inductive biases and vice\n versa? Recent work combines feature-wise transformations with stronger\n inductive bias methods\n ,\n which could be an optimal middle ground. Also, to what extent are FiLM’s\n task representation properties\n inherent to FiLM, and to what extent do they emerge from other features\n of neural networks (i.e. non-linearities, FiLM generator\n depth, etc.)? If you are interested in exploring these or other\n questions about FiLM, we recommend looking into the code bases for\n FiLM models for [visual reasoning](https://github.com/ethanjperez/film)\n and [style transfer](https://github.com/tensorflow/magenta/tree/master/magenta/models/arbitrary_image_stylization)\n which we used as a\n starting point for our experiments here.\n \n\n\n\n Finally, the fact that changes on the feature level alone are able to\n compound into large and meaningful modulations of the FiLM-ed network is\n still very surprising to us, and hopefully future work will uncover deeper\n explanations. For now, though, it is a question that\n evokes the even grander mystery of how neural networks in general compound\n simple operations like matrix multiplications and element-wise\n non-linearities into semantically meaningful transformations.", "bibliography_bbl": "", "bibliography_bib": [{"title": "FiLM: Visual Reasoning with a General Conditioning Layer"}, {"title": "Learning visual reasoning without strong priors"}, {"title": "CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning"}, {"title": "Visual Reasoning with Multi-hop Feature Modulation"}, {"title": "GuessWhat?! Visual object discovery through multi-modal dialogue"}, {"title": "ReferItGame: Referring to objects in photographs of natural scenes"}, {"title": "Modulating early visual processing by language"}, {"title": "VQA: visual question answering"}, {"title": "A learned representation for artistic style"}, {"title": "Exploring the structure of a real-time, arbitrary neural artistic stylization network"}, {"title": "Efficient video object segmentation via network modulation"}, {"title": "Arbitrary style transfer in real-time with adaptive instance normalization"}, {"title": "Highway networks"}, {"title": "Long short-term memory"}, {"title": "Squeeze-and-Excitation networks"}, {"title": "On the state of the art of evaluation in neural language models"}, {"title": "Language modeling with gated convolutional networks"}, {"title": "Convolution sequence-to-sequence learning"}, {"title": "Gated-attention readers for text comprehension"}, {"title": "Gated-attention architectures for task-oriented language grounding"}, {"title": "Vizdoom: A doom-based AI research platform for visual reinforcement learning"}, {"title": "Learning to follow language instructions with adversarial reward induction"}, {"title": "Neural module networks"}, {"title": "Overcoming catastrophic forgetting in neural networks"}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks"}, {"title": "Generative adversarial nets"}, {"title": "Conditional image generation with PixelCNN decoders"}, {"title": "WaveNet: A generative model for raw audio"}, {"title": "Dynamic layer normalization for adaptive neural acoustic modeling in speech recognition"}, {"title": "Adaptive batch normalization for practical domain adaptation"}, {"title": "TADAM: Task dependent adaptive metric for improved few-shot learning"}, {"title": "Prototypical networks for few-shot learning"}, {"title": "Devise: A deep visual-semantic embedding model"}, {"title": "Zero-shot learning through cross-modal transfer"}, {"title": "Zero-shot learning by convex combination of semantic embeddings"}, {"title": "HyperNetworks"}, {"title": "Separating style and content with bilinear models"}, {"title": "Visualizing data using t-SNE"}, {"title": "A dataset and architecture for visual reasoning with a working memory"}, {"title": "A parallel computation that assigns canonical object-based frames of reference"}, {"title": "The correlation theory of brain function"}, {"title": "Generating text with recurrent neural networks"}, {"title": "Robust boltzmann machines for recognition and denoising"}, {"title": "Factored conditional restricted Boltzmann machines for modeling motion style"}, {"title": "Combining discriminative features to infer complex trajectories"}, {"title": "Learning where to attend with deep architectures for image tracking"}, {"title": "Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis"}, {"title": "Convolutional learning of spatio-temporal features"}, {"title": "Learning to relate images"}, {"title": "Incorporating side information by adaptive convolution"}, {"title": "Learning multiple visual domains with residual adapters"}, {"title": "Predicting deep zero-shot convolutional neural networks using textual descriptions"}, {"title": "Zero-shot task generalization with multi-task deep reinforcement learning"}, {"title": "Separating style and content"}, {"title": "Facial expression space learning"}, {"title": "Personalized recommendation on dynamic content using predictive bilinear models"}, {"title": "Like like alike: joint friendship and interest propagation in social networks"}, {"title": "Matrix factorization techniques for recommender systems"}, {"title": "Bilinear CNN models for fine-grained visual recognition"}, {"title": "Convolutional two-stream network fusion for video action recognition"}, {"title": "Multimodal compact bilinear pooling for visual question answering and visual grounding"}]}
{"source": "distill", "source_filetype": "html", "converted_with": "python", "paper_version": "", "post_title": "A Discussion of "Adversarial Examples Are Not Bugs, They Are Features": Robust Feature Leakage", "authors": ["Gabriel Goh"], "date_published": "2019-08-06", "data_last_modified": "", "url": "", "abstract": "This article is part of a discussion of the Ilyas et al. paper “Adversarial examples are not bugs, they are features”. You can learn more in the main discussion article.", "author_comment": "", "journal_ref": "distill pub", "doi": "https://doi.org/10.23915/distill.00019.2", "primary_category": "", "categories": "", "citation_level": "", "main_tex_filename": "", "text": "This article is part of a discussion of the Ilyas et al. paper\n *“Adversarial examples are not bugs, they are features”.*\n You can learn more in the\n [main discussion article](https://distill.pub/2019/advex-bugs-discussion/) .\n \n\n\n[Other Comments](https://distill.pub/2019/advex-bugs-discussion/#commentaries)\n[Comment by Ilyas et al.](https://distill.pub/2019/advex-bugs-discussion/response-2/#rebuttal)\n\n\n Ilyas et al.  report a surprising result: a model trained on\n adversarial examples is effective on clean data. They suggest this transfer is driven by adverserial\n examples containing geuinely useful non-robust cues. But an alternate mechanism for the transfer could be a\n kind of “robust feature leakage” where the model picks up on faint robust cues in the attacks.\n \n\n\n\n\n We show that at least 23.5% (out of 88%) of the accuracy can be explained by robust features in\n DrandD\\_\\text{rand}Drand\u200b. This is a weak lower bound, established by a linear model, and does not perclude the\n possibility of further leakage. On the other hand, we find no evidence of leakage in DdetD\\_\\text{det}Ddet\u200b.\n \n\n\n### Lower Bounding Leakage\n\n\n\n Our technique for quantifying leakage consisting of two steps:\n \n\n\n\n1. First, we construct features fi(x)=wiTxf\\_i(x) = w\\_i^Txfi\u200b(x)=wiT\u200bx that are provably robust, in a sense we will soon\n specify.\n2. Next, we train a linear classifier as per ,\n Equation 3 on the datasets D^det\\hat{\\mathcal{D}}\\_{\\text{det}}D^det\u200b and\n D^rand\\hat{\\mathcal{D}}\\_{\\text{rand}}D^rand\u200b (Defined , Table 1) on\n these robust features *only*.\n\n\n\n\n Since Ilyas et al.  only specify robustness in the two class\n case, we propose two possible specifications for what constitutes a *robust feature* in the multiclass\n setting:\n\n \n\n\n **Specification 1**  \nFor at least one of the\n classes, the feature is γ\\gammaγ-robustly useful   with\n γ=0\\gamma = 0γ=0, and the set of valid perturbations equal to an L2L\\_2L2\u200b norm ball with radius 0.25.\n \n\n **Specification 2**   \n\n The feature comes from a robust model for which at least 80% of points in the test set have predictions\n that remain static in a neighborhood of radius 0.25 on the L2L\\_2L2\u200b norm ball.\n \n\n\n We find features that satisfy *both* specifications by using the 10 linear features of a robust linear\n model trained on CIFAR-10. Because the features are linear, the above two conditions can be certified\n analytically. We leave the reader to inspect the weights corresponding to the features manually:\n \n\n\n\nγ\\_0 = 0.214  γ\\_1 = 0.194  γ\\_2 = 0.126  γ\\_3 = 0.126  γ\\_4 = 0.143  γ\\_5 = 0.154  γ\\_6 = 0.172  γ\\_7 = 0.155  γ\\_8 = 0.231  γ\\_9 = 0.212  \n\n 10 Features, FCF\\_CFC\u200b, of robust linear classifier CCC. Each feature is γi\\gamma\\_iγi\u200b-robustly-useful with\n respect to label iii. Visualized are the weights wiw\\_iwi\u200b of features fi(x)=wiTxf\\_i(x) = w\\_i^Txfi\u200b(x)=wiT\u200bx.\n \n\n\n Training a linear model on the above robust features on D^rand\\hat{\\mathcal{D}}\\_{\\text{rand}}D^rand\u200b and testing on the\n CIFAR test set incurs an accuracy of **23.5%** (out of 88%). Doing the same on\n D^det\\hat{\\mathcal{D}}\\_{\\text{det}}D^det\u200b incurs an accuracy of **6.81%** (out of 44%).\n \n\n\n\n The contrasting results suggest that the the two experiements should be interpreted differently. The\n transfer results of D^rand\\hat{\\mathcal{D}}\\_{\\text{rand}}D^rand\u200b in Table 1 of \n should approached with caution: A non-trivial portion of the accuracy can be attributed to robust\n features. Note that this bound is weak: this bound could be possibly be improved if we used nonlinear\n features, e.g. from a robust deep neural network.\n \n\n\n\n The results of D^det\\hat{\\mathcal{D}}\\_{\\text{det}}D^det\u200b in Table 1 of \n however, are on stronger footing. We find no evidence of feature leakage (in fact, we find negative leakage\u2009—\u2009an influx!). We thus conclude that it is plausible the majority of the accuracy is driven by\n non-robust features, exactly the thesis of .\n \n\n\n\n\n To cite Ilyas et al.’s response, please cite their\n [collection of responses](https://distill.pub/2019/advex-bugs-discussion/original-authors/#citation).\n\n\n**Response Summary**: This\n is a valid concern that was actually one of our motivations for creating the\n D^det\\widehat{\\mathcal{D}}\\_{det}D\ndet\u200b dataset (which, as the comment notes, actually\n has *misleading* robust features). The provided experiment further\n improves our understanding of the underlying phenomenon. \n**Response**: This comment raises a valid concern which was in fact one of\n the primary reasons for designing the D^det\\widehat{\\mathcal{D}}\\_{det}D\ndet\u200b dataset.\n In particular, recall the construction of the D^rand\\widehat{\\mathcal{D}}\\_{rand}D\nrand\u200b\n dataset: assign each input a random target label and do PGD towards that label.\n Note that unlike the D^det\\widehat{\\mathcal{D}}\\_{det}D\ndet\u200b dataset (in which the\n target class is deterministically chosen), the D^rand\\widehat{\\mathcal{D}}\\_{rand}D\nrand\u200b\n dataset allows for robust features to actually have a (small) positive\n correlation with the label. \n\n\nTo see how this can happen, consider the following simple setting: we have a\n single feature f(x)f(x)f(x) that is 111 for cats and −1-1−1 for dogs. If ϵ=0.1\\epsilon = 0.1ϵ=0.1\n then f(x)f(x)f(x) is certainly a robust feature. However, randomly assigning labels\n (as in the dataset D^rand\\widehat{\\mathcal{D}}\\_{rand}D\nrand\u200b) would make this feature\n uncorrelated with the assigned label, i.e., we would have that E[f(x)⋅y]=0E[f(x)\\cdot y] = 0E[f(x)⋅y]=0. Performing a\n targeted attack might in this case induce some correlation with the\n assigned label, as we could have E[f(x+η⋅∇f(x))⋅y]>E[f(x)⋅y]=0\\mathbb{E}[f(x+\\eta\\cdot\\nabla\n f(x))\\cdot y] > \\mathbb{E}[f(x)\\cdot y] = 0E[f(x+η⋅∇f(x))⋅y]>E[f(x)⋅y]=0, allowing a model to learn\n to correctly classify new inputs. \n\n\nIn other words, starting from a dataset with no features, one can encode\n robust features within small perturbations. In contrast, in the\n D^det\\widehat{\\mathcal{D}}\\_{det}D\ndet\u200b dataset, the robust features are *correlated\n with the original label* (since the labels are permuted) and since they are\n robust, they cannot be flipped to correlate with the newly assigned (wrong)\n label. Still, the D^rand\\widehat{\\mathcal{D}}\\_{rand}D\nrand\u200b dataset enables us to show\n that (a) PGD-based adversarial examples actually alter features in the data and\n (b) models can learn from human-meaningless/mislabeled training data. The\n D^det\\widehat{\\mathcal{D}}\\_{det}D\ndet\u200b dataset, on the other hand, illustrates that the\n non-robust features are actually sufficient for generalization and can be\n preferred over robust ones in natural settings.\n\n\nThe experiment put forth in the comment is a clever way of showing that such\n leakage is indeed possible. However, we want to stress (as the comment itself\n does) that robust feature leakage does *not* have an impact on our main\n thesis\u2009—\u2009the D^det\\widehat{\\mathcal{D}}\\_{det}D\ndet\u200b dataset explicitly controls\n for robust\n feature leakage (and in fact, allows us to quantify the models’ preference for\n robust features vs non-robust features\u2009—\u2009see Appendix D.6 in the\n [paper](https://arxiv.org/abs/1905.02175)).\n\n\n\n\n You can find more responses in the  [main discussion article](https://distill.pub/2019/advex-bugs-discussion/).", "bibliography_bbl": "", "bibliography_bib": [{"title": "Adversarial examples are not bugs, they are features"}]}
{"source": "distill", "source_filetype": "html", "converted_with": "python", "paper_version": "", "post_title": "Thread: Circuits", "authors": ["Nick Cammarata", "Shan Carter", "Gabriel Goh", "Chris Olah", "Michael Petrov", "Ludwig Schubert", "Chelsea Voss", "Ben Egan", "Swee Kiat Lim"], "date_published": "2020-03-10", "data_last_modified": "", "url": "", "abstract": "In the original narrative of deep learning, each neuron builds progressively more abstract, meaningful features by composing features in the preceding layer. In recent years, there’s been some skepticism of this view, but what happens if you take it really seriously?", "author_comment": "", "journal_ref": "distill pub", "doi": "https://doi.org/10.23915/distill.00024", "primary_category": "", "categories": "", "citation_level": "", "main_tex_filename": "", "text": "In the original narrative of deep learning, each neuron builds\n progressively more abstract, meaningful features by composing features in\n the preceding layer. In recent years, there’s been some skepticism of this\n view, but what happens if you take it really seriously?\n \n\n\n\n InceptionV1 is a classic vision model with around 10,000 unique neurons\u2009—\u2009a large number, but still on a scale that a group effort could attack.\n What if you simply go through the model, neuron by neuron, trying to\n understand each one and the connections between them? The circuits\n collaboration aims to find out.\n \n\n\n\n## Articles & Comments\n\n\n\n The natural unit of publication for investigating circuits seems to be\n short papers on individual circuits or small families of features.\n Compared to normal machine learning papers, this is a small and unusual\n topic for a paper.\n \n\n\n\n To facilitate exploration of this direction, Distill is inviting a\n “thread” of short articles on circuits, interspersed with critical\n commentary by experts in adjacent fields. The thread will be a living\n document, with new articles added over time, organized through an open\n slack channel (#circuits in the\n [Distill slack](http://slack.distill.pub/)). Content in this\n thread should be seen as early stage exploratory research.\n \n\n\nArticles and comments are presented below in chronological order:\n\n\n\n\n\n\n### \n[Zoom In: An Introduction to Circuits](https://distill.pub/2020/circuits/zoom-in/)\n\n\n\n### Authors\n\n\n### Affiliations\n\n\n\n[Chris Olah](https://colah.github.io/),\n [Nick Cammarata](http://nickcammarata.com/),\n [Ludwig Schubert](https://schubert.io/),\n [Gabriel Goh](http://gabgoh.github.io/),\n [Michael Petrov](https://twitter.com/mpetrov),\n [Shan Carter](http://shancarter.com/)\n\n\n\n\n[OpenAI](https://openai.com/)\n\n\n\n\n\n\n\n Does it make sense to treat individual neurons and the connections\n between them as a serious object of study? This essay proposes three\n claims which, if true, might justify serious inquiry into them: the\n existence of meaningful features, the existence of meaningful circuits\n between features, and the universality of those features and circuits.\n   \n  \n\n It also discuses historical successes of science “zooming in,” whether\n we should be concerned about this research being qualitative, and\n approaches to rigorous investigation.\n   \n  \n\n[Read Full Article](https://distill.pub/2020/circuits/zoom-in/)\n\n\n\n\n\n\n\n### \n[An Overview of Early Vision in InceptionV1](https://distill.pub/2020/circuits/early-vision/)\n\n\n\n### Authors\n\n\n### Affiliations\n\n\n\n[Chris Olah](https://colah.github.io/),\n [Nick Cammarata](http://nickcammarata.com/),\n [Ludwig Schubert](https://schubert.io/),\n [Gabriel Goh](http://gabgoh.github.io/),\n [Michael Petrov](https://twitter.com/mpetrov),\n [Shan Carter](http://shancarter.com/)\n\n\n\n\n[OpenAI](https://openai.com/)\n\n\n\n\n\n\n An overview of all the neurons in the first five layers of\n InceptionV1, organized into a taxonomy of “neuron groups.” This\n article sets the stage for future deep dives into particular aspects\n of early vision.\n   \n  \n\n [Read Full Article](https://distill.pub/2020/circuits/early-vision/) \n\n\n\n\n\n\n\n### \n[Curve Detectors](https://distill.pub/2020/circuits/curve-detectors/)\n\n\n\n### Authors\n\n\n### Affiliations\n\n\n\n[Nick Cammarata](http://nickcammarata.com/),\n [Gabriel Goh](http://gabgoh.github.io/),\n [Shan Carter](http://shancarter.com/),\n [Ludwig Schubert](https://schubert.io/),\n [Michael Petrov](https://twitter.com/mpetrov),\n [Chris Olah](https://colah.github.io/)\n\n\n\n\n[OpenAI](https://openai.com/)\n\n\n\n\n\n\n\n\n Every vision model we’ve explored in detail contains neurons which\n detect curves. Curve detectors is the first in a series of three\n articles exploring this neuron family in detail.\n   \n  \n\n[Read Full Article](https://distill.pub/2020/circuits/curve-detectors/)\n\n\n\n\n\n\n\n### \n[Naturally Occurring Equivariance in Neural Networks](https://distill.pub/2020/circuits/equivariance/)\n\n\n\n### Authors\n\n\n### Affiliations\n\n\n\n[Chris Olah](https://colah.github.io/),\n [Nick Cammarata](http://nickcammarata.com/), [Chelsea Voss](https://csvoss.com/),\n [Ludwig Schubert](https://schubert.io/),\n [Gabriel Goh](http://gabgoh.github.io/)\n\n\n\n\n[OpenAI](https://openai.com/)\n\n\n\n\n\n\n\n\n Neural networks naturally learn many transformed copies of the same\n feature, connected by symmetric weights.\n   \n  \n\n[Read Full Article](https://distill.pub/2020/circuits/equivariance/)\n\n\n\n\n\n\n\n### \n[High-Low Frequency Detectors](https://distill.pub/2020/circuits/frequency-edges/)\n\n\n\n### Authors\n\n\n### Affiliations\n\n\n\n[Ludwig Schubert](https://schubert.io/),\n [Chelsea Voss](https://csvoss.com/),\n [Nick Cammarata](http://nickcammarata.com/),\n [Gabriel Goh](http://gabgoh.github.io/),\n [Chris Olah](https://colah.github.io/)\n\n\n\n\n[OpenAI](https://openai.com/)\n\n\n\n\n\n\n A family of early-vision neurons reacting to directional transitions\n from high to low spatial frequency.\n   \n  \n\n[Read Full Article](https://distill.pub/2020/circuits/frequency-edges/)\n\n\n\n\n\n\n\n### \n[Curve Circuits](https://distill.pub/2020/circuits/curve-circuits/)\n\n\n\n### Authors\n\n\n### Affiliations\n\n\n\n[Nick Cammarata](http://nickcammarata.com/),\n [Gabriel Goh](http://gabgoh.github.io/),\n [Shan Carter](http://shancarter.com/),\n [Chelsea Voss](https://csvoss.com/),\n [Ludwig Schubert](https://schubert.io/),\n [Chris Olah](https://colah.github.io/)\n\n\n\n\n[OpenAI](https://openai.com/)\n\n\n\n\n\n\n We reverse engineer a non-trivial learned algorithm from the weights\n of a neural network and use its core ideas to craft an artificial\n artificial neural network from scratch that reimplements it.\n   \n  \n\n[Read Full Article](https://distill.pub/2020/circuits/curve-circuits/)\n\n\n\n\n\n\n\n### \n[Visualizing Weights](https://distill.pub/2020/circuits/visualizing-weights/)\n\n\n\n### Authors\n\n\n### Affiliations\n\n\n\n[Chelsea Voss](https://csvoss.com/),\n [Nick Cammarata](http://nickcammarata.com/),\n [Gabriel Goh](https://gabgoh.github.io/),\n [Michael Petrov](https://twitter.com/mpetrov),\n [Ludwig Schubert](https://schubert.io/),\n Ben Egan,\n [Swee Kiat Lim](https://greentfrapp.github.io/),\n [Chris Olah](https://colah.github.io/)\n\n\n\n\n[OpenAI](https://openai.com/),\n [Mount Royal University](https://mtroyal.ca/),\n [Stanford University](https://stanford.edu/)\n\n\n\n\n\n\n We present techniques for visualizing, contextualizing, and\n understanding neural network weights.\n   \n  \n\n[Read Full Article](https://distill.pub/2020/circuits/visualizing-weights/)\n\n\n\n\n\n\n\n### \n[Branch Specialization](https://distill.pub/2020/circuits/branch-specialization/)\n\n\n\n### Authors\n\n\n### Affiliations\n\n\n\n[Chelsea Voss](https://csvoss.com/),\n [Gabriel Goh](https://gabgoh.github.io/),\n [Nick Cammarata](http://nickcammarata.com/),\n [Michael Petrov](https://twitter.com/mpetrov),\n [Ludwig Schubert](https://schubert.io/),\n [Chris Olah](https://colah.github.io/)\n\n\n\n\n[OpenAI](https://openai.com/)\n\n\n\n\n\n\n When a neural network layer is divided into multiple branches, neurons\n self-organize into coherent groupings.\n   \n  \n\n[Read Full Article](https://distill.pub/2020/circuits/branch-specialization/)\n\n\n\n\n\n\n\n### \n[Weight Banding](https://distill.pub/2020/circuits/weight-banding/)\n\n\n\n### Authors\n\n\n### Affiliations\n\n\n\n[Michael Petrov](https://twitter.com/mpetrov),\n [Chelsea Voss](https://csvoss.com/),\n [Ludwig Schubert](https://schubert.io/),\n [Nick Cammarata](http://nickcammarata.com/),\n [Gabriel Goh](https://gabgoh.github.io/),\n [Chris Olah](https://colah.github.io/)\n\n\n\n\n[OpenAI](https://openai.com/)\n\n\n\n\n\n\n Weights in the final layer of common visual models appear as horizontal bands. We investigate how and why.\n   \n  \n\n[Read Full Article](https://distill.pub/2020/circuits/weight-banding/)\n\n\n\n\n\n\n#### This is a living document\n\n\n\n Expect more articles on this topic, along with critical comments from\n experts.\n \n\n\n\n## Get Involved\n\n\n\n The Circuits thread is open to articles exploring individual features,\n circuits, and their organization within neural networks. Critical\n commentary and discussion of existing articles is also welcome. The thread\n is organized through the open `#circuits` channel on the\n [Distill slack](http://slack.distill.pub/). Articles can be\n suggested there, and will be included at the discretion of previous\n authors in the thread, or in the case of disagreement by an uninvolved\n editor.\n \n\n\n\n If you would like get involved but don’t know where to start, small\n projects may be available if you ask in the channel.\n \n\n\n## About the Thread Format\n\n\n\n Part of Distill’s mandate is to experiment with new forms of scientific\n publishing. We believe that that reconciling faster and more continuous\n approaches to publication with review and discussion is an important open\n problem in scientific publishing.\n \n\n\n\n Threads are collections of short articles, experiments, and critical\n commentary around a narrow or unusual research topic, along with a slack\n channel for real time discussion and collaboration. They are intended to\n be earlier stage than a full Distill paper, and allow for more fluid\n publishing, feedback and discussion. We also hope they’ll allow for wider\n participation. Think of a cross between a Twitter thread, an academic\n workshop, and a book of collected essays.\n \n\n\n\n Threads are very much an experiment. We think it’s possible they’re a\n great format, and also possible they’re terrible. We plan to trial two\n such threads and then re-evaluate our thought on the format.", "bibliography_bbl": "", "bibliography_bib": None}
{"source": "distill", "source_filetype": "html", "converted_with": "python", "paper_version": "", "post_title": "A Discussion of "Adversarial Examples Are Not Bugs, They Are Features": Adversarially Robust Neural Style Transfer", "authors": ["Reiichiro Nakano"], "date_published": "2019-08-06", "data_last_modified": "", "url": "", "abstract": "This article is part of a discussion of the Ilyas et al. paper “Adversarial examples are not bugs, they are features”. You can learn more in the main discussion article.", "author_comment": "", "journal_ref": "distill pub", "doi": "https://doi.org/10.23915/distill.00019.4", "primary_category": "", "categories": "", "citation_level": "", "main_tex_filename": "", "text": "This article is part of a discussion of the Ilyas et al. paper\n *“Adversarial examples are not bugs, they are features”.*\n You can learn more in the\n [main discussion article](https://distill.pub/2019/advex-bugs-discussion/) .\n \n\n\n[Other Comments](https://distill.pub/2019/advex-bugs-discussion/#commentaries)\n[Comment by Ilyas et al.](https://distill.pub/2019/advex-bugs-discussion/response-4/#rebuttal)\n\n\n A figure in Ilyas, et. al. that struck me as particularly\n interesting\n was the following graph showing a correlation between adversarial transferability between architectures and\n their\n tendency to learn similar non-robust features.\n \n\n\n\n![](./A Discussion of \"Adversarial Examples Are Not Bugs, They Are Features\"_ Adversarially Robust Neural Style Transfer_files/transferability.png)\n\n Adversarial transferability vs test accuracy of different architectures trained on ResNet-50′s\n non-robust features.\n \n\n\n One way to interpret this graph is that it shows how well a particular architecture is able to capture\n non-robust features in an image.\n Since the non-robust features are defined by the non-robust features ResNet-50 captures,\n NRFresnetNRF\\_{resnet}NRFresnet\u200b, what this graph really shows is how well an architecture captures NRFresnetNRF\\_{resnet}NRFresnet\u200b.\n \n\n\n\n\n Notice how far back VGG is compared to the other models.\n \n\n\n\n In the unrelated field of neural style transfer, VGG-based neural networks are also quite special since non-VGG architectures are\n known to not work very well This phenomenon is discussed at length in [this\n Reddit thread](https://www.reddit.com/r/MachineLearning/comments/7rrrk3/d_eat_your_vggtables_or_why_does_neural_style/). without some sort of parameterization trick .\n The above interpretation of the graph provides an alternative explanation for this phenomenon.\n **Since VGG is unable to capture non-robust features as well as other architectures, the outputs for style\n transfer actually look more correct to humans!**\nTo follow this argument, note that the perceptual losses used in neural style transfer are\n dependent on matching features learned by a separately trained image classifier. If these learned\n features don’t make sense to humans (non-robust features), the outputs for neural style transfer won’t\n make sense either.\n\n\n\n\n Before proceeding, let’s quickly discuss the results obtained by Mordvintsev, et. al. in [Differentiable Image\n Parameterizations](https://distill.pub/2018/differentiable-parameterizations/), where they show that non-VGG architectures can work for style transfer by using a\n simple technique previously established in feature visualization.\n In their experiment, instead of optimizing the output image in RGB space, they optimize it in Fourier space,\n and run the image through a series of transformations (e.g jitter, rotation, scaling) before passing it\n through the neural network.\n \n\n\n\n Can we reconcile this result with our hypothesis linking neural style transfer and non-robust features?\n \n\n\n\n One possible theory is that all of these image transformations *weaken* or even *destroy*\n non-robust features.\n Since the optimization can no longer reliably manipulate non-robust features to bring down the loss, it is\n forced to use robust features instead, which are presumably more resistant to the applied image\n transformations (a rotated and jittered flappy ear still looks like a flappy ear).\n \n\n\n## A quick experiment\n\n\n\n Testing our hypothesis is fairly straightforward:\n Use an adversarially robust classifier for neural style transfer and see\n what happens.\n \n\n\n\n I evaluated a regularly trained (non-robust) ResNet-50 with a robustly trained ResNet-50 from Engstrom, et.\n al. on their performance on neural style transfer.\n For comparison, I performed the same algorithm with a regular VGG-19\n \xa0.\n \n\n\n\n To ensure a fair comparison despite the different networks having different optimal hyperparameters, I\n performed a small grid search for each image and manually picked the best output per network.\n Further details can be read in a footnote\n \n L-BFGS was used for optimization as it showed faster convergence\n over Adam.\n For ResNet-50, the style layers used were the ReLu outputs after each of the 4 residual blocks,\n [relu2\\_x,relu3\\_x,relu4\\_x,relu5\\_x][relu2\\\\_x, relu3\\\\_x, relu4\\\\_x, relu5\\\\_x][relu2\\_x,relu3\\_x,relu4\\_x,relu5\\_x] while the content layer used was relu4\\_xrelu4\\\\_xrelu4\\_x.\n For VGG-19, style layers [relu1\\_1,relu2\\_1,relu3\\_1,relu4\\_1,relu5\\_1][relu1\\\\_1,relu2\\\\_1,relu3\\\\_1,relu4\\\\_1,relu5\\\\_1][relu1\\_1,relu2\\_1,relu3\\_1,relu4\\_1,relu5\\_1] were used with a content layer\n relu4\\_2relu4\\\\_2relu4\\_2.\n In VGG-19, max pooling layers were replaced with avg pooling layers, as stated in Gatys, et. al.\n \n or observed in the accompanying Colaboratory notebook.\n \n\n\n\n The results of this experiment can be explored in the diagram below.\n \n\n\n\n #style-transfer-slider.juxtapose {\n max-height: 512px;\n max-width: 512px;\n }\n \n\n\n**Content image**\n\n\n\n\n\n* ![](./A Discussion of \"Adversarial Examples Are Not Bugs, They Are Features\"_ Adversarially Robust Neural Style Transfer_files/ben.jpg)\n* ![](./A Discussion of \"Adversarial Examples Are Not Bugs, They Are Features\"_ Adversarially Robust Neural Style Transfer_files/dancing.jpg)\n* ![](./A Discussion of \"Adversarial Examples Are Not Bugs, They Are Features\"_ Adversarially Robust Neural Style Transfer_files/tubingen.jpg)\n* ![](./A Discussion of \"Adversarial Examples Are Not Bugs, They Are Features\"_ Adversarially Robust Neural Style Transfer_files/stata.jpg)\n\n\n**Style image**\n\n\n\n\n\n* ![](./A Discussion of \"Adversarial Examples Are Not Bugs, They Are Features\"_ Adversarially Robust Neural Style Transfer_files/scream.jpg)\n* ![](./A Discussion of \"Adversarial Examples Are Not Bugs, They Are Features\"_ Adversarially Robust Neural Style Transfer_files/starrynight.jpg)\n* ![](./A Discussion of \"Adversarial Examples Are Not Bugs, They Are Features\"_ Adversarially Robust Neural Style Transfer_files/woman.jpg)\n* ![](./A Discussion of \"Adversarial Examples Are Not Bugs, They Are Features\"_ Adversarially Robust Neural Style Transfer_files/picasso.jpg)\n\n\n\xa0 Compare VGG or Robust\n ResNet\n![](./A Discussion of \"Adversarial Examples Are Not Bugs, They Are Features\"_ Adversarially Robust Neural Style Transfer_files/ben_scream_nonrobust.jpg)Non-robust ResNet50![](./A Discussion of \"Adversarial Examples Are Not Bugs, They Are Features\"_ Adversarially Robust Neural Style Transfer_files/ben_scream_robust.jpg)Robust ResNet50\n\n[Reproduce in a Notebook](https://colab.research.google.com/github/reiinakano/adversarially-robust-neural-style-transfer/blob/master/Robust_Neural_Style_Transfer.ipynb)\n\n\"use strict\";\n\n// I don\"t know how to write JavaScript without a bundler. Please someone save me.\n\n(function () {\n\n // Initialize slider\n var currentContent = \"ben\";\n var currentStyle = \"scream\";\n var currentLeft = \"nonrobust\";\n\n var compareVGGCheck = document.getElementById("check-compare-vgg");\n var styleTransferSliderDiv = document.getElementById("style-transfer-slider");\n\n function refreshSlider() {\n while (styleTransferSliderDiv.firstChild) {\n styleTransferSliderDiv.removeChild(styleTransferSliderDiv.firstChild);\n }\n var imgPath1 = \"images/style-transfer/\" + currentContent + \"\\_\" + currentStyle + \"\\_\" + currentLeft + \".jpg\";\n var imgPath2 = \"images/style-transfer/\" + currentContent + \"\\_\" + currentStyle + \"\\_robust.jpg\";\n new juxtapose.JXSlider(\"#style-transfer-slider\", [{\n src: imgPath1, // TODO: Might need to use absolute\\_url?\n label: currentLeft === \"nonrobust\" ? \"Non-robust ResNet50\" : \"VGG\"\n }, {\n src: imgPath2,\n label: \"Robust ResNet50\"\n }], {\n animate: true,\n showLabels: true,\n showCredits: false,\n startingPosition: "50%",\n makeResponsive: true\n });\n }\n\n refreshSlider();\n\n compareVGGCheck.onclick = function (evt) {\n currentLeft = evt.target.checked ? \"vgg\" : \"nonrobust\";\n refreshSlider();\n };\n\n // Initialize selector\n $("#content-select").imagepicker({\n changed: function changed(oldVal, newVal, event) {\n currentContent = newVal;\n refreshSlider();\n }\n });\n $("#style-select").imagepicker({\n changed: function changed(oldVal, newVal, event) {\n currentStyle = newVal;\n refreshSlider();\n }\n });\n})();\n\n Success!\n The robust ResNet shows drastic improvement over the regular ResNet.\n Remember, all we did was switch the ResNet’s weights, the rest of the code for performing style transfer is\n exactly the same!\n \n\n\n\n A more interesting comparison can be done between VGG-19 and the robust ResNet.\n At first glance, the robust ResNet’s outputs seem on par with VGG-19.\n Looking closer, however, the ResNet’s outputs seem slightly noisier and exhibit some artifacts\n This is more obvious when the output image is initialized not with the content image, but with\n Gaussian noise..\n \n\n\n\n\n\n\n![](./A Discussion of \"Adversarial Examples Are Not Bugs, They Are Features\"_ Adversarially Robust Neural Style Transfer_files/vgg_texture.jpg)\n\n\n\n![](./A Discussion of \"Adversarial Examples Are Not Bugs, They Are Features\"_ Adversarially Robust Neural Style Transfer_files/vgg_texture.jpg)\n\n\n Texture synthesized with VGG.  \n\n*Mild artifacts.*\n\n\n\n\n![](./A Discussion of \"Adversarial Examples Are Not Bugs, They Are Features\"_ Adversarially Robust Neural Style Transfer_files/resnet_texture.jpg)\n\n\n\n![](./A Discussion of \"Adversarial Examples Are Not Bugs, They Are Features\"_ Adversarially Robust Neural Style Transfer_files/resnet_texture.jpg)\n\n\n Texture synthesized with robust ResNet.  \n\n*Severe artifacts.*\n\n\n\n\n\n A comparison of artifacts between textures synthesized by VGG and ResNet.\n Interact by hovering around the images.\n This diagram was repurposed from\n [Deconvolution and Checkerboard Artifacts](https://distill.pub/2016/deconv-checkerboard/) \n by Odena, et. al.\n \n\n It is currently unclear exactly what causes these artifacts.\n One theory is that they are checkerboard artifacts\n caused by\n non-divisible kernel size and stride in the convolution layers.\n They could also be artifacts caused by the presence of max pooling layers\n in ResNet.\n An interesting implication is that these artifacts, while problematic, seem orthogonal to the\n problem that\n adversarial robustness solves in neural style transfer.\n \n\n\n## VGG remains a mystery\n\n\n\n Although this experiment started because of an observation about a special characteristic of VGG\n nets, it\n did not provide an explanation for this phenomenon.\n Indeed, if we are to accept the theory that adversarial robustness is the reason VGG works out of\n the box\n with neural style transfer, surely we’d find some indication in existing literature that VGG is\n naturally\n more robust than other architectures.\n \n\n\n\n A few papers\n indeed show\n that VGG architectures are slightly more robust than ResNet.\n However, they also show that AlexNet, not known to work well\n for\n neural style transferAs shown by Dávid Komorowicz\n in\n this [blog post](https://dawars.me/neural-style-transfer-deep-learning/).\n , is\n *above* VGG in terms of this “natural robustness”.\n \n\n\n\n Perhaps adversarial robustness just happens to incidentally fix or cover up the true reason non-VGG\n architectures fail at style transfer (or other similar algorithms\n \n In fact, neural style transfer is not the only pretrained classifier-based iterative image\n optimization\n technique that magically works better with adversarial robustness. In Engstrom, et. al., they show that feature visualization via activation\n maximization works on robust classifiers *without*\n enforcing\n any priors or regularization (e.g. image transformations and decorrelated parameterization) used\n by\n previous work. In a recent chat with Chris\n Olah, he\n pointed out that the aforementioned feature visualization techniques actually work well on VGG\n *without* these priors, just like style transfer!\n \n ) i.e. adversarial robustness is a sufficient but unnecessary condition for good style transfer.\n Whatever the reason, I believe that further examination of VGG is a very interesting direction for\n future\n work.\n \n\n\n\n To cite Ilyas et al.’s response, please cite their\n [collection of responses](https://distill.pub/2019/advex-bugs-discussion/original-authors/#citation).\n\n\n**Response Summary**: Very interesting\n results, highlighting the effect of non-robust features and the utility of\n robust models for downstream tasks. We’re excited to see what kind of impact\n robustly trained models will have in neural network art! We were also really\n intrigued by the mysteriousness of VGG in the context of style transfer\n. As such, we took a\n deeper dive which found some interesting links between robustness and style\n transfer that suggest that perhaps robustness does indeed play a role here. \n\n\n**Response**: These experiments are really cool! It is interesting that\n preventing the reliance of a model on non-robust features improves performance\n on style transfer, even without an explicit task-related objective (i.e. we\n didn’t train the networks to be better for style transfer). \n\n\n We also found the discussion of VGG as a “mysterious network” really\n interesting\u2009—\u2009it would be valuable to understand what factors drive style transfer\n performance more generally. Though not a complete answer, we made a couple of\n observations while investigating further: \n\n\n*Style transfer does work with AlexNet:* One wrinkle in the idea that\n robustness is the “secret ingredient” to style transfer could be that VGG is not\n the most naturally robust network\u2009—\u2009AlexNet is. However, based on our own\n testing, style transfer does seem to work with AlexNet out-of-the-box, as\n long as we use a few early layers in the network (in a similar manner to\n VGG): \n\n\n\n![](./A Discussion of \"Adversarial Examples Are Not Bugs, They Are Features\"_ Adversarially Robust Neural Style Transfer_files/alexnetworks.png)\n\n Style transfer using AlexNet, using conv\\_1 through conv\\_4.\n \n\n\n Observe that even though style transfer still works, there are checkerboard\n patterns emerging\u2009—\u2009this seems to be a similar phenomenon to the one noticed\n in the comment in the context of robust models.\n This might be another indication that these two phenomena (checkerboard\n patterns and style transfer working) are not as intertwined as previously\n thought.\n \n\n\n*From prediction robustness to layer robustness:*  Another\n potential wrinkle here is that both AlexNet and VGG are not that\n much more robust than ResNets (for which style transfer completely fails),\n and yet seem to have dramatically better performance. To try to\n explain this, recall that style transfer is implemented as a minimization of a\n combined objective consisting of a style loss and a content loss. We found,\n however, that the network we use to compute the\n style loss is far more important\n than the one for the content loss. The following demo illustrates this\u2009—\u2009we can\n actually use a non-robust ResNet for the content loss and everything works just\n fine:\n\n\n\n![](./A Discussion of \"Adversarial Examples Are Not Bugs, They Are Features\"_ Adversarially Robust Neural Style Transfer_files/stylematters.png)\n\nStyle transfer seems to be rather\n invariant to the choice of content network used, and very sensitive\n to the style network used.\n\n\nTherefore, from now on, we use a fixed ResNet-50 for the content loss as a\n control, and only worry about the style loss. \n\n\nNow, note that the way that style loss works is by using the first few\n layers of the relevant network. Thus, perhaps it is not about the robustness of\n VGG’s predictions, but instead about the robustness of the layers that we actually use\n for style transfer? \n\n\n To test this hypothesis, we measure the robustness of a layer fff as:\n \n\n\nR(f)=Ex1∼D[maxx′∥f(x′)−f(x1)∥2]Ex1,x2∼D[∥f(x1)−f(x2)∥2]\n R(f) = \\frac{\\mathbb{E}\\_{x\\_1\\sim D}\\left[\\max\\_{x’} \\|f(x’) - f(x\\_1)\\|\\_2 \\right]}\n {\\mathbb{E}\\_{x\\_1, x\\_2 \\sim D}\\left[\\|f(x\\_1) - f(x\\_2)\\|\\_2\\right]}\n R(f)=Ex1\u200b,x2\u200b∼D\u200b[∥f(x1\u200b)−f(x2\u200b)∥2\u200b]Ex1\u200b∼D\u200b[maxx′\u200b∥f(x′)−f(x1\u200b)∥2\u200b]\u200b\n Essentially, this quantity tells us how much we can change the\n output of that layer f(x)f(x)f(x) within a small ball, normalized by how far apart\n representations are between images in general. We’ve plotted this value for\n the first few layers in a couple of different networks below: \n\n\n\n![](./A Discussion of \"Adversarial Examples Are Not Bugs, They Are Features\"_ Adversarially Robust Neural Style Transfer_files/robustnesses.png)\n\nThe robustness R(f)R(f)R(f) of the first\n four layers of VGG16, AlexNet, and robust/standard ResNet-50\n trained on ImageNet.\n\n\n Here, it becomes clear that, the first few layers of VGG and AlexNet are\n actually almost as robust as the first few layers of the robust ResNet!\n This is perhaps a more convincing indication that robustness might have\n something to with VGG’s success in style transfer after all.\n \n\n\n Finally, suppose we restrict style transfer to only use a single layer of\n the network when computing the style lossUsually style transfer uses\n several layers in the loss function to get the most visually appealing results\u2009—\u2009here we’re only interested in whether or not style transfer works (i.e.\n actually confers some style onto the image).. Again, the more\n robust layers seem to indeed work better for style transfer! Since all of the\n layers in the robust ResNet are robust, style transfer yields non-trivial\n results even using the last layer alone. Conversely, VGG and AlexNet seem to\n excel in the earlier layers (where they are non-trivially robust) but fail when\n using exclusively later (non-robust) layers: \n\n\n\n![](./A Discussion of \"Adversarial Examples Are Not Bugs, They Are Features\"_ Adversarially Robust Neural Style Transfer_files/styletransfer.png)\n\n\nStyle transfer using a single layer. The\n names of the layers and their robustness R(f)R(f)R(f) are printed below\n each style transfer result. We find that for both networks, the robust\n layers seem to work (for the robust ResNet, every layer is robust).\n\n\n Of course, there is much more work to be done here, but we are excited\n to see further work into understanding the role of both robustness and the VGG\n in network-based image manipulation. \n\n\n\n\n You can find more responses in the  [main discussion article](https://distill.pub/2019/advex-bugs-discussion/).", "bibliography_bbl": "", "bibliography_bib": [{"title": "Adversarial examples are not bugs, they are features"}, {"title": "Very deep convolutional networks for large-scale image recognition"}, {"title": "A Neural Algorithm of Artistic Style"}, {"title": "Differentiable Image Parameterizations"}, {"title": "Feature Visualization"}, {"title": "Learning Perceptually-Aligned Representations via Adversarial Robustness"}, {"title": "On the limited memory BFGS method for large scale optimization"}, {"title": "Deconvolution and checkerboard artifacts"}, {"title": "Geodesics of learned representations"}, {"title": "Batch Normalization is a Cause of Adversarial Vulnerability"}, {"title": "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations"}, {"title": "Is Robustness the Cost of Accuracy? - A Comprehensive Study on the Robustness of 18 Deep Image Classification Models"}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks"}, {"title": "Neural Style transfer with Deep Learning"}, {"title": "The Building Blocks of Interpretability"}]}
{"source": "distill", "source_filetype": "html", "converted_with": "python", "paper_version": "", "post_title": "Visualizing Neural Networks with the Grand Tour", "authors": ["Mingwei Li", "Zhenge Zhao", "Carlos Scheidegger"], "date_published": "2020-03-16", "data_last_modified": "", "url": "", "abstract": "The Grand Tour is a classic visualization technique for high-dimensional point clouds that projects a high-dimensional dataset into two dimensions.\n\n  Over time, the Grand Tour smoothly animates its projection so that every possible view of the dataset is (eventually) presented to the viewer.\n\n  Unlike modern nonlinear projection methods such as t-SNE and UMAP, the Grand Tour is fundamentally a linear method.\n\n  In this article, we show how to leverage the linearity of the Grand Tour to enable a number of capabilities that are uniquely useful to visualize the behavior of neural networks.\n  \n  Concretely, we present three use cases of interest: visualizing the training process as the network weights change, visualizing the layer-to-layer behavior as the data goes through the network and visualizing both how adversarial examples are crafted and how they fool a neural network.", "author_comment": "", "journal_ref": "distill pub", "doi": "https://doi.org/10.23915/distill.00025", "primary_category": "", "categories": "", "citation_level": "", "main_tex_filename": "", "text": "The Grand Tour is a classic visualization technique for high-dimensional point clouds that *projects* a high-dimensional dataset into two dimensions.\n\n Over time, the Grand Tour smoothly animates its projection so that every possible view of the dataset is (eventually) presented to the viewer.\n\n Unlike modern nonlinear projection methods such as t-SNE and UMAP, the Grand Tour is fundamentally a *linear* method.\n\n In this article, we show how to leverage the linearity of the Grand Tour to enable a number of capabilities that are uniquely useful to visualize the behavior of neural networks.\n \n Concretely, we present three use cases of interest: visualizing the training process as the network weights change, visualizing the layer-to-layer behavior as the data goes through the network and visualizing both how adversarial examples are crafted and how they fool a neural network.\n\n\n\n## Introduction\n\n\n\n Deep neural networks often achieve best-in-class performance in supervised learning contests such as the ImageNet Large Scale Visual Recognition Challenge (ILSVRC).\n \n Unfortunately, their decision process is notoriously hard to interpret, and their training process is often hard to debug.\n \n In this article, we present a method to visualize the responses of a neural network which leverages properties of deep neural networks and properties of the *Grand Tour*.\n\n Notably, our method enables us to more directly reason about the relationship between *changes in the data* and *changes in the resulting visualization*.\n\n As we will show, this data-visual correspondence is central to the method we present, especially when compared to other non-linear projection methods like UMAP and t-SNE.\n\n\n\n\n\n To understand a neural network, we often try to observe its action on input examples (both real and synthesized).\n \n These kinds of visualizations are useful to elucidate the activation patterns of a neural network for a single example, but they might offer less insight about the relationship between different examples, different states of the network as it’s being trained, or how the data in the example flows through the different layers of a single network.\n \n Therefore, we instead aim to enable visualizations of the *context around* our objects of interest: what is the difference between the present training epoch and the next one? How does the classification of a network converge (or diverge) as the image is fed through the network?\n\n Linear methods are attractive because they are particularly easy to reason about.\n\n The Grand Tour works by generating a random, smoothly changing rotation of the dataset, and then projecting the data to the two-dimensional screen: both are linear processes.\n\n Although deep neural networks are clearly not linear processes, they often confine their nonlinearity to a small set of operations, enabling us to still reason about their behavior.\n\n Our proposed method better preserves context by providing more\n consistency: it should be possible to know *how the visualization\n would change, if the data had been different in a particular\n way*.\n\n\n\n## Working Examples\n\n\n\n To illustrate the technique we will present, we trained deep neural\n network models (DNNs) with 3 common image classification datasets:\n MNIST\n \n MNIST contains grayscale images of 10 handwritten digits\n ![](./Visualizing Neural Networks with the Grand Tour_files/mnist.png)\n Image credit to <https://en.wikipedia.org/wiki/File:MnistExamples.png>\n,\n fashion-MNIST\n \n Fashion-MNIST contains grayscale images of 10 types of fashion items:\n ![](./Visualizing Neural Networks with the Grand Tour_files/fashion-mnist.png)\n\n Image credit to <https://towardsdatascience.com/multi-label-classification-and-class-activation-map-on-fashion-mnist-1454f09f5925>\n\n\n and CIFAR-10\n \n CIFAR-10 contains RGB images of 10 classes of objects\n ![](./Visualizing Neural Networks with the Grand Tour_files/cifar-10.png)\n Image credit to <https://www.cs.toronto.edu/~kriz/cifar.html>\n. \n While our architecture is simpler and smaller than current DNNs, it’s still indicative of modern networks, and is complex enough to demonstrate both our proposed techniques and shortcomings of typical approaches.\n\n\n\n\n The following figure presents a simple functional diagram of the neural network we will use throughout the article. The neural network is a sequence of linear (both convolutional\n A convolution calculates weighted sums of regions in the input. \n In neural networks, the learnable weights in convolutional layers are referred to as the kernel.\n For example\n ![](./Visualizing Neural Networks with the Grand Tour_files/conv.gif)\n Image credit to <https://towardsdatascience.com/gentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9>.  \n\n See also [Convolution arithmetic](https://github.com/vdumoulin/conv_arithmetic).\n  and fully-connected\n A fully-connected layer computes output neurons as weighted sum of input neurons. In matrix form, it is a matrix that linearly transforms the input vector into the output vector.\n ), max-pooling, and ReLU\n First introduced by Nair and Hinton, ReLU calculates f(x)=max(0,x)f(x)=max(0,x)f(x)=max(0,x) for each entry in a vector input. Graphically, it is a hinge at the origin: ![](./Visualizing Neural Networks with the Grand Tour_files/relu.png)\n Image credit to <https://pytorch.org/docs/stable/nn.html#relu>\n layers, culminating in a softmax\n Softmax function calculates S(yi)=eyiΣj=1NeyjS(y\\_i)=\\frac{e^{y\\_i}}{\\Sigma\\_{j=1}^{N} e^{y\\_j}}S(yi\u200b)=Σj=1N\u200beyj\u200beyi\u200b\u200b for each entry (yiy\\_iyi\u200b) in a vector input (yyy). For example, ![](./Visualizing Neural Networks with the Grand Tour_files/softmax.png)\n Image credit to <https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/>\n layer.\n\n\n\n \n0123456789Digit0123456789Digitconv\\_5x5(1->10)maxpool\\_2x2ReLUconv\\_5x5(10->20)maxpool\\_2x2ReLUlinear(320->50)ReLUlinear(50->10)ReLUsoftmaxargmaxlinearcomponent-wiseotherepoch: 24/99\n\n\nNeural network opened. The colored blocks are building-block functions (i.e. neural network layers), the gray-scale heatmaps are either the input image or intermediate activation vectors after some layers.\n\n Even though neural networks are capable of incredible feats of classification, deep down, they really are just pipelines of relatively simple functions.\n For images, the input is a 2D array of scalar values for gray scale images or RGB triples for colored images.\n When needed, one can always flatten the 2D array into an equivalent (w⋅h⋅cw \\cdot h \\cdot cw⋅h⋅c) -dimensional vector.\n Similarly, the intermediate values after any one of the functions in composition, or activations of neurons after a layer, can also be seen as vectors in Rn\\mathbb{R}^nRn, where nnn is the number of neurons in the layer. \n The softmax, for example, can be seen as a 10-vector whose values are positive real numbers that sum up to 1.\n This vector view of data in neural network not only allows us represent complex data in a mathematically compact form, but also hints us on how to visualize them in a better way.\n\n\n\n\n Most of the simple functions fall into two categories: they are either linear transformations of their inputs (like fully-connected layers or convolutional layers), or relatively simple non-linear functions that work component-wise (like sigmoid activations\n Sigmoid calculates S(x)=exex+1S(x)=\\frac{e^{x}}{e^{x}+1}S(x)=ex+1ex\u200b for each entry (xxx) in a vector input. Graphically, it is an S-shaped curve.\n ![](./Visualizing Neural Networks with the Grand Tour_files/sigmoid.png)\n Image credit to <https://en.wikipedia.org/wiki/Sigmoid_function>\n \n or ReLU activations).\n Some operations, notably max-pooling\n Max-pooling calculates maximum of a region in the input. For example\n \n![](./Visualizing Neural Networks with the Grand Tour_files/maxpool.gif)\n Image credit to <https://towardsdatascience.com/gentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9>\n and softmax, do not fall into either categories. We will come back to this later.\n\n\n\n\n The above figure helps us look at a single image at a time; however, it does not provide much context to understand the relationship between layers, between different examples, or between different class labels. For that, researchers often turn to more sophisticated visualizations.\n\n\n\n## Using Visualization to Understand DNNs\n\n\n\n Let’s start by considering the problem of visualizing the training process of a DNN.\n When training neural networks, we optimize parameters in the function to minimize a scalar-valued loss function, typically through some form of gradient descent.\n We want the loss to keep decreasing, so we monitor the whole history of training and testing losses over rounds of training (or “epochs”), to make sure that the loss decreases over time. \n The following figure shows a line plot of the training loss for the MNIST classifier.\n\n\n\n\n014215075990.10.512Training EpochLoss\n\n\n\n Although its general trend meets our expectation as the loss steadily decreases, we see something strange around epochs 14 and 21: the curve goes almost flat before starting to drop again.\n What happened? What caused that?\n\n\n\n\n014215075990.050.10.5123Training EpochLoss\n\n\n\n If we separate input examples by their true labels/classes and plot the *per-class* loss like above, we see that the two drops were caused by the classses 1 and 7; the model learns different classes at very different times in the training process. \n Although the network learns to recognize digits 0, 2, 3, 4, 5, 6, 8 and 9 early on, it is not until epoch 14 that it starts successfully recognizing digit 1, or until epoch 21 that it recognizes digit 7.\n If we knew ahead of time to be looking for class-specific error rates, then this chart works well. But what if we didn’t really know what to look for?\n\n\n\n\n In that case, we could consider visualizations of neuron activations (e.g. in the last softmax layer) for *all* examples at once, looking\n to find patterns like class-specific behavior, and other patterns besides.\n Should there be only two neurons in that layer, a simple two-dimensional scatter plot would work.\n However, the points in the softmax layer for our example datasets are 10 dimensional (and in larger-scale classification problems this number can be much larger).\n We need to either show two dimensions at a time (which does not scale well as the number of possible charts grows quadratically),\n or we can use *dimensionality reduction* to map the data into a two dimensional space and show them in a single plot. \n\n\n\n\n### The State-of-the-art Dimensionality Reduction is Non-linear\n\n\n\n Modern dimensionality reduction techniques such as t-SNE and UMAP are capable of impressive feats of summarization, providing two-dimensional images where similar points tend to be clustered together very effectively.\n However, these methods are not particularly good to understand the behavior of neuron activations at a fine scale.\n Consider the aforementioned intriguing feature about the different learning rate that the MNIST classifier has on digit 1 and 7: the network did not learn to recognize digit 1 until epoch 14, digit 7 until epoch 21.\n We compute t-SNE, Dynamic t-SNE, and UMAP projections of the epochs where the phenomenon we described happens.\n Consider now the task of identifying this class-specific behavior during training. As a reminder, in this case, the strange behavior happens with digits 1 and 7, around epochs 14 and 21 respectively.\n While the behavior is not particularly subtle&emdash;digit goes from misclassified to correctly classified&emdash; it is quite hard to notice it in any of the plots below. \n Only on careful inspection we can notice that (for example) in the UMAP plot, the digit 1 which clustered in the bottom in epoch 13 becomes a new tentacle-like feature in epoch 14. \n\n\n\n\n\n\n // let sm1 = createSmallMultiple("#smallmultiple1", \n // [13,14,15, 20,21,22], ["t-SNE", "Dynamic t-SNE", "UMAP"], \n // "mnist", true, highlight\\_digits);\n let sm1 = createSmallMultiple("#smallmultiple1", \n [13,14,15, 20,21,22], ["t-SNE", "Dynamic t-SNE", "UMAP"], \n "mnist", true);\n\n\nSoftmax activations of the MNIST classifier with non-linear dimensionality reduction. Use the buttons on the right to highlight digits 1 and 7 in the plot, or drag rectangles around the charts to select particular point subsets to highlight in the other charts.\n\n One reason that non-linear embeddings fail in elucidating this phenomenon is that, for the particular change in the data, the fail the principle of *data-visual correspondence* . More concretely, the principle states that specific visualization tasks should be modeled as functions that change the data; the visualization sends this change from data to visuals, and\n we can study the extent to which the visualization changes are easily perceptible.\n Ideally, we want the changes in data and visualization to *match in magnitude*: a barely noticeable change in visualization should be due to the smallest possible change in data, and a salient change in visualization should reflect a significant one in data.\n Here, a significant change happened in only a *subset* of data (e.g. all points of digit 1 from epoch 13 to 14), but *all* points in the visualization move dramatically.\n For both UMAP and t-SNE, the position of each single point depends non-trivially on the whole data distribution in such embedding algorithms.\n This property is not ideal for visualization because it fails the data-visual correspondence, making it hard to *infer* the underlying change in data from the change in the visualization.\n\n\n\n\n Non-linear embeddings that have non-convex objectives also tend to be sensitive to initial conditions.\n For example, in MNIST, although the neural network starts to stabilize on epoch 30, t-SNE and UMAP still generate quite different projections between epochs 30, 31 and 32 (in fact, all the way to 99).\n Temporal regularization techniques (such as Dynamic t-SNE) mitigate these consistency issues, but still suffer from other interpretability issues. \n\n\n\n\n\nlet sm2 = createSmallMultiple("#smallmultiple2", [1,5,10,15,20,30,31,32], ["t-SNE", "Dynamic t-SNE", "UMAP", "Linear"], "mnist", true, ()=>{})\n\n\n Now, let’s consider another task, that of identifying classes which the neural network tends to confuse.\n For this example, we will use the Fashion-MNIST dataset and classifier, and consider the confusion among sandals, sneakers and ankle boots.\n If we know ahead of time that these three classes are likely to confuse the classifier, then we can directly design an appropriate linear projection, as can be seen in the last row of the following figure (we found this particular projection using both the Grand Tour and the direct manipulation technique we later describe). The pattern in this case is quite salient, forming a triangle.\n T-SNE, in contrast, incorrectly separates the class clusters (possibly because of an inappropriately-chosen hyperparameter).\n UMAP successfully isolates the three classes, but even in this case it’s not possible to distinguish between three-way confusion for the classifier in epochs 5 and 10 (portrayed in a linear method by the presence of points near the center of the triangle), and multiple two-way confusions in later epochs (evidences by an “empty” center).\n\n\n\n\n\n\n sm3 = createSmallMultiple("#smallmultiple3", \n [2,5,10,20,50,99], ["t-SNE", "UMAP", "Linear"], \n "fashion-mnist", true, \n highlight\\_shoes\\_button, \n highlight\\_shoes,\n );\n \n\nThree-way confusion in fashion-MNIST. Notice that in contrast to non-linear methods, a carefully-constructed linear projection can offer a better visualization of the classifier behavior.\n## Linear Methods to the Rescue\n\n\n\n When given the chance, then, we should prefer methods for which changes in the data produce predictable, visually salient changes in the result, and linear dimensionality reductions often have this property.\n Here, we revisit the linear projections described above in an interface where the user can easily navigate between different training epochs.\n In addition, we introduce another useful capability which is only available to linear methods, that of direct manipulation.\n Each linear projection from nnn dimensions to 222 dimensions can be represented by nnn 2-dimensional vectors which have an intuitive interpretation: they are the vectors that the nnn canonical basis vector in the nnn-dimensional space will be projected to.\n In the context of projecting the final classification layer, this is especially simple to interpret: they are the destinations of an input that is classified with 100% confidence to any one particular class.\n If we provide the user with the ability to change these vectors by dragging around user-interface handles, then users can intuitively set up new linear projections.\n\n\n\n \n This setup provides additional nice properties that explain the salient patterns in the previous illustrations.\n For example, because projections are linear and the coefficients of vectors in the classification layer sum to one, classification outputs that are halfway confident between two classes are projected to vectors that are halfway between the class handles.\n\n\n\n\n\n\n\n\n From\n this linear projection, we can easily identify the learning of \n digit 1 on \n epoch 14 and \n digit 7 on \n epoch 21.\n\n\n This particular property is illustrated clearly in the Fashion-MNIST example below.\n The model confuses sandals, sneakers and ankle boots, as data points form a triangular shape in the softmax layer.\n\n\n\n\n\n\n\n\n This linear projection clearly shows model’s confusion among\n sandals,\n sneakers, and\n ankle boots.\n Similarly, this projection shows the true three-way confusion about\n pullovers,\n coats, and\n shirts.\n (The shirts are also get confused with \n t-shirts/tops. )\n Both projections are found by direct manipulations.\n   \n\n\n\n Examples falling between classes indicate that the model has trouble distinguishing the two, such as sandals vs. sneakers, and sneakers vs. ankle boot classes. \n Note, however, that this does not happen as much for sandals vs. ankle boots: not many examples fall between these two classes. \n Moreover, most data points are projected close to the edge of the triangle. \n This tells us that most confusions happen between two out of the three classes, they are really two-way confusions.\n\n Within the same dataset, we can also see pullovers, coats and shirts filling a triangular *plane*.\n This is different from the sandal-sneaker-ankle-boot case, as examples not only fall on the boundary of a triangle, but also in its interior: a true three-way confusion. \n\n Similarly, in the CIFAR-10 dataset we can see confusion between dogs and cats, airplanes and ships.\n The mixing pattern in CIFAR-10 is not as clear as in fashion-MNIST, because many more examples are misclassified.\n\n\n\n\n\n\n\n\n This linear projection clearly shows model’s confusion between\n cats and\n dogs.\n Similarly, this projection shows the confusion about\n airplanes and\n ships.\n Both projections are found by direct manipulations.\n\n## The Grand Tour\n\n\n\n In the previous section, we took advantage of the fact that we knew which classes to visualize.\n That meant it was easy to design linear projections for the particular tasks at hand.\n But what if we don’t know ahead of time which projection to choose from, because we don’t quite know what to look for?\n Principal Component Analysis (PCA) is the quintessential linear dimensionality reduction method,\n choosing to project the data so as to preserve the most variance possible. \n However, the distribution of data in softmax layers often has similar variance along many axis directions, because each axis concentrates a similar number of examples around the class vector.We are assuming a class-balanced training dataset. Nevertheless, if the training dataset is not balanced, PCA will prefer dimensions with more examples, which might not be help much either.\n As a result, even though PCA projections are interpretable and consistent through training epochs, the first two principal components of softmax activations are not substantially better than the third.\n So which of them should we choose?\n Instead of PCA, we propose to visualize this data by smoothly animating random projections, using a technique called the Grand Tour.\n\n\n\n\nStarting with a random velocity, it smoothly rotates data points around the origin in high dimensional space, and then projects it down to 2D for display. \nHere are some examples of how Grand Tour acts on some (low-dimensional) objects:\n\n\n\n* On a square, the Grand Tour rotates it with a constant angular velocity.\n* On a cube, the Grand Tour rotates it in 3D, and its 2D projection let us see every facet of the cube.\n* On a 4D cube (a *tesseract*), the rotation happens in 4D and the 2D view shows every possible projection.\n\n\n\n\nGrand tours of a square, a cube and a tesseract\n\n\n### The Grand Tour of the Softmax Layer\n\n\n\n We first look at the Grand Tour of the softmax layer. \n The softmax layer is relatively easy to understand because its axes have strong semantics. As we described earlier, the iii-th axis corresponds to network’s *confidence* about predicting that the given input belongs to the iii-th class. \n\n\n\n\n\n\n\n\n\n The Grand Tour of softmax layer in the last (99th) epoch, with \n MNIST, \n fashion-MNIST or \n CIFAR-10 dataset.\n\n\n The Grand Tour of the softmax layer lets us qualitatively assess the performance of our model.\n In the particular case of this article, since we used comparable architectures for three datasets, this also allows us to gauge the relative difficulty of classifying each dataset. \n We can see that data points are most confidently classified for the MNIST dataset, where the digits are close to one of the ten corners of the softmax space. For Fashion-MNIST or CIFAR-10, the separation is not as clean, and more points appear *inside* the volume.\n\n\n\n### The Grand Tour of Training Dynamics\n\n\n\n Linear projection methods naturally give a formulation that is independent of the input points, allowing us to keep the projection fixed while the\n data changes.\n To recap our working example, we trained each of the neural networks for 99 epochs and recorded the entire history of neuron activations on a subset of training and testing examples. We can use the Grand Tour, then, to visualize the actual training process of these networks.\n\n\n\n\n In the beginning when the neural networks are randomly initialized, all examples are placed around the center of the softmax space, with equal weights to each class. \n Through training, examples move to class vectors in the softmax space. The Grand Tour also lets us\n compare visualizations of the training and testing data, giving us a qualitative assessment of over-fitting. \n In the MNIST dataset, the trajectory of testing images through training is consistent with the training set. \n Data points went directly toward the corner of its true class and all classes are stabilized after about 50 epochs.\n On the other hand, in CIFAR-10 there is an *inconsistency* between the training and testing sets. Images from the testing set keep oscillating while most images from training converges to the corresponding class corner. \n In epoch 99, we can clearly see a difference in distribution between these two sets.\n This signals that the model overfits the training set and thus does not generalize well to the testing set. \n\n\n\n\n\n\n\n\n With this view of CIFAR-10\xa0, \n the color of points are more mixed in testing (right) than training (left) set, showing an over-fitting in the training process.\n Compare \n CIFAR-10 \n with \n MNIST\n or fashion-MNIST, \n where there is less difference between training and testing sets.\n\n### The Grand Tour of Layer Dynamics\n\n\n\n Given the presented techniques of the Grand Tour and direct manipulations on the axes, we can in theory visualize and manipulate any intermediate layer of a neural network by itself. Nevertheless, this is not a very satisfying approach, for two reasons:\n \n\n* In the same way that we’ve kept the projection fixed as the training data changed, we would like to “keep the projection fixed”, as the data moves through the layers in the neural network. However, this is not straightforward. For example, different layers in a neural network have different dimensions. How do we connect rotations of one layer to rotations of the other?\n* The class “axis handles” in the softmax layer convenient, but that’s only practical when the dimensionality of the layer is relatively small.\n With hundreds of dimensions, for example, there would be too many axis handles to naturally interact with.\n In addition, hidden layers do not have as clear semantics as the softmax layer, so manipulating them would not be as intuitive.\n\n\n\n\n To address the first problem, we will need to pay closer attention to the way in which layers transform the data that they are given. \n To see how a linear transformation can be visualized in a particularly ineffective way, consider the following (very simple) weights (represented by a matrix AAA) which take a 2-dimensional hidden layer kkk and produce activations in another 2-dimensional layer k+1k+1k+1. The weights simply negate two activations in 2D:\n A=[−1,00,−1]\n A = \\begin{bmatrix}\n -1, 0 \\\\\n 0, -1\n \\end{bmatrix}\n A=[−1,00,−1\u200b]\n Imagine that we wish to visualize the behavior of network as the data moves from layer to layer. One way to interpolate the source x0x\\_0x0\u200b and destination x1=A(x0)=−x0x\\_1 = A(x\\_0) = -x\\_0x1\u200b=A(x0\u200b)=−x0\u200b of this action AAA is by a simple linear interpolation\n\n xt=(1−t)⋅x0+t⋅x1=(1−2t)⋅x0\n x\\_t = (1-t) \\cdot x\\_0 + t \\cdot x\\_1 = (1-2t) \\cdot x\\_0 \n xt\u200b=(1−t)⋅x0\u200b+t⋅x1\u200b=(1−2t)⋅x0\u200b\n for t∈[0,1].t \\in [0,1].t∈[0,1].\n\n Effectively, this strategy reuses the linear projection coefficients from one layer to the next. This is a natural thought, since they have the same dimension.\n However, notice the following: the transformation given by A is a simple rotation of the data. Every linear transformation of the layer k+1k+1k+1 could be encoded simply as a linear transformation of the layer kkk, if only that transformation operated on the negative values of the entries.\n In addition, since the Grand Tour has a rotation itself built-in, for every configuration that gives a certain picture of the layer kkk, there exists a *different* configuration that would yield the same picture for layer k+1k+1k+1, by taking the action of AAA into account.\n In effect, the naive interpolation fails the principle of data-visual correspondence: a simple change in data (negation in 2D/180 degree rotation) results in a drastic change in visualization (all points cross the origin).\n\n\n\n\n This observation points to a more general strategy: when designing a visualization, we should be as explicit as possible about which parts of the input (or process) we seek to capture in our visualizations.\n We should seek to explicitly articulate what are purely representational artifacts that we should discard, and what are the real features a visualization we should *distill* from the representation.\n Here, we claim that rotational factors in linear transformations of neural networks are significantly less important than other factors such as scalings and nonlinearities.\n As we will show, the Grand Tour is particularly attractive in this case because it is can be made to be invariant to rotations in data.\n As a result, the rotational components in the linear transformations of a neural network will be explicitly made invisible.\n\n\n\n\n Concretely, we achieve this by taking advantage of a central theorem of linear algebra. \n The *Singular Value Decomposition* (SVD) theorem shows that *any* linear transformation can be decomposed into a sequence of very simple operations: a rotation, a scaling, and another rotation. \n\n Applying a matrix AAA to a vector xxx is then equivalent to applying those simple operations: xA=xUΣVTx A = x U \\Sigma V^TxA=xUΣVT.\n But remember that the Grand Tour works by rotating the dataset and then projecting it to 2D.\n Combined, these two facts mean that as far as the Grand Tour is concerned, visualizing a vector xxx is the same as visualizing xUx UxU, and visualizing a vector xUΣVTx U \\Sigma V^TxUΣVT is the same as visualizing xUΣx U \\SigmaxUΣ. \n This means that any linear transformation seen by the Grand Tour is equivalent to the transition between xUx UxU and xUΣx U \\SigmaxUΣ - a simple (coordinate-wise) scaling. \n This is explicitly saying that any linear operation (whose matrix is represented in standard bases) is a scaling operation with appropriately chosen orthonormal bases on both sides.\n So the Grand Tour provides a natural, elegant and computationally efficient way to *align* visualizations of activations separated by fully-connected (linear) layers.Convolutional layers are also linear. One can instantly see that by forming the linear transformations between flattened feature maps, or by taking the circulant structure of convolutional layers directly into account\n\n\n\n\n (For the following portion, we reduce the number of data points to 500 and epochs to 50, in order to reduce the amount of data transmitted in a web-based demonstration.)\n With the linear algebra structure at hand, now we are able to trace behaviors and patterns from the softmax back to previous layers.\n In fashion-MNIST, for example, we observe a separation of shoes (sandals, sneakers and ankle boots as a group) from all other classes in the softmax layer. \n Tracing it back to earlier layers, we can see that this separation happened as early as layer 5:\n\n\n\n\n\n\n\nWith layers aligned, it is easy to see the early separation of shoes from this view.\n### The Grand Tour of Adversarial Dynamics\n\n\n\n As a final application scenario, we show how the Grand Tour can also elucidate the behavior of adversarial examples as they are processed by a neural network.\n For this illustration, we use the MNIST dataset, and we adversarially add perturbations to 89 digit 8s to fool the network into thinking they are 0s.\n Previously, we either animated the training dynamics or the layer dynamics.\n We fix a well-trained neural network, and visualize the training process of adversarial examples, since they are often themselves generated by an optimization process. Here, we used the Fast Gradient Sign method.\n Again, because the Grand Tour is a linear method, the change in the positions of the adversarial examples over time can be faithfully attributed to changes in how the neural network perceives the images, rather than potential artifacts of the visualization.\n Let us examine how adversarial examples evolved to fool the network:\n\n\n\n\n\n\n\n\n From this view of softmax, we can see how \n adversarial examples \n evolved from 8s \n into 0s.\n In the corresponding pre-softmax however, these adversarial examples stop around the decision boundary of two classes. \n Show data as images to see the actual images generated in each step, or dots colored by labels.\n\n\n Through this adversarial training, the network eventually claims, with high confidence, that the inputs given are all 0s.\n If we stay in the softmax layer and slide though the adversarial training steps in the plot, we can see adversarial examples move from a high score for class 8 to a high score for class 0.\n Although all adversarial examples are classified as the target class (digit 0s) eventually, some of them detoured somewhere close to the centroid of the space (around the 25th epoch) and then moved towards the target. \n Comparing the actual images of the two groups, we see those that those “detouring” images tend to be noisier.\n\n\n\n\n More interesting, however, is what happens in the intermediate layers.\n In pre-softmax, for example, we see that these fake 0s behave differently from the genuine 0s: they live closer to the decision boundary of two classes and form a plane by themselves. \n\n\n\n## Discussion\n\n\n### Limitations of the Grand Tour\n\n\n\n Early on, we compared several state-of-the-art dimensionality reduction techniques with the Grand Tour, showing that non-linear methods do not have as many desirable properties as the Grand Tour for understanding the behavior of neural networks. \n However, the state-of-the-art non-linear methods come with their own strength. \n Whenever geometry is concerned, like the case of understanding multi-way confusions in the softmax layer, linear methods are more interpretable because they preserve certain geometrical structures of data in the projection. \n When topology is the main focus, such as when we want to cluster the data or we need dimensionality reduction for downstream models that are less sensitive to geometry, we might choose non-linear methods such as UMAP or t-SNE for they have more freedom in projecting the data, and will generally make better use of the fewer dimensions available. \n\n\n\n### The Power of Animation and Direct Manipulation\n\n\n\n When comparing linear projections with non-linear dimensionality reductions, we used small multiples to contrast training epochs and dimensionality reduction methods.\n The Grand Tour, on the other hand, uses a single animated view.\n When comparing small multiples and animations, there is no general consensus on which one is better than the other in the literature, aside. \n from specific settings such as dynamic graph drawing , or concerns about incomparable contents  between small multiples and animated plots.\n Regardless of these concerns, in our scenarios, the use of animation comes naturally from the direct manipulation and the existence of a continuum of rotations for the Grand Tour to operate in.\n\n\n\n### Non-sequential Models\n\n\n\n In our work we have used models that are purely “sequential”, in the sense that the layers can be put in numerical ordering, and that the activations for\n the n+1n+1n+1-th layer are a function exclusively of the activations at the nnn-th layer. \n In recent DNN architectures, however, it is common to have non-sequential parts such as highway  branches or dedicated branches for different tasks . \n With our technique, one can visualize neuron activations on each such branch, but additional research is required to incorporate multiple branches directly.\n\n\n\n### Scaling to Larger Models\n\n\n\n Modern architectures are also wide. Especially when convolutional layers are concerned, one could run into issues with scalability if we see such layers as a large sparse matrix acting on flattened multi-channel images.\n For the sake of simplicity, in this article we brute-forced the computation of the alignment of such convolutional layers by writing out their explicit matrix representation. \n However, the singular value decomposition of multi-channel 2D convolutions can be computed efficiently , which can be then be directly used for alignment, as we described above.\n\n\n\n\nfunction toggle(event, id){\n let caller = d3.select(event.target); //DOM that received the event\n let callerIsActive = caller.classed("clickable-active");\n\n let selection = d3.select(id); //DOM to be toggled\n let isHidden = selection.classed("hidden");\n\n selection.classed("hidden", !isHidden);\n caller.classed("clickable-active", !callerIsActive); //change the indicator (+/- sign) besides the caller\n}\n\n## \nTechnical Details\n\n\n\n\nThis section presents the technical details necessary to implement the direct manipulation of axis handles and data points, as well as how to implement the projection consistency technique for layer transitions.\n\n### Notation\n\n\n\n In this section, our notational convention is that data points are represented as row vectors.\n An entire dataset is laid out as a matrix, where each row is a data point, and each column represents a different feature/dimension.\n As a result, when a linear transformation is applied to the data, the row vectors (and the data matrix overall) are left-multiplied by the transformation matrix.\n This has a side benefit that when applying matrix multiplications in a chain, the formula reads from left to right and aligns with a commutative diagram.\n For example, when a data matrix XXX is multiplied by a matrix MMM to generate YYY, in formula we write XM=YXM = YXM=Y, the letters have the same order in diagram:\n\n\n\nX↦MY\n X \n \\overset{M}{\\mapsto}\n Y\nX↦M\u200bY\n\nFurthermore, if the SVD of MMM is M=UΣVTM = U \\Sigma V^{T}M=UΣVT, we have XUΣVT=YX U \\Sigma V^{T} = YXUΣVT=Y, and the diagram\nX↦U↦Σ↦VTY\n X \n \\overset{U}{\\mapsto} \n \\overset{\\Sigma}{\\mapsto} \n \\overset{V^T}{\\mapsto} Y\nX↦U\u200b↦Σ\u200b↦VT\u200bY\nnicely aligns with the formula.\n\n### \nDirect Manipulation\n\n\n\n\n The direct manipulations we presented earlier provide explicit control over the possible projections for the data points.\n We provide two modes: directly manipulating class axes (the “axis mode”), or directly manipulating a group of data points through their centroid (the “data point mode”).\n Based on the dimensionality and axis semantics, as discussed in [Layer Dynamics](https://distill.pub/2020/grand-tour/#layer-dynamics), we may prefer one mode than the other.\n \n We will see that the axis mode is a special case of data point mode, because we can view an axis handle as a particular “fictitious” point in the dataset.\n Because of its simplicity, we will first introduce the axis mode.\n\n\n\n#### \n The Axis Mode\n\n\n\n\n The implied semantics of direct manipulation is that when a user drags an UI element (in this case, an axis handle), they are signaling to the system that they wished that the corresponding\n data point had been projected to the location where the UI element was dropped, rather than where it was dragged from.\n In our case the overall projection is a rotation (originally determined by the Grand Tour), and an arbitrary user manipulation might not necessarily generate a new projection that is also a rotation. Our goal, then, is to find a new rotation which satisfies the user request and is close to the previous state of the Grand Tour projection, so that the resulting state satisfies the user request.\n\n In a nutshell, when user drags the ithi^{th}ith axis handle by (dx,dy)(dx, dy)(dx,dy), we add them to the first two entries of the ithi^{th}ith row of the Grand Tour matrix, and then perform [Gram-Schmidt orthonormalization](https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process) on the rows of the new matrix.\n \n Rows have to be reordered such that the ithi^{th}ith row is considered first in the Gram-Schmidt procedure.\n \n\n\n\n\n Before we see in detail why this works well, let us formalize the process of the Grand Tour on a standard basis vector eie\\_iei\u200b. \n As shown in the diagram below, eie\\_iei\u200b goes through an orthogonal Grand Tour matrix GTGTGT to produce a rotated version of itself, ei~\\tilde{e\\_i}ei\u200b~\u200b. \n Then, π2\\pi\\_2π2\u200b is a function that keeps only the first two entries of ei~\\tilde{e\\_i}ei\u200b~\u200b and gives the 2D coordinate of the handle to be shown in the plot, (xi,yi)(x\\_i, y\\_i)(xi\u200b,yi\u200b).\n\n\n\nei↦GTei~↦π2(xi,yi)\n e\\_i \\overset{GT}{\\mapsto} \\tilde{e\\_i} \\overset{\\pi\\_2}{\\mapsto} (x\\_i, y\\_i)\nei\u200b↦GT\u200bei\u200b~\u200b↦π2\u200b\u200b(xi\u200b,yi\u200b)\n\n When user drags an axis handle on the screen canvas, they induce a delta change Δ=(dx,dy)\\Delta = (dx, dy)Δ=(dx,dy) on the xyxyxy-plane. \n The coordinate of the handle becomes:\n (xi(new),yi(new)):=(xi+dx,yi+dy)(x\\_i^{(new)}, y\\_i^{(new)})\xa0:= (x\\_i+dx, y\\_i+dy)(xi(new)\u200b,yi(new)\u200b):=(xi\u200b+dx,yi\u200b+dy)\n Note that xix\\_ixi\u200b and yiy\\_iyi\u200b are the first two coordinates of the axis handle in high dimensions after the Grand Tour rotation, so a delta change on (xi,yi)(x\\_i, y\\_i)(xi\u200b,yi\u200b) induces a delta change Δ~:=(dx,dy,0,0,⋯)\\tilde{\\Delta}\xa0:= (dx, dy, 0, 0, \\cdots)Δ~:=(dx,dy,0,0,⋯) on ei~\\tilde{e\\_i}ei\u200b~\u200b:\n ei~↦Δ~ei~+Δ~\\tilde{e\\_i} \\overset{\\tilde{\\Delta}}{\\mapsto} \\tilde{e\\_i} + \\tilde{\\Delta}ei\u200b~\u200b↦Δ~\u200bei\u200b~\u200b+Δ~\n\n\n\n\n To find a nearby Grand Tour rotation that respects this change, first note that ei~\\tilde{e\\_i}ei\u200b~\u200b is exactly the ithi^{th}ith row of orthogonal Grand Tour matrix GTGTGT\n\n Recall that the convention is that vectors are in row form and linear transformations are matrices that are multiplied on the right.\n So eie\\_iei\u200b is a row vector whose iii-th entry is 111 (and 000s elsewhere) and ei~:=ei⋅GT\\tilde{e\\_i}\xa0:= e\\_i \\cdot GTei\u200b~\u200b:=ei\u200b⋅GT is the iii-th row of GTGTGT\n. \n Naturally, we want the new matrix to be the original GTGTGT with its ithi^{th}ith row replaced by ei~+Δ~\\tilde{e\\_i}+\\tilde{\\Delta}ei\u200b~\u200b+Δ~, i.e. we should add dxdxdx and dydydy to the (i,1)(i,1)(i,1)-th entry and (i,2)(i,2)(i,2)-th entry of GTGTGT respectively:\n GT~←GT\\widetilde{GT} \\leftarrow GTGT\n←GT\nGT~i,1←GTi,1+dx\\widetilde{GT}\\_{i,1} \\leftarrow GT\\_{i,1} + dxGT\ni,1\u200b←GTi,1\u200b+dx\nGT~i,2←GTi,2+dy\\widetilde{GT}\\_{i,2} \\leftarrow GT\\_{i,2} + dyGT\ni,2\u200b←GTi,2\u200b+dy\n However, GT~\\widetilde{GT}GT\n is not orthogonal for arbitrary (dx,dy)(dx, dy)(dx,dy).\n In order to find an approximation to GT~\\widetilde{GT}GT\n that is orthogonal, we apply [Gram-Schmidt orthonormalization](https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process) on the rows of GT~\\widetilde{GT}GT\n, with the ithi^{th}ith row considered first in the Gram-Schmidt process:\n GT(new):=GramSchmidt(GT~)GT^{(new)}\xa0:= \\textsf{GramSchmidt}(\\widetilde{GT})GT(new):=GramSchmidt(GT\n)\n Note that the ithi^{th}ith row is normalized to a unit vector during the Gram-Schmidt, so the resulting position of the handle is \n ei~(new)=normalize(ei~+Δ~)\\tilde{e\\_i}^{(new)} = \\textsf{normalize}(\\tilde{e\\_i} + \\tilde{\\Delta})ei\u200b~\u200b(new)=normalize(ei\u200b~\u200b+Δ~)\n which may not be exactly the same as ei~+Δ~\\tilde{e\\_i}+\\tilde{\\Delta}ei\u200b~\u200b+Δ~, as the following figure shows\n \n However, for any Δ~\\tilde{\\Delta}Δ~, the norm of the difference is bounded above by ∣∣Δ~∣∣||\\tilde{\\Delta}||∣∣Δ~∣∣, as the following figure proves.\n ![](./Visualizing Neural Networks with the Grand Tour_files/direct-manipulation-rotation-2d-proof.png)\n\n \xa0.\n\n\n\n![](./Visualizing Neural Networks with the Grand Tour_files/direct-manipulation-rotation-2d.png)\n \n#### \n The Data Point Mode\n\n\n\n\n We now explain how we directly manipulate data points. \n Technically speaking, this method only considers one point at a time.\n For a group of points, we compute their centroid and directly manipulate this single point with this method.\n Thinking more carefully about the process in axis mode gives us a way to drag any single point.\n Recall that in axis mode, we added user’s manipulation Δ~:=(dx,dy,0,0,⋯)\\tilde{\\Delta}\xa0:= (dx, dy, 0, 0, \\cdots)Δ~:=(dx,dy,0,0,⋯) to the position of the ithi^{th}ith axis handle ei~\\tilde{e\\_i}ei\u200b~\u200b.\n This induces a delta change in the ithi^{th}ith row of the Grand Tour matrix GTGTGT.\n Next, as the first step in Gram-Schmidt, we normalized this row: \n GTi(new):=normalize(GT~i)=normalize(ei~+Δ~)\n GT\\_i^{(new)}\xa0:= \\textsf{normalize}(\\widetilde{GT}\\_i) = \\textsf{normalize}(\\tilde{e\\_i} + \\tilde{\\Delta})\n GTi(new)\u200b:=normalize(GT\ni\u200b)=normalize(ei\u200b~\u200b+Δ~)\n These two steps make the axis handle move from ei~\\tilde{e\\_i}ei\u200b~\u200b to ei~(new):=normalize(ei~+Δ~)\\tilde{e\\_i}^{(new)}\xa0:= \\textsf{normalize}(\\tilde{e\\_i}+\\tilde{\\Delta})ei\u200b~\u200b(new):=normalize(ei\u200b~\u200b+Δ~).\n\n\n\n\n Looking at the geometry of this movement, the “add-delta-then-normalize” on ei~\\tilde{e\\_i}ei\u200b~\u200b is equivalent to a *rotation* from ei~\\tilde{e\\_i}ei\u200b~\u200b towards ei~(new)\\tilde{e\\_i}^{(new)}ei\u200b~\u200b(new), illustrated in the figure below. \n This geometric interpretation can be directly generalized to any arbitrary data point.\n\n\n\n![](./Visualizing Neural Networks with the Grand Tour_files/direct-manipulation-rotation-3d.png)\n\n The figure shows the case in 3D, but in higher dimensional space it is essentially the same, since the two vectors ei~\\tilde{e\\_i}ei\u200b~\u200b and ei~+Δ~\\tilde{e\\_i}+\\tilde{\\Delta}ei\u200b~\u200b+Δ~ only span a 2-subspace.\n Now we have a nice geometric intuition about direct manipulation: dragging a point induces a *simple rotation*\n[Simple rotations](https://en.wikipedia.org/wiki/Rotations_in_4-dimensional_Euclidean_space#Simple_rotations) are rotations with only one [plane of rotation](https://en.wikipedia.org/wiki/Plane_of_rotation#Simple_rotations).\n in high dimensional space.\n This intuition is precisely how we implemented our direct manipulation on arbitrary data points, which we will specify as below.\n\n\n\n\n Generalizing this observation from axis handle to arbitrary data point, we want to find the rotation that moves the centroid of a selected subset of data points c~\\tilde{c}c~ to \n c~(new):=(c~+Δ~)⋅∣∣c~∣∣/∣∣c~+Δ~∣∣\n \\tilde{c}^{(new)}\xa0:= (\\tilde{c} + \\tilde{\\Delta}) \\cdot ||\\tilde{c}|| / ||\\tilde{c} + \\tilde{\\Delta}||\n c~(new):=(c~+Δ~)⋅∣∣c~∣∣/∣∣c~+Δ~∣∣\n\n\n\n![](./Visualizing Neural Networks with the Grand Tour_files/direct-manipulation-rotation-2d-perp.png)\n\n First, the angle of rotation can be found by their cosine similarity:\n θ=arccos(⟨c~,c~(new)⟩∣∣c~∣∣⋅∣∣c~(new)∣∣) \\theta = \\textrm{arccos}(\n \\frac{\\langle \\tilde{c}, \\tilde{c}^{(new)} \\rangle}{||\\tilde{c}|| \\cdot ||\\tilde{c}^{(new)}||}\n )θ=arccos(∣∣c~∣∣⋅∣∣c~(new)∣∣⟨c~,c~(new)⟩\u200b)\n Next, to find the matrix form of the rotation, we need a convenient basis.\n Let QQQ be a change of (orthonormal) basis matrix in which the first two rows form the 2-subspace span(c~,c~(new))\\textrm{span}(\\tilde{c}, \\tilde{c}^{(new)})span(c~,c~(new)).\n For example, we can let its first row to be normalize(c~)\\textsf{normalize}(\\tilde{c})normalize(c~), second row to be its orthonormal complement normalize(c~⊥(new))\\textsf{normalize}(\\tilde{c}^{(new)}\\_{\\perp})normalize(c~⊥(new)\u200b) in span(c~,c~(new))\\textrm{span}(\\tilde{c}, \\tilde{c}^{(new)})span(c~,c~(new)), and the remaining rows complete the whole space:\n c~⊥(new):=c~−∣∣c~∣∣⋅cosθc~(new)∣∣c~(new)∣∣\n \\tilde{c}^{(new)}\\_{\\perp} \n \xa0:= \\tilde{c} - ||\\tilde{c}|| \\cdot cos \\theta \\frac{\\tilde{c}^{(new)}}{||\\tilde{c}^{(new)}||}\n c~⊥(new)\u200b:=c~−∣∣c~∣∣⋅cosθ∣∣c~(new)∣∣c~(new)\u200b\nQ:=[⋯normalize(c~)⋯⋯normalize(c~⊥(new))⋯P]\n Q\xa0:=\n \\begin{bmatrix}\n \\cdots \\textsf{normalize}(\\tilde{c}) \\cdots \\\\\n \\cdots \\textsf{normalize}(\\tilde{c}^{(new)}\\_{\\perp}) \\cdots \\\\\n P\n \\end{bmatrix}\n Q:=⎣⎡\u200b⋯normalize(c~)⋯⋯normalize(c~⊥(new)\u200b)⋯P\u200b⎦⎤\u200b\n where PPP completes the remaining space.\n\n Making use of QQQ, we can find the matrix that rotates the plane span(c~,c~(new))\\textrm{span}(\\tilde{c}, \\tilde{c}^{(new)})span(c~,c~(new)) by the angle θ\\thetaθ:\n ρ=QT[cosθsinθ00⋯−sinθcosθ00⋯00⋮⋮I]Q=:QTR1,2(θ)Q\n \\rho = Q^T\n \\begin{bmatrix}\n \\cos \\theta& \\sin \\theta& 0& 0& \\cdots\\\\\n -\\sin \\theta& \\cos \\theta& 0& 0& \\cdots\\\\\n 0& 0& \\\\ \n \\vdots& \\vdots& & I& \\\\\n \\end{bmatrix}\n Q\n =: Q^T R\\_{1,2}(\\theta) Q\n ρ=QT⎣⎢⎢⎢⎢⎡\u200bcosθ−sinθ0⋮\u200bsinθcosθ0⋮\u200b00\u200b00I\u200b⋯⋯\u200b⎦⎥⎥⎥⎥⎤\u200bQ=:QTR1,2\u200b(θ)Q\n The new Grand Tour matrix is the matrix product of the original GTGTGT and ρ\\rhoρ:\n GT(new):=GT⋅ρ\n GT^{(new)}\xa0:= GT \\cdot \\rho\n GT(new):=GT⋅ρ\n Now we should be able to see the connection between axis mode and data point mode.\n In data point mode, finding QQQ can be done by Gram-Schmidt: Let the first basis be c~\\tilde{c}c~, find the orthogonal component of c~(new)\\tilde{c}^{(new)}c~(new) in span(c~,c~(new))\\textrm{span}(\\tilde{c}, \\tilde{c}^{(new)})span(c~,c~(new)), repeatedly take a random vector, find its orthogonal component to the span of the current basis vectors and add it to the basis set. \n In axis mode, the ithi^{th}ith-row-first Gram-Schmidt does the rotation and change of basis in one step.\n\n\n\n \n \n### \n\n Layer Transitions\n\n\n\n#### \n ReLU Layers\n\n\n\n When the lthl^{th}lth layer is a ReLU function, the output activation is Xl=ReLU(Xl−1)X^{l} = ReLU(X^{l-1})Xl=ReLU(Xl−1). Since ReLU does not change the dimensionality and the function is taken coordinate wise, we can animate the transition by a simple linear interpolation: for a time parameter t∈[0,1]t \\in [0,1]t∈[0,1],\n X(l−1)→l(t):=(1−t)Xl−1+tXl\n X^{(l-1) \\to l}(t)\xa0:= (1-t) X^{l-1} + t X^{l}\n X(l−1)→l(t):=(1−t)Xl−1+tXl\n \n#### \n Linear Layers\n\n\n\n Transitions between linear layers can seem complicated, but as we will show, this comes from choosing mismatching bases on either side of the transition. \n If Xl=Xl−1MX^{l} = X^{l-1} MXl=Xl−1M where M∈Rm×nM \\in \\mathbb{R}^{m \\times n}M∈Rm×n is the matrix of a linear transformation, then it has a singular value decomposition (SVD):\n M=UΣVTM = U \\Sigma V^TM=UΣVT\n where U∈Rm×mU \\in \\mathbb{R}^{m \\times m}U∈Rm×m and VT∈Rn×nV^T \\in \\mathbb{R}^{n \\times n}VT∈Rn×n are orthogonal, Σ∈Rm×n\\Sigma \\in \\mathbb{R}^{m \\times n}Σ∈Rm×n is diagonal.\n For arbitrary UUU and VTV^TVT, the transformation on Xl−1X^{l-1}Xl−1 is a composition of a rotation (UUU), scaling (Σ\\SigmaΣ) and another rotation (VTV^TVT), which can look complicated. \n However, consider the problem of relating the Grand Tour view of layer XlX^{l}Xl to that of layer Xl+1X^{l+1}Xl+1. The Grand Tour has a single parameter that represents the current rotation of the dataset. Since our goal is to keep the transition consistent, we notice that UUU and VTV^TVT have essentially no significance - they are just rotations to the view that can be exactly “canceled” by changing the rotation parameter of the Grand Tour in either layer.\n Hence, instead of showing MMM, we seek for the transition to animate only the effect of Σ\\SigmaΣ.\n Σ\\SigmaΣ is a coordinate-wise scaling, so we can animate it similar to the ReLU after the proper change of basis.\n Given Xl=Xl−1UΣVTX^{l} = X^{l-1} U \\Sigma V^TXl=Xl−1UΣVT, we have\n (XlV)=(Xl−1U)Σ\n (X^{l}V) = (X^{l-1}U)\\Sigma\n (XlV)=(Xl−1U)Σ\n For a time parameter t∈[0,1]t \\in [0,1]t∈[0,1],\n X(l−1)→l(t):=(1−t)(Xl−1U)+t(XlV)=(1−t)(Xl−1U)+t(Xl−1UΣ)\n X^{(l-1) \\to l}(t)\xa0:= (1-t) (X^{l-1}U) + t (X^{l}V) = (1-t) (X^{l-1}U) + t (X^{l-1} U \\Sigma)\n X(l−1)→l(t):=(1−t)(Xl−1U)+t(XlV)=(1−t)(Xl−1U)+t(Xl−1UΣ)\n \n#### \n Convolutional Layers\n\n\n\n Convolutional layers can be represented as special linear layers.\n With a change of representation, we can animate a convolutional layer like the previous section.\n For 2D convolutions this change of representation involves flattening the input and output, and repeating the kernel pattern in a sparse matrix M∈Rm×nM \\in \\mathbb{R}^{m \\times n}M∈Rm×n, where mmm and nnn are the dimensionalities of the input and output respectively.\n This change of representation is only practical for a small dimensionality (e.g. up to 1000), since we need to solve SVD for linear layers.\n However, the singular value decomposition of multi-channel 2D convolutions can be computed efficiently , which can be then be directly used for alignment.\n \n#### \n Max-pooling Layers\n\n\n\n Animating max-pooling layers is nontrivial because max-pooling is neither linear A max-pooling layer is piece-wise linear or coordinate-wise.\n We replace it by average-pooling and scaling by the ratio of the average to the max.\n We compute the matrix form of average-pooling and use its SVD to align the view before and after this layer. \n Functionally, our operations have equivalent results to max-pooling, but this introduces\n unexpected artifacts. For example, the max-pooling version of the vector [0.9,0.9,0.9,1.0][0.9, 0.9, 0.9, 1.0][0.9,0.9,0.9,1.0] should “give no credit” to the 0.90.90.9 entries; our implementation, however, will\n attribute about 25% of the result in the downstream layer to each those coordinates.\n \n \n\n## Conclusion\n\n\n\n As powerful as t-SNE and UMAP are, they often fail to offer the correspondences we need, and such correspondences can come, surprisingly, from relatively simple methods like the Grand Tour. The Grand Tour method we presented is particularly useful when direct manipulation from the user is available or desirable.\n We believe that it might be possible to design methods that highlight the best of both worlds, using non-linear dimensionality reduction to create intermediate, relatively low-dimensional representations of the activation layers, and using the Grand Tour and direct manipulation to compute the final projection.\n\n\n\n", "bibliography_bbl": "", "bibliography_bib": [{"title": "The grand tour: a tool for viewing multidimensional data"}, {"title": "Visualizing data using t-SNE"}, {"title": "Umap: Uniform manifold approximation and projection for dimension reduction"}, {"title": "Intriguing properties of neural networks"}, {"title": "ImageNet Large Scale Visual Recognition Challenge"}, {"title": "The mythos of model interpretability"}, {"title": "Visualizing dataflow graphs of deep learning models in tensorflow"}, {"title": "An algebraic process for visualization design"}, {"title": "Feature visualization"}, {"title": "MNIST handwritten digit database"}, {"title": "Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms"}, {"title": "Learning multiple layers of features from tiny images"}, {"title": "Rectified linear units improve restricted boltzmann machines"}, {"title": "Visualizing time-dependent data using dynamic t-SNE"}, {"title": "How to use t-sne effectively"}, {"title": "We Recommend a Singular Value Decomposition"}, {"title": "The singular values of convolutional layers"}, {"title": "Explaining and harnessing adversarial examples"}, {"title": "Animation, small multiples, and the effect of mental map preservation in dynamic graphs"}, {"title": "Animation: can it facilitate?"}, {"title": "Highway networks"}, {"title": "Going deeper with convolutions"}]}
{"source": "distill", "source_filetype": "html", "converted_with": "python", "paper_version": "", "post_title": "Visualizing memorization in RNNs", "authors": ["Andreas Madsen"], "date_published": "2019-03-25", "data_last_modified": "", "url": "", "abstract": "Memorization in Recurrent Neural Networks (RNNs) continues to pose a challenge in many applications. We’d like RNNs to be able to store information over many timesteps and retrieve it when it becomes relevant\u2009—\u2009but vanilla RNNs often struggle to do this.", "author_comment": "", "journal_ref": "distill pub", "doi": "https://doi.org/10.23915/distill.00016", "primary_category": "", "categories": "", "citation_level": "", "main_tex_filename": "", "text": "Memorization in Recurrent Neural Networks (RNNs) continues to pose a challenge\n in many applications. We’d like RNNs to be able to store information over many\n timesteps and retrieve it when it becomes relevant\u2009—\u2009but vanilla RNNs often struggle to do this.\n \n\n\n\n Several network architectures have been proposed to tackle aspects of this problem, such\n as Long-Short-Term Memory (LSTM)\n units and Gated Recurrent Units (GRU).\n However, the practical problem of memorization still poses a challenge.\n As such, developing new recurrent units that are better at memorization\n continues to be an active field of research.\n \n\n\n\n To compare a recurrent unit against its alternatives, both past and recent\n papers, such as the Nested LSTM paper by Monzi et al.\n , heavily rely on quantitative\n comparisons. These comparisons often measure accuracy or\n cross entropy loss on standard problems such as Penn Treebank, Chinese\n Poetry Generation, or text8, where the task is to predict the\n next character given existing input.\n \n\n\n\n While quantitative comparisons are useful, they only provide partial\n insight into the how a recurrent unit memorizes. A model can, for example,\n achieve high accuracy and cross entropy loss by just providing highly accurate\n predictions in cases that only require short-term memorization, while\n being inaccurate at predictions that require long-term\n memorization.\n For example, when autocompleting words in a sentence, a model with only short-term understanding could still exhibit high accuracy completing the ends of words once most of the characters are present.\n However, without longer term contextual understanding it won’t be able to predict words when only a few characters are known.\n \n\n\n\n This article presents a qualitative visualization method for comparing\n recurrent units with regards to memorization and contextual understanding.\n The method is applied to the three recurrent units mentioned above: Nested LSTMs, LSTMs, and GRUs.\n \n\n\n## Recurrent Units\n\n\n\n The networks that will be analyzed all use a simple RNN structure:\n \n\n\n\n\nhℓth\\_{\\ell}^{t}hℓt\u200b\n\n Output for layer ℓ\\ellℓ at time ttt.\n \n===\nUnit\\mathrm{Unit}Unit\n\n Recurrent unit of choice.\n \n(hℓ−1t,hℓt−1),\xa0where:\xa0h0t=xt(h\\_{\\ell-1}^{t}, h\\_{\\ell}^{t-1}), \\text{ where: } h\\_{0}^t = x\\_t(hℓ−1t\u200b,hℓt−1\u200b),\xa0where:\xa0h0t\u200b=xt\u200b\nyty^tyt\n===\nSoftmax\\mathrm{Softmax}Softmax\n(hLt)(h\\_L^t)(hLt\u200b)\n\n\n\n In theory, the time dependency allows it in each iteration to know\n about every part of the sequence that came before. However, this time\n dependency typically causes a vanishing gradient problem that results in\n long-term dependencies being ignored during training\n .\n \n\n\n\n\n**Vanishing Gradient:**  where the contribution from the\n earlier steps becomes insignificant in the gradient for the vanilla RNN\n unit.\n \nSoftmax LayerRecurrent LayerRecurrent LayerInput Layer\n\n Several solutions to the vanishing gradient problem have been proposed over\n the years. The most popular are the aforementioned LSTM and GRU units, but this\n is still an area of active research. Both LSTM and GRU are well known\n and [thoroughly explained in literature](http://colah.github.io/posts/2015-08-Understanding-LSTMs/). Recently, Nested LSTMs have also been proposed\n \u2009—\u2009an explanation of Nested LSTMs\n can be found [in the appendix](https://distill.pub/2019/memorization-in-rnns/#appendix-nestedlstm).\n \n\n\n\n* Nested LSTM\n* LSTM\n* GRU\n\n![Nested LSTM Diagram](./Visualizing memorization in RNNs_files/nlstm-web.svg)\n\n**Recurrent Unit, Nested LSTM:** makes the cell update depend on another\n LSTM unit, supposedly this allows more long-term memory compared to\n stacking LSTM layers.\n \n![Long Short Term Memory Diagram](./Visualizing memorization in RNNs_files/lstm-web.svg)\n\n**Recurrent Unit, LSTM:** allows for long-term\n memorization by gateing its update, thereby solving the vanishing gradient\n problem.\n \n![Gated Recurrent Unit Diagram](./Visualizing memorization in RNNs_files/gru-web.svg)\n\n**Recurrent Unit, GRU:** solves the vanishing gradient\n problem without depending on an internal memory state.\n \n\n\n It is not entirely clear why one recurrent unit performs better than another\n in some applications, while in other applications it is another type of\n recurrent unit that performs better. Theoretically they all solve the vanishing\n gradient problem, but in practice their performance is highly application\n dependent.\n \n\n\n\n Understanding why these differences occur is likely an opaque and\n challenging problem. The purpose of this article is to demonstrate a\n visualization technique that can better highlight what these differences\n are. Hopefully, such an understanding can lead to a deeper understanding.\n \n\n\n## Comparing Recurrent Units\n\n\n\n Comparing different Recurrent Units is often more involved than simply comparing the accuracy or cross entropy\n loss. Differences in these high-level quantitative measures\n can have many explanations and may only be because of some small improvement\n in predictions that only requires short-term contextual understanding,\n while it is often the long-term contextual understanding that is of interest.\n \n\n\n### A problem for qualitative analysis\n\n\n\n Therefore a good problem for qualitatively analyzing contextual\n understanding should have a human-interpretable output and depend both on\n long-term and short-term contextual understanding. The typical problems\n that are often used, such as Penn Treebank, Chinese Poetry Generation, or\n text8  generation do not have outputs that are easy to reason about, as they\n require an extensive understanding of either grammar, Chinese poetry, or\n only output a single letter.\n \n\n\n\n To this end, this article studies the autocomplete problem. Each character is mapped\n to a target that represents the entire word. The space leading up to the word should also map to that target.\n This prediction based on the space character is in particular useful for showing contextual understanding.\n \n\n\n\n The autocomplete problem is quite similar to the text8 generation\n problem: the only difference is that instead of predicting the next letter,\n the model predicts an entire word. This makes the output much more\n interpretable. Finally, because of its close relation to text8 generation,\n existing literature on text8 generation is relevant and comparable,\n in the sense that models that work well on text8 generation should work\n well on the autocomplete problem.\n \n\n\n\nUser types input sequence.\nRecurrent neural network processes the sequence.\nThe output for the last character is used.\nThe most likely suggestions are extracted.\n\n parts of north af                           \ncustom input, loading ...\n\nafrica(85.30%)africans(1.40%)african(8.90%)\n\n\n\n**Autocomplete:** An application that has a humanly\n interpretable output, while depending on both short and long-term\n contextual understanding. In this case, the network uses past information\n and understands the next word should be a country.\n \n\n\n\n The output in this figure was produced by the GRU model;\n all model setups are [described in the appendix](https://distill.pub/2019/memorization-in-rnns/#appendix-autocomplete).\n\n \n Try [removing the last letters](javascript:arDemoShort();) to see\n that the network continues to give meaningful suggestions.\n \nYou can also type in your own text.\n ([reset](javascript:arDemoReset();)).\n \n\n\n\n\n\n The autocomplete dataset is constructed from the full\n [text8](http://mattmahoney.net/dc/textdata.html) dataset. The\n recurrent neural networks used to solve the problem have two layers, each\n with 600 units. There are three models, using GRU, LSTM, and Nested LSTM.\n See [the appendix](https://distill.pub/2019/memorization-in-rnns/#appendix-autocomplete) for more details.\n \n\n\n### Connectivity in the Autocomplete Problem\n\n\n\n In the recently published Nested LSTM paper\n , they qualitatively compared their\n Nested LSTM unit to other recurrent units, to show how it memorizes in\n comparison, by visualizing individual cell activations.\n \n\n\n\n This visualization was inspired by Karpathy et al.\n  where they identify cells\n that capture a specific feature. To identify a specific\n feature, this visualization approach works well. However, it is not a useful\n argument for memorization in general as the output is entirely dependent\n on what feature the specific cell captures.\n \n\n\n\n Instead, to get a better idea of how well each model memorizes and uses\n memory for contextual understanding, the connectivity between the desired\n output and the input is analyzed. This is calculated as:\n \n\n\n\n\nconnectivity(\\textrm{connectivity}(connectivity(\nttt\n\n Input time index.\n \n,,,\nt~\\tilde{t}t~\n\n Output time index.\n \n)=) =)=\n∣∣∂(hLt~)k∂xt∣∣2\\left|\\left|\\frac{\\partial (h\\_L^{\\tilde{t}})\\_k}{\\partial x^t}\\right|\\right|\\_2∣∣∣∣∣\u200b∣∣∣∣∣\u200b∂xt∂(hLt~\u200b)k\u200b\u200b∣∣∣∣∣\u200b∣∣∣∣∣\u200b2\u200b\n\n Magnitude of the gradient, between the logits for the desired output (hLt~)k(h\\_L^{\\tilde{t}})\\_k(hLt~\u200b)k\u200b and the input\n xtx^txt.\n \n\n\n\n Exploring the connectivity gives a surprising amount of insight into the\n different models’ ability for long-term contextual understanding. Try and\n interact with the figure below yourself to see what information the\n different models use for their predictions.\n \n\n\n\n context the formal study of grammar is an important part of education from a young age through advanced learning though the rules taught in schools are not a grammar in the sense most linguists useless(8.40%)level(5.90%)left(5.70%)Nested LSTM\n context the formal study of grammar is an important part of education from a young age through advanced learning though the rules taught in schools are not a grammar in the sense most linguists uselevel(26.30%)levels(25.30%)learning(8.10%)LSTM\n context the formal study of grammar is an important part of education from a young age through advanced learning though the rules taught in schools are not a grammar in the sense most linguists uselearning(57.50%)legal(7.60%)length(4.40%)GRU\n\n**Connectivity:** the connection strength between\n the target for the selected character and the input characters is highlighted in green\n ([reset](javascript:connectivitySetIndex(null);)).\n *Hover over or tap the text to change the selected character.*\n\n\n\n Let’s highlight three specific situations:\n \n\n\n\n\n\n1\n\n\n Observe how the models predict the word “learning” with [only the first two\n characters as input](javascript:connectivitySetIndex(106);). The Nested LSTM model barely uses past\n information and thus only suggests common words starting with the letter “l”.\n \n\n\n\n In contrast, the LSTM and GRU models both suggests the word “learning”.\n The GRU model shows stronger connectivity with the word “advanced”,\n and we see in the suggestions that it predicts a higher probability for “learning” than the LSTM model.\n \n\n\n\n\n\n2\n\n\n Examine how the models predict the word “grammar”.\n This word appears twice; when it appears for the first time the models have very little context.\n Thus, no model suggests “grammar” until it has\n [seen at least 4 characters](javascript:connectivitySetIndex(32);).\n \n\n\n\n When “grammar” appears for the second time, the models have more context.\n The GRU model is able to predict the word “grammar” with only [1 character from the word itself](javascript:connectivitySetIndex(159);). The LSTM and Nested LSTM again\n need [at least 4 characters](javascript:connectivitySetIndex(162);).\n \n\n\n\n\n\n3\n\n\n Finally, let’s look at predicting the word “schools”\n [given only the first character](javascript:connectivitySetIndex(141);). As in the other cases,\n the GRU model seems better at using past information for\n contextual understanding.\n \n\n\n\n What makes this case noteworthy is how the LSTM model appears to\n use words from almost the entire sentence as context. However,\n its suggestions are far from correct and have little to do\n with the previous words it seems to use in its prediction.\n This suggests that the LSTM model in this setup is capable of\n long-term memorization, but not long-term contextual understanding.\n \n\n\n\n\n\n1\n2\n3\n\n\n\n*The colored number links above change the connectivity figure’s displayed timestep and explanation.*\n\n\n\n These observations show that the connectivity visualization is a powerful tool\n for comparing models in terms of which previous inputs they use for contextual understanding.\n However, it is only possible to compare models on the same dataset, and\n on a specific example. As such, while these observations may show that\n Nested LSTM is not very capable of long-term contextual understanding in this example;\n these observations may not generalize to other datasets or hyperparameters.\n \n\n\n### Future work; quantitative metric\n\n\n\n From the above observations it appears that short-term contextual understanding\n often involves the word that is being predicted itself. That is, the models switch to\n using previously seen letters from the word itself, as more letters become\n available. In contrast, at the beginning of predicting a word, models\u2009—\u2009especially the\n GRU network\u2009—\u2009use previously seen words as context for the prediction.\n \n\n\n\n This observation suggests a quantitative metric: measure the accuracy given\n how many letters from the word being predicted are already known.\n It is not clear that this is best quantitative metric: it is highly problem dependent,\n and it also doesn’t summarize the model to a single number, which one may wish for a more direct comparison.\n \n\n\n\n024681012141618201.00.90.80.70.60.50.40.30.20.10.005101520100%80%60%40%20%0%characters from wordaccuracyGRU - top 1GRU - top 2GRU - top 3LSTM - top 1LSTM - top 2LSTM - top 3Nested LSTM - top 1Nested LSTM - top 2Nested LSTM - top 3\n\n**Accuracy Graph**: shows the accuracy\n given a fixed number of characters in a word that the RNN has seen.\n 0 characters mean that the RNN has only seen the space leading up\n to the word, including all the previous text which should provide context.\n The different line styles, indicates if the correct word should appear\n among the top 1, 2, or 3 suggestions.\n \n\n\n These results suggest that the GRU model is better at long-term contextual\n understanding, while the LSTM model is better at short-term contextual\n understanding. These observations are valuable, as it justifies why the\n [overall accuracy of the GRU and LSTM models](https://distill.pub/2019/memorization-in-rnns/#ar-overall-accuracy)  are almost identical, while the connectivity visualization shows that\n the GRU model is far better at long-term contextual understanding.\n \n\n\n\n While more detailed quantitative metrics like this provides new insight,\n qualitative analysis like the connectivity figure presented\n in this article still has great value. As the connectivity visualization gives an\n intuitive understanding of how the model works, which a quantitative metric\n cannot. It also shows that a wrong prediction can still be considered a\n useful prediction, such as a synonym or a contextually reasonable\n prediction.\n \n\n\n## Conclusion\n\n\n\n Looking at overall accuracy and cross entropy loss in itself is not that\n interesting. Different models may prioritize either long-term or\n short-term contextual understanding, while both models can have similar\n accuracy and cross entropy.\n \n\n\n\n A qualitative analysis, where one looks at how previous input is used in\n the prediction is therefore also important when judging models. In this\n case, the connectivity visualization together with the autocomplete\n predictions, reveals that the GRU model is much more capable of long-term\n contextual understanding, compared to LSTM and Nested LSTM. In the case of\n LSTM, the difference is much higher than one would guess from just looking\n at the overall accuracy and cross entropy loss alone. This observation is\n not that interesting in itself as it is likely very dependent on the\n hyperparameters, and the specific application.\n \n\n\n\n Much more valuable is that this visualization method makes it possible\n to intuitively understand how the models are different, to a much higher\n degree than just looking at accuracy and cross entropy. For this application,\n it is clear that the GRU model uses repeating words and semantic meaning\n of past words to make its prediction, to a much higher degree than the LSTM\n and Nested LSTM models. This is both a valuable insight when choosing the\n final model, but also essential knowledge when developing better models\n in the future.", "bibliography_bbl": "", "bibliography_bib": [{"title": "Long short-term memory"}, {"title": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"}, {"title": "Nested LSTMs"}, {"title": "The Penn Treebank: Annotating Predicate Argument Structure"}, {"title": "text8 Dataset"}, {"title": "On the difficulty of training recurrent neural networks"}, {"title": "Visualizing and Understanding Recurrent Networks"}]}
{"source": "distill", "source_filetype": "html", "converted_with": "python", "paper_version": "", "post_title": "Branch Specialization", "authors": ["Chelsea Voss", "Gabriel Goh", "Nick Cammarata", "Michael Petrov", "Ludwig Schubert", "Chris Olah"], "date_published": "2021-04-05", "data_last_modified": "", "url": "", "abstract": "\n          This article is part of the Circuits thread, an experimental format collecting invited short articles and critical commentary delving into the inner workings of neural networks.\n      ", "author_comment": "", "journal_ref": "distill pub", "doi": "https://doi.org/10.23915/distill.00024.008", "primary_category": "", "categories": "", "citation_level": "", "main_tex_filename": "", "text": "![](./Branch Specialization_files/multiple-pages.svg)\n\n This article is part of the [Circuits thread](https://distill.pub/2020/circuits/), an experimental format collecting invited short articles and critical commentary delving into the inner workings of neural networks.\n \n\n\n[Visualizing Weights](https://distill.pub/2020/circuits/visualizing-weights/)\n[Weight Banding](https://distill.pub/2020/circuits/weight-banding/)\n\n## Introduction\n\n\n\n If we think of interpretability as a kind of “anatomy of neural networks,” most of the circuits thread has involved studying tiny little veins – looking at the small-scale, at individual neurons and how they connect. However, there are many natural questions that the small-scale approach doesn’t address.\n \n\n\n\n In contrast, the most prominent abstractions in biological anatomy involve larger-scale structures: individual organs like the heart, or entire organ systems like the respiratory system. And so we wonder: is there a “respiratory system” or “heart” or “brain region” of an artificial neural network? Do neural networks have any emergent structures that we could study that are larger-scale than circuits?\n \n\n\n\n This article describes *branch specialization*, one of three larger “structural phenomena” we’ve been able observe in neural networks. (The other two, [equivariance](https://distill.pub/2020/circuits/equivariance/) and [weight banding](https://distill.pub/2020/circuits/weight-banding/), have separate dedicated articles.) Branch specialization occurs when neural network layers are split up into branches. The neurons and circuits tend to self-organize, clumping related functions into each branch and forming larger functional units – a kind of “neural network brain region.” We find evidence that these structures implicitly exist in neural networks without branches, and that branches are simply reifying structures that otherwise exist.\n \n\n\n\n The earliest example of branch specialization that we’re aware of comes from AlexNet. AlexNet is famous as a jump in computer vision, arguably starting the deep learning revolution, but buried in the paper is a fascinating, rarely-discussed detail.\n\n The first two layers of AlexNet are split into two branches which can’t communicate until they rejoin after the second layer. This structure was used to maximize the efficiency of training the model on two GPUs, but the authors noticed something very curious happened as a result. The neurons in the first layer organized themselves into two groups: black-and-white Gabor filters formed on one branch and low-frequency color detectors formed on the other branch.\n \n\n\n\n![](./Branch Specialization_files/Figure_1.png)\n\n\n[1](https://distill.pub/2020/circuits/branch-specialization/#figure-1). Branch specialization in the first two layers of AlexNet. Krizhevsky et al. observed the phenomenon we call branch specialization in the first layer of AlexNet by visualizing their weights to RGB channels; here, we use [feature visualization](https://distill.pub/2017/feature-visualization/) to show how this phenomenon extends to the second layer of each branch.\n \n\n\n\n\n\n Although the first layer of AlexNet is the only example of branch specialization we’re aware of being discussed in the literature, it seems to be a common phenomenon. We find that branch specialization happens in later hidden layers, not just the first layer. It occurs in both low-level and high-level features. It occurs in a wide range of models, including places you might not expect it – for example, residual blocks in resnets can functionally be branches and specialize. Finally, branch specialization appears to surface as a structural phenomenon in plain convolutional nets, even without any particular structure causing it.\n \n\n\n\n Is there a large-scale structure to how neural networks operate? How are features and circuits organized within the model? Does network architecture influence the features and circuits that form? Branch specialization hints at an exciting story related to all of these questions.\n \n\n\n## What is a branch?\n\n\n\n Many neural network architectures have *branches*, sequences of layers which temporarily don’t have access to “parallel” information which is still passed to later layers.\n\nInceptionV1\n has nine sets of four-way branches called “Inception blocks.” has several two-way branches.\nAlexNet\n\n\nResidual Networks aren’t typically thought of as having branches, but residual blocks can be seen as a type of branch.\n\n[2](https://distill.pub/2020/circuits/branch-specialization/#figure-2). Examples of branches in various types of neural network architectures.\n \n\n\n\n\n\n In the past, models with explicitly-labeled branches were popular (such as AlexNet and the Inception family of networks). In more recent years, these have become less common, but residual networks – which can be seen as implicitly having branches in their residual blocks – have become very common. We also sometimes see branched architectures develop automatically in neural architecture search, an approach where the network architecture is learned.\n \n\n\n\n The implicit branching of residual networks has some important nuances. At first glance, every layer is a two-way branch. But because the branches are combined together by addition, we can actually rewrite the model to reveal that the residual blocks can be understood as branches in parallel:\n \n\n\n\n\n\n\n\nWe typically think of residual blocks as sequential layers, building on top of each other.… but we can also conceptualize them as, to some extent, being parallel branches due to the skip connections. This means that residual blocks can potentially specialize.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n+\n\n\n\n\n\n\n\n[3](https://distill.pub/2020/circuits/branch-specialization/#figure-3). Residual blocks as branches in parallel.\n \n\n\n\n\n\n We typically see residual blocks specialize in very deep residual networks (e.g. ResNet-152). One hypothesis for why is that, in these models, the exact depth of a layer doesn’t matter and the branching aspect becomes more important than the sequential aspect.\n \n\n\n\n One of the conceptual weaknesses of normal branching models is that although branches can save parameters, it still requires a lot of parameters to mix values between branches. However, if you buy the branch interpretation of residual networks, you can see them as a strategy to sidestep this: residual networks intermix branches (e.g. block sparse weights) with low-rank connections (projecting all the blocks into the same sum and then back up). This seems like a really elegant way to handle branching. More practically, it suggests that analysis of residual networks might be well-served by paying close attention to the units in the blocks, and that we might expect the residual stream to be unusually polysemantic.\n \n\n\n## Why does branch specialization occur?\n\n\n\n Branch specialization is defined by features organizing between branches. In a normal layer, features are organized randomly: a given feature is just as likely to be any neuron in a layer. But in a branched layer, we often see features of a given type cluster to one branch. The branch has specialized on that type of feature.\n \n\n\n\n How does this happen? Our intuition is that there’s a positive feedback loop during training.\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA1\nB1\nC1\nD1\nA2\nB2\nD2\nD2\nThe first part of the branch is incentivized to form features relevant to the second half.\nThe second half of the branch prefers features which the first half provides primitives for.\n\n\n\n\n\n\n[4](https://distill.pub/2020/circuits/branch-specialization/#figure-4). Hypothetical positive feedback loop of branch specialization during training.\n \n\n\n\n\n\n Another way to think about this is that if you need to cut a neural network into pieces that have limited ability to communicate with each other, it makes sense to organize similar features close together, because they probably need to share more information.\n \n\n\n## Branch specialization beyond the first layer\n\n\n\n So far, the only concrete example we’ve shown of branch specialization is the first and second layer of AlexNet. What about later layers? AlexNet also splits its later layers into branches, after all. This seems to be unexplored, since studying features after the first layer is much harder.For the first layer, one can visualize the RGB weights; for later layers, one needs to use feature visualization.\n\n\n\n\n Unfortunately, branch specialization in the later layers of AlexNet is also very subtle. Instead of one overall split, it’s more like there’s dozens of small clusters of neurons, each cluster being assigned to a branch. It’s hard to be confident that one isn’t just seeing patterns in noise.\n \n\n\n\n But other models have very clear branch specialization in later layers. This tends to happen when a branch constitutes only a very small fraction of a layer, either because there are many branches or because one is much smaller than others. In these cases, the branch can specialize on a very small subset of the features that exist in a layer and reveal a clear pattern.\n \n\n\n\n For example, most of InceptionV1′s layers have a branched structure. The branches have varying numbers of units, and varying convolution sizes. The 5x5 branch is the smallest branch, and also has the largest convolution size. It’s often very specialized:\n\nThe 5x5 branch of mixed3a, a relatively early layer, is specialized on color detection, and especially black-and-white vs. color detection.\nmixed3a\\_5x5: \n\n\n It also contains a disproportionate number of boundary, eye, and fur detectors, many of which share sub-components with curves.\nThis branch contains all 30 of the curve-related features for this layer (all curves, double curves, circles, spirals, S-shape and more features, etc).\nmixed3b\\_5x5: \n\n\nThis branch appears to be specialized in complex shapes and 3D geometry detectors. We don’t have a full taxonomy of this layer to allow for a quantitative assessment.\nmixed4a\\_5x5: \n\n3D Geometry / Complex Shapes\nCurve Related\nBW vs Color\nFur/Eye/Face Related\nOther\nBoundary Detectors\nOther\nOther\nBrightness\nOther Color Contrast\n\n[5](https://distill.pub/2020/circuits/branch-specialization/#figure-5). Examples of branch specialization in `[mixed3a\\_5x5](https://distill.pub/2017/feature-visualization/appendix/googlenet/3a.html#3a-192)`, `[mixed3b\\_5x5](https://distill.pub/2017/feature-visualization/appendix/googlenet/3b.html#3b-320)`, and `[mixed4a\\_5x5](https://distill.pub/2017/feature-visualization/appendix/googlenet/4a.html#4a-396)`.\n \n\n\n\n\n\n This is exceptionally unlikely to have occurred by chance.\n\n For example, all 9 of the black and white vs. color detectors in `mixed3a` are in `mixed3a_5x5`, despite it only being 32 out of the 256 neurons in the layer. The probability of that happening by chance is less than 1/108. For a more extreme example, all 30 of the curve-related features in `mixed3b` are in `mixed3b_5x5`, despite it being only 96 out of the 480 neurons in the layer. The probability of that happening by chance is less than 1/1020.\n\n\n \n\n\n It’s worth noting one confounding factor which might be influencing the specialization. The 5x5 branches are the smallest branches, but also have larger convolutions (5x5 instead of 3x3 or 1x1) than their neighbors.There is something which suggests that the branching plays an essential role: mixed3a and mixed3b are adjacent layers which contain relatively similar features and are at the same scale. If it was only about convolution size, why don’t we see any curves in the `mixed3a_5x5` branch or color in the `mixed3b_5x5` branch?\n\n\n\n## Why is branch specialization consistent?\n\n\n\n Perhaps the most surprising thing about branch specialization is that the same branch specializations seem to occur again and again, across different architectures and tasks.\n \n\n\n\n For example, the branch specialization we observed in AlexNet – the first layer specializing into a black-and-white Gabor branch vs. a low-frequency color branch – is a surprisingly robust phenomenon. It occurs consistently if you retrain AlexNet. It also occurs if you train other architectures with the first few layers split into two branches. It even occurs if you train those models on other natural image datasets, like Places instead of ImageNet. Anecdotally, we also seem to see other types of branch specialization recur. For example, finding branches that seem to specialize in curve detection seems to be quite common (although InceptionV1′s `mixed3b_5x5` is the only one we’ve carefully characterized).\n \n\n\n\n So, why do the same branch specializations occur again and again?\n \n\n\n\n One hypothesis seems very tempting. Notice that many of the same features that form in normal, non-branched models also seem to form in branched models. For example, the first layer of both branched and non-branched models contain Gabor filters and color features. If the same features exist, presumably the same weights exist between them.\n \n\n\n\n Could it be that branching is just surfacing a structure that already exists? Perhaps there’s two different subgraphs between the weights of the first and second conv layer in a normal model, with relatively small weights between them, and when you train a branched model these two subgraphs latch onto the branches.\n\n (This would be directionally similar to work finding modular substructures within neural networks.)\n \n\n\n\n To test this, let’s look at models which have non-branched first and second convolutional layers. Let’s take the weights between them and perform a singular value decomposition (SVD) on the absolute values of the weights. This will show us the main factors of variation in which neurons connect to different neurons in the next layer (irrespective of whether those connections are excitatory or inhibitory).\n \n\n\n\n Sure enough, the singular vector (the largest factor of variation) of the weights between the first two convolutional layers of InceptionV1 is color.\n \n\n\n\n\n\n\n\nfirst convolutional layer\nNeurons in the \norganized by the left singular vectors of |W|.\n\n\nInceptionV1 (tf-slim version) trained on ImageNet.\n\nThe first singular vector separates color and black and white, meaning that’s the largest dimension of variation in which neurons connect to which in the next layer.\nGabor filters and color features are far apart, meaning they tend to connect to different features in the next layer.\n\n\n\nSingular Vector 1 (frequency?)\n\nSingular Vector 0 (color?)\n\n\n\n\nInceptionV1 trained on Places365\n\nOne more, the first singular vector separates color and black and white, meaning that’s the largest dimension of variation in which neurons connect to which in the next layer.\n\n\n\nSingular Vector 1 (frequency?)\n\nSingular Vector 0 (color?)\n\n\n\nSingular Vector 1 (frequency?)\n\nSingular Vector 0 (color?)\n\n\n\nSingular Vector 1 (frequency?)\n\nSingular Vector 0 (color?)\nsecond convolutional layer\nNeurons in the  organized by the right singular vectors of |W|.\n\n[6](https://distill.pub/2020/circuits/branch-specialization/#figure-6). Singular vectors for the first and second convolutional layers of InceptionV1, trained on ImageNet (above) or Places365 (below). One can think of neurons being plotted closer together in this diagram as meaning they likely tend to connect to similar neurons.\n\n \n\n\n\n\n\n We also see that the second factor appears to be [frequency](https://distill.pub/2020/circuits/frequency-edges/). This suggests an interesting prediction: perhaps if we were to split the layer into more than two branches, we’d also observe specialization in frequency in addition to color.\n \n\n\n\n This seems like it may be true. For example, here we see a high-frequency black-and-white branch, a mid-frequency mostly black-and-white branch, a mid-frequency color branch, and a low-frequency color branch.\n \n\n\n\n![](./Branch Specialization_files/Figure_7.png)\n\n\n[7](https://distill.pub/2020/circuits/branch-specialization/#figure-7). We constructed a small ImageNet model with the first layer split into four branches. The rest of the model is roughly an InceptionV1 architecture.\n \n\n\n\n\n## Parallels to neuroscience\n\n\n\n We’ve shown that branch specialization is one example of a structural phenomenon\u2009—\u2009a larger-scale structure in a neural network. It happens in a variety of situations and neural network architectures, and it happens with *consistency* – certain motifs of specialization, such as color, frequency, and curves, happen consistently across different architectures and tasks.\n \n\n\n\n Returning to our comparison with anatomy, although we hesitate to claim explicit parallels to neuroscience, it’s tempting to draw analogies between branch specialization and the existence of regions of the brain focused on particular tasks.\n The visual cortex, the auditory cortex, Broca’s area and Wernicke’s area\n \n The subspecialization within the V2 area of the primate visual cortex is another strong example from neuroscience. One type of stripe within V2 is sensitive to orientation or luminance, whereas the other type of stripe contains color-selective neurons.\n\n We are grateful to Patrick Mineault for noting this analogy, and for further noting that the high-frequency features are consistent with some of the known representations of high-level features in the primate V2 area.\n\n –\xa0these are all examples of brain areas with such consistent specialization across wide populations of people that neuroscientists and psychologists have been able to characterize as having remarkably consistent functions.\n\n As researchers without expertise in neuroscience, we’re uncertain how useful this connection is, but it may be worth considering whether branch specialization can be a useful model of how specialization might emerge in biological neural networks.\n \n\n\n\n![](./Branch Specialization_files/multiple-pages.svg)\n\n This article is part of the [Circuits thread](https://distill.pub/2020/circuits/), an experimental format collecting invited short articles and critical commentary delving into the inner workings of neural networks.\n \n\n\n[Visualizing Weights](https://distill.pub/2020/circuits/visualizing-weights/)\n[Weight Banding](https://distill.pub/2020/circuits/weight-banding/)\n\n\n .comment {\n background-color: hsl(54, 78%, 96%);\n border-left: solid hsl(54, 33%, 67%) 1px;\n padding: 1em;\n color: hsla(0, 0%, 0%, 0.67);\n margin-top: 1em;\n }\n\n .comment h3 {\n font-size: 100%;\n font-weight: bold;\n text-transform: uppercase;\n margin-top: 0px;\n }\n\n .comment .commenter-description {\n font-style: italic;\n margin-bottom: 1em;\n margin-top: 0px;\n }\n .comment .commenter-description, .comment .commenter-description a {\n color: #777;\n font-size: 80%;\n line-height: 160%;\n }\n .comment p {\n margin-bottom: 0px;\n }\n .comment div {\n margin-top: 1em;\n }\n\n \n\n### Comment\n\n\n\n**[Matthew Nolan](https://www.ed.ac.uk/discovery-brain-sciences/our-staff/research-groups/matthew-nolan)** is Professor of Neural Circuits and Computation at the Centre for Discovery Brain Sciences and Simons Initiative for the Developing Brain, University of Edinburgh.\n **Ian Hawes** is a PhD student in the Wellcome Trust programme in Translational Neuroscience at the University of Edinburgh.\n \n\nAs neuroscientists we’re excited by this work as it offers fresh theoretical perspectives on long-standing questions about how brains are organised and how they develop. Branching and specialisation are found throughout the brain. A well studied example is the dorsal and ventral visual streams, which are associated with spatial and non-spatial visual processing. At the microcircuit level neurons in each pathway are similar. However, recordings of neural activity demonstrate remarkable specialisation; classic experiments from the 1970s and 80s established the idea that the ventral stream enables identification of objects whereas the dorsal stream represents their location. Since then, much has been learned about signal processing in these pathways but fundamental questions such as why there are multiple streams and how they are established remain unanswered.\n\n\n\n\nFrom the perspective of a neuroscientist, a striking result from the investigation of branch specialization by Voss and her colleagues is that robust branch specialisation emerges in the absence of any complex branch specific design rules. Their analyses show that specialisation is similar within and across architectures, and across different training tasks. The implication here is that no specific instructions are required for branch specialisation to emerge. Indeed, their analyses suggest that it even emerges in the absence of predetermined branches. By contrast, the intuition of many neuroscientists would be that specialisation of different areas of the neocortex requires developmental mechanisms that are specific to each area. For neuroscientists aiming to understand how perceptual and cognitive functions of the brain arise, an important idea here is that developmental mechanisms that drive the separation of cortical pathways, such as the dorsal and ventral visual streams, may be absolutely critical.\n\n\n\n\nWhile the parallels between branch specialization in artificial neural networks and neural circuits in the brain are striking, there are clearly major differences and many outstanding questions. From the perspective of building artificial neural networks, we wonder if branch specific tuning of individual units and their connectivity rules would enhance performance? In the brain, there is good evidence that the activation functions of individual neurons are fine-tuned between and even within distinct neural circuits. If this fine tuning confers benefits to the brain then we might expect similar benefits in artificial networks. From the perspective of understanding the brain, we wonder whether branch specialisation could help make experimentally testable predictions? If artificial networks can be engineered with branches that have organisation similar to branching pathways in the brain, then manipulations to these networks could be compared to experimental manipulations achieved with optogenetic and chemogenetic strategies. Given that many brain disorders involve changes to specific neural populations, similar strategies could give insights into how these pathological changes alter brain functions. For example, very specific populations of neurons are disrupted in early stages of Alzheimer’s disease. By disrupting corresponding units in neural network models one could explore the resulting computational deficits and possible strategies for restoration of cognitive functions.", "bibliography_bbl": "", "bibliography_bib": [{"title": "Imagenet classification with deep convolutional neural networks"}, {"title": "Visualizing higher-layer features of a deep network"}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps"}, {"title": "Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks"}, {"title": "Feature Visualization"}, {"title": "Going deeper with convolutions"}, {"title": "Neural architecture search with reinforcement learning"}, {"title": "Neural networks are surprisingly modular"}, {"title": "Are Neural Nets Modular? Inspecting Functional Modularity Through Differentiable Weight Masks"}, {"title": "Segregation of form, color, and stereopsis in primate area 18"}, {"title": "Representation of Angles Embedded within Contour Stimuli in Area V2 of Macaque Monkeys"}]}
{"source": "distill", "source_filetype": "html", "converted_with": "python", "paper_version": "", "post_title": "A Discussion of "Adversarial Examples Are Not Bugs, They Are Features": Adversarial Example Researchers Need to Expand What is Meant by "Robustness"", "authors": ["Justin Gilmer", "Dan Hendrycks"], "date_published": "2019-08-06", "data_last_modified": "", "url": "", "abstract": "\n        This article is part of a discussion of the Ilyas et al. paper\n        “Adversarial examples are not bugs, they are features”.\n        You can learn more in the\n        \n            main discussion article\n        .\n    ", "author_comment": "", "journal_ref": "distill pub", "doi": "https://doi.org/10.23915/distill.00019.1", "primary_category": "", "categories": "", "citation_level": "", "main_tex_filename": "", "text": "\n\n #rebuttal,\n .comment-info {\n background-color: hsl(54, 78%, 96%);\n border-left: solid hsl(54, 33%, 67%) 1px;\n padding: 1em;\n color: hsla(0, 0%, 0%, 0.67);\n }\n\n #header-info {\n margin-top: 0;\n margin-bottom: 1.5rem;\n display: grid;\n grid-template-columns: 65px max-content 1fr;\n grid-template-areas:\n "icon explanation explanation"\n "icon back comment";\n grid-column-gap: 1.5em;\n }\n\n #header-info .icon-multiple-pages {\n grid-area: icon;\n padding: 0.5em;\n content: url(images/multiple-pages.svg);\n }\n\n #header-info .explanation {\n grid-area: explanation;\n font-size: 85%;\n }\n\n #header-info .back {\n grid-area: back;\n }\n\n #header-info .back::before {\n\n content: "←";\n margin-right: 0.5em;\n }\n\n #header-info .comment {\n grid-area: comment;\n scroll-behavior: smooth;\n }\n\n #header-info .comment::before {\n content: "↓";\n margin-right: 0.5em;\n }\n\n #header-info a.back,\n #header-info a.comment {\n font-size: 80%;\n font-weight: 600;\n border-bottom: none;\n text-transform: uppercase;\n color: #2e6db7;\n display: block;\n margin-top: 0.25em;\n letter-spacing: 0.25px;\n }\n\n\n\n\n This article is part of a discussion of the Ilyas et al. paper\n *“Adversarial examples are not bugs, they are features”.*\n You can learn more in the\n [main discussion article](https://distill.pub/2019/advex-bugs-discussion/) .\n \n\n\n[Other Comments](https://distill.pub/2019/advex-bugs-discussion/#commentaries)\n[Comment by Ilyas et al.](https://distill.pub/2019/advex-bugs-discussion/response-1/#rebuttal)\n\n\n The hypothesis in Ilyas et. al. is a special case of a more general principle that is well accepted in the\n distributional robustness literature\u2009—\u2009models lack robustness to distribution shift because they latch onto\n superficial correlations in the data. Naturally, the same principle also explains adversarial examples\n because they arise from a worst-case analysis of distribution shift. To obtain a more complete understanding\n of robustness, adversarial example researchers should connect their work to the more general problem of\n distributional robustness rather than remaining solely fixated on small gradient perturbations.\n \n\n\n## Detailed Response\n\n\n\n The main hypothesis in Ilyas et al. (2019) happens to be a special case of a more general principle that is\n commonly accepted in the robustness to distributional shift literature \n: a model’s lack of\n robustness is largely because the model latches onto superficial statistics in the data. In the image\n domain, these statistics may be unused by\u2009—\u2009and unintuitive to\u2009—\u2009humans, yet they may be useful for\n generalization in i.i.d. settings. Separate experiments eschewing gradient perturbations and studying\n robustness beyond adversarial perturbations show similar results. For example, a recent work \n demonstrates that models can generalize to the test examples by learning from high-frequency information\n that is both naturally occurring and also inconspicuous. Concretely, models were trained and tested with an\n extreme high-pass filter applied to the data. The resulting high-frequency features appear completely\n grayscale to humans, yet models are able to achieve 50% top-1 accuracy on ImageNet-1K solely from these\n natural features that usually are “invisible.” These hard-to-notice features can be made conspicuous by\n normalizing the filtered image to have unit variance pixel statistics in the figure below.\n \n\n\n\n![](./A Discussion of \"Adversarial Examples Are Not Bugs, They Are Features\"_ Adversarial Example Researchers Need to Expand What is Meant by \"Robustness\"_files/figure-1-cropped.png)\n\n[1](https://distill.pub/2019/advex-bugs-discussion/response-1/#figure-1)\n Models can achieve high accuracy using information from the input that would be unrecognizable\n to humans. Shown above are models trained and tested with aggressive high and low pass filtering applied\n to the inputs. With aggressive low-pass filtering, the model is still above 30% on ImageNet when the\n images appear to be simple globs of color. In the case of high-pass (HP) filtering, models can achieve\n above 50% accuracy using features in the input that are nearly invisible to humans. As shown on the\n right hand side, the high pass filtered images needed be normalized in order to properly visualize the\n high frequency features.\n \n\n\n Given the plethora of useful correlations that exist in natural data, we should expect that our models will\n learn to exploit them. However, models relying on superficial statistics can poorly generalize should these\n same statistics become corrupted after deployment. To obtain a more complete understanding of model\n robustness,  measured test error after perturbing every image in the test set by a\n Fourier basis vector,\n as shown in Figure 2. The naturally trained model is robust to low-frequency perturbations, but,\n interestingly, lacks robustness in the mid to high frequencies. In contrast, adversarial training improves\n robustness to mid- and high-frequency perturbations, while sacrificing performance on low frequency\n perturbations. For instance adversarial training degrades performance on the low-frequency fog corruption\n  from 85.7% to 55.3%. Adversarial training similarly degrades robustness to\n contrast and low-pass\n filtered noise. By taking a broader view of robustness beyond tiny ℓp\\ell\\_pℓp\u200b norm perturbations, we discover\n that adversarially trained models are actually not “robust.” They are instead biased towards different kinds\n of superficial statistics. As a result, adversarial training can sacrifice robustness in real-world\n settings.\n\n \n\n\n\n![](./A Discussion of \"Adversarial Examples Are Not Bugs, They Are Features\"_ Adversarial Example Researchers Need to Expand What is Meant by \"Robustness\"_files/figure-2-cropped.png)\n\n[2](https://distill.pub/2019/advex-bugs-discussion/response-1/#figure-2)\n Model sensitivity to additive noise aligned with different Fourier basis vectors on CIFAR-10.\n We fix the additive noise to have ℓ2\\ell\\_2ℓ2\u200b norm 4 and evaluate three models: a naturally trained model,\n an\n adversarially trained model, and a model trained with Gaussian data augmentation. Error rates are\n averaged over 1000 randomly sampled images from the test set. In the bottom row we show images perturbed\n with noise along the corresponding Fourier basis vector. The naturally trained model is highly sensitive\n to additive noise in all but the lowest frequencies. Both adversarial training and Gaussian data\n augmentation dramatically improve robustness in the higher frequencies while sacrificing the robustness\n of the naturally trained model in the lowest frequencies (i.e. in both models, blue area in the middle\n is smaller compared to that of the naturally trained model).\n \n\n\n How, then, can the research community create models that robustly generalize in the real world, given that\n adversarial training can harm robustness to distributional shift? To do so, the research community must take\n a broader view of robustness and accept that ℓp\\ell\\_pℓp\u200b adversarial robustness is highly limited and mostly\n detached from security and real-world robustness . While often thought an\n idiosyncratic quirk of deep\n neural network classifiers, adversarial examples are not a counterintuitive mystery plaguing otherwise\n superhuman classifiers. Instead, adversarial examples are in fact expected of models which lack robustness\n to noise . They should not be surprising given the brittleness observed in\n numerous synthetic\u2009—\u2009and even\n natural \u2009—\u2009conditions. Models reliably exhibit poor performance when they are\n evaluated on distributions\n slightly different from the training distribution. For all that, current benchmarks do not expose these\n failure modes. The upshot is that we need to design harder and more diverse test sets, and we should not\n continue to be singularly fixated on studying specific gradient perturbations. As we move forward in\n robustness research, we should focus on the various ways in which models are fragile, and design more\n comprehensive benchmarks accordingly . As long as models lack\n robustness to\n distributional shift, there will always be errors to find adversarially.\n\n \n\n\n\n To cite Ilyas et al.’s response, please cite their\n [collection of responses](https://distill.pub/2019/advex-bugs-discussion/original-authors/#citation).\n\n\n**Response Summary**: The demonstration of models that learn from\n high-frequency components of the data is interesting and nicely aligns with our\n findings. Now, even though susceptibility to noise could indeed arise from\n non-robust useful features, this kind of brittleness (akin to adversarial examples)\n of ML models has been so far predominantly viewed as a consequence of model\n “bugs” that will be eliminated by “better” models. Finally, we agree that our\n models need to be robust to a much broader set of perturbations\u2009—\u2009expanding the\n set of relevant perturbations will help identify even more non-robust features\n and further distill the useful features we actually want our models to rely on.\n \n\n\n**Response**: The fact that models can learn to classify correctly based\n purely on the high-frequency component of the training set is neat! This nicely\n complements one of our [takeaways](https://distill.pub/2019/advex-bugs-responses/rebuttal/#takeaway1): models\n will rely on useful features even if these features appear incomprehensible to humans.\n\n\n Also, while non-robustness to noise can be an indicator of models using\n non-robust useful features, this is not how the phenomenon was predominantly viewed.\n More often than not, the brittleness of ML models to noise was instead regarded\n as an innate shortcoming of the models, e.g., due to poor margins. (This view is\n even more prevalent in the adversarial robustness community.) Thus, it was often\n expected that progress towards “better”/”bug-free” models will lead to them\n being more robust to noise and adversarial examples.\n\n\n Finally, we fully agree that the set of LpL\\_pLp\u200b-bounded perturbations is a very\n small subset of the perturbations we want our models to be robust to. Note,\n however, that the focus of our work is human-alignment\u2009—\u2009to that end, we\n demonstrate that models rely on features sensitive to patterns that are\n imperceptible to humans. Thus, the existence of other families of\n incomprehensible but useful features would provide even more support for our\n thesis\u2009—\u2009identifying and characterizing such features is an interesting area for\n future research.\n\n\n\n\n You can find more responses in the  [main discussion article](https://distill.pub/2019/advex-bugs-discussion/).\n\n", "bibliography_bbl": "", "bibliography_bib": [{"title": "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations"}, {"title": "Measuring the tendency of CNNs to Learn Surface Statistical Regularities"}, {"title": "Nightmare at Test Time: Robust Learning by Feature Deletion"}, {"title": "A Robust Minimax Approach to Classification"}, {"title": "Generalisation in humans and deep neural networks"}, {"title": "A Fourier Perspective on Model Robustness in Computer Vision"}, {"title": "Motivating the Rules of the Game for Adversarial Example Research"}, {"title": "Adversarial Examples Are a Natural Consequence of Test Error in Noise"}, {"title": "Robustness of classifiers: from adversarial to random noise"}, {"title": "Natural Adversarial Examples"}, {"title": "{MNIST-C:} {A} Robustness Benchmark for Computer Vision"}, {"title": "{NICO:} {A} Dataset Towards Non-I.I.D. Image Classification"}, {"title": "Do ImageNet Classifiers Generalize to ImageNet?"}, {"title": "The Elephant in the Room"}, {"title": "Using Videos to Evaluate Image Model Robustness"}]}
{"source": "distill", "source_filetype": "html", "converted_with": "python", "paper_version": "", "post_title": "A Discussion of "Adversarial Examples Are Not Bugs, They Are Features": Adversarially Robust Neural Style Transfer", "authors": ["Reiichiro Nakano"], "date_published": "2019-08-06", "data_last_modified": "", "url": "", "abstract": "This article is part of a discussion of the Ilyas et al. paper “Adversarial examples are not bugs, they are features”. You can learn more in the main discussion article.", "author_comment": "", "journal_ref": "distill pub", "doi": "https://doi.org/10.23915/distill.00019.4", "primary_category": "", "categories": "", "citation_level": "", "main_tex_filename": "", "text": "This article is part of a discussion of the Ilyas et al. paper\n *“Adversarial examples are not bugs, they are features”.*\n You can learn more in the\n [main discussion article](https://distill.pub/2019/advex-bugs-discussion/) .\n \n\n\n[Other Comments](https://distill.pub/2019/advex-bugs-discussion/#commentaries)\n[Comment by Ilyas et al.](https://distill.pub/2019/advex-bugs-discussion/response-4/#rebuttal)\n\n\n A figure in Ilyas, et. al. that struck me as particularly\n interesting\n was the following graph showing a correlation between adversarial transferability between architectures and\n their\n tendency to learn similar non-robust features.\n \n\n\n\n![](./Adversarially Robust Neural Style Transfer_files/transferability.png)\n\n Adversarial transferability vs test accuracy of different architectures trained on ResNet-50′s\n non-robust features.\n \n\n\n One way to interpret this graph is that it shows how well a particular architecture is able to capture\n non-robust features in an image.\n Since the non-robust features are defined by the non-robust features ResNet-50 captures,\n NRFresnetNRF\\_{resnet}NRFresnet\u200b, what this graph really shows is how well an architecture captures NRFresnetNRF\\_{resnet}NRFresnet\u200b.\n \n\n\n\n\n Notice how far back VGG is compared to the other models.\n \n\n\n\n In the unrelated field of neural style transfer, VGG-based neural networks are also quite special since non-VGG architectures are\n known to not work very well This phenomenon is discussed at length in [this\n Reddit thread](https://www.reddit.com/r/MachineLearning/comments/7rrrk3/d_eat_your_vggtables_or_why_does_neural_style/). without some sort of parameterization trick .\n The above interpretation of the graph provides an alternative explanation for this phenomenon.\n **Since VGG is unable to capture non-robust features as well as other architectures, the outputs for style\n transfer actually look more correct to humans!**\nTo follow this argument, note that the perceptual losses used in neural style transfer are\n dependent on matching features learned by a separately trained image classifier. If these learned\n features don’t make sense to humans (non-robust features), the outputs for neural style transfer won’t\n make sense either.\n\n\n\n\n Before proceeding, let’s quickly discuss the results obtained by Mordvintsev, et. al. in [Differentiable Image\n Parameterizations](https://distill.pub/2018/differentiable-parameterizations/), where they show that non-VGG architectures can work for style transfer by using a\n simple technique previously established in feature visualization.\n In their experiment, instead of optimizing the output image in RGB space, they optimize it in Fourier space,\n and run the image through a series of transformations (e.g jitter, rotation, scaling) before passing it\n through the neural network.\n \n\n\n\n Can we reconcile this result with our hypothesis linking neural style transfer and non-robust features?\n \n\n\n\n One possible theory is that all of these image transformations *weaken* or even *destroy*\n non-robust features.\n Since the optimization can no longer reliably manipulate non-robust features to bring down the loss, it is\n forced to use robust features instead, which are presumably more resistant to the applied image\n transformations (a rotated and jittered flappy ear still looks like a flappy ear).\n \n\n\n## A quick experiment\n\n\n\n Testing our hypothesis is fairly straightforward:\n Use an adversarially robust classifier for neural style transfer and see\n what happens.\n \n\n\n\n I evaluated a regularly trained (non-robust) ResNet-50 with a robustly trained ResNet-50 from Engstrom, et.\n al. on their performance on neural style transfer.\n For comparison, I performed the same algorithm with a regular VGG-19\n \xa0.\n \n\n\n\n To ensure a fair comparison despite the different networks having different optimal hyperparameters, I\n performed a small grid search for each image and manually picked the best output per network.\n Further details can be read in a footnote\n \n L-BFGS was used for optimization as it showed faster convergence\n over Adam.\n For ResNet-50, the style layers used were the ReLu outputs after each of the 4 residual blocks,\n [relu2\\_x,relu3\\_x,relu4\\_x,relu5\\_x][relu2\\\\_x, relu3\\\\_x, relu4\\\\_x, relu5\\\\_x][relu2\\_x,relu3\\_x,relu4\\_x,relu5\\_x] while the content layer used was relu4\\_xrelu4\\\\_xrelu4\\_x.\n For VGG-19, style layers [relu1\\_1,relu2\\_1,relu3\\_1,relu4\\_1,relu5\\_1][relu1\\\\_1,relu2\\\\_1,relu3\\\\_1,relu4\\\\_1,relu5\\\\_1][relu1\\_1,relu2\\_1,relu3\\_1,relu4\\_1,relu5\\_1] were used with a content layer\n relu4\\_2relu4\\\\_2relu4\\_2.\n In VGG-19, max pooling layers were replaced with avg pooling layers, as stated in Gatys, et. al.\n \n or observed in the accompanying Colaboratory notebook.\n \n\n\n\n The results of this experiment can be explored in the diagram below.\n \n\n\n\n #style-transfer-slider.juxtapose {\n max-height: 512px;\n max-width: 512px;\n }\n \n\n\n**Content image**\n\n\n\n\n\n* ![](./Adversarially Robust Neural Style Transfer_files/ben.jpg)\n* ![](./Adversarially Robust Neural Style Transfer_files/dancing.jpg)\n* ![](./Adversarially Robust Neural Style Transfer_files/tubingen.jpg)\n* ![](./Adversarially Robust Neural Style Transfer_files/stata.jpg)\n\n\n**Style image**\n\n\n\n\n\n* ![](./Adversarially Robust Neural Style Transfer_files/scream.jpg)\n* ![](./Adversarially Robust Neural Style Transfer_files/starrynight.jpg)\n* ![](./Adversarially Robust Neural Style Transfer_files/woman.jpg)\n* ![](./Adversarially Robust Neural Style Transfer_files/picasso.jpg)\n\n\n\xa0 Compare VGG or Robust\n ResNet\n![](./Adversarially Robust Neural Style Transfer_files/ben_scream_nonrobust.jpg)Non-robust ResNet50![](./Adversarially Robust Neural Style Transfer_files/ben_scream_robust.jpg)Robust ResNet50\n\n[Reproduce in a Notebook](https://colab.research.google.com/github/reiinakano/adversarially-robust-neural-style-transfer/blob/master/Robust_Neural_Style_Transfer.ipynb)\n\n\"use strict\";\n\n// I don\"t know how to write JavaScript without a bundler. Please someone save me.\n\n(function () {\n\n // Initialize slider\n var currentContent = \"ben\";\n var currentStyle = \"scream\";\n var currentLeft = \"nonrobust\";\n\n var compareVGGCheck = document.getElementById("check-compare-vgg");\n var styleTransferSliderDiv = document.getElementById("style-transfer-slider");\n\n function refreshSlider() {\n while (styleTransferSliderDiv.firstChild) {\n styleTransferSliderDiv.removeChild(styleTransferSliderDiv.firstChild);\n }\n var imgPath1 = \"images/style-transfer/\" + currentContent + \"\\_\" + currentStyle + \"\\_\" + currentLeft + \".jpg\";\n var imgPath2 = \"images/style-transfer/\" + currentContent + \"\\_\" + currentStyle + \"\\_robust.jpg\";\n new juxtapose.JXSlider(\"#style-transfer-slider\", [{\n src: imgPath1, // TODO: Might need to use absolute\\_url?\n label: currentLeft === \"nonrobust\" ? \"Non-robust ResNet50\" : \"VGG\"\n }, {\n src: imgPath2,\n label: \"Robust ResNet50\"\n }], {\n animate: true,\n showLabels: true,\n showCredits: false,\n startingPosition: "50%",\n makeResponsive: true\n });\n }\n\n refreshSlider();\n\n compareVGGCheck.onclick = function (evt) {\n currentLeft = evt.target.checked ? \"vgg\" : \"nonrobust\";\n refreshSlider();\n };\n\n // Initialize selector\n $("#content-select").imagepicker({\n changed: function changed(oldVal, newVal, event) {\n currentContent = newVal;\n refreshSlider();\n }\n });\n $("#style-select").imagepicker({\n changed: function changed(oldVal, newVal, event) {\n currentStyle = newVal;\n refreshSlider();\n }\n });\n})();\n\n Success!\n The robust ResNet shows drastic improvement over the regular ResNet.\n Remember, all we did was switch the ResNet’s weights, the rest of the code for performing style transfer is\n exactly the same!\n \n\n\n\n A more interesting comparison can be done between VGG-19 and the robust ResNet.\n At first glance, the robust ResNet’s outputs seem on par with VGG-19.\n Looking closer, however, the ResNet’s outputs seem slightly noisier and exhibit some artifacts\n This is more obvious when the output image is initialized not with the content image, but with\n Gaussian noise..\n \n\n\n\n\n\n\n![](./Adversarially Robust Neural Style Transfer_files/vgg_texture.jpg)\n\n\n\n![](./Adversarially Robust Neural Style Transfer_files/vgg_texture.jpg)\n\n\n Texture synthesized with VGG.  \n\n*Mild artifacts.*\n\n\n\n\n![](./Adversarially Robust Neural Style Transfer_files/resnet_texture.jpg)\n\n\n\n![](./Adversarially Robust Neural Style Transfer_files/resnet_texture.jpg)\n\n\n Texture synthesized with robust ResNet.  \n\n*Severe artifacts.*\n\n\n\n\n\n A comparison of artifacts between textures synthesized by VGG and ResNet.\n Interact by hovering around the images.\n This diagram was repurposed from\n [Deconvolution and Checkerboard Artifacts](https://distill.pub/2016/deconv-checkerboard/) \n by Odena, et. al.\n \n\n It is currently unclear exactly what causes these artifacts.\n One theory is that they are checkerboard artifacts\n caused by\n non-divisible kernel size and stride in the convolution layers.\n They could also be artifacts caused by the presence of max pooling layers\n in ResNet.\n An interesting implication is that these artifacts, while problematic, seem orthogonal to the\n problem that\n adversarial robustness solves in neural style transfer.\n \n\n\n## VGG remains a mystery\n\n\n\n Although this experiment started because of an observation about a special characteristic of VGG\n nets, it\n did not provide an explanation for this phenomenon.\n Indeed, if we are to accept the theory that adversarial robustness is the reason VGG works out of\n the box\n with neural style transfer, surely we’d find some indication in existing literature that VGG is\n naturally\n more robust than other architectures.\n \n\n\n\n A few papers\n indeed show\n that VGG architectures are slightly more robust than ResNet.\n However, they also show that AlexNet, not known to work well\n for\n neural style transferAs shown by Dávid Komorowicz\n in\n this [blog post](https://dawars.me/neural-style-transfer-deep-learning/).\n , is\n *above* VGG in terms of this “natural robustness”.\n \n\n\n\n Perhaps adversarial robustness just happens to incidentally fix or cover up the true reason non-VGG\n architectures fail at style transfer (or other similar algorithms\n \n In fact, neural style transfer is not the only pretrained classifier-based iterative image\n optimization\n technique that magically works better with adversarial robustness. In Engstrom, et. al., they show that feature visualization via activation\n maximization works on robust classifiers *without*\n enforcing\n any priors or regularization (e.g. image transformations and decorrelated parameterization) used\n by\n previous work. In a recent chat with Chris\n Olah, he\n pointed out that the aforementioned feature visualization techniques actually work well on VGG\n *without* these priors, just like style transfer!\n \n ) i.e. adversarial robustness is a sufficient but unnecessary condition for good style transfer.\n Whatever the reason, I believe that further examination of VGG is a very interesting direction for\n future\n work.\n \n\n\n\n To cite Ilyas et al.’s response, please cite their\n [collection of responses](https://distill.pub/2019/advex-bugs-discussion/original-authors/#citation).\n\n\n**Response Summary**: Very interesting\n results, highlighting the effect of non-robust features and the utility of\n robust models for downstream tasks. We’re excited to see what kind of impact\n robustly trained models will have in neural network art! We were also really\n intrigued by the mysteriousness of VGG in the context of style transfer\n. As such, we took a\n deeper dive which found some interesting links between robustness and style\n transfer that suggest that perhaps robustness does indeed play a role here. \n\n\n**Response**: These experiments are really cool! It is interesting that\n preventing the reliance of a model on non-robust features improves performance\n on style transfer, even without an explicit task-related objective (i.e. we\n didn’t train the networks to be better for style transfer). \n\n\n We also found the discussion of VGG as a “mysterious network” really\n interesting\u2009—\u2009it would be valuable to understand what factors drive style transfer\n performance more generally. Though not a complete answer, we made a couple of\n observations while investigating further: \n\n\n*Style transfer does work with AlexNet:* One wrinkle in the idea that\n robustness is the “secret ingredient” to style transfer could be that VGG is not\n the most naturally robust network\u2009—\u2009AlexNet is. However, based on our own\n testing, style transfer does seem to work with AlexNet out-of-the-box, as\n long as we use a few early layers in the network (in a similar manner to\n VGG): \n\n\n\n![](./Adversarially Robust Neural Style Transfer_files/alexnetworks.png)\n\n Style transfer using AlexNet, using conv\\_1 through conv\\_4.\n \n\n\n Observe that even though style transfer still works, there are checkerboard\n patterns emerging\u2009—\u2009this seems to be a similar phenomenon to the one noticed\n in the comment in the context of robust models.\n This might be another indication that these two phenomena (checkerboard\n patterns and style transfer working) are not as intertwined as previously\n thought.\n \n\n\n*From prediction robustness to layer robustness:*  Another\n potential wrinkle here is that both AlexNet and VGG are not that\n much more robust than ResNets (for which style transfer completely fails),\n and yet seem to have dramatically better performance. To try to\n explain this, recall that style transfer is implemented as a minimization of a\n combined objective consisting of a style loss and a content loss. We found,\n however, that the network we use to compute the\n style loss is far more important\n than the one for the content loss. The following demo illustrates this\u2009—\u2009we can\n actually use a non-robust ResNet for the content loss and everything works just\n fine:\n\n\n\n![](./Adversarially Robust Neural Style Transfer_files/stylematters.png)\n\nStyle transfer seems to be rather\n invariant to the choice of content network used, and very sensitive\n to the style network used.\n\n\nTherefore, from now on, we use a fixed ResNet-50 for the content loss as a\n control, and only worry about the style loss. \n\n\nNow, note that the way that style loss works is by using the first few\n layers of the relevant network. Thus, perhaps it is not about the robustness of\n VGG’s predictions, but instead about the robustness of the layers that we actually use\n for style transfer? \n\n\n To test this hypothesis, we measure the robustness of a layer fff as:\n \n\n\nR(f)=Ex1∼D[maxx′∥f(x′)−f(x1)∥2]Ex1,x2∼D[∥f(x1)−f(x2)∥2]\n R(f) = \\frac{\\mathbb{E}\\_{x\\_1\\sim D}\\left[\\max\\_{x’} \\|f(x’) - f(x\\_1)\\|\\_2 \\right]}\n {\\mathbb{E}\\_{x\\_1, x\\_2 \\sim D}\\left[\\|f(x\\_1) - f(x\\_2)\\|\\_2\\right]}\n R(f)=Ex1\u200b,x2\u200b∼D\u200b[∥f(x1\u200b)−f(x2\u200b)∥2\u200b]Ex1\u200b∼D\u200b[maxx′\u200b∥f(x′)−f(x1\u200b)∥2\u200b]\u200b\n Essentially, this quantity tells us how much we can change the\n output of that layer f(x)f(x)f(x) within a small ball, normalized by how far apart\n representations are between images in general. We’ve plotted this value for\n the first few layers in a couple of different networks below: \n\n\n\n![](./Adversarially Robust Neural Style Transfer_files/robustnesses.png)\n\nThe robustness R(f)R(f)R(f) of the first\n four layers of VGG16, AlexNet, and robust/standard ResNet-50\n trained on ImageNet.\n\n\n Here, it becomes clear that, the first few layers of VGG and AlexNet are\n actually almost as robust as the first few layers of the robust ResNet!\n This is perhaps a more convincing indication that robustness might have\n something to with VGG’s success in style transfer after all.\n \n\n\n Finally, suppose we restrict style transfer to only use a single layer of\n the network when computing the style lossUsually style transfer uses\n several layers in the loss function to get the most visually appealing results\u2009—\u2009here we’re only interested in whether or not style transfer works (i.e.\n actually confers some style onto the image).. Again, the more\n robust layers seem to indeed work better for style transfer! Since all of the\n layers in the robust ResNet are robust, style transfer yields non-trivial\n results even using the last layer alone. Conversely, VGG and AlexNet seem to\n excel in the earlier layers (where they are non-trivially robust) but fail when\n using exclusively later (non-robust) layers: \n\n\n\n![](./Adversarially Robust Neural Style Transfer_files/styletransfer.png)\n\n\nStyle transfer using a single layer. The\n names of the layers and their robustness R(f)R(f)R(f) are printed below\n each style transfer result. We find that for both networks, the robust\n layers seem to work (for the robust ResNet, every layer is robust).\n\n\n Of course, there is much more work to be done here, but we are excited\n to see further work into understanding the role of both robustness and the VGG\n in network-based image manipulation. \n\n\n\n\n You can find more responses in the  [main discussion article](https://distill.pub/2019/advex-bugs-discussion/).", "bibliography_bbl": "", "bibliography_bib": [{"title": "Adversarial examples are not bugs, they are features"}, {"title": "Very deep convolutional networks for large-scale image recognition"}, {"title": "A Neural Algorithm of Artistic Style"}, {"title": "Differentiable Image Parameterizations"}, {"title": "Feature Visualization"}, {"title": "Learning Perceptually-Aligned Representations via Adversarial Robustness"}, {"title": "On the limited memory BFGS method for large scale optimization"}, {"title": "Deconvolution and checkerboard artifacts"}, {"title": "Geodesics of learned representations"}, {"title": "Batch Normalization is a Cause of Adversarial Vulnerability"}, {"title": "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations"}, {"title": "Is Robustness the Cost of Accuracy? - A Comprehensive Study on the Robustness of 18 Deep Image Classification Models"}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks"}, {"title": "Neural Style transfer with Deep Learning"}, {"title": "The Building Blocks of Interpretability"}]}
{"source": "distill", "source_filetype": "html", "converted_with": "python", "paper_version": "", "post_title": "A Discussion of "Adversarial Examples Are Not Bugs, They Are Features": Learning from Incorrectly Labeled Data", "authors": ["Eric Wallace"], "date_published": "2019-08-06", "data_last_modified": "", "url": "", "abstract": "This article is part of a discussion of the Ilyas et al. paper “Adversarial examples are not bugs, they are features”. You can learn more in the main discussion article.", "author_comment": "", "journal_ref": "distill pub", "doi": "https://doi.org/10.23915/distill.00019.6", "primary_category": "", "categories": "", "citation_level": "", "main_tex_filename": "", "text": "This article is part of a discussion of the Ilyas et al. paper\n *“Adversarial examples are not bugs, they are features”.*\n You can learn more in the\n [main discussion article](https://distill.pub/2019/advex-bugs-discussion/) .\n \n\n\n[Other Comments](https://distill.pub/2019/advex-bugs-discussion/#commentaries)\n[Comment by Ilyas et al.](https://distill.pub/2019/advex-bugs-discussion/response-6/#rebuttal)\n\n\n Section 3.2 of Ilyas et al. (2019) shows that training a model on only adversarial errors leads to\n non-trivial generalization on the original test set. We show that these experiments are a specific case of\n learning from errors. We start with a counterintuitive result\u2009—\u2009we take a completely mislabeled training set\n (without modifying the inputs) and use it to train a model that generalizes to the original test set. We\n then show that this result, and the results of Ilyas et al. (2019), are a special case of model\n distillation. In particular, since the incorrect labels are generated using a trained model, information\n about the trained model is being “leaked” into the dataset.\n We begin with the following question: what if we took the images in the training set (without any\n adversarial perturbations) and mislabeled them? Since the inputs are unmodified and mislabeled, intuition\n says that a model trained on this dataset should not generalize to the correctly-labeled test set.\n Nevertheless, we show that this intuition fails\u2009—\u2009a model *can* generalize.\n We first train a ResNet-18 on the CIFAR-10 training set for two epochs. The model reaches a training\n accuracy of 62.5% and a test accuracy of 63.1%. Next, we run the model on all of the 50,000 training data\n points and relabel them according to the model’s predictions. Then, we filter out *all the correct\n predictions*. We are now left with an incorrectly labeled training set of size 18,768. We show four\n examples on the left of the Figure below:\n \n\n\n\n![](./A Discussion of \"Adversarial Examples Are Not Bugs, They Are Features\"_ Learning from Incorrectly Labeled Data_files/image1.png)\n\n[1](https://distill.pub/2019/advex-bugs-discussion/response-6/#figure-1)\n\n\n\n We then randomly initialize a new ResNet-18 and train it only on this mislabeled dataset. We train for 50\n epochs and reach an accuracy of 49.7% on the *original* test set. The new model has only ever seen\n incorrectly labeled, unperturbed images but can still non-trivially generalize.\n \n\n\n## This is Model Distillation Using Incorrect Predictions\n\n\n\n How can this model and the models in Ilyas et al. (2019) generalize without seeing any correctly labeled\n data? Here, we show that since the incorrect labels are generated using a trained model, information is\n being “leaked” about that trained model into the mislabeled examples. In particular, this an indirect form\n of model distillation\u2009—\u2009training on this dataset allows a new\n model to somewhat recover the features of the original model.\n \n\n\n\n We first illustrate this distillation phenomenon using a two-dimensional problem. Then, we explore other\n peculiar forms of distillation for neural networks\u2009—\u2009-we transfer knowledge despite the inputs being from\n another task.\n \n\n\n### Two-dimensional Illustration of Model Distillation\n\n\n\n We construct a dataset of adversarial examples using a two-dimensional binary classification problem. We\n generate 32 random two-dimensional data points in [0,1]2[0,1]^2[0,1]2 and assign each point a random binary label. We\n then train a small feed-forward neural network on these examples, predicting 32/32 of the examples correctly\n (panel (a) in the Figure below).\n \n\n\n\n![](./A Discussion of \"Adversarial Examples Are Not Bugs, They Are Features\"_ Learning from Incorrectly Labeled Data_files/image2.png)\n\n[2](https://distill.pub/2019/advex-bugs-discussion/response-6/#figure-2)\n\n\n\n Next, we create adversarial examples for the original model using an l∞l\\_{\\infty}l∞\u200b ball of radius\n ϵ=0.12\\epsilon=0.12ϵ=0.12. In panel (a) of the Figure above, we display the ϵ\\epsilonϵ-ball around each training\n point. In panel (b), we show the adversarial examples which cause the model to change its prediction (from\n correct to incorrect). We train a new feed-forward neural network on this dataset, resulting in the model in\n panel (c).\n \n\n\n\n Although this new model has never seen a correctly labeled example, it is able to perform non-trivially on\n the original dataset, predicting 23/3223/3223/32 of the inputs correctly (panel (d) in the Figure). The new model’s\n decision boundary loosely matches the original model’s decision boundary, i.e., the original model has been\n somewhat distilled after training on its adversarial examples. This two-dimensional problem presents an\n illustrative version of the intriguing result that distillation can be performed using incorrect\n predictions.\n\n \n\n\n### \n Other Peculiar Forms of Distillation\n\n\n\n Our experiments show that we can distill models using mislabeled examples. In what other peculiar ways can\n we learn about the original model? Can we use only *out-of-domain* data?\n \n\n\n\n We train a simple CNN model on MNIST, reaching 99.1% accuracy. We then run this model on the FashionMNIST\n training set and save its argmax predictions. The resulting dataset is nonsensical to humans\u2009—\u2009a “dress” is\n labeled as an “8″.\n \n\n\n\n![](./A Discussion of \"Adversarial Examples Are Not Bugs, They Are Features\"_ Learning from Incorrectly Labeled Data_files/image3.png)\n\n[3](https://distill.pub/2019/advex-bugs-discussion/response-6/#figure-3)\n\n\n\n We then initialize a new CNN model and train it on this mislabeled FashionMNIST data. The resulting model\n reaches 91.04% accuracy on the MNIST test set. Furthermore, if we normalize the FashionMNIST images using\n the mean and variance statistics for MNIST, the model reaches 94.5% accuracy on the MNIST test set. This is\n another instance of recovering a functionally similar model to the original despite the new model only\n training on erroneous predictions.\n \n\n\n### \n Summary\n\n\n\n These results show that training a model using mislabeled adversarial examples is a special case of learning\n from prediction errors. In other words, the perturbations added to adversarial examples in Section 3.2 of\n Ilyas et al. (2019) are not necessary to enable learning.\n \n\n\n\n To cite Ilyas et al.’s response, please cite their\n [collection of responses](https://distill.pub/2019/advex-bugs-discussion/original-authors/#citation).\n\n\n**Response\n Summary**: Note that since our experiments work across different architectures,\n “distillation” in weight space does not occur. The only distillation that can\n arise is “feature space” distillation, which is actually exactly our hypothesis.\n In particular, feature-space distillation would not work in [World 1](https://distill.pub/2019/advex-bugs-discussion/original-authors/#world1)\u2009—\u2009if the\n adversarial examples we generated did not exploit useful features, we should not\n have been able to “distill” a useful model from them. (In fact, one might think\n of normal model training as just “feature distillation” of the humans that\n labeled the dataset.) Furthermore, the hypothesis that all we need is enough\n model-consistent points in order to recover a model, seems to be disproven by\n Preetum’s [“bugs-only dataset”](https://distill.pub/2019/advex-bugs-discussion/response-5)\n and other (e.g. ) settings. \n**Response**: Since our experiments work across different architectures,\n “distillation” in weight space cannot arise. Thus, from what we understand, the\n “distillation” hypothesis suggested here is referring to “feature distillation”\n (i.e. getting models which use the same features as the original), which is\n actually precisely our hypothesis too. Notably, this feature distillation would\n not be possible if adversarial examples did not rely on “flipping” features that\n are good for classification (see [World\n 1](https://distill.pub/2019/advex-bugs-discussion/original-authors/#world1) and\n [World 2](https://distill.pub/2019/advex-bugs-discussion/original-authors/#world2))\u2009—\u2009in that case, the distilled\n model would only use features that generalize poorly, and would thus generalize\n poorly itself. \n\n\n Moreover, we would argue that in the experiments presented (learning from\n mislabeled data), the same kind of distillation is happening. For instance, a\n moderately accurate model might associate “green background” with “frog” thus\n labeling “green” images as “frogs” (e.g., the horse in the comment’s figure).\n Training a new model on this dataset will thus associate “green” with “frog”\n achieving non-trivial accuracy on the test set (similarly for the “learning MNIST\n from Fashion-MNIST” experiment in the comment). This corresponds exactly to\n learning features from labels, akin to how deep networks “distill” a good\n decision boundary from human annotators. In fact, we find these experiments\n a very interesting illustration of feature distillation that complements\n our findings. \n\n\n We also note that an analogy to logistic regression here is only possible\n due to the low VC-dimension of linear classifiers (namely, these classifiers\n have dimension ddd). In particular, given any classifier with VC-dimension\n kkk, we need at least kkk points to fully specify the classifier. Conversely, neural\n networks have been shown to have extremely large VC-dimension (in particular,\n bigger than the size of the training set ). So even though\n labelling d+1d+1d+1 random\n points model-consistently is sufficient to recover a linear model, it is not\n necessarily sufficient to recover a deep neural network. For instance, Milli et\n al.  are not able to reconstruct a ResNet-18\n using only its predictions on random Gaussian inputs. (Note that we are using a\n ResNet-50 in our experiments.) \n\n\n Finally, it seems that the only potentially problematic explanation for\n our experiments (namely, that enough model-consistent points can recover a\n classifier) is [disproved by Preetum’s experiment](https://distill.pub/2019/advex-bugs-discussion/response-5).\n In particular, Preetum is able to design a\n dataset where training on mislabeled inputs *that are model-consistent*\n does not at all recover the decision boundary of the original model. More\n generally, the “model distillation” perspective raised here is unable to\n distinguish between the dataset created by Preetum below, and those created\n with standard PGD (as in our D^det\\widehat{\\mathcal{D}}\\_{det}D\ndet\u200b and\n D^rand\\widehat{\\mathcal{D}}\\_{rand}D\nrand\u200b datasets).\n \n\n\n\n\n You can find more responses in the  [main discussion article](https://distill.pub/2019/advex-bugs-discussion/).", "bibliography_bbl": "", "bibliography_bib": [{"title": "Distilling the Knowledge in a Neural Network"}, {"title": "Model reconstruction from model explanations"}, {"title": "Understanding deep learning requires rethinking generalization"}]}
{"source": "distill", "source_filetype": "html", "converted_with": "python", "paper_version": "", "post_title": "A Discussion of "Adversarial Examples Are Not Bugs, They Are Features": Two Examples of Useful, Non-Robust Features", "authors": ["Gabriel Goh"], "date_published": "2019-08-06", "data_last_modified": "", "url": "", "abstract": "This article is part of a discussion of the Ilyas et al. paper “Adversarial examples are not bugs, they are features”. You can learn more in the main discussion article.", "author_comment": "", "journal_ref": "distill pub", "doi": "https://doi.org/10.23915/distill.00019.3", "primary_category": "", "categories": "", "citation_level": "", "main_tex_filename": "", "text": "This article is part of a discussion of the Ilyas et al. paper\n *“Adversarial examples are not bugs, they are features”.*\n You can learn more in the\n [main discussion article](https://distill.pub/2019/advex-bugs-discussion/) .\n \n\n\n[Other Comments](https://distill.pub/2019/advex-bugs-discussion/#commentaries)\n[Comment by Ilyas et al.](https://distill.pub/2019/advex-bugs-discussion/response-3/#rebuttal)\n\n\n Ilyas et al.  define a *feature* as a function fff that\n takes xxx from the *data distribution* (x,y)∼D(x,y) \\sim \\mathcal{D}(x,y)∼D into a real number, restricted to have\n mean zero and unit variance. A feature is said to be *useful* if it has high correlation with the\n label. But in the presence of an adversary Ilyas et al.  argues\n the metric that truly matters is a feature’s *robust usefulness*,\n \n\n\n\nE[inf∥δ∥≤ϵyf(x+δ)],\n \\mathbf{E}\\left[\\inf\\_{\\|\\delta\\|\\leq\\epsilon}yf(x+\\delta)\\right],\n E[∥δ∥≤ϵinf\u200byf(x+δ)],\n\n\n its correlation with the label while under attack. Ilyas et al. \n suggests that in addition to the pedestrian, robust features we know and love (such as the color of the\n sky), our models may also be taking advantage of useful, non-robust features, some of which may even lie\n beyond the threshold of human intuition. This begs the question: what might such non-robust features look\n like?\n \n\n\n### Non-Robust Features in Linear Models\n\n\n\n\n Our search is simplified when we realize the following: non-robust features are not unique to the complex,\n nonlinear models encountered in deep learning. As Ilyas et al \n observe, they arise even in the humblest of models\u2009—\u2009the linear one. Thus, we restrict our attention\n to linear features of the form:\n\n \n\n\n\n\nf(x)=aTx∥a∥ΣwhereΣ=E[xxT]andE[x]=0.f(x) = \\frac{a^Tx}{\\|a\\|\\_\\Sigma}\\qquad \\text{where} \\qquad \\Sigma = \\mathbf{E}[xx^T] \\quad\n \\text{and} \\quad \\mathbf{E}[x] = 0.\n f(x)=∥a∥Σ\u200baTx\u200bwhereΣ=E[xxT]andE[x]=0.\n\n\n The robust usefulness of a linear feature admits an elegant decomposition\n  This\n E[inf∥δ∥≤ϵyf(x+δ)]=E[yf(x)+inf∥δ∥≤ϵyf(δ)]=E[yf(x)+inf∥δ∥≤ϵyaTδ∥a∥Σ]=E[yf(x)+inf∥δ∥≤ϵaTδ∥a∥Σ]=E[yf(x)]−ϵ∥a∥∗∥a∥Σ\n \\begin{aligned}\n \\mathbf{E}\\left[\\inf\\_{\\|\\delta\\|\\leq\\epsilon}yf(x+\\delta)\\right] &\n =\\mathbf{E}\\left[yf(x)+\\inf\\_{\\|\\delta\\|\\leq\\epsilon}yf(\\delta)\\right]\\\\\n & =\\mathbf{E}\\left[yf(x)+\\inf\\_{\\|\\delta\\|\\leq\\epsilon}y\\frac{a^{T}\\delta}{\\|a\\|\\_{\\Sigma}}\\right]\\\\\n &\n =\\mathbf{E}\\left[yf(x)+\\frac{\\inf\\_{\\|\\delta\\|\\leq\\epsilon}a^{T}\\delta}{\\|a\\|\\_{\\Sigma}}\\right]=\\mathop{\\mathbf{E}[yf(x)]}-\\epsilon\\frac{\\|a\\|\\_{*}}{\\|a\\|\\_{\\Sigma}}\n \\end{aligned}\n E[∥δ∥≤ϵinf\u200byf(x+δ)]\u200b=E[yf(x)+∥δ∥≤ϵinf\u200byf(δ)]=E[yf(x)+∥δ∥≤ϵinf\u200by∥a∥Σ\u200baTδ\u200b]=E[yf(x)+∥a∥Σ\u200binf∥δ∥≤ϵ\u200baTδ\u200b]=E[yf(x)]−ϵ∥a∥Σ\u200b∥a∥∗\u200b\u200b\u200b\n into two terms:\n\n \n\n\n\n .undomargin {\n position: relative;\n left: -1em;\n top: 0.2em;\n }\n \n\n\n\n\n\nE[inf∥δ∥≤ϵyf(x+δ)]\n \\mathbf{E}\\left[\\inf\\_{\\|\\delta\\|\\leq\\epsilon}yf(x+\\delta)\\right]\n E[∥δ∥≤ϵinf\u200byf(x+δ)]\n\n\n\n\n===\n\n\n\n\nE[yf(x)]\\mathop{\\mathbf{E}[yf(x)]}E[yf(x)]\n\n\n\n\n−-−\n\n\n\n\nϵ∥a∥∗∥a∥Σ\\epsilon\\frac{\\|a\\|\\_{*}}{\\|a\\|\\_{\\Sigma}}ϵ∥a∥Σ\u200b∥a∥∗\u200b\u200b\n\n\n\n\n\n The robust usefulness of a feature\n \n\n\n the correlation of the feature with the label\n \n\n\n the feature’s non-robustness\n \n\n\n\n In the above equation ∥⋅∥∗\\|\\cdot\\|\\_*∥⋅∥∗\u200b deontes the dual norm of ∥⋅∥\\|\\cdot\\|∥⋅∥.\n This decomposition gives us an instrument for visualizing any set of linear features aia\\_iai\u200b in a two\n dimensional plot.\n \n\n\n\n Plotted below is the binary classification task of separating *truck* and *frog* in CIFAR-10 on\n the set of features aia\\_iai\u200b corresponding to the ithi^{th}ith singular vector of the data.\n \n\n\n\n Subject to an\n L\\_2\n adversery, observe that high frequency features are both less useful and\n less robust.\n  Useful Non-Useful A  B  C  D  E  F  \n Pareto frontier of points in the non-robustness and usefulness space.\n \n\n  \\log \\sum\\_i \\!y\\_if(x\\_i) Log usefulness. Measured by the feature\"s empirical correlation with the\n positive label.\n\n \\log \\left( \\frac{\\|a\\_i\\|\\_\\Sigma}{\\|a\\_i\\|} \\right) =\n \\log(\\lambda\\_i) Feature’s log robustness. When\n a\\_i\"s\n are the\n i^{th}\n eigenvalues of\n \\Sigma\n , the robustness reduces to the\n i^{th}\n singular value of\n \\lambda\\_i   ABCDEFf-12-11-10-9-8-7-6-5-4-3-4-3-2-10  \n\n\n The elusive non-robust useful features, however, seem conspicuously absent in the above plot.\n Fortunately, we can construct such features by strategically combining elements of this basis.\n \n\n\n\n We demonstrate two constructions:\n \n\n\n\n\n\n**Ensembles** The work of Tsipras et al  suggests a collection of non-robust and non-useful features, if sufficiently uncorrelated, can be ensembled into a single useful, non-robust useful feature f.\n\n f-12-11-10-9-8-7-6-5-4-3-4-3-2-1    We ensemble all features below a certain threshold of robustness.         This process is illustrated above numerically. We choose a set of non-robust features by excluding all features above a threshold, and naively ensembling them according to:\n\n \\sum\\_i \\text{sign}(\\mathbf{E}[y\\_{i}a\\_{i}^{T}x])a\\_{i}^{T} and normalizing. This construction is closest in spirit to the author\"s idea of what a non-robust feature looks like.\n\n\n**Containments** But now consider now an alternative construction. If we interpolate a robust useful feature 1and a non-robust useless feature 2, we obtain, surprisingly, a non-robust feature f too.\n\n 12f-12-11-10-9-8-7-6-5-4-3-4-3-2-1     a\\_\\text{robust}     a\\_\\text{non-robust}   We illustrate this by interpolating between two features, one robust and one non-robust\n\n (1-\\alpha) \\cdot a\\_{\\text{non-robust}} + \\alpha \\cdot a\\_{\\text{robust}}, \n and normalizing. This construction reveals a flaw of using the definition of *robust usefulness* discussed above — non-robust, useful features can arise from such cross contamination rather than something more fundamental.\n \n\n\n\n\n\n It is surprising, thus, that the experiments of Madry et al. \n (with deterministic perturbations) *do* distinguish between the non-robust useful\n features generated from ensembles and containments. A succinct definition of a robust feature that peels\n these two worlds apart is yet to exist, and remains an open problem for the machine learning community.\n \n\n\n\n To cite Ilyas et al.’s response, please cite their\n [collection of responses](https://distill.pub/2019/advex-bugs-discussion/original-authors/#citation).\n\n\n**Response Summary**: The construction of explicit non-robust features is\n very interesting and makes progress towards the challenge of visualizing some of\n the useful non-robust features detected by our experiments. We also agree that\n non-robust features arising as “distractors” is indeed not precluded by our\n theoretical framework, even if it is precluded by our experiments.\n This simple theoretical framework sufficed for reasoning about and\n predicting the outcomes of our experiments\n We also presented a theoretical setting where we can\n analyze things fully rigorously in Section 4 of our paper..\n However, this comment rightly identifies finding a more comprehensive\n definition of feature as an important future research direction.\n \n\n\n**Response**: These experiments (visualizing the robustness and\n usefulness of different linear features) are very interesting! They both further\n corroborate the existence of useful, non-robust features and make progress\n towards visualizing what these non-robust features actually look like. \n\n\nWe also appreciate the point made by the provided construction of non-robust\n features (as defined in our theoretical framework) that are combinations of\n useful+robust and useless+non-robust features. Our theoretical framework indeed\n enables such a scenario, even if\u2009—\u2009as the commenter already notes\u2009—\u2009our\n experimental results do not. (In this sense, the experimental results and our [main takeaway](https://distill.pub/2019/advex-bugs-discussion/rebuttal/#takeaway1)  are actually stronger than our theoretical\n framework technically captures.) Specifically, in such a scenario, during the\n construction of the D^det\\widehat{\\mathcal{D}}\\_{det}D\ndet\u200b dataset, only the non-robust\n and useless term of the feature would be flipped. Thus, a classifier trained on\n such a dataset would associate the predictive robust feature with the\n *wrong* label and would thus not generalize on the test set. In contrast,\n our experiments show that classifiers trained on D^det\\widehat{\\mathcal{D}}\\_{det}D\ndet\u200b\n do generalize.\n\n\nOverall, our focus while developing our theoretical framework was on\n enabling us to formally describe and predict the outcomes of our experiments. As\n the comment points out, putting forth a theoretical framework that captures\n non-robust features in a very precise way is an important future research\n direction in itself. \n\n\n\n\n You can find more responses in the  [main discussion article](https://distill.pub/2019/advex-bugs-discussion/).", "bibliography_bbl": "", "bibliography_bib": [{"title": "Adversarial examples are not bugs, they are features"}]}
{"source": "distill", "source_filetype": "html", "converted_with": "python", "paper_version": "", "post_title": "Growing Neural Cellular Automata", "authors": ["Alexander Mordvintsev", "Ettore Randazzo", "Eyvind Niklasson", "Michael Levin"], "date_published": "2020-02-11", "data_last_modified": "", "url": "", "abstract": "This article is part of the Differentiable Self-organizing Systems Thread, an experimental format collecting invited short articles delving into differentiable self-organizing systems, interspersed with critical commentary from several experts in adjacent fields.", "author_comment": "", "journal_ref": "distill pub", "doi": "https://doi.org/10.23915/distill.00023", "primary_category": "", "categories": "", "citation_level": "", "main_tex_filename": "", "text": "This article is part of the\n [Differentiable Self-organizing Systems Thread](https://distill.pub/2020/selforg/),\n an experimental format collecting invited short articles delving into\n differentiable self-organizing systems, interspersed with critical\n commentary from several experts in adjacent fields.\n \n\n\n[Differentiable Self-organizing Systems Thread](https://distill.pub/2020/selforg/)\n[Self-classifying MNIST Digits](https://distill.pub/2020/selforg/mnist/)\n\n\n Most multicellular organisms begin their life as a single egg cell - a\n single cell whose progeny reliably self-assemble into highly complex\n anatomies with many organs and tissues in precisely the same arrangement\n each time. The ability to build their own bodies is probably the most\n fundamental skill every living creature possesses. Morphogenesis (the\n process of an organism’s shape development) is one of the most striking\n examples of a phenomenon called *self-organisation*. Cells, the tiny\n building blocks of bodies, communicate with their neighbors to decide the\n shape of organs and body plans, where to grow each organ, how to\n interconnect them, and when to eventually stop. Understanding the interplay\n of the emergence of complex outcomes from simple rules and\n homeostatic\n Self-regulatory feedback loops trying maintain the body in a stable state\n or preserve its correct overall morphology under external\n perturbations\n feedback loops is an active area of research\n . What is clear\n is that evolution has learned to exploit the laws of physics and computation\n to implement the highly robust morphogenetic software that runs on\n genome-encoded cellular hardware.\n \n\n\n\n This process is extremely robust to perturbations. Even when the organism is\n fully developed, some species still have the capability to repair damage - a\n process known as regeneration. Some creatures, such as salamanders, can\n fully regenerate vital organs, limbs, eyes, or even parts of the brain!\n Morphogenesis is a surprisingly adaptive process. Sometimes even a very\n atypical development process can result in a viable organism - for example,\n when an early mammalian embryo is cut in two, each half will form a complete\n individual - monozygotic twins!\n \n\n\n\n The biggest puzzle in this field is the question of how the cell collective\n knows what to build and when to stop. The sciences of genomics and stem cell\n biology are only part of the puzzle, as they explain the distribution of\n specific components in each cell, and the establishment of different types\n of cells. While we know of many genes that are *required* for the\n process of regeneration, we still do not know the algorithm that is\n *sufficient* for cells to know how to build or remodel complex organs\n to a very specific anatomical end-goal. Thus, one major lynch-pin of future\n work in biomedicine is the discovery of the process by which large-scale\n anatomy is specified within cell collectives, and how we can rewrite this\n information to have rational control of growth and form. It is also becoming\n clear that the software of life possesses numerous modules or subroutines,\n such as “build an eye here”, which can be activated with simple signal\n triggers. Discovery of such subroutines and a\n mapping out of the developmental logic is a new field at the intersection of\n developmental biology and computer science. An important next step is to try\n to formulate computational models of this process, both to enrich the\n conceptual toolkit of biologists and to help translate the discoveries of\n biology into better robotics and computational technology.\n \n\n\n\n Imagine if we could design systems of the same plasticity and robustness as\n biological life: structures and machines that could grow and repair\n themselves. Such technology would transform the current efforts in\n regenerative medicine, where scientists and clinicians seek to discover the\n inputs or stimuli that could cause cells in the body to build structures on\n demand as needed. To help crack the puzzle of the morphogenetic code, and\n also exploit the insights of biology to create self-repairing systems in\n real life, we try to replicate some of the desired properties in an\n *in silico* experiment.\n \n\n\n## Model\n\n\n\n Those in engineering disciplines and researchers often use many kinds of\n simulations incorporating local interaction, including systems of partial\n derivative equation (PDEs), particle systems, and various kinds of Cellular\n Automata (CA). We will focus on Cellular Automata models as a roadmap for\n the effort of identifying cell-level rules which give rise to complex,\n regenerative behavior of the collective. CAs typically consist of a grid of\n cells being iteratively updated, with the same set of rules being applied to\n each cell at every step. The new state of a cell depends only on the states\n of the few cells in its immediate neighborhood. Despite their apparent\n simplicity, CAs often demonstrate rich, interesting behaviours, and have a\n long history of being applied to modeling biological phenomena.\n \n\n\n\n Let’s try to develop a cellular automata update rule that, starting from a\n single cell, will produce a predefined multicellular pattern on a 2D grid.\n This is our analogous toy model of organism development. To design the CA,\n we must specify the possible cell states, and their update function. Typical\n CA models represent cell states with a set of discrete values, although\n variants using vectors of continuous values exist. The use of continuous\n values has the virtue of allowing the update rule to be a differentiable\n function of the cell’s neighbourhood’s states. The rules that guide\n individual cell behavior based on the local environment are analogous to the\n low-level hardware specification encoded by the genome of an organism.\n Running our model for a set amount of steps from a starting configuration\n will reveal the patterning behavior that is enabled by such hardware.\n \n\n\n\n So - what is so special about differentiable update rules? They will allow\n us to use the powerful language of loss functions to express our wishes, and\n the extensive existing machinery around gradient-based numerical\n optimization to fulfill them. The art of stacking together differentiable\n functions, and optimizing their parameters to perform various tasks has a\n long history. In recent years it has flourished under various names, such as\n (Deep) Neural Networks, Deep Learning or Differentiable Programming.\n \n\n\n\n\n\n\n\nA single update step of the model.\n\n\n### Cell State\n\n\n\n We will represent each cell state as a vector of 16 real values (see the\n figure above). The first three channels represent the cell color visible to\n us (RGB). The target pattern has color channel values in range [0.0,1.0][0.0, 1.0][0.0,1.0]\n and an α\\alphaα equal to 1.0 for foreground pixels, and 0.0 for background.\n \n\n\n\n The alpha channel (α\\alphaα) has a special meaning: it demarcates living\n cells, those belonging to the pattern being grown. In particular, cells\n having α>0.1\\alpha > 0.1α>0.1 and their neighbors are considered “living”. Other\n cells are “dead” or empty and have their state vector values explicitly set\n to 0.0 at each time step. Thus cells with α>0.1\\alpha > 0.1α>0.1 can be thought of\n as “mature”, while their neighbors with α≤0.1\\alpha \\leq 0.1α≤0.1 are “growing”, and\n can become mature if their alpha passes the 0.1 threshold.\n \n\n\n\n![](./Growing Neural Cellular Automata_files/alive2.svg)\n\nstate⃗→0.00\\vec{state} \\rightarrow 0.00state⃗→0.00 when no neighbour with α>0.10\\alpha > 0.10α>0.10\n\n\n\n Hidden channels don’t have a predefined meaning, and it’s up to the update\n rule to decide what to use them for. They can be interpreted as\n concentrations of some chemicals, electric potentials or some other\n signaling mechanism that are used by cells to orchestrate the growth. In\n terms of our biological analogy - all our cells share the same genome\n (update rule) and are only differentiated by the information encoded the\n chemical signalling they receive, emit, and store internally (their state\n vectors).\n \n\n\n### Cellular Automaton rule\n\n\n\n Now it’s time to define the update rule. Our CA runs on a regular 2D grid of\n 16-dimensional vectors, essentially a 3D array of shape [height, width, 16].\n We want to apply the same operation to each cell, and the result of this\n operation can only depend on the small (3x3) neighborhood of the cell. This\n is heavily reminiscent of the convolution operation, one of the cornerstones\n of signal processing and differential programming. Convolution is a linear\n operation, but it can be combined with other per-cell operations to produce\n a complex update rule, capable of learning the desired behaviour. Our cell\n update rule can be split into the following phases, applied in order:\n \n\n\n\n**Perception.** This step defines what each cell perceives of\n the environment surrounding it. We implement this via a 3x3 convolution with\n a fixed kernel. One may argue that defining this kernel is superfluous -\n after all we could simply have the cell learn the requisite perception\n kernel coefficients. Our choice of fixed operations are motivated by the\n fact that real life cells often rely only on chemical gradients to guide the\n organism development. Thus, we are using classical Sobel filters to estimate\n the partial derivatives of cell state channels in the x⃗\\vec{x}x⃗ and\n y⃗\\vec{y}y⃗\u200b directions, forming a 2D gradient vector in each direction, for\n each state channel. We concatenate those gradients with the cells own\n states, forming a 16∗2+16=4816*2+16=4816∗2+16=48 dimensional *perception vector*, or\n rather *percepted vector,* for each cell.\n \n\n\n\ndef perceive(state\\_grid):\n\n\nsobel\\_x = [[-1, 0, +1],\n\n\n[-2, 0, +2],\n\n\n[-1, 0, +1]]\n\n\nsobel\\_y = transpose(sobel\\_x)\n\n\n# Convolve sobel filters with states\n\n\n# in x, y and channel dimension.\n\n\ngrad\\_x = conv2d(sobel\\_x, state\\_grid)\n\n\ngrad\\_y = conv2d(sobel\\_y, state\\_grid)\n\n\n# Concatenate the cell’s state channels,\n\n\n# the gradients of channels in x and\n\n\n# the gradient of channels in y.\n\n\nperception\\_grid = concat(\n\n\nstate\\_grid, grad\\_x, grad\\_y, axis=2)\n\n\nreturn perception\\_grid\n\n\n\n\n**Update rule.** Each cell now applies a series of operations\n to the perception vector, consisting of typical differentiable programming\n building blocks, such as 1x1-convolutions and ReLU nonlinearities, which we\n call the cell’s “update rule”. Recall that the update rule is learned, but\n every cell runs the same update rule. The network parametrizing this update\n rule consists of approximately 8,000 parameters. Inspired by residual neural\n networks, the update rule outputs an incremental update to the cell’s state,\n which applied to the cell before the next time step. The update rule is\n designed to exhibit “do-nothing” initial behaviour - implemented by\n initializing the weights of the final convolutional layer in the update rule\n with zero. We also forego applying a ReLU to the output of the last layer of\n the update rule as the incremental updates to the cell state must\n necessarily be able to both add or subtract from the state.\n \n\n\n\ndef update(perception\\_vector):\n\n\n# The following pseudocode operates on\n\n\n# a single cell’s perception vector.\n\n\n# Our reference implementation uses 1D\n\n\n# convolutions for performance reasons.\n\n\nx = dense(perception\\_vector, output\\_len=128)\n\n\nx = relu(x)\n\n\nds = dense(x, output\\_len=16, weights\\_init=0.0)\n\n\nreturn ds\n\n\n\n\n**Stochastic cell update.** Typical cellular automata update\n all cells simultaneously. This implies the existence of a global clock,\n synchronizing all cells. Relying on global synchronisation is not something\n one expects from a self-organising system. We relax this requirement by\n assuming that each cell performs an update independently, waiting for a\n random time interval between updates. To model this behaviour we apply a\n random per-cell mask to update vectors, setting all update values to zero\n with some predefined probability (we use 0.5 during training). This\n operation can be also seen as an application of per-cell dropout to update\n vectors.\n \n\n\n\ndef stochastic\\_update(state\\_grid, ds\\_grid):\n\n\n# Zero out a random fraction of the updates.\n\n\nrand\\_mask = cast(random(64, 64) < 0.5, float32)\n\n\nds\\_grid = ds\\_grid * rand\\_mask\n\n\nreturn state\\_grid + ds\\_grid\n\n\n\n\n**Living cell masking.** We want to model the growth process\n that starts with a single cell, and don’t want empty cells to participate in\n computations or carry any hidden state. We enforce this by explicitly\n setting all channels of empty cells to zeros. A cell is considered empty if\n there is no “mature” (alpha>0.1) cell in its 3x3 neightborhood.\n \n\n\n\ndef alive\\_masking(state\\_grid):\n\n\n# Take the alpha channel as the measure of “life”.\n\n\nalive = max\\_pool(state\\_grid[:,\xa0:, 3], (3,3)) > 0.1\n\n\nstate\\_grid = state\\_grid * cast(alive, float32)\n\n\nreturn state\\_grid\n\n\n\n## Experiment 1: Learning to Grow\n\n\n\n\n\n\n\nTraining regime for learning a target pattern.\n\n\n\n In our first experiment, we simply train the CA to achieve a target image\n after a random number of updates. This approach is quite naive and will run\n into issues. But the challenges it surfaces will help us refine future\n attempts.\n \n\n\n\n We initialize the grid with zeros, except a single seed cell in the center,\n which will have all channels except RGB\n We set RGB channels of the seed to zero because we want it to be visible\n on the white background.\n set to one. Once the grid is initialized, we iteratively apply the update\n rule. We sample a random number of CA steps from the [64, 96]\n This should be a sufficient number of steps to grow the pattern of the\n size we work with (40x40), even considering the stochastic nature of our\n update rule.\n range for each training step, as we want the pattern to be stable across a\n number of iterations. At the last step we apply pixel-wise L2 loss between\n RGBA channels in the grid and the target pattern. This loss can be\n differentiably optimized\n We observed training instabilities, that were manifesting themselves as\n sudden jumps of the loss value in the later stages of the training. We\n managed to mitigate them by applying per-variable L2 normalization to\n parameter gradients. This may have the effect similar to the weight\n normalization . Other training\n parameters are available in the accompanying source code.\n with respect to the update rule parameters by backpropagation-through-time,\n the standard method of training recurrent neural networks.\n \n\n\n\n Once the optimisation converges, we can run simulations to see how our\n learned CAs grow patterns starting from the seed cell. Let’s see what\n happens when we run it for longer than the number of steps used during\n training. The animation below shows the behaviour of a few different models,\n trained to generate different emoji patterns.\n \n\n\n\n\n\n\n\n\n Your browser does not support the video tag.\n \n\n Many of the patterns exhibit instability for longer time periods.\n   \n  \n\n[Reproduce in a Notebook](https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=4O4tzfe-GRJ7)\n\n\n\n\n We can see that different training runs can lead to models with drastically\n different long term behaviours. Some tend to die out, some don’t seem to\n know how to stop growing, but some happen to be almost stable! How can we\n steer the training towards producing persistent patterns all the time?\n \n\n\n## Experiment 2: What persists, exists\n\n\n\n One way of understanding why the previous experiment was unstable is to draw\n a parallel to dynamical systems. We can consider every cell to be a\n dynamical system, with each cell sharing the same dynamics, and all cells\n being locally coupled amongst themselves. When we train our cell update\n model we are adjusting these dynamics. Our goal is to find dynamics that\n satisfy a number of properties. Initially, we wanted the system to evolve\n from the seed pattern to the target pattern - a trajectory which we achieved\n in Experiment 1. Now, we want to avoid the instability we observed - which\n in our dynamical system metaphor consists of making the target pattern an\n attractor.\n \n\n\n\n One strategy to achieve this is letting the CA iterate for much longer time\n and periodically applying the loss against the target, training the system\n by backpropagation through these longer time intervals. Intuitively we claim\n that with longer time intervals and several applications of loss, the model\n is more likely to create an attractor for the target shape, as we\n iteratively mold the dynamics to return to the target pattern from wherever\n the system has decided to venture. However, longer time periods\n substantially increase the training time and more importantly, the memory\n requirements, given that the entire episode’s intermediate activations must\n be stored in memory for a backwards-pass to occur.\n \n\n\n\n Instead, we propose a “sample pool” based strategy to a similar effect. We\n define a pool of seed states to start the iterations from, initially filled\n with the single black pixel seed state. We then sample a batch from this\n pool which we use in our training step. To prevent the equivalent of\n “catastrophic forgetting” we replace one sample in this batch with the\n original, single-pixel seed state. After concluding the training step\xa0, we\n replace samples in the pool that were sampled for the batch with the output\n states from the training step over this batch. The animation below shows a\n random sample of the entries in the pool every 20 training steps.\n \n\n\n\ndef pool\\_training():\n\n\n# Set alpha and hidden channels to (1.0).\n\n\nseed = zeros(64, 64, 16)\n\n\nseed[64//2, 64//2, 3:] = 1.0\n\n\ntarget = targets[‘lizard’]\n\n\npool = [seed] * 1024\n\n\nfor i in range(training\\_iterations):\n\n\nidxs, batch = pool.sample(32)\n\n\n# Sort by loss, descending.\n\n\nbatch = sort\\_desc(batch, loss(batch))\n\n\n# Replace the highest-loss sample with the seed.\n\n\nbatch[0] = seed\n\n\n# Perform training.\n\n\noutputs, loss = train(batch, target)\n\n\n# Place outputs back in the pool.\n\n\npool[idxs] = outputs\n\n\n\n\n\n\n\n\n\n Your browser does not support the video tag.\n \n\n A random sample of the patterns in the pool during training, sampled\n every 20 training steps.   \n  \n\n[Reproduce in a Notebook](https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=B4JAbAJf6Alw)\n\n\n\n\n Early on in the training process, the random dynamics in the system allow\n the model to end up in various incomplete and incorrect states. As these\n states are sampled from the pool, we refine the dynamics to be able to\n recover from such states. Finally, as the model becomes more robust at going\n from a seed state to the target state, the samples in the pool reflect this\n and are more likely to be very close to the target pattern, allowing the\n training to refine these almost completed patterns further.\n \n\n\n\n Essentially, we use the previous final states as new starting points to\n force our CA to learn how to persist or even improve an already formed\n pattern, in addition to being able to grow it from a seed. This makes it\n possible to add a periodical loss for significantly longer time intervals\n than otherwise possible, encouraging the generation of an attractor as the\n target shape in our coupled system. We also noticed that reseeding the\n highest loss sample in the batch, instead of a random one, makes training\n more stable at the initial stages, as it helps to clean up the low quality\n states from the pool.\n \n\n\n\n Here is what a typical training progress of a CA rule looks like. The cell\n rule learns to stabilize the pattern in parallel to refining its features.\n \n\n\n\n\n\n\n\n\n Your browser does not support the video tag.\n \n\n CA behaviour at training steps 100, 500, 1000, 4000.   \n  \n\n[Reproduce in a Notebook](https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=nqvkfl9W4ODI)\n\n\n\n## Experiment 3: Learning to regenerate\n\n\n\n In addition to being able to grow their own bodies, living creatures are\n great at maintaining them. Not only does worn out skin get replaced with new\n skin, but very heavy damage to complex vital organs can be regenerated in\n some species. Is there a chance that some of the models we trained above\n have regenerative capabilities?\n \n\n\n\n\n\n\n\n\n Your browser does not support the video tag.\n \n\n Patterns exhibit some regenerative properties upon being damaged, but\n not full re-growth.   \n  \n\n[Reproduce in a Notebook](https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=S5JRLGxX1dnX)\n\n\n\n\n The animation above shows three different models trained using the same\n settings. We let each of the models develop a pattern over 100 steps, then\n damage the final state in five different ways: by removing different halves\n of the formed pattern, and by cutting out a square from the center. Once\n again, we see that these models show quite different out-of-training mode\n behaviour. For example “the lizard” develops quite strong regenerative\n capabilities, without being explicitly trained for it!\n \n\n\n\n Since we trained our coupled system of cells to generate an attractor\n towards a target shape from a single cell, it was likely that these systems,\n once damaged, would generalize towards non-self-destructive reactions.\n That’s because the systems were trained to grow, stabilize, and never\n entirely self-destruct. Some of these systems might naturally gravitate\n towards regenerative capabilities, but nothing stops them from developing\n different behaviors such as explosive mitoses (uncontrolled growth),\n unresponsiveness to damage (overstabilization), or even self destruction,\n especially for the more severe types of damage.\n \n\n\n\n If we want our model to show more consistent and accurate regenerative\n capabilities, we can try to increase the basin of attraction for our target\n pattern - increase the space of cell configurations that naturally gravitate\n towards our target shape. We will do this by damaging a few pool-sampled\n states before each training step. The system now has to be capable of\n regenerating from states damaged by randomly placed erasing circles. Our\n hope is that this will generalize to regenerational capabilities from\n various types of damage.\n \n\n\n\n\n\n\n\n\n Your browser does not support the video tag.\n \n\n Damaging samples in the pool encourages the learning of robust\n regenerative qualities. Row 1 are samples from the pool, Row 2 are their\n respective states after iterating the model.  \n  \n\n[Reproduce in a Notebook](https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=QeXZKb5v2gxj)\n\n\n\n\n The animation above shows training progress, which includes sample damage.\n We sample 8 states from the pool. Then we replace the highest-loss sample\n (top-left-most in the above) with the seed state, and damage the three\n lowest-loss (top-right-most) states by setting a random circular region\n within the pattern to zeros. The bottom row shows states after iteration\n from the respective top-most starting state. As in Experiment 2, the\n resulting states get injected back into the pool.\n \n\n\n\n\n\n\n\n\n Your browser does not support the video tag.\n \n\n Patterns exposed to damage during training exhibit astounding\n regenerative capabilities.   \n  \n\n[Reproduce in a Notebook](https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=TDzJM69u4_8p)\n\n\n\n\n As we can see from the animation above, models that were exposed to damage\n during training are much more robust, including to types of damage not\n experienced in the training process (for instance rectangular damage as\n above).\n \n\n\n## Experiment 4: Rotating the perceptive field\n\n\n\n As previously described, we model the cell’s perception of its neighbouring\n cells by estimating the gradients of state channels in x⃗\\vec{x}x⃗ and\n y⃗\\vec{y}y⃗\u200b using Sobel filters. A convenient analogy is that each agent has\n two sensors (chemosensory receptors, for instance) pointing in orthogonal\n directions that can sense the gradients in the concentration of certain\n chemicals along the axis of the sensor. What happens if we rotate those\n sensors? We can do this by rotating the Sobel kernels.\n \n\n\n\n[KxKy]=[cosθ−sinθsinθcosθ]∗[SobelxSobely] \\begin{bmatrix} K\\_x \\\\ K\\_y \\end{bmatrix} = \\begin{bmatrix} \\cos \\theta &\n -\\sin \\theta \\\\ \\sin \\theta & \\cos \\theta \\end{bmatrix} * \\begin{bmatrix}\n Sobel\\_x \\\\ Sobel\\_y \\end{bmatrix} [Kx\u200bKy\u200b\u200b]=[cosθsinθ\u200b−sinθcosθ\u200b]∗[Sobelx\u200bSobely\u200b\u200b]\n\n\n This simple modification of the perceptive field produces rotated versions\n of the pattern for an angle of choosing without retraining as seen below.\n \n\n\n\n\n\n\n![](./Growing Neural Cellular Automata_files/rotation.png)\n\n Rotating the axis along which the perception step computes gradients\n brings about rotated versions of the pattern.   \n  \n\n[Reproduce in a Notebook](https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=1CVR9MeYnjuY)\n\n\n\n\n In a perfect world, not quantized by individual cells in a pixel-lattice,\n this would not be too surprising, as, after all, one would expect the\n perceived gradients in x⃗\\vec{x}x⃗ and y⃗\\vec{y}y⃗\u200b to be invariant to the chosen\n angle - a simple change of frame of reference. However, it is important to\n note that things are not as simple in a pixel based model. Rotating pixel\n based graphics involves computing a mapping that’s not necessarily bijective\n and classically involves interpolating between pixels to achieve the desired\n result. This is because a single pixel, when rotated, will now likely\n overlap several pixels. The successful growth of patterns as above suggests\n a certain robustness to the underlying conditions outside of those\n experienced during training.\n \n\n\n## Related Work\n\n\n### CA and PDEs\n\n\n\n There exists an extensive body of literature that describes the various\n flavours of cellular automata and PDE systems, and their applications to\n modelling physical, biological or even social systems. Although it would be\n impossible to present a just overview of this field in a few lines, we will\n describe some prominent examples that inspired this work. Alan Turing\n introduced his famous Turing patterns back in 1952\n , suggesting how\n reaction-diffusion systems can be a valid model for chemical behaviors\n during morphogenesis. A particularly inspiring reaction-diffusion model that\n stood the test of time is the Gray-Scott model\n , which shows an extreme variety of\n behaviors controlled by just a few variables.\n \n\n\n\n Ever since von Neumann introduced CAs\n  as models for self-replication they\n have captivated researchers’ minds, who observed extremely complex\n behaviours emerging from very simple rules. Likewise, the a broader audience\n outside of academia were seduced by CA’s life-like behaviours thanks to\n Conway’s Game of Life . Perhaps\n motivated in part by the proof that something as simple as the Rule 110 is\n Turing complete, Wolfram’s “*A New Kind of Science”*\n asks for a paradigm shift centered\n around the extensive usage of elementary computer programs such as CA as\n tools for understanding the world.\n \n\n\n\n More recently, several researchers generalized Conway’s Game of life to work\n on more continuous domains. We were particularly inspired by Rafler’s\n SmoothLife  and Chan’s Lenia\n , the latter of\n which also discovers and classifies entire species of “lifeforms”.\n \n\n\n\n A number of researchers have used evolutionary algorithms to find CA rules\n that reproduce predefined simple patterns\n .\n For example, J. Miller  proposed an\n experiment similar to ours, using evolutionary algorithms to design a CA\n rule that could build and regenerate the French flag, starting from a seed\n cell.\n \n\n\n### Neural Networks and Self-Organisation\n\n\n\n The close relation between Convolutional Neural Networks and Cellular\n Automata has already been observed by a number of researchers\n . The\n connection is so strong it allowed us to build Neural CA models using\n components readily available in popular ML frameworks. Thus, using a\n different jargon, our Neural CA could potentially be named “Recurrent\n Residual Convolutional Networks with ‘per-pixel’ Dropout”.\n \n\n\n\n The Neural GPU\n  offers\n a computational architecture very similar to ours, but applied in the\n context of learning multiplication and a sorting algorithm.\n \n\n\n\n Looking more broadly, we think that the concept of self-organisation is\n finding its way into mainstream machine learning with popularisation of\n Graph Neural Network  models.\n Typically, GNNs run a repeated computation across vertices of a (possibly\n dynamic) graph. Vertices communicate locally through graph edges, and\n aggregate global information required to perform the task over multiple\n rounds of message exchanges, just as atoms can be thought of as\n communicating with each other to produce the emergent properties of a\n molecule , or even points of a point\n cloud talk to their neighbors to figure out their global shape\n .\n \n\n\n\n Self-organization also appeared in fascinating contemporary work using more\n traditional dynamic graph networks, where the authors evolved\n Self-Assembling Agents to solve a variety of virtual tasks\n .\n \n\n\n### Swarm Robotics\n\n\n\n One of the most remarkable demonstrations of the power of self-organisation\n is when it is applied to swarm modeling. Back in 1987, Reynolds’ Boids\n  simulated the flocking behaviour of birds with\n just a tiny set of handcrafted rules. Nowadays, we can embed tiny robots\n with programs and test their collective behavior on physical agents, as\n demonstrated by work such as Mergeable Nervous Systems\n  and Kilobots\n . To the best of our knowledge, programs\n embedded into swarm robots are currently designed by humans. We hope our\n work can serve as an inspiration for the field and encourage the design of\n collective behaviors through differentiable modeling.\n \n\n\n## Discussion\n\n\n### Embryogenetic Modeling\n\n\n\n\n\n\n\n\n Your browser does not support the video tag.\n \n\n Regeneration-capable 2-headed planarian, the creature that inspired this\n work \n  \n  \n\n[Reproduce in a Notebook](https://colab.research.google.com/github/google-research/self-organising-systems/blob/master/notebooks/growing_ca.ipynb#scrollTo=fQ1u2MqFy7Ni)\n\n\n\n\n This article describes a toy embryogenesis and regeneration model. This is a\n major direction for future work, with many applications in biology and\n beyond. In addition to the implications for understanding the evolution and\n control of regeneration, and harnessing this understanding for biomedical\n repair, there is the field of bioengineering. As the field transitions from\n synthetic biology of single cell collectives to a true synthetic morphology\n of novel living machines , it\n will be essential to develop strategies for programming system-level\n capabilities, such as anatomical homeostasis (regenerative repair). It has\n long been known that regenerative organisms can restore a specific\n anatomical pattern; however, more recently it’s been found that the target\n morphology is not hard coded by the DNA, but is maintained by a\n physiological circuit that stores a setpoint for this anatomical homeostasis\n . Techniques are\n now available for re-writing this setpoint, resulting for example\n  in 2-headed flatworms\n that, when cut into pieces in plain water (with no more manipulations)\n result in subsequent generations of 2-headed regenerated worms (as shown\n above). It is essential to begin to develop models of the computational\n processes that store the system-level target state for swarm behavior\n , so that efficient strategies can be developed for rationally editing this\n information structure, resulting in desired large-scale outcomes (thus\n defeating the inverse problem that holds back regenerative medicine and many\n other advances).\n \n\n\n### Engineering and machine learning\n\n\n\n The models described in this article run on the powerful GPU of a modern\n computer or a smartphone. Yet, let’s speculate about what a “more physical”\n implementation of such a system could look like. We can imagine it as a grid\n of tiny independent computers, simulating individual cells. Each of those\n computers would require approximately 10Kb of ROM to store the “cell\n genome”: neural network weights and the control code, and about 256 bytes of\n RAM for the cell state and intermediate activations. The cells must be able\n to communicate their 16-value state vectors to neighbors. Each cell would\n also require an RGB-diode to display the color of the pixel it represents. A\n single cell update would require about 10k multiply-add operations and does\n not have to be synchronised across the grid. We propose that cells might\n wait for random time intervals between updates. The system described above\n is uniform and decentralised. Yet, our method provides a way to program it\n to reach the predefined global state, and recover this state in case of\n multi-element failures and restarts. We therefore conjecture this kind of\n modeling may be used for designing reliable, self-organising agents. On the\n more theoretical machine learning front, we show an instance of a\n decentralized model able to accomplish remarkably complex tasks. We believe\n this direction to be opposite to the more traditional global modeling used\n in the majority of contemporary work in the deep learning field, and we hope\n this work to be an inspiration to explore more decentralized learning\n modeling.\n \n\n\n\n![](./Growing Neural Cellular Automata_files/multiple-pages.svg)\n\n This article is part of the\n [Differentiable Self-organizing Systems Thread](https://distill.pub/2020/selforg/),\n an experimental format collecting invited short articles delving into\n differentiable self-organizing systems, interspersed with critical\n commentary from several experts in adjacent fields.\n \n\n\n[Differentiable Self-organizing Systems Thread](https://distill.pub/2020/selforg/)\n[Self-classifying MNIST Digits](https://distill.pub/2020/selforg/mnist/)", "bibliography_bbl": "", "bibliography_bib": [{"title": "Top-down models in biology: explanation and control of complex living systems above the molecular level"}, {"title": "Re-membering the body: applications of computational neuroscience to the top-down control of regeneration of limbs and other complex organs"}, {"title": "Transmembrane voltage potential controls embryonic eye patterning in Xenopus laevis"}, {"title": "Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks"}, {"title": "The chemical basis of morphogenesis"}, {"title": "Complex Patterns in a Simple System"}, {"title": "Theory of Self-Reproducing Automata"}, {"title": "MATHEMATICAL GAMES"}, {"title": "A New Kind of Science"}, {"title": "Generalization of Conway\"s "Game of Life" to a continuous domain - SmoothLife"}, {"title": "Lenia: Biology of Artificial Life"}, {"title": "Intrinsically Motivated Exploration for Automated Discovery of Patterns in Morphogenetic Systems"}, {"title": "Evolving Self-organizing Cellular Automata Based on Neural Network Genotypes"}, {"title": "CA-NEAT: Evolved Compositional Pattern Producing Networks for Cellular Automata Morphogenesis and Replication"}, {"title": "Evolving a Self-Repairing, Self-Regulating, French Flag Organism"}, {"title": "Learning Cellular Automaton Dynamics with Neural Networks"}, {"title": "Cellular automata as convolutional neural networks"}, {"title": "Neural GPUs Learn Algorithms"}, {"title": "Improving the Neural GPU Architecture for Algorithm Learning"}, {"title": "A Comprehensive Survey on Graph Neural Networks"}, {"title": "Convolutional Networks on Graphs for Learning Molecular Fingerprints"}, {"title": "Dynamic Graph CNN for Learning on Point Clouds"}, {"title": "Learning to Control Self- Assembling Morphologies: A Study of Generalization via Modularity"}, {"title": "Flocks, Herds and Schools: A Distributed Behavioral Model"}, {"title": "Mergeable nervous systems for robots"}, {"title": "Kilobot: A low cost scalable robot system for collective behaviors"}, {"title": "What Bodies Think About: Bioelectric Computation Outside the Nervous System"}, {"title": "A scalable pipeline for designing reconfigurable organisms"}, {"title": "Perspective: The promise of multi-cellular engineered living systems"}, {"title": "Physiological inputs regulate species-specific anatomy during embryogenesis and regeneration"}, {"title": "Long-range neural and gap junction protein-mediated cues control polarity during planarian regeneration"}, {"title": "Long-Term, Stochastic Editing of Regenerative Anatomy via Targeting Endogenous Bioelectric Gradients"}, {"title": "Pattern Regeneration in Coupled Networks"}, {"title": "Bioelectrical control of positional information in development and regeneration: A review of conceptual and computational advances"}, {"title": "Modeling Cell Migration in a Simulated Bioelectrical Signaling Network for Anatomical Regeneration"}, {"title": "Investigating the effects of noise on a cell-to-cell communication mechanism for structure regeneration"}, {"title": "Social Intelligence"}, {"title": "Inceptionism: Going deeper into neural networks"}]}
{"source": "distill", "source_filetype": "html", "converted_with": "python", "paper_version": "", "post_title": "Understanding RL Vision", "authors": ["Jacob Hilton", "Nick Cammarata", "Shan Carter", "Gabriel Goh", "Chris Olah"], "date_published": "2020-11-17", "data_last_modified": "", "url": "", "abstract": "In this article, we apply interpretability techniques to a reinforcement learning (RL) model trained to play the video game CoinRun. Using attribution  combined with dimensionality reduction as in, we build an interface for exploring the objects detected by the model, and how they influence its value function and policy. We leverage this interface in several ways.", "author_comment": "", "journal_ref": "distill pub", "doi": "https://doi.org/10.23915/distill.00029", "primary_category": "", "categories": "", "citation_level": "", "main_tex_filename": "", "text": "In this article, we apply interpretability techniques to a reinforcement learning (RL) model trained to play the video game CoinRun . Using attribution  combined with dimensionality reduction as in , we build an interface for exploring the objects detected by the model, and how they influence its value function and policy. We leverage this interface in several ways.\n \n\n\n* **[Dissecting failure](https://distill.pub/2020/understanding-rl-vision/#dissecting-failure).** We perform a step-by-step analysis of the agent’s behavior in cases where it failed to achieve the maximum reward, allowing us to understand what went wrong, and why. For example, one case of failure was caused by an obstacle being temporarily obscured from view.\n* **[Hallucinations](https://distill.pub/2020/understanding-rl-vision/#hallucinations).** We find situations when the model “hallucinated” a feature not present in the observation, thereby explaining inaccuracies in the model’s value function. These were brief enough that they did not affect the agent’s behavior.\n* **[Model editing](https://distill.pub/2020/understanding-rl-vision/#model-editing).** We hand-edit the weights of the model to blind the agent to certain hazards, without otherwise changing the agent’s behavior. We verify the effects of these edits by checking which hazards cause the new agents to fail. Such editing is only made possible by our previous analysis, and thus provides a quantitative validation of this analysis.\n\n\n\n Our results depend on levels in CoinRun being procedurally-generated, leading us to formulate a [diversity hypothesis](https://distill.pub/2020/understanding-rl-vision/#diversity-hypothesis) for interpretability. If it is correct, then we can expect RL models to become more interpretable as the environments they are trained on become more diverse. We provide evidence for our hypothesis by measuring the relationship between interpretability and generalization.\n \n\n\n\n Finally, we provide a thorough [investigation](https://distill.pub/2020/understanding-rl-vision/#feature-visualization) of several interpretability techniques in the context of RL vision, and pose a number of [questions](https://distill.pub/2020/understanding-rl-vision/#questions) for further research.\n \n\n\n\n## Our CoinRun model\n\n\n\n CoinRun is a side-scrolling platformer in which the agent must dodge enemies and other traps and collect the coin at the end of the level.\n \n\n\n\n\n\n\nOur trained model playing CoinRun. **Left**: full resolution. **Right**: 64x64 RGB observations given to the model.\n\n\n CoinRun is procedurally-generated, meaning that each new level encountered by the agent is randomly generated from scratch. This incentivizes the model to learn how to spot the different kinds of objects in the game, since it cannot get away with simply memorizing a small number of specific trajectories .We use the original version of CoinRun , not the version from Procgen Benchmark , which is slightly different. To play CoinRun yourself, please follow the instructions [here](https://github.com/openai/coinrun).\n\n\n\n\n Here are some examples of the objects used, along with walls and floors, to generate CoinRun levels.\n \n\n\n\n\n\n| Full resolution | \n |  |  | \n | \n |  |  |\n| Model resolution | \n |  |  | \n | \n |  |  |\n|  | The agent, in mid air (left) and about to jump (right). The agent also appears in beige, blue and green. | Coins, which have to be collected. | Stationary buzzsaw obstacles, which must be dodged. | Enemies, which must be dodged, moving left and right. There are several alternative sprites, all with white trails. | Boxes, which the agent can both move past and land on top of. | Lava at the bottom of a chasm. | The velocity info painted into the top left of each observation, indicating the agent’s horizontal and vertical velocities.Painting in the velocity info allows the model to infer the agent’s motion from a single frame. The shade of the left square indicates the agent’s horizontal velocity (black for left at full speed, white for right at full speed), and the shade of the right square indicates the agent’s vertical velocity (black for down at full speed, white for up at full speed). In this example, the agent is moving forward and about to land (and is thus moving right and down). |\n\n\n\n\n There are 9 actions available to the agent in CoinRun:\n \n\n\n\n\n\n| ← | → |  | Left and right change the agent’s horizontal velocity. They still work while the agent is in mid-air, but have less of an effect. |\n| ↓ |  |  | Down cancels a jump if used immediately after up, and steps the agent down from boxes. |\n| ↑ | ↖ | ↗ | Up causes the agent to jump after the next non-up action. Diagonal directions have the same effect as both component directions combined. |\n| A | B | C | A, B and C do nothing.The original version of CoinRun only has 1 “do nothing” action, but our version ended up with 3 when “A” and “B” actions were added to be used in other games. For consistency, we have relabeled the original “do nothing” action as “C”. |\n\n\n\n\n We trained a convolutional neural network on CoinRun for around 2 billion timesteps, using PPO , an actor-critic algorithm.We used the standard PPO hyperparameters for CoinRun , except that we used twice as many copies of the environment per worker and twice and many workers. The effect of these changes was to increase the effective batch size, which seemed to be necessary to reach the same performance with our smaller architecture. The architecture of our network is described in [Appendix C](https://distill.pub/2020/understanding-rl-vision/#architecture). We used a non-recurrent network, to avoid any need to visualize multiple frames at once. Thus our model observes a single downsampled 64x64 image, and outputs a value function (an estimate of the total future time-discounted reward) and a policy (a probability distribution over the actions, from which the next action is sampled).\n \n\n\n\n\n\n\n\n\n\n\n\nobservation\n\nCNN\n\n\n\n\n\nvalue function\n\n\n\n\n\n\n\n\n\n\nlogits\n\nsoftmax\npolicy\n\nSchematic of a typical non-recurrent convolutional actor-critic model, such as ours.\n\n\n Since the only available reward is a fixed bonus for collecting the coin, the value function estimates the time-discountedWe use a discount rate of 0.999 per timestep. probability that the agent will successfully complete the level.\n \n\n\n## Model analysis\n\n\n\n Having trained a strong RL agent, we were curious to see what it had learned. Following , we developed an interface for examining trajectories of the agent playing the game. This incorporates attribution from a hidden layer that recognizes objects, which serves to highlight objects that positively or negatively influence a particular network output. By applying dimensionality reduction, we obtain attribution vectors whose components correspond to different types of object, which we indicate using different colors.\n \n\n\n\n Here is our interface for a typical trajectory, with the value function as the network output. It reveals the model using obstacles, coins, enemies and more to compute the value function.\n \n\n\n\n\n  \n\n| Value function attribution |\n| --- |\n\n \n\n| Observation\n  Video game pixels seen by the model | Positive attribution\n (good news)  Color overlay shows objects predictive of success | Negative attribution\n (bad news)  Color overlay shows objects predictive of failure |\n| --- | --- | --- |\n|     |      |      |\n\n Attribution legend\n (hover to isolate)   \n Colors correspond to vector components after dimensionality reduction, icons show dataset examples, and labels are hand-composed   \n   Buzzsaw  \nobstacle  Coin  Enemy  \nmoving  \nleft  Agent  \nor enemy  \nmoving right  Buzzsaw  \nobstacle  \nor platform  Platform  Velocity  \ninfo or left-  \nfacing wall  Step hover  \nto show residual \n Everything  \nelse\n  Timeline ← ►   \n Speed:\u2003\u2002FPS →   \n        Value function   \n Estimated discounted win probability 93.9%    \n\n\n### Dissecting failure\n\n\n\n Our fully-trained model fails to complete around 1 in every 200 levels. We explored a few of these failures using our interface, and found that we were usually able to understand why they occurred.\n \n\n\n\n The failure often boils down to the fact that the model has no memory, and must therefore choose its action based only on the current observation. It is also common for some unlucky sampling of actions from the agent’s policy to be partly responsible.\n \n\n\n\n Here are some cherry-picked examples of failures, carefully analyzed step-by-step.\n \n\n\n\n\n| \n **Buzzsaw obstacle obscured by enemy**\n\n\n **Stepping down to avoid jumping**\n\n\n **Landing platform moving off-screen**\n | \n\n The agent moves too far to the right while in mid-air as a result of a buzzsaw obstacle being temporarily hidden from view by a moving enemy. The buzzsaw comes back into view, but too late to avoid a collision.\n \n\n The agent presses down in a bid to delay a jump. This causes the agent to inadvertently step down from a box and onto an enemy.\n \n\n The agent fails to move far enough to the right while in mid-air, as a result of the platform where it was intending to land moving below the field of view.\n \n |\n\n\n\n\n\n\nPrev  \n\nStart\n\n\n►\n\n\nNext  \n\nEnd\n\n  \n\n\nTimestep **1** of **18**\n\n**Timestep 1**: The agent moves right, invited by the unoccupied floor (• •) in front of it.**Timestep 2**: The agent prepares to jump, seeing the wall (•) up ahead.**Timestep 3**: The agent jumps by releasing up, analyzing something (◦) on the wall. (The "residual" feature can be triggered by a number of different objects, and can be viewed by hovering over the last legend item.)**Timesteps 4–6**: The agent is in mid-air, trying to control its horizontal motion. Its policy has higher entropy as its actions matter less at the start of a jump (due to the entropy bonus used in PPO).**Timesteps 4–6**: The agent is in mid-air, trying to control its horizontal motion. Its policy has higher entropy as its actions matter less at the start of a jump (due to the entropy bonus used in PPO).**Timesteps 4–6**: The agent is in mid-air, trying to control its horizontal motion. Its policy has higher entropy as its actions matter less at the start of a jump (due to the entropy bonus used in PPO).**Timesteps 7–8**: The agent moves right to be able to reach the top of the wall (•), though it is slightly discouraged from doing so by the prescence of the enemy moving right (•).**Timesteps 7–8**: The agent moves right to be able to reach the top of the wall (•), though it is slightly discouraged from doing so by the prescence of the enemy moving right (•).**Timesteps 9–13**: Having adjusted its horizontal motion, the agent returns to a policy with higher entropy. It is paying attention to the enemy moving right (•), which it is on track to avoid, but it cannot see the buzzsaw obstacle (•) obscured from view behind it. With some bad luck, the agent happens to move right at every timestep during this period.**Timesteps 9–13**: Having adjusted its horizontal motion, the agent returns to a policy with higher entropy. It is paying attention to the enemy moving right (•), which it is on track to avoid, but it cannot see the buzzsaw obstacle (•) obscured from view behind it. With some bad luck, the agent happens to move right at every timestep during this period.**Timesteps 9–13**: Having adjusted its horizontal motion, the agent returns to a policy with higher entropy. It is paying attention to the enemy moving right (•), which it is on track to avoid, but it cannot see the buzzsaw obstacle (•) obscured from view behind it. With some bad luck, the agent happens to move right at every timestep during this period.**Timesteps 9–13**: Having adjusted its horizontal motion, the agent returns to a policy with higher entropy. It is paying attention to the enemy moving right (•), which it is on track to avoid, but it cannot see the buzzsaw obstacle (•) obscured from view behind it. With some bad luck, the agent happens to move right at every timestep during this period.**Timesteps 9–13**: Having adjusted its horizontal motion, the agent returns to a policy with higher entropy. It is paying attention to the enemy moving right (•), which it is on track to avoid, but it cannot see the buzzsaw obstacle (•) obscured from view behind it. With some bad luck, the agent happens to move right at every timestep during this period.**Timestep 14**: Finally the buzzsaw obstacle (•) comes into view, and the agent tries to move left to avoid it.**Timestep 15**: For some reason the agent no longer seems to realize that it is on track to collide with the buzzsaw obstacle (•), and returns to a policy with higher entropy. With further bad luck, the agent again happens to move right.**Timesteps 16–17**: The danger of the buzzsaw obstacle (•) becomes clear to the agent again. The agent tries to move left to avoid it, and also prepares to immediately jump upon landing. Pulling off the jump actually matters more to its chances of survival, since it is already moving right.**Timesteps 16–17**: The danger of the buzzsaw obstacle (•) becomes clear to the agent again. The agent tries to move left to avoid it, and also prepares to immediately jump upon landing. Pulling off the jump actually matters more to its chances of survival, since it is already moving right.**Timestep 18**: The agent makes a futile attempt to avoid the buzzsaw obstacle (•) by releasing up to jump, but it is too late, and the episode is terminated.**Timestep 1**: The agent prepares to jump, seeing the wall (•) immediately in front of it. The agent cannot jump when it is in mid-air, but jumping is only triggered when up is released, and it is allowed to start pressing up before landing.**Timesteps 2–3**: The agent\"s policy is dominated by the 3 upward directions (which delay the jump) and down (which cancels it). Any other action would trigger the jump. So the agent is trying to delay the jump, partly because of the enemy moving left (•), which both positively influences these actions and negatively influences others.**Timesteps 2–3**: The agent\"s policy is dominated by the 3 upward directions (which delay the jump) and down (which cancels it). Any other action would trigger the jump. So the agent is trying to delay the jump, partly because of the enemy moving left (•), which both positively influences these actions and negatively influences others.**Timestep 4**: The agent\"s policy is still dominated by the 3 updward directions and down, and at this point the agent happens to move down. Unfortunately, the agent does not seem to have taken into account that this will cause it to step down from the box it is standing on and onto an enemy.**Timestep 5**: The agent is now happier to jump by releasing up, based on the position of the enemy moving left (•), seeming not to realize that the jump has already been cancelled.**Timesteps 6–7**: The agent returns to a policy dominated by the 3 upward directions and down, seemingly influenced by the wall (•) in front of it and, for some reason, the buzzsaw obstacle (••) behind it. The agent\"s apparent confusion may be because it is already doomed, no matter which actions it takes.**Timesteps 6–7**: The agent returns to a policy dominated by the 3 upward directions and down, seemingly influenced by the wall (•) in front of it and, for some reason, the buzzsaw obstacle (••) behind it. The agent\"s apparent confusion may be because it is already doomed, no matter which actions it takes.**Timestep 1**: The agent moves right across the platform (• •) it is on.**Timestep 2**: The agent prepares to jump, seeing the wall (•) across the other side of the chasm ahead of it. Even though it is not yet at the edge of the platform it is currently on, it is timing its jump so as to land on the nearest platform up ahead, perhaps because it can see that there are no obstacles or enemies there.**Timestep 3**: The agent jumps by releasing up.**Timesteps 4–6**: The agent is in mid-air, trying to control its horizontal motion. Its policy has higher entropy as its actions matter less at the start of a jump (due to the entropy bonus used in PPO). Unfortunately, it happens to move left at every timestep during this period.**Timesteps 4–6**: The agent is in mid-air, trying to control its horizontal motion. Its policy has higher entropy as its actions matter less at the start of a jump (due to the entropy bonus used in PPO). Unfortunately, it happens to move left at every timestep during this period.**Timesteps 4–6**: The agent is in mid-air, trying to control its horizontal motion. Its policy has higher entropy as its actions matter less at the start of a jump (due to the entropy bonus used in PPO). Unfortunately, it happens to move left at every timestep during this period.**Timesteps 7–8**: The agent notices that its horizontal velocity (• • •) has been reduced, and tries to move right to compensate. (Several features read from the velocity info as a secondary purpose.) With some bad luck, it moves up but not right on its first move.**Timesteps 7–8**: The agent notices that its horizontal velocity (• • •) has been reduced, and tries to move right to compensate. (Several features read from the velocity info as a secondary purpose.) With some bad luck, it moves up but not right on its first move.**Timesteps 9–13**: The platforms have now moved below the agent\"s field of view. With little to go by, its policy has high entropy, with some bias towards moving right rather than left. With further bad luck, this rightward bias is not reflected in the actions sampled.**Timesteps 9–13**: The platforms have now moved below the agent\"s field of view. With little to go by, its policy has high entropy, with some bias towards moving right rather than left. With further bad luck, this rightward bias is not reflected in the actions sampled.**Timesteps 9–13**: The platforms have now moved below the agent\"s field of view. With little to go by, its policy has high entropy, with some bias towards moving right rather than left. With further bad luck, this rightward bias is not reflected in the actions sampled.**Timesteps 9–13**: The platforms have now moved below the agent\"s field of view. With little to go by, its policy has high entropy, with some bias towards moving right rather than left. With further bad luck, this rightward bias is not reflected in the actions sampled.**Timesteps 9–13**: The platforms have now moved below the agent\"s field of view. With little to go by, its policy has high entropy, with some bias towards moving right rather than left. With further bad luck, this rightward bias is not reflected in the actions sampled.**Timesteps 14–18**: As the wall (•) across the other side of the chasm comes back into view, the agent can see that its horizontal velocity (•) is not high enough, and so it tries to move right.**Timesteps 14–18**: As the wall (•) across the other side of the chasm comes back into view, the agent can see that its horizontal velocity (•) is not high enough, and so it tries to move right.**Timesteps 14–18**: As the wall (•) across the other side of the chasm comes back into view, the agent can see that its horizontal velocity (•) is not high enough, and so it tries to move right.**Timesteps 14–18**: As the wall (•) across the other side of the chasm comes back into view, the agent can see that its horizontal velocity (•) is not high enough, and so it tries to move right.**Timesteps 14–18**: As the wall (•) across the other side of the chasm comes back into view, the agent can see that its horizontal velocity (•) is not high enough, and so it tries to move right.**Timesteps 19–27**: The agent realizes from the wall (•) of the chasm that it has failed to complete the jump. The value function plummets to below 5%, and the policy degenerates.**Timesteps 19–27**: The agent realizes from the wall (•) of the chasm that it has failed to complete the jump. The value function plummets to below 5%, and the policy degenerates.**Timesteps 19–27**: The agent realizes from the wall (•) of the chasm that it has failed to complete the jump. The value function plummets to below 5%, and the policy degenerates.**Timesteps 19–27**: The agent realizes from the wall (•) of the chasm that it has failed to complete the jump. The value function plummets to below 5%, and the policy degenerates.**Timesteps 19–27**: The agent realizes from the wall (•) of the chasm that it has failed to complete the jump. The value function plummets to below 5%, and the policy degenerates.**Timesteps 19–27**: The agent realizes from the wall (•) of the chasm that it has failed to complete the jump. The value function plummets to below 5%, and the policy degenerates.**Timesteps 19–27**: The agent realizes from the wall (•) of the chasm that it has failed to complete the jump. The value function plummets to below 5%, and the policy degenerates.**Timesteps 19–27**: The agent realizes from the wall (•) of the chasm that it has failed to complete the jump. The value function plummets to below 5%, and the policy degenerates.**Timesteps 19–27**: The agent realizes from the wall (•) of the chasm that it has failed to complete the jump. The value function plummets to below 5%, and the policy degenerates.\n\n\n\n  \n\n| \n Attribution for: | → | show choices |  | \n Policy: (probabilities and sampled action) | ←→↓↑↗↖ABC |\n| --- | --- | --- | --- | --- | --- |\n\n \n\n| Choose attribution: | \n Value function |  ←  →  ↓  ↑  ↗  ↖  A  B  C  |\n| --- | --- | --- |\n\n \n\n| Observation\n  | Positive attribution\n  | Negative attribution\n  |\n| --- | --- | --- |\n|     |      |      |\n\n \n Attribution totals\n (colors summed over spatial positions) 0        \n Attribution legend\n (hover to isolate)    \n   Buzzsaw  \nobstacle  Coin  Enemy  \nmoving  \nleft  Agent  \nor enemy  \nmoving right  Buzzsaw  \nobstacle  \nor platform  Platform  Velocity  \ninfo or left-  \nfacing wall  Step hover  \nto show residual \n Everything  \nelse\n  \n  \n\n| \n Attribution for: | Value function | show choices |  | \n Policy: (probabilities and sampled action) | ←→↓↑↗↖ABC |\n| --- | --- | --- | --- | --- | --- |\n\n \n\n| Choose attribution: | \n Value function |  ←  →  ↓  ↑  ↗  ↖  A  B  C  |\n| --- | --- | --- |\n\n \n\n| Observation\n  | Positive attribution\n  | Negative attribution\n  |\n| --- | --- | --- |\n|     |      |      |\n\n \n Attribution totals\n (colors summed over spatial positions) 0        \n Attribution legend\n (hover to isolate)    \n   Buzzsaw  \nobstacle  Coin  Enemy  \nmoving  \nleft  Agent  \nor enemy  \nmoving right  Buzzsaw  \nobstacle  \nor platform  Platform  Velocity  \ninfo or left-  \nfacing wall  Step hover  \nto show residual \n Everything  \nelse\n  \n  \n\n| \n Attribution for: | Value function | show choices |  | \n Policy: (probabilities and sampled action) | ←→↓↑↗↖ABC |\n| --- | --- | --- | --- | --- | --- |\n\n \n\n| Choose attribution: | \n Value function |  ←  →  ↓  ↑  ↗  ↖  A  B  C  |\n| --- | --- | --- |\n\n \n\n| Observation\n  | Positive attribution\n  | Negative attribution\n  |\n| --- | --- | --- |\n|     |      |      |\n\n \n Attribution totals\n (colors summed over spatial positions) 0        \n Attribution legend\n (hover to isolate)    \n   Buzzsaw  \nobstacle  Coin  Enemy  \nmoving  \nleft  Agent  \nor enemy  \nmoving right  Buzzsaw  \nobstacle  \nor platform  Platform  Velocity  \ninfo or left-  \nfacing wall  Step hover  \nto show residual \n Everything  \nelse\n  \n\n\n### Hallucinations\n\n\n\n We searched for errors in the model using generalized advantage estimation (GAE) ,We use the same GAE hyperparameters as in training, namely γ=0.999\\gamma=0.999γ=0.999 and λ=0.95\\lambda=0.95λ=0.95. which measures how successful each action turned out relative to the agent’s expectations. An unusually high or low GAE indicates that either something unexpected occurred, or the agent’s expectations were miscalibrated. Filtering for such timesteps can therefore find problems with the value function or policy.\n \n\n\n\n Using our interface, we found a couple of cases in which the model “hallucinated” a feature not present in the observation, causing the value function to spike.\n \n\n\n\n\n| \n **Coin hallucination**\n\n\n **Buzzsaw hallucination**\n | \n\n At one point the value function spiked upwards from 95% to 98% for a single timestep. This was due to a curved yellow-brown shape in the background, which happened to appear next to a wall, being mistaken for a coin.\n \n\n At another point the value function spiked downwards from 94% to 85% for a single timestep. This was due to the agent, colored in gray-blue and crouching against a mottled background, being mistaken for a buzzsaw obstacle. An actual buzzsaw was also present in the observation, but the main effect was from the misjudged agent, as shown by the larger red circle around the agent (hover over the first legend item to isolate).\n \n |\n\n\n\n\n        Value function  98.0%         \n\n| Value function attribution |\n| --- |\n\n \n\n| Observation\n  | Positive attribution\n  | Negative attribution\n  |\n| --- | --- | --- |\n|     |      |      |\n\n \n Attribution totals\n (colors summed over spatial positions) 0        \n Attribution legend\n (hover to isolate)    \n   Buzzsaw  \nobstacle  Coin  Enemy  \nmoving  \nleft  Agent  \nor enemy  \nmoving right  Buzzsaw  \nobstacle  \nor platform  Platform  Velocity  \ninfo or left-  \nfacing wall  Step hover  \nto show residual \n Everything  \nelse\n  \n        Value function  85.0%         \n\n| Value function attribution |\n| --- |\n\n \n\n| Observation\n  | Positive attribution\n  | Negative attribution\n  |\n| --- | --- | --- |\n|     |      |      |\n\n \n Attribution totals\n (colors summed over spatial positions) 0        \n Attribution legend\n (hover to isolate)    \n   Buzzsaw  \nobstacle  Coin  Enemy  \nmoving  \nleft  Agent  \nor enemy  \nmoving right  Buzzsaw  \nobstacle  \nor platform  Platform  Velocity  \ninfo or left-  \nfacing wall  Step hover  \nto show residual \n Everything  \nelse\n  \n\n\n### Model editing\n\n\n\n Our analysis so far has been mostly qualitative. To quantitatively validate our analysis, we hand-edited the model to make the agent blind to certain features identified by our interface: buzzsaw obstacles in one case, and left-moving enemies in another. Our method for this can be thought of as a primitive form of [circuit](https://distill.pub/2020/circuits/)-editing , and we explain it in detail in [Appendix A](https://distill.pub/2020/understanding-rl-vision/#model-editing-method).\n \n\n\n\n We evaluated each edit by measuring the percentage of levels that the new agent failed to complete, broken down by the object that the agent collided with to cause the failure. Our results show that our edits were successful and targeted, with no statistically measurable effects on the agent’s other abilities.The data for this plot are as follows.  \nPercentage of levels failed due to: buzzsaw obstacle / enemy moving left / enemy moving right / multiple or other:  \n- Original model: 0.37% / 0.16% / 0.12% / 0.08%  \n- Buzzsaw obstacle blindness: 12.76% / 0.16% / 0.08% / 0.05%  \n- Enemy moving left blindness: 0.36% / 4.69% / 0.97% / 0.07%  \nEach model was tested on 10,000 levels.\n\n\n\n\n *{stroke-linecap:butt;stroke-linejoin:round;} Original modelBuzzsaw obstacleblindnessEnemy moving leftblindness0%2%4%6%8%10%12%Percentage of levels failedFailure rate by causeCausesBuzzsaw obstacleEnemy moving leftEnemy moving rightMultiple or other\nResults of testing each model on 10,000 levels. Note that moving enemies can change direction.\n\n\n We did not manage to achieve complete blindness, however: the buzzsaw-edited model still performed significantly better than the original model did when we made the buzzsaws completely invisible.Our results on the version of the game with invisible buzzsaws are as follows.  \nPercentage of levels failed due to: buzzsaw obstacle / enemy moving left / enemy moving right / multiple or other:  \nOriginal model, invisible buzzsaws: 32.20% / 0.05% / 0.05% / 0.05%  \nWe tested the model on 10,000 levels.  \nWe experimented briefly with iterating the editing procedure, but were not able to achieve more than around 50% buzzsaw blindness by this metric without affecting the model’s other abilities. This implies that the model has other ways of detecting buzzsaws than the feature identified by our interface.\n \n\n\n\n Here are the original and edited models playing some cherry-picked levels.\n \n\n\n\n\n\n| \n **Level 1**\n\n\n **Level 2**\n\n\n **Level 3**\n\n\n\n►\n\n\n►\n\n\n►\n\n | \n\n\n\n\n\n\n\n\n\n\n\n\n\n | \n\n\n\n\n\n\n\n\n\n\n\n\n\n | \n\n\n\n\n\n\n\n\n\n\n\n\n\n |\n|  | Original model. | Buzzsaw obstacle blindness. | Enemy moving left blindness. |\n\n\n\n## The diversity hypothesis\n\n\n\n All of the above analysis uses the same hidden layer of our network, the third of five convolutional layers, since it was much harder to find interpretable features at other layers. Interestingly, the level of abstraction at which this layer operates – finding the locations of various in-game objects – is exactly the level at which CoinRun levels are randomized using procedural generation. Furthermore, we found that training on many randomized levels was essential for us to be able to find any interpretable features at all.\n \n\n\n\n This led us to suspect that the diversity introduced by CoinRun’s randomization is linked to the formation of interpretable features. We call this the diversity hypothesis:\n \n\n\n\n> \n>  Interpretable features tend to arise (at a given level of abstraction) if and only if the training distribution is diverse enough (at that level of abstraction).\n>  \n\n\n\n Our explanation for this hypothesis is as follows. For the forward implication (“only if”), we only expect features to be interpretable if they are general enough, and when the training distribution is not diverse enough, models have no incentive to develop features that generalize instead of overfitting. For the reverse implication (“if”), we do not expect it to hold in a strict sense: diversity on its own is not enough to guarantee the development of interpretable features, since they must also be relevant to the task. Rather, our intention with the reverse implication is to hypothesize that it holds very often in practice, as a result of generalization being bottlenecked by diversity.\n \n\n\n\n In CoinRun, procedural generation is used to incentivize the model to learn skills that generalize to unseen levels . However, only the layout of each level is randomized, and correspondingly, we were only able to find interpretable features at the level of abstraction of objects. At a lower level, there are only a handful of visual patterns in the game, and the low-level features of our model seem to consist mostly of memorized color configurations used for picking these out. Similarly, the game’s high-level dynamics follow a few simple rules, and accordingly the high-level features of our model seem to involve mixtures of combinations of objects that are hard to decipher. To explore the other convolutional layers, see the interface [here](https://openaipublic.blob.core.windows.net/rl-clarity/attribution/demo/interface.html).\n \n\n\n### Interpretability and generalization\n\n\n\n To test our hypothesis, we made the training distribution less diverse, by training the agent on a fixed set of 100 levels. This dramatically reduced our ability to interpret the model’s features. Here we display an interface for the new model, generated in the same way as the one [above](https://distill.pub/2020/understanding-rl-vision/#interface). The smoothly increasing value function suggests that the model has memorized the number of timesteps until the end of the level, and the features it uses for this focus on irrelevant background objects. Similar overfitting occurs for other video games with a limited number of levels .\n \n\n\n\n\n  \n\n| Value function attribution |\n| --- |\n\n \n\n| Observation\n  Video game pixels seen by the model | Positive attribution\n (good news)  Color overlay shows objects predictive of success | Negative attribution\n (bad news)  Color overlay shows objects predictive of failure |\n| --- | --- | --- |\n|     |      |      |\n\n Attribution legend\n (hover to isolate)   \n Colors correspond to vector components after dimensionality reduction, icons show dataset examples, and labels are hand-composed   \n   \u2003???  \u2003???  \u2003???  \u2003???  \u2003???  \u2003???  \u2003???  Agent, walls  \nand velocity  \ninfo? hover  \nto show residual \n Everything  \nelse\n  Timeline ← ►   \n Speed:\u2003\u2002FPS →   \n        Value function   \n Estimated discounted win probability 94.4%    \n\n\n\n We attempted to quantify this effect by varying the number of levels used to train the agent, and evaluating the 8 features identified by our interface on how interpretable they were.The interfaces used for this evaluation can be found [here](https://openaipublic.blob.core.windows.net/rl-clarity/attribution/finite_levels/index.html). Features were scored based on how consistently they focused on the same objects, and whether the value function attribution made sense – for example, background objects should not be relevant. This process was subjective and noisy, but that may be unavoidable. We also measured the generalization ability of each model, by testing the agent on unseen levels .The data for this plot are as follows.  \n- Number of training levels: 100 / 300 / 1000 / 3,000 / 10,000 / 30,000 / 100,000  \n- Percentage of levels completed (train, run 1): 99.96% / 99.82% / 99.67% / 99.65% / 99.47% / 99.55% / 99.57%  \n- Percentage of levels completed (train, run 2): 99.97% / 99.86% / 99.70% / 99.46% / 99.39% / 99.50% / 99.37%  \n- Percentage of levels completed (test, run 1): 61.81% / 66.95% / 74.93% / 89.87% / 97.53% / 98.66% / 99.25%  \n- Percentage of levels completed (test, run 2): 64.13% / 67.64% / 73.46% / 90.36% / 97.44% / 98.89% / 99.35%  \n- Percentage of features interpretable (researcher 1, run 1): 52.5% / 22.5% / 11.25% / 45% / 90% / 75% / 91.25%  \n- Percentage of features interpretable (researcher 2, run 1): 8.75% / 8.75% / 10% / 26.25% / 56.25% / 90% / 70%  \n- Percentage of features interpretable (researcher 1, run 2): 15% / 13.75% / 15% / 23.75% / 53.75% / 90% / 96.25%  \n- Percentage of features interpretable (researcher 2, run 2): 3.75% / 6.25% / 21.25% / 45% / 72.5% / 83.75% / 77.5%  \nPercentages of levels completed are estimated by sampling 10,000 levels with replacement.\n\n\n\n\n *{stroke-linecap:butt;stroke-linejoin:round;} 10210310410560%70%80%90%100%Percentage of levels completedGeneralizationTrainTest1021031041050%20%40%60%80%100%Percentage of features interpretableInterpretable features0.00.20.40.60.81.0Number of training levels0.00.20.40.60.81.0\nComparison of models trained on different numbers of levels. Two models were trained for each number of levels, and two researchers independently evaluated how interpretable the features of each model were, without being shown the number of levels.Our methodology had some flaws. Firstly, the researchers were not completely blind to the number of levels: for example, it is possible to infer something about the number of levels from the smoothness of graphs of the value function, since with fewer levels the model is better able to memorize the number of timesteps until the end of the level. Secondly, since evaluations are somewhat tedious, we stopped them once we thought the trend had become clear, introducing some selection bias. Therefore these results should be considered primarily illustrative. Each model was tested on 10,000 train and 10,000 test levels sampled with replacement. Shaded areas in the left plot show the range of values over both models, though these are mostly too narrow to be visible. Error bars in the right plot show ±1 population standard deviation over all four model–researcher pairs.\n\n\n Our results illustrate how diversity may lead to interpretable features via generalization, lending support to the diversity hypothesis. Nevertheless, we still consider the hypothesis to be highly unproven.\n \n\n\n## Feature visualization\n\n\n\n[Feature visualization](https://distill.pub/2017/feature-visualization/)  answers questions about what certain parts of a network\u2009are looking for by generating examples. This can be done by applying gradient descent to the input image, starting from random noise, with the objective of activating a particular neuron or group of neurons. While this method works well for an image classifier trained on ImageNet , for our CoinRun model it yields only featureless clouds of color. Only for the first layer, which computes simple convolutions of the input, does the method produce comparable visualizations for the two models.\n \n\n\n\n\n\n\n|  | ImageNet | CoinRun |\n| --- | --- | --- |\n| \n First layer\n  | \n\n | \n\n |\n| \n Intermediate layer\n  | \n\n | \n\n |\n\n\nComparison of gradient-based feature visualization for CNNs trained on ImageNet (GoogLeNet ) and on CoinRun (architecture described [below](https://distill.pub/2020/understanding-rl-vision/#architecture)). Each image was chosen to activate a neuron in the center, with the 3 images corresponding to the first 3 channels. Jittering was applied between optimization steps of up to 2 pixels for the first layer, and up to 8 pixels for the intermediate layer (mixed4a for ImageNet, [2b](https://distill.pub/2020/understanding-rl-vision/#architecture) for CoinRun).\n\n\n\n Gradient-based feature visualization has previously been shown to struggle with RL models trained on Atari games . To try to get it to work for CoinRun, we varied the method in a number of ways. Nothing we tried had any noticeable effect on the quality of the visualizations.\n \n\n\n* **Transformation robustness.** This is the method of stochastically jittering, rotating and scaling the image between optimization steps, to search for examples that are robust to these transformations . We tried both increasing and decreasing the size of the jittering. Rotating and scaling are less appropriate for CoinRun, since the observations themselves are not invariant to these transformations.\n* **Penalizing extremal colors.**By an “extremal” color we mean one of the 8 colors with maximal or minimal RGB values (black, white, red, green, blue, yellow, cyan and magenta). Noticing that our visualizations tend to use extremal colors towards the middle, we tried including in the visualization objective an L2 penalty of various strengths on the activations of the first layer, which successfully reduced the size of the extremally-colored region but did not otherwise help.\n* **Alternative objectives.** We tried using an alternative optimization objective , such as the caricature objective.The caricature objective is to maximize the dot product between the activations of the input image and the activations of a reference image. Caricatures are often an especially easy type of feature visualization to make work, and helpful for getting a first glance into what features a model has. They are demonstrated in [this notebook](https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/misc/feature_inversion_caricatures.ipynb). A more detailed manuscript by its authors  is forthcoming. We also tried using dimensionality reduction, as described [below](https://distill.pub/2020/understanding-rl-vision/#dataset-examples), to choose non-axis-aligned directions in activation space to maximize.\n* **Low-level visual diversity.** In an attempt to broaden the distribution of images seen by the model, we retrained it on a version of the game with procedurally-generated sprites. We additionally tried adding noise to the images, both independent per-pixel noise and spatially-correlated noise. Finally, we experimented briefly with adversarial training , though we did not pursue this line of inquiry very far.\n\n\n\n As shown [below](https://distill.pub/2020/understanding-rl-vision/#dataset-examples), we were able to use dataset examples to identify a number of channels that pick out human-interpretable features. It is therefore striking how resistant gradient-based methods were to our efforts. We believe that this is because solving CoinRun does not ultimately require much visual ability. Even with our modifications, it is possible to solve the game using simple visual shortcuts, such as picking out certain small configurations of pixels. These shortcuts work well on the narrow distribution of images on which the model is trained, but behave unpredictably in the full space of images in which gradient-based optimization takes place.\n \n\n\n\n Our analysis here provides further insight into the [diversity hypothesis](https://distill.pub/2020/understanding-rl-vision/#diversity-hypothesis). In support of the hypothesis, we have examples of features that are hard to interpret in the absence of diversity. But there is also evidence that the hypothesis may need to be refined. Firstly, it seems to be a lack of diversity at a low level of abstraction that harms our ability to interpret features at all levels of abstraction, which could be due to the fact that gradient-based feature visualization needs to back-propagate through earlier layers. Secondly, the failure of our efforts to increase low-level visual diversity suggests that diversity may need to be assessed in the context of the requirements of the task.\n \n\n\n### Dataset example-based feature visualization\n\n\n\n As an alternative to gradient-based feature visualization, we use dataset examples. This idea has a long history, and can be thought of as a heavily-regularized form of feature visualization . In more detail, we sample a few thousand observations infrequently from the agent playing the game, and pass them through the model. We then apply a dimensionality reduction method known as non-negative matrix factorization (NMF) to the activation channels .More precisely, we find a non-negative approximate low-rank factorization of the matrix obtained by flattening the spatial dimensions of the activations into the batch dimension. This matrix has one row per observation *per spatial position* and one column per channel: thus the dimensionality reduction does not use spatial information. For each of the resulting channels (which correspond to weighted combinations of the original channels), we choose the observations and spatial positions with the strongest activation (with a limited number of examples per position, for diversity), and display a patch from the observation at that position.\n \n\n\n\n\n![](./Understanding RL Vision_files/layer_2b_feature_0.png)Short left-facing wall\n![](./Understanding RL Vision_files/layer_2b_feature_1.png)Velocity info or left edge of screen\n![](./Understanding RL Vision_files/layer_2b_feature_2.png)Long left-facing wall\n![](./Understanding RL Vision_files/layer_2b_feature_3.png)Left end of platform\n![](./Understanding RL Vision_files/layer_2b_feature_4.png)Right end of platform\n![](./Understanding RL Vision_files/layer_2b_feature_5.png)Buzzsaw obstacle or platform\n![](./Understanding RL Vision_files/layer_2b_feature_6.png)Coin\n![](./Understanding RL Vision_files/layer_2b_feature_7.png)Top/right edge of screen\n![](./Understanding RL Vision_files/layer_2b_feature_8.png)Left end of platform\n![](./Understanding RL Vision_files/layer_2b_feature_9.png)Step\n![](./Understanding RL Vision_files/layer_2b_feature_10.png)Agent or enemy moving right\n![](./Understanding RL Vision_files/layer_2b_feature_11.png)Left edge of box\n![](./Understanding RL Vision_files/layer_2b_feature_12.png)Right end of platform\n![](./Understanding RL Vision_files/layer_2b_feature_13.png)Buzzsaw obstacle\n![](./Understanding RL Vision_files/layer_2b_feature_14.png)Top left corner of box\n![](./Understanding RL Vision_files/layer_2b_feature_15.png)Left end of platform or bottom/right of screen?\n\nDataset example-based feature visualizations for 16 NMF directions of layer [2b](https://distill.pub/2020/understanding-rl-vision/#architecture) of our CoinRun model. The grey-white checkerboard represents the edge of the screen. The labels are hand-composed.\n\n\n Unlike gradient-based feature visualization, this method finds some meaning to the different directions in activation space. However, it may still fail to provide a complete picture for each direction, since it only shows a limited number of dataset examples, and with limited context.\n \n\n\n### Spatially-aware feature visualization\n\n\n\n CoinRun observations differ from natural images in that they are much less spatially invariant. For example, the agent always appears in the center, and the agent’s velocity is always encoded in the top left. As a result, some features detect unrelated things at different spatial positions, such as reading the agent’s velocity in the top left while detecting an unrelated object elsewhere. To account for this, we developed a spatially-aware version of dataset example-based feature visualization, in which we fix each spatial position in turn, and choose the observation with the strongest activation at that position (with a limited number of reuses of the same observation, for diversity). This creates a spatial correspondence between visualizations and observations.\n \n\n\n\n Here is such a visualization for a feature that responds strongly to coins. The white squares in the top left show that the feature also responds strongly to the horizontal velocity info when it is white, corresponding to the agent moving right at full speed.\n \n\n\n\n\n![](./Understanding RL Vision_files/feature_vis_spatial.png)\n\nSpatially-aware dataset example-based feature visualization for the coin-detecting NMF direction of layer [2b](https://distill.pub/2020/understanding-rl-vision/#architecture). Transparency (revealing the diagonally-striped background) indicates a weak response, so the left half of the visualization is mostly transparent because coins never appear in the left half of observations.\n\n## Attribution\n\n\n\n Attribution  answers questions about the relationships between neurons. It is most commonly used to see how the input to a network affects a particular output – for example, in RL  – but it can also be applied to the activations of hidden layers . Although there are many approaches to attribution we could have used, we chose the method of integrated gradients . We explain in [Appendix B](https://distill.pub/2020/understanding-rl-vision/#integrated-gradients) how we applied this method a hidden layer, and how positive value function attribution can be thought of as “good news” and negative value function attribution can as “bad news”.\n \n\n\n### Dimensionality reduction for attribution\n\n\n\n We showed [above](https://distill.pub/2020/understanding-rl-vision/#dataset-examples) that a dimensionality reduction method known as non-negative matrix factorization (NMF) could be applied to the channels of activations to produce meaningful directions in activation space . We found that it is even more effective to apply NMF not to activations, but to value function attributionsAs before, we obtain the NMF directions by sampling a few thousand observations infrequently from the agent playing the game, computing the attributions, flattening the spatial dimensions into the batch dimension, and applying NMF. (working around the fact that NMF can only be applied to non-negative matricesOur workaround is to separate out the positive and negative parts of the attributions and concatenate them along the batch dimension. We could also have concatenated them along the channel dimension.). Both methods tend to produce NMF directions that are close to one-hot, and so can be thought of as picking out the most relevant channels. However, when reducing to a small number of dimensions, using attributions usually picks out more salient features, because attribution takes into account not just what neurons respond to but also whether their response matters.\n \n\n\n\n Following , after applying NMF to attributions, we visualize them by assigning a different color to each of the resulting channels. We overlay these visualizations over the observation  and contextualize each channel using feature visualization , making use of [dataset example-based feature visualization](https://distill.pub/2020/understanding-rl-vision/#dataset-examples). This gives a basic version of our interface, which allows us to see the effect of the main features at different spatial positions.\n \n\n\n\n\n\n\n| Observation | Positive attribution (good news) | Negative attribution (bad news) |\n| --- | --- | --- |\n| \n\n\n\n | \n\n\n\n\n\n\n | \n\n\n\n\n\n\n |\n\n\n\nLegend (hover to isolate)\n  \n\n\n\n\n\n\n\nBuzzsaw  \nobstacle\n\n\n\n\n\n\n\n\nCoin\n\n\n\n\n\n\n\n\nEnemy  \nmoving  \nleft\n\n\n\n\n\n\n\n\nAgent  \nor enemy  \nmoving right\n\n\n\n\n Value function attribution for a cherry-picked observation using layer [2b](https://distill.pub/2020/understanding-rl-vision/#architecture) of our CoinRun model, reduced to 4 channels using attribution-based NMF. The dataset example-based feature visualizations of these directions reveal more salient features than the visualizations of the first 4 activation-based NMF directions from the preceding section.\n \n\n\n\n For the [full version](https://distill.pub/2020/understanding-rl-vision/#interface) of our interface, we simply repeat this for an entire trajectory of the agent playing the game. We also incorporate video controls, a timeline view of compressed observations , and additional information, such as model outputs and sampled actions. Together these allow the trajectory to be easily explored and understood.\n \n\n\n### Attribution discussion\n\n\n\n Attributions for our CoinRun model have some interesting properties that would be unusual for an ImageNet model.\n \n\n\n* **Sparsity.** Attribution tends to be concentrated in a very small number of spatial positions and (post-NMF) channels. For example, in the figure above, the top 10 position–channel pairs account for more than 80% of the total absolute attribution. This might be explained by our [earlier](https://distill.pub/2020/understanding-rl-vision/#feature-visualization-discussion) hypothesis that the model identifies objects by picking out certain small configurations of pixels. Because of this sparsity, we smooth out attribution over nearby spatial positions for the full version of our interface, so that the amount of visual space taken up can be used to judge attribution strength. This trades off some spatial precision for more precision with magnitudes.\n* **Unexpected sign.** Value function attribution usually has the sign one would expect: positive for coins, negative for enemies, and so on. However, this is sometimes not the case. For example, in the figure above, the red channel that detects buzzsaw obstacles has both positive and negative attribution in two neighboring spatial positions towards the left. Our best guess is that this phenomenon is a result of statistical [collinearity](https://en.wikipedia.org/wiki/Multicollinearity), caused by certain correlations in the procedural level generation together with the agent’s behavior. These could be visual, such as correlations between nearby pixels, or more abstract, such as both coins and long walls appearing at the end of every level. As a toy example, supposing the value function ought to increase by 2% when the end of the level becomes visible, the model could either increase the value function by 1% for coins and 1% for long walls, or by 3% for coins and −1% for long walls, and the effect would be similar.\n* **Outlier frames.** When an unusual event causes the network to output extreme values, attribution can behave especially strangely. For example, in the [buzzsaw hallucination](https://distill.pub/2020/understanding-rl-vision/#hallucinations) frame, most features have a significant amount of both positive and negative attribution. We do not have a good explanation for this, but perhaps features are interacting in more complicated ways than usual. Moreover, in these cases there is often a significant component of the attribution lying outside the space spanned by the NMF directions, which we display as an additional “residual” feature. This could be because each frame is weighted equally when computing NMF, so outlier frames have little influence over the NMF directions.\n\n\n\n These considerations suggest that some care may be required when interpreting attributions.\n \n\n\n## Questions for further research\n\n\n### The [diversity hypothesis](https://distill.pub/2020/understanding-rl-vision/#diversity-hypothesis)\n\n\n1. **Validity.** Does the diversity hypothesis hold in other contexts, both within and outside of reinforcement learning?\n2. **Relationship to generalization.** What is the three-way relationship between diversity, interpretable features and generalization? Do non-interpretable features indicate that a model will fail to generalize in certain ways? Generalization refers implicitly to an underlying distribution – how should this distribution be chosen?For example, to measure generalization for CoinRun models trained on a limited number of levels, we used the distribution over all possible procedurally-generated levels. However, to formalize the sense in which CoinRun is not diverse in its visual patterns or dynamics rules, one would need a distribution over levels from a wider class of games.\n3. **Caveats.** How are interpretable features affected by other factors, such as the choice of task or algorithm, and how do these interact with diversity? Speculatively, do big enough models obtain interpretable features via the double descent phenomenon , even in the absence of diversity?\n4. **Quantification.** Can we quantitatively predict how much diversity is needed for interpretable features, perhaps using generalization metrics? Can we be precise about what is meant by an “interpretable feature” and a “level of abstraction”?\n\n\n### Interpretability in the absence of diversity\n\n\n1. **Pervasiveness of non-diverse features.** Do “non-diverse features”, by which we mean the hard-to-interpret features that tend to arise in the absence of diversity, remain when diversity is present? Is there a connection between these non-diverse features and the “non-robust features” that have been posited to explain adversarial examples ?\n2. **Coping with non-diverse levels of abstraction.** Are there levels of abstraction at which even broad distributions like ImageNet remain non-diverse, and how can we best interpret models at these levels of abstraction?\n3. **Gradient-based feature visualization.** Why does gradient-based feature visualization [break down](https://distill.pub/2020/understanding-rl-vision/#feature-visualization) in the absence of diversity, and can it be made to work using transformation robustness, regularization, data augmentation, adversarial training, or other techniques? What property of the optimization leads to the clouds of [extremal colors](https://distill.pub/2020/understanding-rl-vision/#extremal-colors)?\n4. **Trustworthiness of dataset examples and attribution.** How reliable and trustworthy can we make very heavily-regularized versions of feature visualization, such as those based on [dataset examples](https://distill.pub/2020/understanding-rl-vision/#dataset-examples)?Heavily-regularized feature visualization may be untrustworthy by failing to separate the things causing certain behavior from the things that merely correlate with those causes . What explains the [strange behavior](https://distill.pub/2020/understanding-rl-vision/#attribution-discussion) of attribution, and how trustworthy is it?\n\n\n### Interpretability in the RL framework\n\n\n1. **Non-visual and abstract features.** What are the best methods for interpreting models with non-visual inputs? Even vision models may also have interpretable abstract features, such as relationships between objects or anticipated events: will any method of generating examples be enough to understand these, or do we need an entirely new approach? For models with memory, how can we interpret their hidden states ?\n2. **Improving reliability.** How can we best identify, understand and correct rare [failures](https://distill.pub/2020/understanding-rl-vision/#dissecting-failure) and [other errors](https://distill.pub/2020/understanding-rl-vision/#hallucinations) in RL models? Can we actually improve models by [model editing](https://distill.pub/2020/understanding-rl-vision/#model-editing), rather than merely degrading them?\n3. **Modifying training.** In what ways can we train RL models to make them more interpretable without a significant performance cost, such as by altering architectures or adding auxiliary predictive losses?\n4. **Leveraging the environment.** How can we enrich interfaces using RL-specific data, such as trajectories of agent–environment interaction, state distributions, and advantage estimates? What are the benefits of incorporating user–environment interaction, such as for exploring counterfactuals?\n\n\n### What we would like to see from further research and why\n\n\n\n We are motivated to study interpretability for RL for two reasons.\n \n\n\n* **To be able to interpret RL models.** RL can be applied to an enormous variety of tasks, and seems likely to be a part of increasingly influential AI systems. It is therefore important to be able to scrutinize RL models and to understand how they might fail. This may also benefit RL research through an improved understanding of the pitfalls of different algorithms and environments.\n* **As a testbed for interpretability techniques.** RL models pose a number of distinctive challenges for interpretability techniques. In particular, environments like CoinRun straddle the boundary between memorization and generalization, making them useful for studying the [diversity hypothesis](https://distill.pub/2020/understanding-rl-vision/#diversity-hypothesis) and related ideas.\n\n\n\n We think that large neural networks are currently the most likely type of model to be used in highly capable and influential AI systems in the future. Contrary to the traditional perception of neural networks as black boxes, we think that there is a fighting chance that we will be able to clearly and thoroughly understand the behavior even of very large networks. We are therefore most excited by neural network interpretability research that scores highly according to the following criteria.\n \n\n\n* **Scalability.** The takeaways of the research should have some chance of scaling to harder problems and larger networks. If the techniques themselves do not scale, they should at least reveal some relevant insight that might.\n* **Trustworthiness.** Explanations should be faithful to the model. Even if they do not tell the full story, they should at least not be biased in some fatal way (such as by using an approval-based objective that leads to bad explanations that sound good, or by depending on another model that badly distorts information).\n* **Exhaustiveness.** This may turn out to be impossible at scale, but we should strive for techniques that explain every essential feature of our models. If there are theoretical limits to exhaustiveness, we should try to understand these.\n* **Low cost.** Our techniques should not be significantly more computationally expensive than training the model. We hope that we will not need to train models differently for them to be interpretable, but if we do, we should try to minimize both the computational expense and any performance cost, so that interpretable models are not disincentivized from being used in practice.\n\n\n\n Our proposed questions reflect this perspective. One of the reasons we emphasize diversity relates to exhaustiveness. If “non-diverse features” remain when diversity is present, then our current techniques are not exhaustive and could end up missing important features of more capable models. Developing tools to understand non-diverse features may shed light on whether this is likely to be a problem.\n \n\n\n\n We think there may be significant mileage in simply applying existing interpretability techniques, with attention to detail, to more models. Indeed, this was the mindset with which we initially approached this project. If the diversity hypothesis is correct, then this may become easier as we train our models to perform more complex tasks. Like early biologists encountering a new species, there may be a lot we can glean from taking a magnifying glass to the creatures in front of us.\n \n\n\n## Supplementary material\n\n\n* **Code.** Utilities for computing feature visualization, attribution and dimensionality reduction for our models can be found in `lucid.scratch.rl_util`, a submodule of [Lucid](https://github.com/tensorflow/lucid). We demonstrate these in a [![](./Understanding RL Vision_files/colab.svg) notebook](https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/misc/rl_util.ipynb).\n* **Model weights.** The weights of our model are available for download, along with those of a number of other models, including the models trained on different numbers of levels, the edited models, and models trained on all 16 of the Procgen Benchmark  games. These are indexed [here](https://openaipublic.blob.core.windows.net/rl-clarity/attribution/models/index.html).\n* **More interfaces.** We generated an expanded version of our interface for every convolutional layer in our model, which can be found [here](https://openaipublic.blob.core.windows.net/rl-clarity/attribution/demo/interface.html). We also generated similar interfaces for each of our other models, which are indexed [here](https://openaipublic.blob.core.windows.net/rl-clarity/attribution/index.html).\n* **Interface code.** The code used to generate the expanded version of our interface can be found [here](https://github.com/openai/understanding-rl-vision).", "bibliography_bbl": "", "bibliography_bib": [{"title": "Quantifying generalization in reinforcement learning"}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps"}, {"title": "Visualizing and understanding convolutional networks"}, {"title": "Striving for simplicity: The all convolutional net"}, {"title": "Grad-CAM: Visual explanations from deep networks via gradient-based localization"}, {"title": "Interpretable explanations of black boxes by meaningful perturbation"}, {"title": "PatternNet and PatternLRP--Improving the interpretability of neural networks"}, {"title": "The (un)reliability of saliency methods"}, {"title": "Axiomatic attribution for deep networks"}, {"title": "The Building Blocks of Interpretability"}, {"title": "Leveraging Procedural Generation to Benchmark Reinforcement Learning"}, {"title": "Proximal policy optimization algorithms"}, {"title": "High-dimensional continuous control using generalized advantage estimation"}, {"title": "Thread: Circuits"}, {"title": "General Video Game AI: A multi-track framework for evaluating agents, games and content generation algorithms"}, {"title": "Obstacle Tower: A Generalization Challenge in Vision, Control, and Planning"}, {"title": "Observational Overfitting in Reinforcement Learning"}, {"title": "Feature Visualization"}, {"title": "Visualizing higher-layer features of a deep network"}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images"}, {"title": "Inceptionism: Going deeper into neural networks"}, {"title": "Plug & play generative networks: Conditional iterative generation of images in latent space"}, {"title": "Imagenet: A large-scale hierarchical image database"}, {"title": "Going deeper with convolutions"}, {"title": "An Atari model zoo for analyzing, visualizing, and comparing deep reinforcement learning agents"}, {"title": "Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents"}, {"title": "Caricatures"}, {"title": "Towards deep learning models resistant to adversarial attacks"}, {"title": "Intriguing properties of neural networks"}, {"title": "Visualizing and understanding Atari agents"}, {"title": "Explain Your Move: Understanding Agent Actions Using Specific and Relevant Feature Attribution"}, {"title": "Video Interface: Assuming Multiple Perspectives on a Video Exposes Hidden Structure"}, {"title": "Reconciling modern machine-learning practice and the classical bias--variance trade-off"}, {"title": "Adversarial examples are not bugs, they are features"}, {"title": "A Discussion of "Adversarial Examples Are Not Bugs, They Are Features""}, {"title": "Human-level performance in 3D multiplayer games with population-based reinforcement learning"}, {"title": "Solving Rubik"s Cube with a Robot Hand"}, {"title": "Dota 2 with Large Scale Deep Reinforcement Learning"}, {"title": "Does Attribution Make Sense?"}, {"title": "IMPALA: Scalable distributed deep-RL with importance weighted actor-learner architectures"}]}
