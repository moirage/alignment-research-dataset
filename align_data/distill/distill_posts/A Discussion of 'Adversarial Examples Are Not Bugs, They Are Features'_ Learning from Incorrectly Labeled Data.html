<!DOCTYPE html>
<!-- saved from url=(0058)https://distill.pub/2019/advex-bugs-discussion/response-6/ -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><script src="./A Discussion of &#39;Adversarial Examples Are Not Bugs, They Are Features&#39;_ Learning from Incorrectly Labeled Data_files/template.v2.js"></script><style>body {transition: opacity ease-in 0.2s; } 
body[unresolved] {opacity: 0; display: block; overflow: hidden; position: relative; } 
</style><meta name="viewport" content="width=device-width, initial-scale=1"><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><script async="" src="./A Discussion of &#39;Adversarial Examples Are Not Bugs, They Are Features&#39;_ Learning from Incorrectly Labeled Data_files/analytics.js"></script><script>
window.addEventListener('WebComponentsReady', function() {
  console.warn('WebComponentsReady');
  const loaderTag = document.createElement('script');
  loaderTag.src = 'https://distill.pub/template.v2.js';
  document.head.insertBefore(loaderTag, document.head.firstChild);
});
</script><script src="./A Discussion of &#39;Adversarial Examples Are Not Bugs, They Are Features&#39;_ Learning from Incorrectly Labeled Data_files/webcomponents-loader.js"></script><script src="./A Discussion of &#39;Adversarial Examples Are Not Bugs, They Are Features&#39;_ Learning from Incorrectly Labeled Data_files/webcomponents-hi.js"></script>
    
    
    <style id="distill-prerendered-styles" type="text/css">/*
 * Copyright 2018 The Distill Template Authors
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

html {
  font-size: 14px;
	line-height: 1.6em;
  /* font-family: "Libre Franklin", "Helvetica Neue", sans-serif; */
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
  /*, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";*/
  text-size-adjust: 100%;
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
}

@media(min-width: 768px) {
  html {
    font-size: 16px;
  }
}

body {
  margin: 0;
}

a {
  color: #004276;
}

figure {
  margin: 0;
}

table {
	border-collapse: collapse;
	border-spacing: 0;
}

table th {
	text-align: left;
}

table thead {
  border-bottom: 1px solid rgba(0, 0, 0, 0.05);
}

table thead th {
  padding-bottom: 0.5em;
}

table tbody :first-child td {
  padding-top: 0.5em;
}

pre {
  overflow: auto;
  max-width: 100%;
}

p {
  margin-top: 0;
  margin-bottom: 1em;
}

sup, sub {
  vertical-align: baseline;
  position: relative;
  top: -0.4em;
  line-height: 1em;
}

sub {
  top: 0.4em;
}

.kicker,
.marker {
  font-size: 15px;
  font-weight: 600;
  color: rgba(0, 0, 0, 0.5);
}


/* Headline */

@media(min-width: 1024px) {
  d-title h1 span {
    display: block;
  }
}

/* Figure */

figure {
  position: relative;
  margin-bottom: 2.5em;
  margin-top: 1.5em;
}

figcaption+figure {

}

figure img {
  width: 100%;
}

figure svg text,
figure svg tspan {
}

figcaption,
.figcaption {
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
}

@media(min-width: 1024px) {
figcaption,
.figcaption {
    font-size: 13px;
  }
}

figure.external img {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

figcaption a {
  color: rgba(0, 0, 0, 0.6);
}

figcaption b,
figcaption strong, {
  font-weight: 600;
  color: rgba(0, 0, 0, 1.0);
}
/*
 * Copyright 2018 The Distill Template Authors
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

@supports not (display: grid) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    display: block;
    padding: 8px;
  }
}

.base-grid,
distill-header,
d-title,
d-abstract,
d-article,
d-appendix,
distill-appendix,
d-byline,
d-footnote-list,
d-citation-list,
distill-footer {
  display: grid;
  justify-items: stretch;
  grid-template-columns: [screen-start] 8px [page-start kicker-start text-start gutter-start middle-start] 1fr 1fr 1fr 1fr 1fr 1fr 1fr 1fr [text-end page-end gutter-end kicker-end middle-end] 8px [screen-end];
  grid-column-gap: 8px;
}

.grid {
  display: grid;
  grid-column-gap: 8px;
}

@media(min-width: 768px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start middle-start text-start] 45px 45px 45px 45px 45px 45px 45px 45px [ kicker-end text-end gutter-start] 45px [middle-end] 45px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 16px;
  }

  .grid {
    grid-column-gap: 16px;
  }
}

@media(min-width: 1000px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 50px [middle-start] 50px [text-start kicker-end] 50px 50px 50px 50px 50px 50px 50px 50px [text-end gutter-start] 50px [middle-end] 50px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 16px;
  }

  .grid {
    grid-column-gap: 16px;
  }
}

@media(min-width: 1180px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 32px;
  }

  .grid {
    grid-column-gap: 32px;
  }
}




.base-grid {
  grid-column: screen;
}

/* .l-body,
d-article > *  {
  grid-column: text;
}

.l-page,
d-title > *,
d-figure {
  grid-column: page;
} */

.l-gutter {
  grid-column: gutter;
}

.l-text,
.l-body {
  grid-column: text;
}

.l-page {
  grid-column: page;
}

.l-body-outset {
  grid-column: middle;
}

.l-page-outset {
  grid-column: page;
}

.l-screen {
  grid-column: screen;
}

.l-screen-inset {
  grid-column: screen;
  padding-left: 16px;
  padding-left: 16px;
}


/* Aside */

d-article aside {
  grid-column: gutter;
  font-size: 12px;
  line-height: 1.6em;
  color: rgba(0, 0, 0, 0.6)
}

@media(min-width: 768px) {
  aside {
    grid-column: gutter;
  }

  .side {
    grid-column: gutter;
  }
}
/*
 * Copyright 2018 The Distill Template Authors
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

d-title {
  padding: 2rem 0 1.5rem;
  contain: layout style;
  overflow-x: hidden;
}

@media(min-width: 768px) {
  d-title {
    padding: 4rem 0 1.5rem;
  }
}

d-title h1 {
  grid-column: text;
  font-size: 40px;
  font-weight: 700;
  line-height: 1.1em;
  margin: 0 0 0.5rem;
}

@media(min-width: 768px) {
  d-title h1 {
    font-size: 50px;
  }
}

d-title p {
  font-weight: 300;
  font-size: 1.2rem;
  line-height: 1.55em;
  grid-column: text;
}

d-title .status {
  margin-top: 0px;
  font-size: 12px;
  color: #009688;
  opacity: 0.8;
  grid-column: kicker;
}

d-title .status span {
  line-height: 1;
  display: inline-block;
  padding: 6px 0;
  border-bottom: 1px solid #80cbc4;
  font-size: 11px;
  text-transform: uppercase;
}
/*
 * Copyright 2018 The Distill Template Authors
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

d-byline {
  contain: style;
  overflow: hidden;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  font-size: 0.8rem;
  line-height: 1.8em;
  padding: 1.5rem 0;
  min-height: 1.8em;
}


d-byline .byline {
  grid-template-columns: 1fr 1fr;
  grid-column: text;
}

@media(min-width: 768px) {
  d-byline .byline {
    grid-template-columns: 1fr 1fr 1fr 1fr;
  }
}

d-byline .authors-affiliations {
  grid-column-end: span 2;
  grid-template-columns: 1fr 1fr;
  margin-bottom: 1em;
}

@media(min-width: 768px) {
  d-byline .authors-affiliations {
    margin-bottom: 0;
  }
}

d-byline h3 {
  font-size: 0.6rem;
  font-weight: 400;
  color: rgba(0, 0, 0, 0.5);
  margin: 0;
  text-transform: uppercase;
}

d-byline p {
  margin: 0;
}

d-byline a,
d-article d-byline a {
  color: rgba(0, 0, 0, 0.8);
  text-decoration: none;
  border-bottom: none;
}

d-article d-byline a:hover {
  text-decoration: underline;
  border-bottom: none;
}

d-byline p.author {
  font-weight: 500;
}

d-byline .affiliations {

}
/*
 * Copyright 2018 The Distill Template Authors
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

d-article {
  contain: layout style;
  overflow-x: hidden;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  padding-top: 2rem;
  color: rgba(0, 0, 0, 0.8);
}

d-article > * {
  grid-column: text;
}

@media(min-width: 768px) {
  d-article {
    font-size: 16px;
  }
}

@media(min-width: 1024px) {
  d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
  }
}


/* H2 */


d-article .marker {
  text-decoration: none;
  border: none;
  counter-reset: section;
  grid-column: kicker;
  line-height: 1.7em;
}

d-article .marker:hover {
  border: none;
}

d-article .marker span {
  padding: 0 3px 4px;
  border-bottom: 1px solid rgba(0, 0, 0, 0.2);
  position: relative;
  top: 4px;
}

d-article .marker:hover span {
  color: rgba(0, 0, 0, 0.7);
  border-bottom: 1px solid rgba(0, 0, 0, 0.7);
}

d-article h2 {
  font-weight: 600;
  font-size: 24px;
  line-height: 1.25em;
  margin: 2rem 0 1.5rem 0;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  padding-bottom: 1rem;
}

@media(min-width: 1024px) {
  d-article h2 {
    font-size: 36px;
  }
}

/* H3 */

d-article h3 {
  font-weight: 700;
  font-size: 18px;
  line-height: 1.4em;
  margin-bottom: 1em;
  margin-top: 2em;
}

@media(min-width: 1024px) {
  d-article h3 {
    font-size: 20px;
  }
}

/* H4 */

d-article h4 {
  font-weight: 600;
  text-transform: uppercase;
  font-size: 14px;
  line-height: 1.4em;
}

d-article a {
  color: inherit;
}

d-article p,
d-article ul,
d-article ol,
d-article blockquote {
  margin-top: 0;
  margin-bottom: 1em;
  margin-left: 0;
  margin-right: 0;
}

d-article blockquote {
  border-left: 2px solid rgba(0, 0, 0, 0.2);
  padding-left: 2em;
  font-style: italic;
  color: rgba(0, 0, 0, 0.6);
}

d-article a {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
  text-decoration: none;
}

d-article a:hover {
  border-bottom: 1px solid rgba(0, 0, 0, 0.8);
}

d-article .link {
  text-decoration: underline;
  cursor: pointer;
}

d-article ul,
d-article ol {
  padding-left: 24px;
}

d-article li {
  margin-bottom: 1em;
  margin-left: 0;
  padding-left: 0;
}

d-article li:last-child {
  margin-bottom: 0;
}

d-article pre {
  font-size: 14px;
  margin-bottom: 20px;
}

d-article hr {
  grid-column: screen;
  width: 100%;
  border: none;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  margin-top: 60px;
  margin-bottom: 60px;
}

d-article section {
  margin-top: 60px;
  margin-bottom: 60px;
}

d-article span.equation-mimic {
  font-family: georgia;
  font-size: 115%;
  font-style: italic;
}

d-article > d-code,
d-article section > d-code  {
  display: block;
}

d-article > d-math[block],
d-article section > d-math[block]  {
  display: block;
}

@media (max-width: 768px) {
  d-article > d-code,
  d-article section > d-code,
  d-article > d-math[block],
  d-article section > d-math[block] {
      overflow-x: scroll;
      -ms-overflow-style: none;  // IE 10+
      overflow: -moz-scrollbars-none;  // Firefox
  }

  d-article > d-code::-webkit-scrollbar,
  d-article section > d-code::-webkit-scrollbar,
  d-article > d-math[block]::-webkit-scrollbar,
  d-article section > d-math[block]::-webkit-scrollbar {
    display: none;  // Safari and Chrome
  }
}

d-article .citation {
  color: #668;
  cursor: pointer;
}

d-include {
  width: auto;
  display: block;
}

d-figure {
  contain: layout style;
}

/* KaTeX */

.katex, .katex-prerendered {
  contain: style;
  display: inline-block;
}

/* Tables */

d-article table {
  border-collapse: collapse;
  margin-bottom: 1.5rem;
  border-bottom: 1px solid rgba(0, 0, 0, 0.2);
}

d-article table th {
  border-bottom: 1px solid rgba(0, 0, 0, 0.2);
}

d-article table td {
  border-bottom: 1px solid rgba(0, 0, 0, 0.05);
}

d-article table tr:last-of-type td {
  border-bottom: none;
}

d-article table th,
d-article table td {
  font-size: 15px;
  padding: 2px 8px;
}

d-article table tbody :first-child td {
  padding-top: 2px;
}
/*
 * Copyright 2018 The Distill Template Authors
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

span.katex-display {
  text-align: left;
  padding: 8px 0 8px 0;
  margin: 0.5em 0 0.5em 1em;
}

span.katex {
  -webkit-font-smoothing: antialiased;
  color: rgba(0, 0, 0, 0.8);
  font-size: 1.18em;
}
/*
 * Copyright 2018 The Distill Template Authors
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

@media print {

  @page {
    size: 8in 11in;
    @bottom-right {
      content: counter(page) " of " counter(pages);
    }
  }

  html {
    /* no general margins -- CSS Grid takes care of those */
  }

  p, code {
    page-break-inside: avoid;
  }

  h2, h3 {
    page-break-after: avoid;
  }

  d-header {
    visibility: hidden;
  }

  d-footer {
    display: none!important;
  }

}
</style>
    <style>
        .subgrid {
	grid-column: screen;
	display: grid;
	grid-template-columns: inherit;
	grid-template-rows: inherit;
	grid-column-gap: inherit;
	grid-row-gap: inherit;
}

d-figure.base-grid {
	grid-column: screen;
	background: hsl(0, 0%, 97%);
	padding: 20px 0;
	border-top: 1px solid rgba(0, 0, 0, 0.1);
	border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

d-figure {
	margin-bottom: 1em;
	position: relative;
}

d-figure > figure {
	margin-top: 0;
	margin-bottom: 0;
}

.shaded-figure {
	background-color: hsl(0, 0%, 97%);
	border-top: 1px solid hsla(0, 0%, 0%, 0.1);
	border-bottom: 1px solid hsla(0, 0%, 0%, 0.1);
	padding: 30px 0;
}

.pointer {
	position: absolute;
	width: 26px;
	height: 26px;
	top: 26px;
	left: -48px;
}

a.figure-number,
a.section-number {
	border-bottom-color: hsla(206, 90%, 20%, 0.3);
	text-transform: uppercase;
	font-size: 0.85em;
	color: hsla(206, 90%, 20%, 0.7);
}

a.figure-number::before {
	content: "Figure ";
}

#rebuttal,
.response-info {
	margin: 1em 0;
	background-color: hsl(228, 50%, 97%);
	border-left: solid hsl(229, 50%, 25%) 3px;
	padding: 1em;
}

#rebuttal,
.rebuttal-info {
	color: hsl(129, 50%, 15%);
	background-color: hsl(128, 50%, 97%);
	border-left: solid hsl(128, 50%, 25%) 3px;
	margin-bottom: 0.5em;
}

#rebuttal figure {
	background: white;
	padding: 1em;
	border-radius: 1em;
}

#rebuttal p:last-of-type {
	margin-bottom: 0;
}

    </style>
<link rel="stylesheet" href="./A Discussion of &#39;Adversarial Examples Are Not Bugs, They Are Features&#39;_ Learning from Incorrectly Labeled Data_files/katex.min.css" crossorigin="anonymous">
    
    <link rel="icon" type="image/png" href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAYAAACqaXHeAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA99JREFUeNrsG4t1ozDMzQSM4A2ODUonKBucN2hugtIJ6E1AboLcBiQTkJsANiAb9OCd/OpzMWBJBl5TvaeXPiiyJetry0J8wW3D3QpjRh3GjneXDq+fSQA9s2mH9x3KDhN4foJfCb8N/Jrv+2fnDn8vLRQOplWHVYdvHZYdZsBcZP1vBmh/n8DzEmhUQDPaOuP9pFuY+JwJHwHnCLQE2tnWBGEyXozY9xCUgHMhhjE2I4heVWtgIkZ83wL6Qgxj1obfWBxymPwe+b00BCCRNPbwfb60yleAkkBHGT5AEehIYz7eJrFDMF9CvH4wwhcGHiHMneFvLDQwlwvMLQq58trRcYBWfYn0A0OgHWQUSu25mE+BnoYKnnEJoeIWAifzOv7vLWd2ZKRfWAIme3tOiUaQ3UnLkb0xj1FxRIeEGKaGIHOs9nEgLaaA9i0JRYo1Ic67wJW86KSKE/ZAM8KuVMk8ITVhmxUxJ3Cl2xlm9Vtkeju1+mpCQNxaEGNCY8bs9X2YqwNoQeGjBWut/ma0QAWy/TqAsHx9wSya3I5IRxOfTC+leG+kA/4vSeEcGBtNUN6byhu3+keEZCQJUNh8MAO7HL6H8pQLnsW/Hd4T4lv93TPjfM7A46iEEqbB5EDOvwYNW6tGNZzT/o+CZ6sqZ6wUtR/wf7mi/VL8iNciT6rHih48Y55b4nKCHJCCzb4y0nwFmin3ZEMIoLfZF8F7nncFmvnWBaBj7CGAYA/WGJsUwHdYqVDwAmNsUgAx4CGgAA7GOOxADYOFWOaIKifuVYzmOpREqA21Mo7aPsgiY1PhOMAmxtR+AUbYH3Id2wc0SAFIQTsn9IUGWR8k9jx3vtXSiAacFxTAGakBk9UudkNECd6jLe+6HrshshvIuC6IlLMRy7er+JpcKma24SlE4cFZSZJDGVVrsNvitQhQrDhW0jfiOLfFd47C42eHT56D/BK0To+58Ahj+cAT8HT1UWlfLZCCd/uKawzU0Rh2EyIX/Icqth3niG8ybNroezwe6khdCNxRN+l4XGdOLVLlOOt2hTRJlr1ETIuMAltVTMz70mJrkdGAaZLSmnBEqmAE32JCMmuTlCnRgsBENtOUpHhvvsYIL0ibnBkaC6QvKcR7738GKp0AKnim7xgUSNv1bpS8QwhBt8r+EP47v/oyRK/S34yJ9nT+AN0Tkm4OdB9E4BsmXM3SnMlRFUrtp6IDpV2eKzdYvF3etm3KhQksbOLChGkSmcBdmcEwvqkrMy5BzL00NZeu3qPYJOOuCc+5NjcWKXQxFvTa3NoXJ4d8in7fiAUuTt781dkvuHX4K8AA2Usy7yNKLy0AAAAASUVORK5CYII=">
    <link href="https://distill.pub/rss.xml" rel="alternate" type="application/rss+xml" title="Articles from Distill">
  
    <title>A Discussion of 'Adversarial Examples Are Not Bugs, They Are Features': Learning from Incorrectly Labeled Data</title>
    
    <link rel="canonical" href="https://distill.pub/2019/advex-bugs-discussion/response-6">
    
    <!--  https://schema.org/Article -->
    <meta property="description" itemprop="description" content="Section 3.2 of Ilyas et al. (2019) shows that training a model on only adversarial errors leads to non-trivial generalization on the original test set. We show that these experiments are a specific case of learning from errors.">
    <meta property="article:published" itemprop="datePublished" content="2019-08-06">
    <meta property="article:created" itemprop="dateCreated" content="2019-08-06">
    
    <meta property="article:modified" itemprop="dateModified" content="2019-08-07T20:58:16.000Z">
    
    <meta property="article:author" content="Eric Wallace">
    <!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
    <meta property="og:type" content="article">
    <meta property="og:title" content="A Discussion of &#39;Adversarial Examples Are Not Bugs, They Are Features&#39;: Learning from Incorrectly Labeled Data">
    <meta property="og:description" content="Section 3.2 of Ilyas et al. (2019) shows that training a model on only adversarial errors leads to non-trivial generalization on the original test set. We show that these experiments are a specific case of learning from errors.">
    <meta property="og:url" content="https://distill.pub/2019/advex-bugs-discussion/response-6">
    <meta property="og:image" content="https://distill.pub/2019/advex-bugs-discussion/response-6/thumbnail.jpg">
    <meta property="og:locale" content="en_US">
    <meta property="og:site_name" content="Distill">
  
    <!--  https://dev.twitter.com/cards/types/summary -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="A Discussion of &#39;Adversarial Examples Are Not Bugs, They Are Features&#39;: Learning from Incorrectly Labeled Data">
    <meta name="twitter:description" content="Section 3.2 of Ilyas et al. (2019) shows that training a model on only adversarial errors leads to non-trivial generalization on the original test set. We show that these experiments are a specific case of learning from errors.">
    <meta name="twitter:url" content="https://distill.pub/2019/advex-bugs-discussion/response-6">
    <meta name="twitter:image" content="https://distill.pub/2019/advex-bugs-discussion/response-6/thumbnail.jpg">
    <meta name="twitter:image:width" content="560">
    <meta name="twitter:image:height" content="295">
  
      <!--  https://scholar.google.com/intl/en/scholar/inclusion.html#indexing -->
    <meta name="citation_title" content="A Discussion of &#39;Adversarial Examples Are Not Bugs, They Are Features&#39;: Learning from Incorrectly Labeled Data">
    <meta name="citation_fulltext_html_url" content="https://distill.pub/2019/advex-bugs-discussion/response-6">
    <meta name="citation_volume" content="4">
    <meta name="citation_issue" content="8">
    <meta name="citation_firstpage" content="e00019.6">
    <meta name="citation_doi" content="10.23915/distill.00019.6">
    <meta name="citation_journal_title" content="Distill">
    <meta name="citation_journal_abbrev" content="Distill">
    <meta name="citation_issn" content="2476-0757">
    <meta name="citation_fulltext_world_readable" content="">
    <meta name="citation_online_date" content="2019/08/06">
    <meta name="citation_publication_date" content="2019/08/06">
    <meta name="citation_author" content="Wallace, Eric">
    <meta name="citation_author_institution" content="UC Berkeley">
    <meta name="citation_reference" content="citation_title=Distilling the Knowledge in a Neural Network;citation_author=Geoffrey Hinton;citation_author=Oriol Vinyals;citation_author=Jeff Dean;citation_publication_date=2015;citation_arxiv_id=1503.02531;">
    <meta name="citation_reference" content="citation_title=Model reconstruction from model explanations;citation_author=Smitha Milli;citation_author=Ludwig Schmidt;citation_author=Anca D Dragan;citation_author=Moritz Hardt;citation_publication_date=2018;citation_arxiv_id=1807.05185;">
    <meta name="citation_reference" content="citation_title=Understanding deep learning requires rethinking generalization;citation_author=Chiyuan Zhang;citation_author=Samy Bengio;citation_author=Moritz Hardt;citation_author=Benjamin Recht;citation_author=Oriol Vinyals;citation_publication_date=2016;">
<style id="svelte-1eb0vow-style">.visual-toc.svelte-1eb0vow{counter-reset:toc-heading;display:grid;grid-auto-flow:dense;grid-template-columns:1fr 1fr 1fr;grid-gap:16px}@media(min-width: 1000px){.visual-toc.svelte-1eb0vow{grid-gap:8px;grid-template-columns:1fr 1fr 1fr 1fr 1fr 1fr}}@media(min-width: 1180px){.visual-toc.svelte-1eb0vow{grid-gap:20px}}.visual-toc-item.svelte-1eb0vow{display:flex;flex-flow:column}.visual-toc-top.svelte-1eb0vow{flex-grow:1;border:1px solid #E5E5E5;border-radius:5px;overflow:hidden;text-decoration:none;transition:box-shadow 0.35s, transform 0.35s;transform:scale(1.0);display:flex;flex-flow:column}.visual-toc-top.svelte-1eb0vow:hover{box-shadow:0px 1px 4px rgba(0,0,0,0.05);transform:scale(1.02);transition:box-shadow 0.15s, transform 0.15s}.visual-toc-heading.svelte-1eb0vow,.visual-toc-subheading.svelte-1eb0vow{display:block;line-height:1.3em;font-size:85%;padding:0.5em 1em 1em 1em}.visual-toc-heading.svelte-1eb0vow{counter-increment:toc-heading;color:#333;font-weight:600}.visual-toc-heading.svelte-1eb0vow::before{display:block;content:"Section " counter(toc-heading);font-weight:400;text-transform:uppercase;font-size:0.6rem;color:#666}.visual-toc-subheading.svelte-1eb0vow{display:none;color:#666;font-size:75%}.visual-toc-colab.svelte-1eb0vow{border-radius:5px;border:dashed 1px rgba(0,0,0,0.1);margin-top:1em;padding-left:1.2em;padding-right:1.2em;padding-top:0.25em;padding-bottom:0.25em;text-transform:uppercase;color:#aaa;font-size:10.5px;line-height:24px}.visual-toc-colab.svelte-1eb0vow>img.svelte-1eb0vow{position:relative;top:4px}.visual-toc-item.svelte-1eb0vow:hover .visual-toc-colab>img.svelte-1eb0vow{filter:unset}.visual-toc-colab.svelte-1eb0vow:hover{background-color:hsl(0, 0%, 97%);border-color:rgba(0,0,0,0.2);color:#888}a.svelte-1eb0vow{display:block;text-decoration:none;cursor:pointer}a.svelte-1eb0vow canvas{width:100%}</style></head>

<body distill-prerendered=""><distill-header distill-prerendered="">
<style>
distill-header {
  position: relative;
  height: 60px;
  background-color: hsl(200, 60%, 15%);
  width: 100%;
  box-sizing: border-box;
  z-index: 2;
  color: rgba(0, 0, 0, 0.8);
  border-bottom: 1px solid rgba(0, 0, 0, 0.08);
  box-shadow: 0 1px 6px rgba(0, 0, 0, 0.05);
}
distill-header .content {
  height: 70px;
  grid-column: page;
}
distill-header a {
  font-size: 16px;
  height: 60px;
  line-height: 60px;
  text-decoration: none;
  color: rgba(255, 255, 255, 0.8);
  padding: 22px 0;
}
distill-header a:hover {
  color: rgba(255, 255, 255, 1);
}
distill-header svg {
  width: 24px;
  position: relative;
  top: 4px;
  margin-right: 2px;
}
@media(min-width: 1080px) {
  distill-header {
    height: 70px;
  }
  distill-header a {
    height: 70px;
    line-height: 70px;
    padding: 28px 0;
  }
  distill-header .logo {
  }
}
distill-header svg path {
  fill: none;
  stroke: rgba(255, 255, 255, 0.8);
  stroke-width: 3px;
}
distill-header .logo {
  font-size: 17px;
  font-weight: 200;
}
distill-header .nav {
  float: right;
  font-weight: 300;
}
distill-header .nav a {
  font-size: 12px;
  margin-left: 24px;
  text-transform: uppercase;
}
</style>
<div class="content">
  <a href="https://distill.pub/" class="logo">
    <svg viewBox="-607 419 64 64">
  <path d="M-573.4,478.9c-8,0-14.6-6.4-14.6-14.5s14.6-25.9,14.6-40.8c0,14.9,14.6,32.8,14.6,40.8S-565.4,478.9-573.4,478.9z"></path>
</svg>

    Distill
  </a>
  <nav class="nav">
    <a href="https://distill.pub/about/">About</a>
    <a href="https://distill.pub/prize/">Prize</a>
    <a href="https://distill.pub/journal/">Submit</a>
  </nav>
</div>
</distill-header>

    <d-front-matter>
        <script type="text/json">{
  "title": "A Discussion of 'Adversarial Examples Are Not Bugs, They Are Features': Learning from Incorrectly Labeled Data",
  "description": "Section 3.2 of Ilyas et al. (2019) shows that training a model on only adversarial errors leads to non-trivial generalization on the original test set. We show that these experiments are a specific case of learning from errors.",
  "authors": [
    {
      "author": "Eric Wallace",
      "authorURL": "http://www.ericswallace.com/",
      "affiliation": "UC Berkeley",
      "affiliationURL": "https://www.berkeley.edu/"
    }
  ],
  "katex": {
    "delimiters": [
      {
        "left": "$",
        "right": "$",
        "display": false
      },
      {
        "left": "$$",
        "right": "$$",
        "display": true
      }
    ]
  }
  }</script>
    </d-front-matter>

    <d-title>
        <h1>Learning from Incorrectly Labeled Data</h1>
    </d-title>

    <d-byline>
  <div class="byline grid">
    <div class="authors-affiliations grid">
      <h3>Authors</h3>
      <h3>Affiliations</h3>
      
        <p class="author">
          
            <a class="name" href="http://www.ericswallace.com/">Eric Wallace</a>
        </p>
        <p class="affiliation">
        <a class="affiliation" href="https://www.berkeley.edu/">UC Berkeley</a>
        </p>
      
    </div>
    <div>
      <h3>Published</h3>
      
        <p>Aug. 6, 2019</p> 
    </div>
    <div>
      <h3>DOI</h3>
      
        <p><a href="https://doi.org/10.23915/distill.00019.6">10.23915/distill.00019.6</a></p>
    </div>
  </div>
</d-byline><d-article>

        <style>
    #rebuttal,
    .comment-info {
        background-color: hsl(54, 78%, 96%);
        border-left: solid hsl(54, 33%, 67%) 1px;
        padding: 1em;
        color: hsla(0, 0%, 0%, 0.67);
    }

    #header-info {
        margin-top: 0;
        margin-bottom: 1.5rem;
        display: grid;
        grid-template-columns: 65px max-content 1fr;
        grid-template-areas:
            "icon explanation explanation"
            "icon back comment";
        grid-column-gap: 1.5em;
    }

    #header-info .icon-multiple-pages {
        grid-area: icon;
        padding: 0.5em;
        content: url(images/multiple-pages.svg);
    }

    #header-info .explanation {
        grid-area: explanation;
        font-size: 85%;
    }

    #header-info .back {
        grid-area: back;
    }

    #header-info .back::before {

        content: "←";
        margin-right: 0.5em;
    }

    #header-info .comment {
        grid-area: comment;
        scroll-behavior: smooth;
    }

    #header-info .comment::before {
        content: "↓";
        margin-right: 0.5em;
    }

    #header-info a.back,
    #header-info a.comment {
        font-size: 80%;
        font-weight: 600;
        border-bottom: none;
        text-transform: uppercase;
        color: #2e6db7;
        display: block;
        margin-top: 0.25em;
        letter-spacing: 0.25px;
    }
</style>

<section id="header-info" class="comment-info">
    <div class="icon-multiple-pages"></div>
    <p class="explanation">
        This article is part of a discussion of the Ilyas et al. paper
        <em>“Adversarial examples are not bugs, they are features”.</em>
        You can learn more in the
        <a href="https://distill.pub/2019/advex-bugs-discussion/">
            main discussion article
        </a>.
    </p>
    <a id="header-info-back-link" class="back" href="https://distill.pub/2019/advex-bugs-discussion/#commentaries">Other Comments</a>
    <a id="header-info-comment-link" class="comment" href="https://distill.pub/2019/advex-bugs-discussion/response-6/#rebuttal">Comment by Ilyas et al.</a>
</section>


        <p>
            Section 3.2 of Ilyas et al. (2019) shows that training a model on only adversarial errors leads to
            non-trivial generalization on the original test set. We show that these experiments are a specific case of
            learning from errors. We start with a counterintuitive result — we take a completely mislabeled training set
            (without modifying the inputs) and use it to train a model that generalizes to the original test set. We
            then show that this result, and the results of Ilyas et al. (2019), are a special case of model
            distillation. In particular, since the incorrect labels are generated using a trained model, information
            about the trained model is being “leaked” into the dataset.
            We begin with the following question: what if we took the images in the training set (without any
            adversarial perturbations) and mislabeled them? Since the inputs are unmodified and mislabeled, intuition
            says that a model trained on this dataset should not generalize to the correctly-labeled test set.
            Nevertheless, we show that this intuition fails — a model <i>can</i> generalize.
            We first train a ResNet-18 on the CIFAR-10 training set for two epochs. The model reaches a training
            accuracy of 62.5% and a test accuracy of 63.1%. Next, we run the model on all of the 50,000 training data
            points and relabel them according to the model’s predictions. Then, we filter out <i>all the correct
                predictions</i>. We are now left with an incorrectly labeled training set of size 18,768. We show four
            examples on the left of the Figure below:
        </p>

        <figure id="figure-1">
            <img src="./A Discussion of &#39;Adversarial Examples Are Not Bugs, They Are Features&#39;_ Learning from Incorrectly Labeled Data_files/image1.png">
            <figcaption>
                <a href="https://distill.pub/2019/advex-bugs-discussion/response-6/#figure-1" class="figure-number">1</a>
            </figcaption>
        </figure>

        <p>
            We then randomly initialize a new ResNet-18 and train it only on this mislabeled dataset. We train for 50
            epochs and reach an accuracy of 49.7% on the <i>original</i> test set. The new model has only ever seen
            incorrectly labeled, unperturbed images but can still non-trivially generalize.
        </p>

        <h2>This is Model Distillation Using Incorrect Predictions</h2>

        <p>
            How can this model and the models in Ilyas et al. (2019) generalize without seeing any correctly labeled
            data? Here, we show that since the incorrect labels are generated using a trained model, information is
            being “leaked” about that trained model into the mislabeled examples. In particular, this an indirect form
            of model distillation<d-cite key="hinton2015distilling"></d-cite> — training on this dataset allows a new
            model to somewhat recover the features of the original model.
        </p>

        <p>
            We first illustrate this distillation phenomenon using a two-dimensional problem. Then, we explore other
            peculiar forms of distillation for neural networks — -we transfer knowledge despite the inputs being from
            another task.
        </p>

        <h3>Two-dimensional Illustration of Model Distillation</h3>

        <p>
            We construct a dataset of adversarial examples using a two-dimensional binary classification problem. We
            generate 32 random two-dimensional data points in <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><msup><mo>]</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">[0,1]^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="base"><span class="mopen">[</span><span class="mord mathrm">0</span><span class="mpunct">,</span><span class="mord mathrm">1</span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathrm mtight">2</span></span></span></span></span></span></span></span></span></span></span></span> and assign each point a random binary label. We
            then train a small feed-forward neural network on these examples, predicting 32/32 of the examples correctly
            (panel (a) in the Figure below).
        </p>

        <figure id="figure-2">
            <img src="./A Discussion of &#39;Adversarial Examples Are Not Bugs, They Are Features&#39;_ Learning from Incorrectly Labeled Data_files/image2.png">
            <figcaption>
                <a href="https://distill.pub/2019/advex-bugs-discussion/response-6/#figure-2" class="figure-number">2</a>
            </figcaption>
        </figure>

        <p>
            Next, we create adversarial examples for the original model using an <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>l</mi><mi mathvariant="normal">∞</mi></msub></mrow><annotation encoding="application/x-tex">l_{\infty}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="base"><span class="mord"><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.01968em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">∞</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span></span></span></span></span> ball of radius
            <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi><mo>=</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>1</mn><mn>2</mn></mrow><annotation encoding="application/x-tex">\epsilon=0.12</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">ϵ</span><span class="mrel">=</span><span class="mord mathrm">0</span><span class="mord mathrm">.</span><span class="mord mathrm">1</span><span class="mord mathrm">2</span></span></span></span></span>. In panel (a) of the Figure above, we display the <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">ϵ</span></span></span></span></span>-ball around each training
            point. In panel (b), we show the adversarial examples which cause the model to change its prediction (from
            correct to incorrect). We train a new feed-forward neural network on this dataset, resulting in the model in
            panel (c).
        </p>

        <p>
            Although this new model has never seen a correctly labeled example, it is able to perform non-trivially on
            the original dataset, predicting <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>2</mn><mn>3</mn><mi mathvariant="normal">/</mi><mn>3</mn><mn>2</mn></mrow><annotation encoding="application/x-tex">23/32</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mord mathrm">2</span><span class="mord mathrm">3</span><span class="mord mathrm">/</span><span class="mord mathrm">3</span><span class="mord mathrm">2</span></span></span></span></span> of the inputs correctly (panel (d) in the Figure). The new model’s
            decision boundary loosely matches the original model’s decision boundary, i.e., the original model has been
            somewhat distilled after training on its adversarial examples. This two-dimensional problem presents an
            illustrative version of the intriguing result that distillation can be performed using incorrect
            predictions.

        </p>

        <h3>
            Other Peculiar Forms of Distillation
        </h3>

        <p>
            Our experiments show that we can distill models using mislabeled examples. In what other peculiar ways can
            we learn about the original model? Can we use only <i>out-of-domain</i> data?
        </p>

        <p>
            We train a simple CNN model on MNIST, reaching 99.1% accuracy. We then run this model on the FashionMNIST
            training set and save its argmax predictions. The resulting dataset is nonsensical to humans — a “dress” is
            labeled as an “8″.
        </p>

        <figure id="figure-3">
            <img src="./A Discussion of &#39;Adversarial Examples Are Not Bugs, They Are Features&#39;_ Learning from Incorrectly Labeled Data_files/image3.png">
            <figcaption>
                <a href="https://distill.pub/2019/advex-bugs-discussion/response-6/#figure-3" class="figure-number">3</a>
            </figcaption>
        </figure>

        <p>
            We then initialize a new CNN model and train it on this mislabeled FashionMNIST data. The resulting model
            reaches 91.04% accuracy on the MNIST test set. Furthermore, if we normalize the FashionMNIST images using
            the mean and variance statistics for MNIST, the model reaches 94.5% accuracy on the MNIST test set. This is
            another instance of recovering a functionally similar model to the original despite the new model only
            training on erroneous predictions.
        </p>

        <h3>
            Summary
        </h3>

        <p>
            These results show that training a model using mislabeled adversarial examples is a special case of learning
            from prediction errors. In other words, the perturbations added to adversarial examples in Section 3.2 of
            Ilyas et al. (2019) are not necessary to enable learning.
        </p>


        <div class="comment-info">
    To cite Ilyas et al.’s response, please cite their
    <a href="https://distill.pub/2019/advex-bugs-discussion/original-authors/#citation">collection of responses</a>.
</div>


        <section id="rebuttal">

            <b>Response
                Summary</b>: Note that since our experiments work across different architectures,
            “distillation” in weight space does not occur. The only distillation that can
            arise is “feature space” distillation, which is actually exactly our hypothesis.
            In particular, feature-space distillation would not work in <a href="https://distill.pub/2019/advex-bugs-discussion/original-authors/#world1">
                World 1</a> — if the
            adversarial examples we generated did not exploit useful features, we should not
            have been able to “distill” a useful model from them. (In fact, one might think
            of normal model training as just “feature distillation” of the humans that
            labeled the dataset.) Furthermore, the hypothesis that all we need is enough
            model-consistent points in order to recover a model, seems to be disproven by
            Preetum’s <a href="https://distill.pub/2019/advex-bugs-discussion/response-5">“bugs-only dataset”</a>
            and other (e.g. <d-cite key="milli2018model"></d-cite>) settings. <p></p>

            <p><b>Response</b>: Since our experiments work across different architectures,
                “distillation” in weight space cannot arise. Thus, from what we understand, the
                “distillation” hypothesis suggested here is referring to “feature distillation”
                (i.e. getting models which use the same features as the original), which is
                actually precisely our hypothesis too. Notably, this feature distillation would
                not be possible if adversarial examples did not rely on “flipping” features that
                are good for classification (see <a href="https://distill.pub/2019/advex-bugs-discussion/original-authors/#world1">World
                    1</a> and
                <a href="https://distill.pub/2019/advex-bugs-discussion/original-authors/#world2">World 2</a>) — in that case, the distilled
                model would only use features that generalize poorly, and would thus generalize
                poorly itself. </p>

            <p> Moreover, we would argue that in the experiments presented (learning from
                mislabeled data), the same kind of distillation is happening. For instance, a
                moderately accurate model might associate “green background” with “frog” thus
                labeling “green” images as “frogs” (e.g., the horse in the comment’s figure).
                Training a new model on this dataset will thus associate “green” with “frog”
                achieving non-trivial accuracy on the test set (similarly for the “learning MNIST
                from Fashion-MNIST” experiment in the comment). This corresponds exactly to
                learning features from labels, akin to how deep networks “distill” a good
                decision boundary from human annotators. In fact, we find these experiments
                a very interesting illustration of feature distillation that complements
                our findings. </p>

            <p> We also note that an analogy to logistic regression here is only possible
                due to the low VC-dimension of linear classifiers (namely, these classifiers
                have dimension <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">d</span></span></span></span></span>). In particular, given any classifier with VC-dimension
                <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span></span></span>, we need at least <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span></span></span> points to fully specify the classifier. Conversely, neural
                networks have been shown to have extremely large VC-dimension (in particular,
                bigger than the size of the training set <d-cite key="zhang2016understanding"></d-cite>). So even though
                labelling <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>d</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">d+1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="base"><span class="mord mathit">d</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span></span> random
                points model-consistently is sufficient to recover a linear model, it is not
                necessarily sufficient to recover a deep neural network. For instance, Milli et
                al. <d-cite key="milli2018model"></d-cite> are not able to reconstruct a ResNet-18
                using only its predictions on random Gaussian inputs. (Note that we are using a
                ResNet-50 in our experiments.) </p>

            <p> Finally, it seems that the only potentially problematic explanation for
                our experiments (namely, that enough model-consistent points can recover a
                classifier) is <a href="https://distill.pub/2019/advex-bugs-discussion/response-5">disproved by Preetum’s experiment</a>.
                In particular, Preetum is able to design a
                dataset where training on mislabeled inputs <em>that are model-consistent</em>
                does not at all recover the decision boundary of the original model. More
                generally, the “model distillation” perspective raised here is unable to
                distinguish between the dataset created by Preetum below, and those created
                with standard PGD (as in our <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mover accent="true"><mrow><mrow><mi mathvariant="script">D</mi></mrow></mrow><mo stretchy="true">^</mo></mover><mrow><mi>d</mi><mi>e</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\widehat{\mathcal{D}}_{det}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.92333em;"></span><span class="strut bottom" style="height:1.07333em;vertical-align:-0.15em;"></span><span class="base"><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.92333em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathcal" style="margin-right:0.02778em;">D</span></span></span></span><span class="svg-align" style="top:-3.6833299999999998em;width:calc(100% - 0.11112em);margin-left:0.11112em;"><span class="pstrut" style="height:3em;"></span><span style="height:0.24em;"><svg width="100%" height="0.24em" viewBox="0 0 1062 239" preserveAspectRatio="none">
<path d="M529 0h5l519 115c5 1 9 5 9 10 0 1-1 2-1 3l-4 22
c-1 5-5 9-11 9h-2L532 67 19 159h-2c-5 0-9-4-11-9l-5-22c-1-6 2-12 8-13z"></path></svg></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">d</span><span class="mord mathit mtight">e</span><span class="mord mathit mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span></span></span></span></span> and
                <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mover accent="true"><mrow><mrow><mi mathvariant="script">D</mi></mrow></mrow><mo stretchy="true">^</mo></mover><mrow><mi>r</mi><mi>a</mi><mi>n</mi><mi>d</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\widehat{\mathcal{D}}_{rand}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.92333em;"></span><span class="strut bottom" style="height:1.07333em;vertical-align:-0.15em;"></span><span class="base"><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.92333em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathcal" style="margin-right:0.02778em;">D</span></span></span></span><span class="svg-align" style="top:-3.6833299999999998em;width:calc(100% - 0.11112em);margin-left:0.11112em;"><span class="pstrut" style="height:3em;"></span><span style="height:0.24em;"><svg width="100%" height="0.24em" viewBox="0 0 1062 239" preserveAspectRatio="none">
<path d="M529 0h5l519 115c5 1 9 5 9 10 0 1-1 2-1 3l-4 22
c-1 5-5 9-11 9h-2L532 67 19 159h-2c-5 0-9-4-11-9l-5-22c-1-6 2-12 8-13z"></path></svg></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight" style="margin-right:0.02778em;">r</span><span class="mord mathit mtight">a</span><span class="mord mathit mtight">n</span><span class="mord mathit mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span></span></span></span></span> datasets).
            </p>
        </section>

        <div class="comment-info">
    You can find more responses in the <a href="https://distill.pub/2019/advex-bugs-discussion/"> main discussion article</a>.
</div>


    </d-article>



    <d-appendix>
<style>

d-appendix {
  contain: layout style;
  font-size: 0.8em;
  line-height: 1.7em;
  margin-top: 60px;
  margin-bottom: 0;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  color: rgba(0,0,0,0.5);
  padding-top: 60px;
  padding-bottom: 48px;
}

d-appendix h3 {
  grid-column: page-start / text-start;
  font-size: 15px;
  font-weight: 500;
  margin-top: 1em;
  margin-bottom: 0;
  color: rgba(0,0,0,0.65);
}

d-appendix h3 + * {
  margin-top: 1em;
}

d-appendix ol {
  padding: 0 0 0 15px;
}

@media (min-width: 768px) {
  d-appendix ol {
    padding: 0 0 0 30px;
    margin-left: -30px;
  }
}

d-appendix li {
  margin-bottom: 1em;
}

d-appendix a {
  color: rgba(0, 0, 0, 0.6);
}

d-appendix > * {
  grid-column: text;
}

d-appendix > d-footnote-list,
d-appendix > d-citation-list,
d-appendix > distill-appendix {
  grid-column: screen;
}

</style>


        <d-footnote-list style="display: none;">
<style>

d-footnote-list {
  contain: layout style;
}

d-footnote-list > * {
  grid-column: text;
}

d-footnote-list a.footnote-backlink {
  color: rgba(0,0,0,0.3);
  padding-left: 0.5em;
}

</style>

<h3>Footnotes</h3>
<ol></ol>
</d-footnote-list>
        <d-citation-list distill-prerendered="true"><style>
d-citation-list {
  contain: style;
}

d-citation-list .references {
  grid-column: text;
}

d-citation-list .references .title {
  font-weight: 500;
}
</style><h3 id="references">References</h3><ol id="references-list" class="references"><li id="hinton2015distilling"><span class="title">Distilling the Knowledge in a Neural Network</span>   <a href="http://arxiv.org/pdf/1503.02531.pdf">[PDF]</a><br>Hinton, G., Vinyals, O. and Dean, J., 2015. </li><li id="milli2018model"><span class="title">Model reconstruction from model explanations</span> <br>Milli, S., Schmidt, L., Dragan, A.D. and Hardt, M., 2018. arXiv preprint arXiv:1807.05185. </li><li id="zhang2016understanding"><span class="title">Understanding deep learning requires rethinking generalization</span> <br>Zhang, C., Bengio, S., Hardt, M., Recht, B. and Vinyals, O., 2016. International Conference on Learning Representations (ICLR). </li></ol></d-citation-list>
    <distill-appendix>
<style>
  distill-appendix {
    contain: layout style;
  }

  distill-appendix .citation {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  distill-appendix > * {
    grid-column: text;
  }
</style>

    <h3 id="updates-and-corrections">Updates and Corrections</h3>
    <p>
    If you see mistakes or want to suggest changes, please <a href="https://github.com/distillpub/post--ilyas-response-6/issues/new">create an issue on GitHub</a>. </p>
    
    <h3 id="reuse">Reuse</h3>
    <p>Diagrams and text are licensed under Creative Commons Attribution <a href="https://creativecommons.org/licenses/by/4.0/">CC-BY 4.0</a> with the <a class="github" href="https://github.com/distillpub/post--ilyas-response-6">source available on GitHub</a>, unless noted otherwise. The figures that have been reused from other sources don’t fall under this license and can be recognized by a note in their caption: “Figure from …”.</p>
    
    <h3 id="citation">Citation</h3>
    <p>For attribution in academic contexts, please cite this work as</p>
    <pre class="citation short">Wallace, "A Discussion of 'Adversarial Examples Are Not Bugs, They Are Features': Learning from Incorrectly Labeled Data", Distill, 2019.</pre>
    <p>BibTeX citation</p>
    <pre class="citation long">@article{wallace2019a,
  author = {Wallace, Eric},
  title = {A Discussion of 'Adversarial Examples Are Not Bugs, They Are Features': Learning from Incorrectly Labeled Data},
  journal = {Distill},
  year = {2019},
  note = {https://distill.pub/2019/advex-bugs-discussion/response-6},
  doi = {10.23915/distill.00019.6}
}</pre>
    </distill-appendix></d-appendix>

    <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
    <d-bibliography><script type="text/json">[["hinton2015distilling",{"author":"Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff","year":"2015","month":"03","title":"Distilling the Knowledge in a Neural Network","url":"https://arxiv.org/abs/1503.02531","type":"article"}],["milli2018model",{"title":"Model reconstruction from model explanations","author":"Milli, Smitha and Schmidt, Ludwig and Dragan, Anca D and Hardt, Moritz","journal":"arXiv preprint arXiv:1807.05185","year":"2018","type":"article"}],["zhang2016understanding",{"title":"Understanding deep learning requires rethinking generalization","author":"Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol","booktitle":"International Conference on Learning Representations (ICLR)","year":"2016","type":"InProceedings"}]]</script></d-bibliography>

<script type="text/javascript" src="./A Discussion of &#39;Adversarial Examples Are Not Bugs, They Are Features&#39;_ Learning from Incorrectly Labeled Data_files/index.bundle.js"></script>
<distill-footer>
<style>

:host {
  color: rgba(255, 255, 255, 0.5);
  font-weight: 300;
  padding: 2rem 0;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  background-color: hsl(180, 5%, 15%); /*hsl(200, 60%, 15%);*/
  text-align: left;
  contain: content;
}

.footer-container .logo svg {
  width: 24px;
  position: relative;
  top: 4px;
  margin-right: 2px;
}

.footer-container .logo svg path {
  fill: none;
  stroke: rgba(255, 255, 255, 0.8);
  stroke-width: 3px;
}

.footer-container .logo {
  font-size: 17px;
  font-weight: 200;
  color: rgba(255, 255, 255, 0.8);
  text-decoration: none;
  margin-right: 6px;
}

.footer-container {
  grid-column: text;
}

.footer-container .nav {
  font-size: 0.9em;
  margin-top: 1.5em;
}

.footer-container .nav a {
  color: rgba(255, 255, 255, 0.8);
  margin-right: 6px;
  text-decoration: none;
}

</style>

<div class="footer-container">

  <a href="https://distill.pub/" class="logo">
    <svg viewBox="-607 419 64 64">
  <path d="M-573.4,478.9c-8,0-14.6-6.4-14.6-14.5s14.6-25.9,14.6-40.8c0,14.9,14.6,32.8,14.6,40.8S-565.4,478.9-573.4,478.9z"></path>
</svg>

    Distill
  </a> is dedicated to clear explanations of machine learning

  <div class="nav">
    <a href="https://distill.pub/about/">About</a>
    <a href="https://distill.pub/journal/">Submit</a>
    <a href="https://distill.pub/prize/">Prize</a>
    <a href="https://distill.pub/archive/">Archive</a>
    <a href="https://distill.pub/rss.xml">RSS</a>
    <a href="https://github.com/distillpub">GitHub</a>
    <a href="https://twitter.com/distillpub">Twitter</a>
    &nbsp;&nbsp;&nbsp;&nbsp; ISSN 2476-0757
  </div>

</div>

</distill-footer><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-83741880-1', 'auto');
  ga('send', 'pageview');
</script></body></html>