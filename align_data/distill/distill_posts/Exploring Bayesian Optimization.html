<!DOCTYPE html>
<html lang="en"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8"><script src="Exploring%20Bayesian%20Optimization_files/template.js"></script><style>body {transition: opacity ease-in 0.2s; } 
body[unresolved] {opacity: 0; display: block; overflow: hidden; position: relative; } 
</style><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"><script async="" src="Exploring%20Bayesian%20Optimization_files/analytics.js"></script><script>
window.addEventListener('WebComponentsReady', function() {
  console.warn('WebComponentsReady');
  const loaderTag = document.createElement('script');
  loaderTag.src = 'https://distill.pub/template.v2.js';
  document.head.insertBefore(loaderTag, document.head.firstChild);
});
</script><script src="Exploring%20Bayesian%20Optimization_files/webcomponents-loader.js"></script><script src="Exploring%20Bayesian%20Optimization_files/webcomponents-hi.js"></script>
  
  
  <title>Exploring Bayesian Optimization</title>
  <style id="distill-prerendered-styles" type="text/css">/*
 * Copyright 2018 The Distill Template Authors
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

html {
  font-size: 14px;
	line-height: 1.6em;
  /* font-family: "Libre Franklin", "Helvetica Neue", sans-serif; */
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
  /*, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";*/
  text-size-adjust: 100%;
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
}

@media(min-width: 768px) {
  html {
    font-size: 16px;
  }
}

body {
  margin: 0;
}

a {
  color: #004276;
}

figure {
  margin: 0;
}

table {
	border-collapse: collapse;
	border-spacing: 0;
}

table th {
	text-align: left;
}

table thead {
  border-bottom: 1px solid rgba(0, 0, 0, 0.05);
}

table thead th {
  padding-bottom: 0.5em;
}

table tbody :first-child td {
  padding-top: 0.5em;
}

pre {
  overflow: auto;
  max-width: 100%;
}

p {
  margin-top: 0;
  margin-bottom: 1em;
}

sup, sub {
  vertical-align: baseline;
  position: relative;
  top: -0.4em;
  line-height: 1em;
}

sub {
  top: 0.4em;
}

.kicker,
.marker {
  font-size: 15px;
  font-weight: 600;
  color: rgba(0, 0, 0, 0.5);
}


/* Headline */

@media(min-width: 1024px) {
  d-title h1 span {
    display: block;
  }
}

/* Figure */

figure {
  position: relative;
  margin-bottom: 2.5em;
  margin-top: 1.5em;
}

figcaption+figure {

}

figure img {
  width: 100%;
}

figure svg text,
figure svg tspan {
}

figcaption,
.figcaption {
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
}

@media(min-width: 1024px) {
figcaption,
.figcaption {
    font-size: 13px;
  }
}

figure.external img {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

figcaption a {
  color: rgba(0, 0, 0, 0.6);
}

figcaption b,
figcaption strong, {
  font-weight: 600;
  color: rgba(0, 0, 0, 1.0);
}
/*
 * Copyright 2018 The Distill Template Authors
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

@supports not (display: grid) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    display: block;
    padding: 8px;
  }
}

.base-grid,
distill-header,
d-title,
d-abstract,
d-article,
d-appendix,
distill-appendix,
d-byline,
d-footnote-list,
d-citation-list,
distill-footer {
  display: grid;
  justify-items: stretch;
  grid-template-columns: [screen-start] 8px [page-start kicker-start text-start gutter-start middle-start] 1fr 1fr 1fr 1fr 1fr 1fr 1fr 1fr [text-end page-end gutter-end kicker-end middle-end] 8px [screen-end];
  grid-column-gap: 8px;
}

.grid {
  display: grid;
  grid-column-gap: 8px;
}

@media(min-width: 768px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start middle-start text-start] 45px 45px 45px 45px 45px 45px 45px 45px [ kicker-end text-end gutter-start] 45px [middle-end] 45px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 16px;
  }

  .grid {
    grid-column-gap: 16px;
  }
}

@media(min-width: 1000px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 50px [middle-start] 50px [text-start kicker-end] 50px 50px 50px 50px 50px 50px 50px 50px [text-end gutter-start] 50px [middle-end] 50px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 16px;
  }

  .grid {
    grid-column-gap: 16px;
  }
}

@media(min-width: 1180px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 32px;
  }

  .grid {
    grid-column-gap: 32px;
  }
}




.base-grid {
  grid-column: screen;
}

/* .l-body,
d-article > *  {
  grid-column: text;
}

.l-page,
d-title > *,
d-figure {
  grid-column: page;
} */

.l-gutter {
  grid-column: gutter;
}

.l-text,
.l-body {
  grid-column: text;
}

.l-page {
  grid-column: page;
}

.l-body-outset {
  grid-column: middle;
}

.l-page-outset {
  grid-column: page;
}

.l-screen {
  grid-column: screen;
}

.l-screen-inset {
  grid-column: screen;
  padding-left: 16px;
  padding-left: 16px;
}


/* Aside */

d-article aside {
  grid-column: gutter;
  font-size: 12px;
  line-height: 1.6em;
  color: rgba(0, 0, 0, 0.6)
}

@media(min-width: 768px) {
  aside {
    grid-column: gutter;
  }

  .side {
    grid-column: gutter;
  }
}
/*
 * Copyright 2018 The Distill Template Authors
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

d-title {
  padding: 2rem 0 1.5rem;
  contain: layout style;
  overflow-x: hidden;
}

@media(min-width: 768px) {
  d-title {
    padding: 4rem 0 1.5rem;
  }
}

d-title h1 {
  grid-column: text;
  font-size: 40px;
  font-weight: 700;
  line-height: 1.1em;
  margin: 0 0 0.5rem;
}

@media(min-width: 768px) {
  d-title h1 {
    font-size: 50px;
  }
}

d-title p {
  font-weight: 300;
  font-size: 1.2rem;
  line-height: 1.55em;
  grid-column: text;
}

d-title .status {
  margin-top: 0px;
  font-size: 12px;
  color: #009688;
  opacity: 0.8;
  grid-column: kicker;
}

d-title .status span {
  line-height: 1;
  display: inline-block;
  padding: 6px 0;
  border-bottom: 1px solid #80cbc4;
  font-size: 11px;
  text-transform: uppercase;
}
/*
 * Copyright 2018 The Distill Template Authors
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

d-byline {
  contain: style;
  overflow: hidden;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  font-size: 0.8rem;
  line-height: 1.8em;
  padding: 1.5rem 0;
  min-height: 1.8em;
}


d-byline .byline {
  grid-template-columns: 1fr 1fr;
  grid-column: text;
}

@media(min-width: 768px) {
  d-byline .byline {
    grid-template-columns: 1fr 1fr 1fr 1fr;
  }
}

d-byline .authors-affiliations {
  grid-column-end: span 2;
  grid-template-columns: 1fr 1fr;
  margin-bottom: 1em;
}

@media(min-width: 768px) {
  d-byline .authors-affiliations {
    margin-bottom: 0;
  }
}

d-byline h3 {
  font-size: 0.6rem;
  font-weight: 400;
  color: rgba(0, 0, 0, 0.5);
  margin: 0;
  text-transform: uppercase;
}

d-byline p {
  margin: 0;
}

d-byline a,
d-article d-byline a {
  color: rgba(0, 0, 0, 0.8);
  text-decoration: none;
  border-bottom: none;
}

d-article d-byline a:hover {
  text-decoration: underline;
  border-bottom: none;
}

d-byline p.author {
  font-weight: 500;
}

d-byline .affiliations {

}
/*
 * Copyright 2018 The Distill Template Authors
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

d-article {
  contain: layout style;
  overflow-x: hidden;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  padding-top: 2rem;
  color: rgba(0, 0, 0, 0.8);
}

d-article > * {
  grid-column: text;
}

@media(min-width: 768px) {
  d-article {
    font-size: 16px;
  }
}

@media(min-width: 1024px) {
  d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
  }
}


/* H2 */


d-article .marker {
  text-decoration: none;
  border: none;
  counter-reset: section;
  grid-column: kicker;
  line-height: 1.7em;
}

d-article .marker:hover {
  border: none;
}

d-article .marker span {
  padding: 0 3px 4px;
  border-bottom: 1px solid rgba(0, 0, 0, 0.2);
  position: relative;
  top: 4px;
}

d-article .marker:hover span {
  color: rgba(0, 0, 0, 0.7);
  border-bottom: 1px solid rgba(0, 0, 0, 0.7);
}

d-article h2 {
  font-weight: 600;
  font-size: 24px;
  line-height: 1.25em;
  margin: 2rem 0 1.5rem 0;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  padding-bottom: 1rem;
}

@media(min-width: 1024px) {
  d-article h2 {
    font-size: 36px;
  }
}

/* H3 */

d-article h3 {
  font-weight: 700;
  font-size: 18px;
  line-height: 1.4em;
  margin-bottom: 1em;
  margin-top: 2em;
}

@media(min-width: 1024px) {
  d-article h3 {
    font-size: 20px;
  }
}

/* H4 */

d-article h4 {
  font-weight: 600;
  text-transform: uppercase;
  font-size: 14px;
  line-height: 1.4em;
}

d-article a {
  color: inherit;
}

d-article p,
d-article ul,
d-article ol,
d-article blockquote {
  margin-top: 0;
  margin-bottom: 1em;
  margin-left: 0;
  margin-right: 0;
}

d-article blockquote {
  border-left: 2px solid rgba(0, 0, 0, 0.2);
  padding-left: 2em;
  font-style: italic;
  color: rgba(0, 0, 0, 0.6);
}

d-article a {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
  text-decoration: none;
}

d-article a:hover {
  border-bottom: 1px solid rgba(0, 0, 0, 0.8);
}

d-article .link {
  text-decoration: underline;
  cursor: pointer;
}

d-article ul,
d-article ol {
  padding-left: 24px;
}

d-article li {
  margin-bottom: 1em;
  margin-left: 0;
  padding-left: 0;
}

d-article li:last-child {
  margin-bottom: 0;
}

d-article pre {
  font-size: 14px;
  margin-bottom: 20px;
}

d-article hr {
  grid-column: screen;
  width: 100%;
  border: none;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  margin-top: 60px;
  margin-bottom: 60px;
}

d-article section {
  margin-top: 60px;
  margin-bottom: 60px;
}

d-article span.equation-mimic {
  font-family: georgia;
  font-size: 115%;
  font-style: italic;
}

d-article > d-code,
d-article section > d-code  {
  display: block;
}

d-article > d-math[block],
d-article section > d-math[block]  {
  display: block;
}

@media (max-width: 768px) {
  d-article > d-code,
  d-article section > d-code,
  d-article > d-math[block],
  d-article section > d-math[block] {
      overflow-x: scroll;
      -ms-overflow-style: none;  // IE 10+
      overflow: -moz-scrollbars-none;  // Firefox
  }

  d-article > d-code::-webkit-scrollbar,
  d-article section > d-code::-webkit-scrollbar,
  d-article > d-math[block]::-webkit-scrollbar,
  d-article section > d-math[block]::-webkit-scrollbar {
    display: none;  // Safari and Chrome
  }
}

d-article .citation {
  color: #668;
  cursor: pointer;
}

d-include {
  width: auto;
  display: block;
}

d-figure {
  contain: layout style;
}

/* KaTeX */

.katex, .katex-prerendered {
  contain: style;
  display: inline-block;
}

/* Tables */

d-article table {
  border-collapse: collapse;
  margin-bottom: 1.5rem;
  border-bottom: 1px solid rgba(0, 0, 0, 0.2);
}

d-article table th {
  border-bottom: 1px solid rgba(0, 0, 0, 0.2);
}

d-article table td {
  border-bottom: 1px solid rgba(0, 0, 0, 0.05);
}

d-article table tr:last-of-type td {
  border-bottom: none;
}

d-article table th,
d-article table td {
  font-size: 15px;
  padding: 2px 8px;
}

d-article table tbody :first-child td {
  padding-top: 2px;
}
/*
 * Copyright 2018 The Distill Template Authors
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

span.katex-display {
  text-align: left;
  padding: 8px 0 8px 0;
  margin: 0.5em 0 0.5em 1em;
}

span.katex {
  -webkit-font-smoothing: antialiased;
  color: rgba(0, 0, 0, 0.8);
  font-size: 1.18em;
}
/*
 * Copyright 2018 The Distill Template Authors
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

@media print {

  @page {
    size: 8in 11in;
    @bottom-right {
      content: counter(page) " of " counter(pages);
    }
  }

  html {
    /* no general margins -- CSS Grid takes care of those */
  }

  p, code {
    page-break-inside: avoid;
  }

  h2, h3 {
    page-break-after: avoid;
  }

  d-header {
    visibility: hidden;
  }

  d-footer {
    display: none!important;
  }

}
</style>
  <link rel="stylesheet" type="text/css" href="Exploring%20Bayesian%20Optimization_files/styles.css">
  <script defer="defer" src="Exploring%20Bayesian%20Optimization_files/hider.js"></script>
  <script defer="defer" src="Exploring%20Bayesian%20Optimization_files/gif-slider.js"></script>

<link rel="stylesheet" href="Exploring%20Bayesian%20Optimization_files/katex.css" crossorigin="anonymous">
    
    <link rel="icon" type="image/png" href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAYAAACqaXHeAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA99JREFUeNrsG4t1ozDMzQSM4A2ODUonKBucN2hugtIJ6E1AboLcBiQTkJsANiAb9OCd/OpzMWBJBl5TvaeXPiiyJetry0J8wW3D3QpjRh3GjneXDq+fSQA9s2mH9x3KDhN4foJfCb8N/Jrv+2fnDn8vLRQOplWHVYdvHZYdZsBcZP1vBmh/n8DzEmhUQDPaOuP9pFuY+JwJHwHnCLQE2tnWBGEyXozY9xCUgHMhhjE2I4heVWtgIkZ83wL6Qgxj1obfWBxymPwe+b00BCCRNPbwfb60yleAkkBHGT5AEehIYz7eJrFDMF9CvH4wwhcGHiHMneFvLDQwlwvMLQq58trRcYBWfYn0A0OgHWQUSu25mE+BnoYKnnEJoeIWAifzOv7vLWd2ZKRfWAIme3tOiUaQ3UnLkb0xj1FxRIeEGKaGIHOs9nEgLaaA9i0JRYo1Ic67wJW86KSKE/ZAM8KuVMk8ITVhmxUxJ3Cl2xlm9Vtkeju1+mpCQNxaEGNCY8bs9X2YqwNoQeGjBWut/ma0QAWy/TqAsHx9wSya3I5IRxOfTC+leG+kA/4vSeEcGBtNUN6byhu3+keEZCQJUNh8MAO7HL6H8pQLnsW/Hd4T4lv93TPjfM7A46iEEqbB5EDOvwYNW6tGNZzT/o+CZ6sqZ6wUtR/wf7mi/VL8iNciT6rHih48Y55b4nKCHJCCzb4y0nwFmin3ZEMIoLfZF8F7nncFmvnWBaBj7CGAYA/WGJsUwHdYqVDwAmNsUgAx4CGgAA7GOOxADYOFWOaIKifuVYzmOpREqA21Mo7aPsgiY1PhOMAmxtR+AUbYH3Id2wc0SAFIQTsn9IUGWR8k9jx3vtXSiAacFxTAGakBk9UudkNECd6jLe+6HrshshvIuC6IlLMRy7er+JpcKma24SlE4cFZSZJDGVVrsNvitQhQrDhW0jfiOLfFd47C42eHT56D/BK0To+58Ahj+cAT8HT1UWlfLZCCd/uKawzU0Rh2EyIX/Icqth3niG8ybNroezwe6khdCNxRN+l4XGdOLVLlOOt2hTRJlr1ETIuMAltVTMz70mJrkdGAaZLSmnBEqmAE32JCMmuTlCnRgsBENtOUpHhvvsYIL0ibnBkaC6QvKcR7738GKp0AKnim7xgUSNv1bpS8QwhBt8r+EP47v/oyRK/S34yJ9nT+AN0Tkm4OdB9E4BsmXM3SnMlRFUrtp6IDpV2eKzdYvF3etm3KhQksbOLChGkSmcBdmcEwvqkrMy5BzL00NZeu3qPYJOOuCc+5NjcWKXQxFvTa3NoXJ4d8in7fiAUuTt781dkvuHX4K8AA2Usy7yNKLy0AAAAASUVORK5CYII=">
    <link href="https://distill.pub/rss.xml" rel="alternate" type="application/rss+xml" title="Articles from Distill">
  
    <title>Exploring Bayesian Optimization</title>
    
    <link rel="canonical" href="https://distill.pub/2020/bayesian-optimization">
    
    <!--  https://schema.org/Article -->
    <meta property="description" itemprop="description" content="How to tune hyperparameters for your machine learning model using Bayesian optimization.">
    <meta property="article:published" itemprop="datePublished" content="2020-05-05">
    <meta property="article:created" itemprop="dateCreated" content="2020-05-05">
    
    <meta property="article:modified" itemprop="dateModified" content="2021-09-23T18:37:48.000Z">
    
    <meta property="article:author" content="Apoorv Agnihotri">
    <meta property="article:author" content="Nipun Batra">
    <!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
    <meta property="og:type" content="article">
    <meta property="og:title" content="Exploring Bayesian Optimization">
    <meta property="og:description" content="How to tune hyperparameters for your machine learning model using Bayesian optimization.">
    <meta property="og:url" content="https://distill.pub/2020/bayesian-optimization">
    <meta property="og:image" content="https://distill.pub/2020/bayesian-optimization/thumbnail.jpg">
    <meta property="og:locale" content="en_US">
    <meta property="og:site_name" content="Distill">
  
    <!--  https://dev.twitter.com/cards/types/summary -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Exploring Bayesian Optimization">
    <meta name="twitter:description" content="How to tune hyperparameters for your machine learning model using Bayesian optimization.">
    <meta name="twitter:url" content="https://distill.pub/2020/bayesian-optimization">
    <meta name="twitter:image" content="https://distill.pub/2020/bayesian-optimization/thumbnail.jpg">
    <meta name="twitter:image:width" content="560">
    <meta name="twitter:image:height" content="295">
  
      <!--  https://scholar.google.com/intl/en/scholar/inclusion.html#indexing -->
    <meta name="citation_title" content="Exploring Bayesian Optimization">
    <meta name="citation_fulltext_html_url" content="https://distill.pub/2020/bayesian-optimization">
    <meta name="citation_volume" content="5">
    <meta name="citation_issue" content="5">
    <meta name="citation_firstpage" content="e26">
    <meta name="citation_doi" content="10.23915/distill.00026">
    <meta name="citation_journal_title" content="Distill">
    <meta name="citation_journal_abbrev" content="Distill">
    <meta name="citation_issn" content="2476-0757">
    <meta name="citation_fulltext_world_readable" content="">
    <meta name="citation_online_date" content="2020/05/05">
    <meta name="citation_publication_date" content="2020/05/05">
    <meta name="citation_author" content="Agnihotri, Apoorv">
    <meta name="citation_author" content="Batra, Nipun">
    <meta name="citation_reference" content="citation_title=A statistical approach to some basic mine valuation problems on the Witwatersrand ;citation_author=D.G. Krige;citation_publication_date=1951;citation_journal_title=Journal of the Southern African Institute of Mining and Metallurgy;citation_volume=52;citation_number=6;">
    <meta name="citation_reference" content="citation_title=Active Learning Literature Survey;citation_author=Burr Settles;citation_publication_date=2009;citation_number=1648;">
    <meta name="citation_reference" content="citation_title=Active learning: theory and applications;citation_author=Simon Tong;citation_publication_date=2001;">
    <meta name="citation_reference" content="citation_title=Taking the Human Out of the Loop: A Review of Bayesian Optimization;citation_author=B. Shahriari;citation_author=K. Swersky;citation_author=Z. Wang;citation_author=R. P. Adams;citation_author=N. de Freitas;citation_publication_date=2016;citation_journal_title=Proceedings of the IEEE;citation_volume=104;citation_number=1;">
    <meta name="citation_reference" content="citation_title=A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning;citation_author=Eric Brochu;citation_author=Vlad M. Cora;citation_author=Nando De Freitas;citation_publication_date=2010;citation_journal_title=CoRR;citation_volume=abs/1012.2599;">
    <meta name="citation_reference" content="citation_title=A Visual Exploration of Gaussian Processes;citation_author=Jochen Görtler;citation_author=Rebecca Kehlbeck;citation_author=Oliver Deussen;citation_publication_date=2019;citation_journal_title=Distill;">
    <meta name="citation_reference" content="citation_title=Gaussian Processes in Machine Learning;citation_author=Carl Edward Rasmussen;citation_publication_date=2004;">
    <meta name="citation_reference" content="citation_title=Bayesian approach to global optimization and application to multiobjective and constrained problems;citation_author=J B. Mockus;citation_author=Linas Mockus;citation_publication_date=1991;citation_journal_title=Journal of Optimization Theory and Applications;citation_volume=70;">
    <meta name="citation_reference" content="citation_title=On The Likelihood That One Unknown Probability Exceeds Another In View Of The Evidence Of Two Samples;citation_author=William R Thompson;citation_publication_date=1933;citation_journal_title=Biometrika;citation_volume=25;citation_number=3-4;">
    <meta name="citation_reference" content="citation_title=Using Confidence Bounds for Exploitation-Exploration Trade-Offs;citation_author=Peter Auer;citation_publication_date=2003;citation_journal_title=J. Mach. Learn. Res.;citation_volume=3;citation_number=null;">
    <meta name="citation_reference" content="citation_title=Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design;citation_author=Niranjan Srinivas;citation_author=Andreas Krause;citation_author=Sham M. Kakade;citation_author=Matthias Seeger;citation_publication_date=2009;citation_journal_title=arXiv e-prints;">
    <meta name="citation_reference" content="citation_title=Practical Bayesian Optimization of Machine Learning Algorithms;citation_author=Jasper Snoek;citation_author=Hugo Larochelle;citation_author=Ryan P. Adams;citation_publication_date=2012;">
    <meta name="citation_reference" content="citation_title=Algorithms for Hyper-Parameter Optimization;citation_author=James Bergstra;citation_author=R\'{e}mi Bardenet;citation_author=Yoshua Bengio;citation_author=Bal\'{a}zs K\'{e}gl;citation_publication_date=2011;">
    <meta name="citation_reference" content="citation_title=Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures;citation_author=J. Bergstra;citation_author=D. Yamins;citation_author=D. D. Cox;citation_publication_date=2013;">
    <meta name="citation_reference" content="citation_title=Scikit-learn: Machine Learning in {P}ython;citation_author=F. Pedregosa;citation_author=G. Varoquaux;citation_author=A. Gramfort;citation_author=V. Michel;citation_author=B. Thirion;citation_author=O. Grisel;citation_author=M. Blondel;citation_author=P. Prettenhofer;citation_author=R. Weiss;citation_author=V. Dubourg;citation_author=J. Vanderplas;citation_author=A. Passos;citation_author=D. Cournapeau;citation_author=M. Brucher;citation_author=M. Perrot;citation_author=E. Duchesnay;citation_publication_date=2011;citation_journal_title=Journal of Machine Learning Research;citation_volume=12;">
    <meta name="citation_reference" content="citation_title=Bayesian Optimization with Gradients;citation_author=Jian Wu;citation_author=Matthias Poloczek;citation_author=Andrew G Wilson;citation_author=Peter Frazier;citation_publication_date=2017;">
    <meta name="citation_reference" content="citation_title=Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization;citation_author=Liam Li;citation_author=Kevin Jamieson;citation_author=Giulia DeSalvo;citation_author=Afshin Rostamizadeh;citation_author=Ameet Talwalkar;citation_publication_date=2018;citation_journal_title=Journal of Machine Learning Research;citation_volume=18-185;">
    <meta name="citation_reference" content="citation_title=Fast Bayesian Optimization of Machine Learning Hyperparameters on Large Datasets;citation_author=Aaron Klein;citation_author=Stefan Falkner;citation_author=Simon Bartels;citation_author=Philipp Hennig;citation_author=Frank Hutter;citation_publication_date=2017;citation_volume=54;">
    <meta name="citation_reference" content="citation_title=Safe Exploration for Optimization with Gaussian Processes;citation_author=Yanan Sui;citation_author=Alkis Gotovos;citation_author=Joel W. Burdick;citation_author=Andreas Krause;citation_publication_date=2015;">
    <meta name="citation_reference" content="citation_title=Scalable Bayesian Optimization Using Deep Neural Networks;citation_author=Jasper Snoek;citation_author=Oren Rippel;citation_author=Kevin Swersky;citation_author=Ryan Kiros;citation_author=Nadathur Satish;citation_author=Narayanan Sundaram;citation_author=Md. Mostofa Ali Patwary;citation_author=Prabhat Prabhat;citation_author=Ryan P. Adams;citation_publication_date=2015;">
    <meta name="citation_reference" content="citation_title=Portfolio Allocation for Bayesian Optimization;citation_author=Matthew Hoffman;citation_author=Eric Brochu;citation_author=Nando de Freitas;citation_publication_date=2011;">
    <meta name="citation_reference" content="citation_title=Bayesian Optimization for Sensor Set Selection;citation_author=R. Garnett;citation_author=M. A. Osborne;citation_author=S. J. Roberts;citation_publication_date=2010;">
    <meta name="citation_reference" content="citation_title=Constrained Bayesian Optimization with Noisy Experiments;citation_author=Benjamin Letham;citation_author=Brian Karrer;citation_author=Guilherme Ottoni;citation_author=Eytan Bakshy;citation_publication_date=2019;citation_journal_title=Bayesian Anal.;citation_volume=14;citation_number=2;">
    <meta name="citation_reference" content="citation_title=Parallel Bayesian Global Optimization of Expensive Functions;citation_author=Jialei Wang;citation_author=Scott C. Clark;citation_author=Eric Liu;citation_author=Peter I. Frazier;citation_publication_date=2016;citation_journal_title=arXiv e-prints;">
</head>

<body onload="appendInputButtons();" distill-prerendered="" class="vsc-initialized" data-new-gr-c-s-check-loaded="8.899.0" data-gr-ext-installed=""><distill-header distill-prerendered="">
<style>
distill-header {
  position: relative;
  height: 60px;
  background-color: hsl(200, 60%, 15%);
  width: 100%;
  box-sizing: border-box;
  z-index: 2;
  color: rgba(0, 0, 0, 0.8);
  border-bottom: 1px solid rgba(0, 0, 0, 0.08);
  box-shadow: 0 1px 6px rgba(0, 0, 0, 0.05);
}
distill-header .content {
  height: 70px;
  grid-column: page;
}
distill-header a {
  font-size: 16px;
  height: 60px;
  line-height: 60px;
  text-decoration: none;
  color: rgba(255, 255, 255, 0.8);
  padding: 22px 0;
}
distill-header a:hover {
  color: rgba(255, 255, 255, 1);
}
distill-header svg {
  width: 24px;
  position: relative;
  top: 4px;
  margin-right: 2px;
}
@media(min-width: 1080px) {
  distill-header {
    height: 70px;
  }
  distill-header a {
    height: 70px;
    line-height: 70px;
    padding: 28px 0;
  }
  distill-header .logo {
  }
}
distill-header svg path {
  fill: none;
  stroke: rgba(255, 255, 255, 0.8);
  stroke-width: 3px;
}
distill-header .logo {
  font-size: 17px;
  font-weight: 200;
}
distill-header .nav {
  float: right;
  font-weight: 300;
}
distill-header .nav a {
  font-size: 12px;
  margin-left: 24px;
  text-transform: uppercase;
}
</style>
<div class="content">
  <a href="https://distill.pub/" class="logo">
    <svg viewBox="-607 419 64 64">
  <path d="M-573.4,478.9c-8,0-14.6-6.4-14.6-14.5s14.6-25.9,14.6-40.8c0,14.9,14.6,32.8,14.6,40.8S-565.4,478.9-573.4,478.9z"></path>
</svg>

    Distill
  </a>
  <nav class="nav">
    <a href="https://distill.pub/about/">About</a>
    <a href="https://distill.pub/prize/">Prize</a>
    <a href="https://distill.pub/journal/">Submit</a>
  </nav>
</div>
</distill-header>

  <d-front-matter>
    <script id="distill-front-matter" type="text/json">
      {
        "title": "Exploring Bayesian Optimization",
        "description": "How to tune hyperparameters for your machine learning model using Bayesian optimization.",
        "authors": [{
            "author": "Apoorv Agnihotri",
            "authorURL": "https://apoorvagnihotri.github.io/",
            "affiliations": [{
              "name": "Indian Insitute of Technology Gandhinagar",
              "affiliationURL": "https://www.iitgn.ac.in/"
            }]
          },
          {
            "author": "Nipun Batra",
            "authorURL": "https://nipunbatra.github.io/",
            "affiliations": [{
              "name": "Indian Insitute of Technology Gandhinagar",
              "affiliationURL": "https://www.iitgn.ac.in/"
            }]
          }
        ],
        "katex": {
          "delimiters": [{
            "left": "$$",
            "right": "$$",
            "display": false
          }]
        }
      }
    </script>
  </d-front-matter>

  <d-title style="padding-bottom: 0"><h1>Exploring Bayesian Optimization</h1>
    <p>Breaking Bayesian Optimization into small, sizeable chunks.</p>
  </d-title>

  <d-byline>
  <div class="byline grid">
    <div class="authors-affiliations grid">
      <h3>Authors</h3>
      <h3>Affiliations</h3>
      
        <p class="author">
          
            <a class="name" href="https://apoorvagnihotri.github.io/">Apoorv Agnihotri</a>
        </p>
        <p class="affiliation">
        <span class="affiliation">Indian Insitute of Technology Gandhinagar</span>
        </p>
      
        <p class="author">
          
            <a class="name" href="https://nipunbatra.github.io/">Nipun Batra</a>
        </p>
        <p class="affiliation">
        <span class="affiliation">Indian Insitute of Technology Gandhinagar</span>
        </p>
      
    </div>
    <div>
      <h3>Published</h3>
      
        <p>May 5, 2020</p> 
    </div>
    <div>
      <h3>DOI</h3>
      
        <p><a href="https://doi.org/10.23915/distill.00026">10.23915/distill.00026</a></p>
    </div>
  </div>
</d-byline>

  <d-article style="overflow-x: unset;">
    <p>
      Many modern machine learning algorithms have a large number of 
hyperparameters. To effectively use these algorithms, we need to pick 
good hyperparameter values.

      In this article, we talk about Bayesian Optimization, a suite of 
techniques often used to tune hyperparameters. More generally, Bayesian 
Optimization can be used to optimize any black-box function.
    </p>
    <p>

    </p>

    <!-- <p>
      We first study using Bayesian Optimization to maximize (optimize) a black-box function.
    </p> -->

    <h1>Mining Gold!</h1>
    <p>
      Let us start with the example of gold mining. Our goal is to mine for gold in an unknown land<d-footnote id="d-footnote-1">Interestingly, our example is similar to one of the first use of Gaussian Processes (also called kriging)<d-cite key="goldKridge"></d-cite>, where Prof. Krige modeled the gold concentrations using a Gaussian Process.</d-footnote>.
      For now, we assume that the gold is distributed about a line. We 
want to find the location along this line with the maximum gold while 
only drilling a few times (as drilling is expensive).
    </p>

    <p>
      Let us suppose that the gold distribution <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span></span> looks something like the function below. It is bi-modal, with a maximum value around <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">x = 5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">x</span><span class="mrel">=</span><span class="mord mathrm">5</span></span></span></span></span>. For now, let us not worry about the X-axis or the Y-axis units.
    </p>

    <figure class="smaller-img">
      <d-figure><img src="Exploring%20Bayesian%20Optimization_files/GT.svg"></d-figure>
    </figure>

    <p>
      Initially, we have no idea about the gold distribution. We can 
learn the gold distribution by drilling at different locations. However,
 this drilling is costly. Thus, we want to <!-- <strong> -->minimize the number of drillings required<!-- </strong> --> while still <!-- <strong> -->finding the location of maximum gold quickly<!-- </strong> -->.
    </p>

    <p>
      We now discuss two common objectives for the gold mining problem.
    </p>

    <ul>
      <li>
        <p>
          <strong>Problem 1: Best Estimate of Gold Distribution (Active Learning)</strong><br>
          In this problem, we want to accurately estimate the gold distribution on the new land.<!--  along the line, using a small number of drillings.  --> We can not drill at every location due to the prohibitive cost. Instead, we should drill at locations providing <strong>high information</strong> about the gold distribution. This problem is akin to
          <strong>
            Active Learning<d-cite key="settles2009active,Tong2001"></d-cite>
          </strong>.
        </p>
      </li>

      <li>
        <p>
          <strong>Problem 2: Location of Maximum Gold (Bayesian Optimization)</strong><br>
          In this problem, we want to find the location of the maximum 
gold content. We, again, can not drill at every location. Instead, we 
should drill at locations showing <strong>high promise</strong> about the gold content.<!-- , using a small number of drillings. --> This problem is akin to
          <strong>
            Bayesian Optimization<d-cite key="humanOut,nandoBOtut"></d-cite>
          </strong>.
        </p>
      </li>
    </ul>

    <p>
      We will soon see how these two problems are related, but not the same.
    </p>



    <h2>Active Learning</h2>

    <p>
      For many machine learning problems, unlabeled data is readily 
available. However, labeling (or querying) is often expensive. As an 
example, for a speech-to-text task, the annotation requires expert(s) to
 label words and sentences manually. Similarly, in our gold mining 
problem, drilling (akin to labeling) is expensive. 
    </p>

    <p>
      Active learning minimizes labeling costs while maximizing modeling
 accuracy. While there are various methods in active learning 
literature, we look at <strong>uncertainty reduction</strong>. This 
method proposes labeling the point whose model uncertainty is the 
highest. Often, the variance acts as a measure of uncertainty.
    </p>

    <p>
      Since we only know the true value of our function at a few points, we need a <em>surrogate model</em>
 for the values our function takes elsewhere. This surrogate should be 
flexible enough to model the true function. Using a Gaussian Process 
(GP) is a common choice, both because of its flexibility and its ability
 to give us uncertainty estimates
      <d-footnote id="d-footnote-2">
          Gaussian Process supports setting of priors by using specific 
kernels and mean functions. One might want to look at this excellent 
Distill article<d-cite key="görtler2019a"></d-cite> on Gaussian Processes<d-cite key="Rasmussen2004"></d-cite> to learn more.<br><br>
          Please find <a href="https://youtu.be/EnXxO3BAgYk">this</a> amazing video from Javier González on Gaussian Processes. 
      </d-footnote>.
    </p>

    <p>
      Our surrogate model starts with a prior of <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span></span> — in the case of gold, we pick a prior assuming that it’s smoothly distributed
      <d-footnote id="d-footnote-3">
        Specifics: We use a Matern 5/2 kernel due to its property of favoring doubly differentiable functions. <!-- In contrast, Matern 3/2 favors singly differentiable functions. --> See <a href="http://www.gaussianprocess.org/gpml/">Rasmussen and Williams 2004</a> and <a href="https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.Matern.html">scikit-learn</a>, for details regarding the Matern kernel.
      </d-footnote>.
      As we evaluate points (drilling), we get more data for our surrogate to learn from, updating it according to Bayes’ rule.
    </p>

    <figure class="l-page">
      <d-figure><img src="Exploring%20Bayesian%20Optimization_files/prior2posterior.png"></d-figure>
    <figcaption>
      Each new data point updates our surrogate model, moving it closer 
to the ground truth. The black line and the grey shaded region indicate 
the mean <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mi>μ</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">(\mu)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mopen">(</span><span class="mord mathit">μ</span><span class="mclose">)</span></span></span></span></span> and uncertainty <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mi>μ</mi><mo>±</mo><mi>σ</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">(\mu \pm \sigma)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mopen">(</span><span class="mord mathit">μ</span><span class="mbin">±</span><span class="mord mathit" style="margin-right:0.03588em;">σ</span><span class="mclose">)</span></span></span></span></span> in our gold distribution estimate before and after drilling.
    </figcaption>
    </figure>

    <p>
      In the above example, we started with uniform uncertainty. But after our first update, the posterior is certain near <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mo>=</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>5</mn></mrow><annotation encoding="application/x-tex">x = 0.5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">x</span><span class="mrel">=</span><span class="mord mathrm">0</span><span class="mord mathrm">.</span><span class="mord mathrm">5</span></span></span></span></span> and uncertain away from it. We could just keep adding more training points and obtain a more certain estimate of <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span></span>.
    </p>

    <p>
      However, we want to minimize the number of evaluations. Thus, we 
should choose the next query point “smartly” using active learning. 
Although there are many ways to pick smart points, we will be picking 
the most uncertain one.
    </p>

    <p>
      This gives us the following procedure for Active Learning:
    </p>
  <!-- 
    <h3>Surrogate Model</h3>
    <p>
      Active learning (along with Bayesian Optimization, which we see later) employs a surrogate model for modeling the unknown true function <d-math>f(x)</d-math>. The surrogate ideally models the true function closely. 
    </p>

    <h3>Bayesian Update</h3>
    <p>
      Every evaluation (drilling) of <d-math>f(x)</d-math> gives the surrogate model more data to learn. We can apply Bayes rule to obtain the surrogate posterior. At the end of an evaluation, the posterior becomes the prior for the next evaluation.
    </p>

    <p>
      Gaussian Processes are commonly used as surrogate models. One can set priors of a Gaussian Process (GP) by using specific kernels and mean functions. Moreover, GPs provide uncertainty estimates, which we leverage in both active learning and Bayesian Optimization.
    </p>

    <h3>Gaussian Processes</h3>

    <p>
      One might want to look at this excellent Distill article<d-cite key="görtler2019a"></d-cite> on Gaussian Processes<d-cite key="Rasmussen2004"></d-cite>.
      We use Gaussian Process regression to model the gold distribution.
    </p>

    <p>
      Let us visualize our true function <d-math>f(x)</d-math>. The gold distribution is bi-modal, with a maximum value around <d-math>x = 5</d-math>. For now, let us not worry about the X-axis or the Y-axis units.
    </p>
    <figure class="smaller-img">
      <d-figure><img src="images/MAB_gifs/GT.svg" /></d-figure>
    </figure>


    <h4 id="priormodel">Prior Model</h4>

    <p>
      We define a prior over a set of functions based on our initial beliefs of the black-box function<d-cite key="Rasmussen2004"></d-cite>. We consider the gold distribution to be smooth by setting the  prior appropriately. <d-footnote>We use a Matern 5/2 kernel due to its property of favoring doubly differentiable functions. In contrast, Matern 3/2 favors singly differentiable functions.

      See <a href="http://www.gaussianprocess.org/gpml/">Rasmussen and Williams 2004</a> and <a href="https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.Matern.html">scikit-learn</a>, for details regarding the Matern kernel.
    </d-footnote> The black line and the grey shaded region indicate the mean (<d-math>\mu</d-math>) and uncertainty  (<d-math>\mu \pm \sigma </d-math>) in our gold distribution estimate before drilling.

    </p>

    <figure>
      <d-figure><img src="images/MAB_gifs/prior.svg" /></d-figure>
    </figure>

    <h4 id="addingtrainingdata">Adding Training Data</h4>

    <p>
      Let us now add the point <d-math>< x = 0.5, \ y = f(0.5) ></d-math> to the training set.
    </p>

    <figure>
      <d-figure><img src="images/MAB_gifs/posterior.svg" /></d-figure>
    </figure>

    <p>
      We see our surrogate's posterior is certainty near <d-math>x = 0.5</d-math> and uncertain away from it. We can similarly add more training points and obtain a more certain estimate of <d-math>f(x)</d-math>. However, we want to minimize the number of evaluations. Thus, we should choose the next query point "smartly" using active learning. !-- <d-footnote>We can transform the data and fit a GP over <d-math>\log\left(f(x)\right)</d-math> instead of <d-math>f(x)</d-math> to ensure the  predictions are non-negative</d-footnote> .  --
    We now discuss the key idea of active learning.</p>
    <h3 id="activelearningprocedure">Active Learning Procedure
    </h3>
  -->

    <ol>
      <li>Choose and add the point with the highest uncertainty to the training set (by querying/labeling that point)</li>
      <li>Train on the new training set</li>
      <li>Go to #1 till convergence or budget elapsed</li>
    </ol>

    <p>
      Let us now visualize this process and see how our posterior changes at every iteration (after each drilling).
    </p>
    <figure class="gif-slider">
      <d-figure><img src="Exploring%20Bayesian%20Optimization_files/0_004.png"></d-figure>
    <div class="controls"><div><button onclick="changePng(this.parentNode.parentNode.parentNode, false)" class="stepper button-left"><svg width="10" height="10" viewBox="0 0 10 10" style="margin-top: 3px; margin-left: -1px;"><path d="M 10 0 L 0 5 L 10 10 z" fill="#888"></path></svg></button></div><div class="slidecontainer"><input oninput="changePng(this.parentNode.parentNode.parentNode, false, this.value);" type="range" min="0" max="9" value="0" class="slider"></div><div><button onclick="changePng(this.parentNode.parentNode.parentNode, true)" class="stepper button-right"><svg width="10" height="10" viewBox="0 0 10 10" style="margin-top: 3px;"><path d="M 0 0 L 10 5 L 0 10 z" fill="#888"></path></svg></button></div></div></figure>


    <p>The visualization shows that one can estimate the true 
distribution in a few iterations. Furthermore, the most uncertain 
positions are often the farthest points from the current evaluation 
points. At every iteration, active learning <strong>explores</strong> the domain to make the estimates better.
    </p>

    <h2 id="bayesianoptimization">Bayesian Optimization</h2>
    <p>
      In the previous section, we picked points in order to determine an
 accurate model of the gold content. But what if our goal is simply to 
find the location of maximum gold content? Of course, we could do active
 learning to estimate the  true function accurately and then find its 
maximum. But that seems pretty wasteful — why should we use evaluations 
improving our estimates of regions where the function expects low gold 
content when we only care about the maximum?
    </p>

    <p>
      This is the core question in Bayesian Optimization: “Based on what
 we know so far, which point should we evaluate next?” Remember that 
evaluating each point is expensive, so we want to pick carefully! In the
 active learning case, we picked the most uncertain point, exploring the
 function. But in Bayesian Optimization, we need to balance exploring 
uncertain regions, which might unexpectedly have high gold content, 
against focusing on regions we already know have higher gold content (a 
kind of exploitation).
    </p>

    <p>
      We make this decision with something called an acquisition 
function. Acquisition functions are heuristics for how desirable it is 
to evaluate a point, based on our present model<d-footnote id="d-footnote-4">More details on acquisition functions can be accessed at on this <a href="https://botorch.org/docs/acquisition">link</a>.</d-footnote>. We will spend much of this section going through different options for acquisition functions.
    </p>

    <p>
      This brings us to how Bayesian Optimization works. At every step, 
we determine what the best point to evaluate next is according to the 
acquisition function by optimizing it. We then update our model and 
repeat this process to determine the next point to evaluate.
    </p>

    <p>
      You may be wondering what’s “Bayesian” about Bayesian Optimization
 if we’re just optimizing these acquisition functions. Well, at every 
step we maintain a model describing our estimates and uncertainty at 
each point, which we update according to Bayes’ rule<d-cite key="settles2009active"></d-cite> at each step. Our acquisition functions are based on this model, and nothing would be possible without them!
    </p>
    
    <!-- 
      <p>
        In this problem, we aim to find the location of maximum gold content. One way to find the maximum would be first to run active learning to estimate the true function accurately, and then find its maximum. However, should we waste evaluations to improve the estimates, when we are only concerned with finding the maximum? Assuming that our black-box function is smooth, it might be a good idea to evaluate at or near locations where our surrogate model's prediction is the highest, i.e., to <strong>exploit</strong>. However, due to the limited evaluations, our model's predictions are inaccurate. One can improve the model by evaluating at points with high variance or performing <strong>exploration</strong>. Bayesian Optimization combines <strong>exploitation</strong> and <strong>exploration</strong>, whereas active learning solely <strong>explores</strong>.
      </p> 
    -->
    
    <h3>Formalizing Bayesian Optimization</h3>

      <span style="padding-bottom: 1em;">
        Let us now formally introduce Bayesian Optimization. Our goal is to find the location (<span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi>x</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mi>d</mi></msup></mrow></mrow><annotation encoding="application/x-tex">{x \in \mathbb{R}^d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.849108em;"></span><span class="strut bottom" style="height:0.888208em;vertical-align:-0.0391em;"></span><span class="base"><span class="mord"><span class="mord mathit">x</span><span class="mrel">∈</span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">d</span></span></span></span></span></span></span></span></span></span></span></span></span>) corresponding to the global maximum (or minimum) of a function <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>:</mo><msup><mi mathvariant="double-struck">R</mi><mi>d</mi></msup><mo>↦</mo><mrow><mi mathvariant="double-struck">R</mi></mrow></mrow><annotation encoding="application/x-tex">f: \mathbb{R}^d \mapsto \mathbb{R}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.849108em;"></span><span class="strut bottom" style="height:1.043548em;vertical-align:-0.19444em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mrel">:</span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">d</span></span></span></span></span></span></span></span><span class="mrel">↦</span><span class="mord"><span class="mord mathbb">R</span></span></span></span></span></span>.
        We present the general constraints in Bayesian Optimization and 
contrast them with the constraints in our gold mining example<d-footnote id="d-footnote-5">The section below is based on the slides/talk from Peter Fraizer at Uber on Bayesian Optimization:
          <ul class="footnote-ul">
            <li> <a href="https://www.youtube.com/watch?v=c4KKvyWW_Xk">Youtube talk</a>,</li>
            <li> <a href="https://people.orie.cornell.edu/pfrazier/Presentations/2018.11.INFORMS.tutorial.pdf">slide deck</a></li>
          </ul>
        </d-footnote>.</span>

      <table>
        <tbody><tr>
          <th><h3>General Constraints</h3></th>
          <th><h3>Constraints in Gold Mining example</h3></th>
        </tr>
        <tr>
          <td><span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.10764em;">f</span></span></span></span></span>’s feasible set <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">A</span></span></span></span></span> is simple,
          e.g., box constraints.</td>
          <td>Our domain in the gold mining problem is a single-dimensional box constraint: <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>0</mn><mo>≤</mo><mi>x</mi><mo>≤</mo><mn>6</mn></mrow><annotation encoding="application/x-tex">0 \leq x \leq 6</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.78041em;vertical-align:-0.13597em;"></span><span class="base"><span class="mord mathrm">0</span><span class="mrel">≤</span><span class="mord mathit">x</span><span class="mrel">≤</span><span class="mord mathrm">6</span></span></span></span></span>.</td>
        </tr>
        <tr>
          <td><span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.10764em;">f</span></span></span></span></span> is continuous but lacks special structure,
          e.g., concavity, that would make it easy to optimize.</td>
          <td>Our true function is neither a convex nor a concave function, resulting in local optimums.</td>
        </tr>
        <tr>
          <td><span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.10764em;">f</span></span></span></span></span> is derivative-free:
          evaluations do not give gradient information.</td>
          <td>Our evaluation (by drilling) of the amount of gold content at a location did not give us any gradient information.</td>
        </tr>
        <tr>
          <td><span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.10764em;">f</span></span></span></span></span> is expensive to evaluate:
          the number of times we can evaluate it
          is severely limited.</td>
          <td>Drilling is costly.</td>
        </tr>
        <tr>
          <td><span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.10764em;">f</span></span></span></span></span> may be noisy. If noise is present, we will assume it is independent and normally distributed, with common but unknown variance.</td>
          <td>We assume noiseless measurements in our modeling (though, 
it is easy to incorporate normally distributed noise for GP regression).</td>
        </tr>
      </tbody></table>

      <p>
        To solve this problem, we will follow the following algorithm:
      </p>

      <p>
      </p><ol>
        <li>
          We first choose a surrogate model for modeling the true function <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.10764em;">f</span></span></span></span></span> and define its <b>prior</b>.
        </li>
        <li>
          Given the set of <b>observations</b> (function evaluations), use Bayes rule to obtain the <b>posterior</b>.
        </li>
        <li>
          Use an acquisition function <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>α</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">\alpha(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.0037em;">α</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span></span>, which is a function of the posterior, to decide the next sample point <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub><mo>=</mo><msub><mtext>argmax</mtext><mi>x</mi></msub><mi>α</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">x_t = \text{argmax}_x \alpha(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mrel">=</span><span class="mord"><span class="mord text"><span class="mord mathrm">argmax</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.057252em;"><span style="top:-2.4558600000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.24414em;"></span></span></span></span></span><span class="mord mathit" style="margin-right:0.0037em;">α</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span></span>.
        </li>
        <li>
          Add newly sampled data to the set of <b>observations</b> and goto step #2 till convergence or budget elapses.
        </li>
      </ol>
    <p></p>

      <!-- <p>
        Our gold mining problem is suited to use Bayesian Optimization. Let us introduce some additional topics before we run to get the maximal gold!
      </p> -->

    <h3>Acquisition Functions</h3>

    <p>
      Acquisition functions are crucial to Bayesian Optimization, and there are a wide variety of options<d-footnote id="d-footnote-6">
        Please find <a href="https://www.cse.wustl.edu/~garnett/cse515t/spring_2015/files/lecture_notes/12.pdf">these</a> slides from Washington University in St. Louis to know more about acquisition functions.
      </d-footnote>. In the following sections, we will go through a number of options, providing intuition and examples.
    </p>

    <!-- <p>
      The core question is Bayesian Optimization is "Based on what we know so far, which point should we evaluate next?" Remember that evaluating each point is expensive, so we want to pick carefully! In the active learning case, we picked the most uncertain point, but in Bayesian Optimization we don't waste precious evaluations on uncertain regions that are not promising.
    </p>

    <p>
      Acquisition functions are heuristics<d-footnote>https://botorch.org/docs/acquisition</d-footnote> for how desirable it is to evaluate a point, based on our present model. At every step, we determine what the best point to evaluate next is according to the acquisition function. We then update our model, and repeat this process to determine the next point to evaluate. 
      <d-footnote>
        Please find <a href="https://www.cse.wustl.edu/~garnett/cse515t/spring_2015/files/lecture_notes/12.pdf">these</a> slides from Washington University in St. Louis to know more about acquisition functions.
      </d-footnote>.
    </p> -->

    <!-- 
      <p>
        Acquisition functions are crucial to Bayesian Optimization, and there are a wide variety of options. In the following sections, we’ll go through a number of options, providing intuition and examples.
      </p>

      <p>
        Our original optimization problem, <d-math>x^* = \text{argmax}_{x \in A} f(x)</d-math> is hard because <d-math>f</d-math> is <b>expensive</b> to evaluate.
        The idea of Bayesian Optimization is to <b>transform</b> the original optimization into a <b>sequence</b> of easier <b>inexpensive</b> optimizations of functions called <b>acquisition functions</b> (<d-math>\alpha(x)</d-math>).
        Intuitively, acquisition functions are heuristics<d-footnote>https://botorch.org/docs/acquisition</d-footnote> that evaluate the utility of a point for maximizing <d-math>f(x)</d-math><d-footnote>Please find <a href="https://www.cse.wustl.edu/~garnett/cse515t/spring_2015/files/lecture_notes/12.pdf">these</a> slides from Washington University in St. Louis to know more and the following </d-footnote>.
        At each step, we <strong>optimize</strong> the <strong>acquisition function</strong> to determine the next point to sample/query.
      </p>
    -->

    <!-- <p>
      Let us re-wind and link the things discussed thus far, by noting the steps of Bayesian Optimization<d-footnote>Please find <a href="https://youtu.be/EnXxO3BAgYk">this</a> amazing video from Javier González at The Gaussian Process Summer School 2019.</d-footnote> and explicitly highlighting the "Bayesian" in Bayesian Optimization.
    </p>
    <p>
      <ol>
        <li>
          We first choose a surrogate model for modeling the true function <d-math>f</d-math> and define its <b>prior</b>.
        </li>
        <li>
          Given the set of <b>observations</b> (function evaluations), use Bayes rule to obtain the <b>posterior</b>.
        </li>
        <li>
          Use an acquisition function <d-math>\alpha(x)</d-math>, which is a function of the posterior, to decide the next sample point <d-math>x_t = \text{argmax}_x \alpha(x)</d-math>.
        </li>
        <li>
          Add newly sampled data to the set of <b>observations</b> and goto step #2 till convergence or budget elapses.
        </li>
      </ol>
    </p>

    <p>
      Thus, the "Bayesian" in Bayesian Optimization is sequentially refining our surrogate's posterior (and thus uncertainty) with each evaluation via Bayes rule<d-cite key="nandoBOLoop"></d-cite>.
    </p>
    <p>Let us now look at a few common acquisition functions.</p> -->

    <h4>Probability of Improvement (PI)</h4>

    <p>
      This acquisition function chooses the next query point as the one which has the highest <em>probability of improvement</em> over the current max <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><msup><mi>x</mi><mo>+</mo></msup><mo>)</mo></mrow><annotation encoding="application/x-tex">f(x^+)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.771331em;"></span><span class="strut bottom" style="height:1.021331em;vertical-align:-0.25em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.771331em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">+</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>. Mathematically, we write the selection of next point as follows, 
    </p>
    <div class="desktop">
      <span><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo>(</mo><msub><mi>α</mi><mrow><mi>P</mi><mi>I</mi></mrow></msub><mo>(</mo><mi>x</mi><mo>)</mo><mo>)</mo><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo>(</mo><mi>P</mi><mo>(</mo><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>≥</mo><mo>(</mo><mi>f</mi><mo>(</mo><msup><mi>x</mi><mo>+</mo></msup><mo>)</mo><mo>+</mo><mi>ϵ</mi><mo>)</mo><mo>)</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">
        x_{t+1} = argmax(\alpha_{PI}(x)) = argmax(P(f(x) \geq (f(x^+) +\epsilon)))
      </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.821331em;"></span><span class="strut bottom" style="height:1.071331em;vertical-align:-0.25em;"></span><span class="base"><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">t</span><span class="mbin mtight">+</span><span class="mord mathrm mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"></span></span></span></span></span><span class="mrel">=</span><span class="mord mathit">a</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit" style="margin-right:0.03588em;">g</span><span class="mord mathit">m</span><span class="mord mathit">a</span><span class="mord mathit">x</span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight" style="margin-right:0.13889em;">P</span><span class="mord mathit mtight" style="margin-right:0.07847em;">I</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord mathit">a</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit" style="margin-right:0.03588em;">g</span><span class="mord mathit">m</span><span class="mord mathit">a</span><span class="mord mathit">x</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mrel">≥</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.821331em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">+</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mbin">+</span><span class="mord mathit">ϵ</span><span class="mclose">)</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span></span>
    </div>
    <div class="mobile_device_480px">
      <span><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mtable><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo>(</mo><msub><mi>α</mi><mrow><mi>P</mi><mi>I</mi></mrow></msub><mo>(</mo><mi>x</mi><mo>)</mo><mo>)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo>(</mo><mi>P</mi><mo>(</mo><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>≥</mo><mo>(</mo><mi>f</mi><mo>(</mo><msup><mi>x</mi><mo>+</mo></msup><mo>)</mo><mo>+</mo><mi>ϵ</mi><mo>)</mo><mo>)</mo><mo>)</mo></mrow></mstyle></mtd></mtr></mtable></mrow><annotation encoding="application/x-tex">
        \begin{aligned}
        x_{t+1} &amp; = argmax(\alpha_{PI}(x))\\
        &amp; = argmax(P(f(x) \geq (f(x^+) +\epsilon)))
        \end{aligned}
      </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.7500000000000002em;"></span><span class="strut bottom" style="height:3.0000000000000004em;vertical-align:-1.2500000000000002em;"></span><span class="base"><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.7500000000000002em;"><span style="top:-3.91em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">t</span><span class="mbin mtight">+</span><span class="mord mathrm mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"></span></span></span></span></span></span></span><span style="top:-2.41em;"><span class="pstrut" style="height:3em;"></span><span class="mord"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2500000000000002em;"></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.7500000000000002em;"><span style="top:-3.91em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mrel">=</span><span class="mord mathit">a</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit" style="margin-right:0.03588em;">g</span><span class="mord mathit">m</span><span class="mord mathit">a</span><span class="mord mathit">x</span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight" style="margin-right:0.13889em;">P</span><span class="mord mathit mtight" style="margin-right:0.07847em;">I</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mclose">)</span></span></span><span style="top:-2.41em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mrel">=</span><span class="mord mathit">a</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit" style="margin-right:0.03588em;">g</span><span class="mord mathit">m</span><span class="mord mathit">a</span><span class="mord mathit">x</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mrel">≥</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.821331em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">+</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mbin">+</span><span class="mord mathit">ϵ</span><span class="mclose">)</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2500000000000002em;"></span></span></span></span></span></span></span></span></span></span></span>
    </div>
    <p>
      where, 
      </p><ul style="list-style-type: none;">
        <li><span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><mo>⋅</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">P(\cdot)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord">⋅</span><span class="mclose">)</span></span></span></span></span> indicates probability</li>
        <li><span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">ϵ</span></span></span></span></span> is a small positive number</li>
        <li>And, <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>x</mi><mo>+</mo></msup><mo>=</mo><msub><mtext>argmax</mtext><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>∈</mo><msub><mi>x</mi><mrow><mn>1</mn><mo>:</mo><mi>t</mi></mrow></msub></mrow></msub><mi>f</mi><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex"> x^+ = \text{argmax}_{x_i \in x_{1:t}}f(x_i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.771331em;"></span><span class="strut bottom" style="height:1.115571em;vertical-align:-0.34424em;"></span><span class="base"><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.771331em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">+</span></span></span></span></span></span></span></span><span class="mrel">=</span><span class="mord"><span class="mord text"><span class="mord mathrm">argmax</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.13322999999999996em;"><span style="top:-2.45586em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathit mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathit mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"></span></span></span></span></span><span class="mrel mtight">∈</span><span class="mord mtight"><span class="mord mathit mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathrm mtight">1</span><span class="mrel mtight">:</span><span class="mord mathit mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.34424em;"></span></span></span></span></span><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> where <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base"><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span></span></span></span></span> is the location queried at <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>i</mi><mrow><mi>t</mi><mi>h</mi></mrow></msup></mrow><annotation encoding="application/x-tex">i^{th}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.849108em;"></span><span class="strut bottom" style="height:0.849108em;vertical-align:0em;"></span><span class="base"><span class="mord"><span class="mord mathit">i</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">t</span><span class="mord mathit mtight">h</span></span></span></span></span></span></span></span></span></span></span></span></span> time step.</li>
      </ul>
    <p></p>
    <p> 
      Looking closely, we are just finding the upper-tail probability 
(or the CDF) of the surrogate posterior. Moreover, if we are using a GP 
as a surrogate the expression above converts to,
    </p>
    <span><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>a</mi><msub><mi>x</mi><mi>x</mi></msub><mi mathvariant="normal">Φ</mi><mrow><mo fence="true">(</mo><mfrac><mrow><msub><mi>μ</mi><mi>t</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo><mo>−</mo><mi>f</mi><mo>(</mo><msup><mi>x</mi><mo>+</mo></msup><mo>)</mo><mo>−</mo><mi>ϵ</mi></mrow><mrow><msub><mi>σ</mi><mi>t</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">x_{t+1} = argmax_x \Phi\left(\frac{\mu_t(x) - f(x^+) - \epsilon}{\sigma_t(x)}\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.45em;"></span><span class="strut bottom" style="height:2.40003em;vertical-align:-0.95003em;"></span><span class="base"><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">t</span><span class="mbin mtight">+</span><span class="mord mathrm mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"></span></span></span></span></span><span class="mrel">=</span><span class="mord mathit">a</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit" style="margin-right:0.03588em;">g</span><span class="mord mathit">m</span><span class="mord mathit">a</span><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mord mathrm">Φ</span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.448331em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathit">μ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mbin">−</span><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.771331em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">+</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mbin">−</span><span class="mord mathit">ϵ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size3">)</span></span></span></span></span></span></span></span>
    <p>
      where, 
      </p><ul style="list-style-type: none;">
        <li><span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">Φ</mi><mo>(</mo><mo>⋅</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">\Phi(\cdot)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mord mathrm">Φ</span><span class="mopen">(</span><span class="mord">⋅</span><span class="mclose">)</span></span></span></span></span> indicates the CDF</li>
      </ul>
    <p></p>

    <p>
      The visualization below shows the calculation of <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>α</mi><mrow><mi>P</mi><mi>I</mi></mrow></msub><mo>(</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">\alpha_{PI}(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mord"><span class="mord mathit" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight" style="margin-right:0.13889em;">P</span><span class="mord mathit mtight" style="margin-right:0.07847em;">I</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span></span>. The orange line represents the current max (plus an <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex"> \epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">ϵ</span></span></span></span></span>) or <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><msup><mi>x</mi><mo>+</mo></msup><mo>)</mo><mo>+</mo><mi>ϵ</mi></mrow><annotation encoding="application/x-tex"> f(x^+) + \epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.771331em;"></span><span class="strut bottom" style="height:1.021331em;vertical-align:-0.25em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.771331em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">+</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mbin">+</span><span class="mord mathit">ϵ</span></span></span></span></span>.
 The violet region shows the probability density at each point. The grey
 regions show the probability density below the current max. The “area” 
of the violet region at each point represents the “probability of 
improvement over current maximum”. The next point to evaluate via the PI
 criteria (shown in dashed blue line) is <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mo>=</mo><mn>6</mn></mrow><annotation encoding="application/x-tex">x = 6</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">x</span><span class="mrel">=</span><span class="mord mathrm">6</span></span></span></span></span>.
    </p>

    <figure>
      <d-figure><img src="Exploring%20Bayesian%20Optimization_files/density_pi.png"></d-figure>
    </figure>

    <h5>Intuition behind <span style="text-transform: none"><span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">ϵ</span></span></span></span></span></span> in PI</h5>

    <p>
      PI uses <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">ϵ</span></span></span></span></span> to strike a balance between exploration and exploitation. <!-- In the following plot, we visualize how changing the value of <d-math>\epsilon</d-math> affects the values of our acquisition function, resulting in different choices of points to evaluate. We selected two points as our candidate points <d-math>x \in \{1.0, 5.0\}</d-math>, and show their tail probabilities in shaded green and orange, respectively. -->
      Increasing <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">ϵ</span></span></span></span></span> results in querying locations with a larger <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.03588em;">σ</span></span></span></span></span> as their probability density is spread.
    </p>

    <p>
      Let us now see the PI acquisition function in action. We start with <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi><mo>=</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>0</mn><mn>7</mn><mn>5</mn></mrow><annotation encoding="application/x-tex">\epsilon=0.075</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">ϵ</span><span class="mrel">=</span><span class="mord mathrm">0</span><span class="mord mathrm">.</span><span class="mord mathrm">0</span><span class="mord mathrm">7</span><span class="mord mathrm">5</span></span></span></span></span>.
    </p>

    <figure class="gif-slider">
      <d-figure><img src="Exploring%20Bayesian%20Optimization_files/0_009.png"></d-figure>
    <div class="controls"><div><button onclick="changePng(this.parentNode.parentNode.parentNode, false)" class="stepper button-left"><svg width="10" height="10" viewBox="0 0 10 10" style="margin-top: 3px; margin-left: -1px;"><path d="M 10 0 L 0 5 L 10 10 z" fill="#888"></path></svg></button></div><div class="slidecontainer"><input oninput="changePng(this.parentNode.parentNode.parentNode, false, this.value);" type="range" min="0" max="9" value="0" class="slider"></div><div><button onclick="changePng(this.parentNode.parentNode.parentNode, true)" class="stepper button-right"><svg width="10" height="10" viewBox="0 0 10 10" style="margin-top: 3px;"><path d="M 0 0 L 10 5 L 0 10 z" fill="#888"></path></svg></button></div></div></figure>
    <p>
      Looking at the graph above, we see that we reach the global maxima in a few iterations<d-footnote id="d-footnote-7">Ties are broken randomly.</d-footnote>.
      Our surrogate possesses a large uncertainty in <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mo>∈</mo><mo>[</mo><mn>2</mn><mo separator="true">,</mo><mn>4</mn><mo>]</mo></mrow><annotation encoding="application/x-tex">x \in [2, 4]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mord mathit">x</span><span class="mrel">∈</span><span class="mopen">[</span><span class="mord mathrm">2</span><span class="mpunct">,</span><span class="mord mathrm">4</span><span class="mclose">]</span></span></span></span></span> in the first few iterations<d-footnote id="d-footnote-8">The proportion of uncertainty is identified by the grey translucent area.</d-footnote>.
      The acquisition function initially <strong>exploits</strong> regions with a high promise<d-footnote id="d-footnote-9">Points in the vicinity of current maxima</d-footnote>, which leads to high uncertainty in the region <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mo>∈</mo><mo>[</mo><mn>2</mn><mo separator="true">,</mo><mn>4</mn><mo>]</mo></mrow><annotation encoding="application/x-tex">x \in [2, 4]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mord mathit">x</span><span class="mrel">∈</span><span class="mopen">[</span><span class="mord mathrm">2</span><span class="mpunct">,</span><span class="mord mathrm">4</span><span class="mclose">]</span></span></span></span></span>.
 This observation also shows that we do not need to construct an 
accurate estimate of the black-box function to find its maximum.
    </p>

    <figure class="gif-slider">
      <d-figure><img src="Exploring%20Bayesian%20Optimization_files/0_013.png"></d-figure>
    <div class="controls"><div><button onclick="changePng(this.parentNode.parentNode.parentNode, false)" class="stepper button-left"><svg width="10" height="10" viewBox="0 0 10 10" style="margin-top: 3px; margin-left: -1px;"><path d="M 10 0 L 0 5 L 10 10 z" fill="#888"></path></svg></button></div><div class="slidecontainer"><input oninput="changePng(this.parentNode.parentNode.parentNode, false, this.value);" type="range" min="0" max="9" value="0" class="slider"></div><div><button onclick="changePng(this.parentNode.parentNode.parentNode, true)" class="stepper button-right"><svg width="10" height="10" viewBox="0 0 10 10" style="margin-top: 3px;"><path d="M 0 0 L 10 5 L 0 10 z" fill="#888"></path></svg></button></div></div></figure>
    <p>
      The visualization above shows that increasing <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">ϵ</span></span></span></span></span> to 0.3, enables us to <strong>explore</strong> more. However, it seems that we are exploring more than required.
    </p>

    <p>
      What happens if we increase <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">ϵ</span></span></span></span></span> a bit more?
    </p>

    <figure class="gif-slider">
      <d-figure><img src="Exploring%20Bayesian%20Optimization_files/0_012.png"></d-figure>
    <div class="controls"><div><button onclick="changePng(this.parentNode.parentNode.parentNode, false)" class="stepper button-left"><svg width="10" height="10" viewBox="0 0 10 10" style="margin-top: 3px; margin-left: -1px;"><path d="M 10 0 L 0 5 L 10 10 z" fill="#888"></path></svg></button></div><div class="slidecontainer"><input oninput="changePng(this.parentNode.parentNode.parentNode, false, this.value);" type="range" min="0" max="9" value="0" class="slider"></div><div><button onclick="changePng(this.parentNode.parentNode.parentNode, true)" class="stepper button-right"><svg width="10" height="10" viewBox="0 0 10 10" style="margin-top: 3px;"><path d="M 0 0 L 10 5 L 0 10 z" fill="#888"></path></svg></button></div></div></figure>
    <p>
      We see that we made things worse! Our model now uses <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">\epsilon = 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">ϵ</span><span class="mrel">=</span><span class="mord mathrm">3</span></span></span></span></span>,
 and we are unable to exploit when we land near the global maximum. 
Moreover, with high exploration, the setting becomes similar to active 
learning.
    </p>

    <p> Our quick experiments above help us conclude that <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">ϵ</span></span></span></span></span> controls the degree of exploration in the PI acquisition function.

    </p><h4 id="expectedimprovementei">Expected Improvement (EI)</h4>

    <p>
      Probability of improvement only looked at <em>how likely</em> is an improvement, but, did not consider <em>how much</em> we can improve. The next criterion, called Expected Improvement (EI), does exactly that<d-footnote id="d-footnote-10">A good introduction to the Expected Improvement acquisition function is by <a href="https://thuijskens.github.io/2016/12/29/bayesian-optimisation/" target="_blank">this post</a> by Thomas Huijskens and <a href="https://people.orie.cornell.edu/pfrazier/Presentations/2018.11.INFORMS.tutorial.pdf" target="_blank">these slides</a> by Peter Frazier</d-footnote>!
      The idea is fairly simple — choose the next query point as the one
 which has the highest expected improvement over the current max <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><msup><mi>x</mi><mo>+</mo></msup><mo>)</mo></mrow><annotation encoding="application/x-tex">f(x^+)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.771331em;"></span><span class="strut bottom" style="height:1.021331em;vertical-align:-0.25em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.771331em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">+</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>, where <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>x</mi><mo>+</mo></msup><mo>=</mo><msub><mtext>argmax</mtext><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>∈</mo><msub><mi>x</mi><mrow><mn>1</mn><mo>:</mo><mi>t</mi></mrow></msub></mrow></msub><mi>f</mi><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex"> x^+ = \text{argmax}_{x_i \in x_{1:t}}f(x_i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.771331em;"></span><span class="strut bottom" style="height:1.115571em;vertical-align:-0.34424em;"></span><span class="base"><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.771331em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">+</span></span></span></span></span></span></span></span><span class="mrel">=</span><span class="mord"><span class="mord text"><span class="mord mathrm">argmax</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.13322999999999996em;"><span style="top:-2.45586em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathit mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathit mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"></span></span></span></span></span><span class="mrel mtight">∈</span><span class="mord mtight"><span class="mord mathit mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathrm mtight">1</span><span class="mrel mtight">:</span><span class="mord mathit mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.34424em;"></span></span></span></span></span><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> and <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base"><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span></span></span></span></span> is the location queried at <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>i</mi><mrow><mi>t</mi><mi>h</mi></mrow></msup></mrow><annotation encoding="application/x-tex">i^{th}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.849108em;"></span><span class="strut bottom" style="height:0.849108em;vertical-align:0em;"></span><span class="base"><span class="mord"><span class="mord mathit">i</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">t</span><span class="mord mathit mtight">h</span></span></span></span></span></span></span></span></span></span></span></span></span> time step.
    </p>

    <p>
      In this acquisition function, <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi><mo>+</mo><msup><mn>1</mn><mrow><mi>t</mi><mi>h</mi></mrow></msup></mrow><annotation encoding="application/x-tex">t + 1^{th}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.849108em;"></span><span class="strut bottom" style="height:0.932438em;vertical-align:-0.08333em;"></span><span class="base"><span class="mord mathit">t</span><span class="mbin">+</span><span class="mord"><span class="mord mathrm">1</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">t</span><span class="mord mathit mtight">h</span></span></span></span></span></span></span></span></span></span></span></span></span> query point, <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">x_{t+1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.638891em;vertical-align:-0.208331em;"></span><span class="base"><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">t</span><span class="mbin mtight">+</span><span class="mord mathrm mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"></span></span></span></span></span></span></span></span></span>, is selected according to the following equation.
    </p>
    <span><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>i</mi><msub><mi>n</mi><mi>x</mi></msub><mrow><mi mathvariant="double-struck">E</mi></mrow><mrow><mo fence="true">(</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><msub><mi>h</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>(</mo><mi>x</mi><mo>)</mo><mo>−</mo><mi>f</mi><mo>(</mo><msup><mi>x</mi><mo>⋆</mo></msup><mo>)</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mtext>&nbsp;</mtext><mi mathvariant="normal">∣</mi><mtext>&nbsp;</mtext><msub><mi mathvariant="script">D</mi><mi>t</mi></msub><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">
      x_{t+1} = argmin_x \mathbb{E} \left( ||h_{t+1}(x) - f(x^\star) || \ | \ \mathcal{D}_t \right)
    </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">t</span><span class="mbin mtight">+</span><span class="mord mathrm mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"></span></span></span></span></span><span class="mrel">=</span><span class="mord mathit">a</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit" style="margin-right:0.03588em;">g</span><span class="mord mathit">m</span><span class="mord mathit">i</span><span class="mord"><span class="mord mathit">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mord"><span class="mord mathbb">E</span></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord mathrm">∣</span><span class="mord mathrm">∣</span><span class="mord"><span class="mord mathit">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">t</span><span class="mbin mtight">+</span><span class="mord mathrm mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mbin">−</span><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.738696em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">⋆</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord mathrm">∣</span><span class="mord mathrm">∣</span><span class="mord mathrm"><span class="mspace">&nbsp;</span><span class="mord mathrm">∣</span></span><span class="mord"><span class="mspace">&nbsp;</span><span class="mord"><span class="mord mathcal" style="margin-right:0.02778em;">D</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span></span></span></span></span></span>
    <p>
      Where, <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.10764em;">f</span></span></span></span></span> is the actual ground truth function, <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">h_{t+1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.902771em;vertical-align:-0.208331em;"></span><span class="base"><span class="mord"><span class="mord mathit">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">t</span><span class="mbin mtight">+</span><span class="mord mathrm mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"></span></span></span></span></span></span></span></span></span> is the posterior mean of the surrogate at <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi><mo>+</mo><msup><mn>1</mn><mrow><mi>t</mi><mi>h</mi></mrow></msup></mrow><annotation encoding="application/x-tex">t+1^{th}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.849108em;"></span><span class="strut bottom" style="height:0.932438em;vertical-align:-0.08333em;"></span><span class="base"><span class="mord mathit">t</span><span class="mbin">+</span><span class="mord"><span class="mord mathrm">1</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">t</span><span class="mord mathit mtight">h</span></span></span></span></span></span></span></span></span></span></span></span></span> timestep, <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi mathvariant="script">D</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\mathcal{D}_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="base"><span class="mord"><span class="mord"><span class="mord mathcal" style="margin-right:0.02778em;">D</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span></span></span></span></span> is the training data <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>{</mo><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo separator="true">,</mo><mi>f</mi><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>)</mo><mo>)</mo><mo>}</mo><mtext>&nbsp;</mtext><mi mathvariant="normal">∀</mi><mi>x</mi><mo>∈</mo><msub><mi>x</mi><mrow><mn>1</mn><mo>:</mo><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\{(x_i,
        f(x_i))\} \ \forall x \in x_{1:t}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mopen">{</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mclose">)</span><span class="mclose">)</span><span class="mclose">}</span><span class="mord mathrm"><span class="mspace">&nbsp;</span><span class="mord mathrm">∀</span></span><span class="mord mathit">x</span><span class="mrel">∈</span><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">1</span><span class="mrel mtight">:</span><span class="mord mathit mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span></span></span></span></span> and <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>x</mi><mo>⋆</mo></msup></mrow><annotation encoding="application/x-tex">x^\star</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.688696em;"></span><span class="strut bottom" style="height:0.688696em;vertical-align:0em;"></span><span class="base"><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.688696em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">⋆</span></span></span></span></span></span></span></span></span></span></span></span> is the actual position where <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.10764em;">f</span></span></span></span></span> takes the maximum value.
    </p>

    <p>
      In essence, we are trying to select the point that minimizes the 
distance to the objective evaluated at the maximum. Unfortunately, we do
 not know the ground truth function, <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.10764em;">f</span></span></span></span></span>. Mockus<d-cite key="mockusEI"></d-cite> proposed
      the following acquisition function to overcome the issue.
    </p>

    <div class="desktop">
      <span><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>a</mi><msub><mi>x</mi><mi>x</mi></msub><mrow><mi mathvariant="double-struck">E</mi></mrow><mrow><mo fence="true">(</mo><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow><mo>{</mo><mn>0</mn><mo separator="true">,</mo><mtext>&nbsp;</mtext><msub><mi>h</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>(</mo><mi>x</mi><mo>)</mo><mo>−</mo><mi>f</mi><mo>(</mo><msup><mi>x</mi><mo>+</mo></msup><mo>)</mo><mo>}</mo><mtext>&nbsp;</mtext><mi mathvariant="normal">∣</mi><mtext>&nbsp;</mtext><msub><mi mathvariant="script">D</mi><mi>t</mi></msub><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">
      x_{t+1} = argmax_x \mathbb{E} \left( {max} \{ 0, \ h_{t+1}(x) - f(x^+) \} \ | \ \mathcal{D}_t \right)
      </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.85em;"></span><span class="strut bottom" style="height:1.20001em;vertical-align:-0.35001em;"></span><span class="base"><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">t</span><span class="mbin mtight">+</span><span class="mord mathrm mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"></span></span></span></span></span><span class="mrel">=</span><span class="mord mathit">a</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit" style="margin-right:0.03588em;">g</span><span class="mord mathit">m</span><span class="mord mathit">a</span><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mord"><span class="mord mathbb">E</span></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mord mathit">m</span><span class="mord mathit">a</span><span class="mord mathit">x</span></span><span class="mopen">{</span><span class="mord mathrm">0</span><span class="mpunct">,</span><span class="mord"><span class="mspace">&nbsp;</span><span class="mord mathit">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">t</span><span class="mbin mtight">+</span><span class="mord mathrm mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mbin">−</span><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.821331em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">+</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">}</span><span class="mord mathrm"><span class="mspace">&nbsp;</span><span class="mord mathrm">∣</span></span><span class="mord"><span class="mspace">&nbsp;</span><span class="mord"><span class="mord mathcal" style="margin-right:0.02778em;">D</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span></span></span></span></span></span>
    </div>
    <div class="mobile_device_480px">
      <span><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mtable><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mtext>&nbsp;</mtext></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>a</mi><msub><mi>x</mi><mi>x</mi></msub><mrow><mi mathvariant="double-struck">E</mi></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mrow><mo fence="true">(</mo><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow><mo>{</mo><mn>0</mn><mo separator="true">,</mo><mtext>&nbsp;</mtext><msub><mi>h</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>(</mo><mi>x</mi><mo>)</mo><mo>−</mo><mi>f</mi><mo>(</mo><msup><mi>x</mi><mo>+</mo></msup><mo>)</mo><mo>}</mo><mtext>&nbsp;</mtext><mi mathvariant="normal">∣</mi><mtext>&nbsp;</mtext><msub><mi mathvariant="script">D</mi><mi>t</mi></msub><mo fence="true">)</mo></mrow></mrow></mstyle></mtd></mtr></mtable></mrow><annotation encoding="application/x-tex">
        \begin{aligned}
          x_{t+1} = \ &amp; argmax_x \mathbb{E} \\
          &amp; \left( {max} \{ 0, \ h_{t+1}(x) - f(x^+) \} \ | \ \mathcal{D}_t \right)
        \end{aligned}
      </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.7550000000000001em;"></span><span class="strut bottom" style="height:3.0100000000000002em;vertical-align:-1.2550000000000001em;"></span><span class="base"><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.7550000000000001em;"><span style="top:-3.915em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">t</span><span class="mbin mtight">+</span><span class="mord mathrm mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"></span></span></span></span></span><span class="mrel">=</span><span class="mspace">&nbsp;</span></span></span><span style="top:-2.4050000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="mord"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2550000000000001em;"></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.7550000000000001em;"><span style="top:-3.915em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="mord mathit">a</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit" style="margin-right:0.03588em;">g</span><span class="mord mathit">m</span><span class="mord mathit">a</span><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mord"><span class="mord mathbb">E</span></span></span></span><span style="top:-2.4050000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mord mathit">m</span><span class="mord mathit">a</span><span class="mord mathit">x</span></span><span class="mopen">{</span><span class="mord mathrm">0</span><span class="mpunct">,</span><span class="mord"><span class="mspace">&nbsp;</span><span class="mord mathit">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">t</span><span class="mbin mtight">+</span><span class="mord mathrm mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mbin">−</span><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.821331em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">+</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">}</span><span class="mord mathrm"><span class="mspace">&nbsp;</span><span class="mord mathrm">∣</span></span><span class="mord"><span class="mspace">&nbsp;</span><span class="mord"><span class="mord mathcal" style="margin-right:0.02778em;">D</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2550000000000001em;"></span></span></span></span></span></span></span></span></span></span></span>
    </div>

    <p>
      where <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><msup><mi>x</mi><mo>+</mo></msup><mo>)</mo></mrow><annotation encoding="application/x-tex">f(x^+)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.771331em;"></span><span class="strut bottom" style="height:1.021331em;vertical-align:-0.25em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.771331em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">+</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> is the maximum value that has been encountered so far. This equation for GP surrogate is an analytical expression shown below.
    </p>

    <div class="desktop">
      <span><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>E</mi><mi>I</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mrow><mo fence="true">{</mo><mtable><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>(</mo><msub><mi>μ</mi><mi>t</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo><mo>−</mo><mi>f</mi><mo>(</mo><msup><mi>x</mi><mo>+</mo></msup><mo>)</mo><mo>−</mo><mi>ϵ</mi><mo>)</mo><mi mathvariant="normal">Φ</mi><mo>(</mo><mi>Z</mi><mo>)</mo><mo>+</mo><msub><mi>σ</mi><mi>t</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo><mi>ϕ</mi><mo>(</mo><mi>Z</mi><mo>)</mo><mo separator="true">,</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>if</mtext><mtext>&nbsp;</mtext><msub><mi>σ</mi><mi>t</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo><mo>&gt;</mo><mn>0</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>0</mn><mo separator="true">,</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>if</mtext><mtext>&nbsp;</mtext><msub><mi>σ</mi><mi>t</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mn>0</mn></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">
        EI(x)=
        \begin{cases}
        (\mu_t(x) - f(x^+) - \epsilon)\Phi(Z) + \sigma_t(x)\phi(Z), &amp; \text{if}\ \sigma_t(x) &gt; 0 \\
        0, &amp; \text{if}\ \sigma_t(x) = 0
        \end{cases}
      </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.75em;"></span><span class="strut bottom" style="height:3.0000299999999998em;vertical-align:-1.25003em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.05764em;">E</span><span class="mord mathit" style="margin-right:0.07847em;">I</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mrel">=</span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">{</span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.69em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mopen">(</span><span class="mord"><span class="mord mathit">μ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mbin">−</span><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.771331em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">+</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mbin">−</span><span class="mord mathit">ϵ</span><span class="mclose">)</span><span class="mord mathrm">Φ</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.07153em;">Z</span><span class="mclose">)</span><span class="mbin">+</span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mord mathit">ϕ</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.07153em;">Z</span><span class="mclose">)</span><span class="mpunct">,</span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathrm">0</span><span class="mpunct">,</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"></span></span></span></span><span class="arraycolsep" style="width:1em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.69em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord text"><span class="mord mathrm">if</span></span><span class="mord"><span class="mspace">&nbsp;</span><span class="mord mathit" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mrel">&gt;</span><span class="mord mathrm">0</span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord text"><span class="mord mathrm">if</span></span><span class="mord"><span class="mspace">&nbsp;</span><span class="mord mathit" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord mathrm">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></span>
    </div>
    <div class="mobile_device_480px">
      <span><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>E</mi><mi>I</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mrow><mo fence="true">{</mo><mtable><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>[</mo><mo>(</mo><msub><mi>μ</mi><mi>t</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo><mo>−</mo><mi>f</mi><mo>(</mo><msup><mi>x</mi><mo>+</mo></msup><mo>)</mo><mo>−</mo><mi>ϵ</mi><mo>)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>&nbsp;</mtext><msub><mi>σ</mi><mi>t</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo><mo>&gt;</mo><mn>0</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mspace width="1em"></mspace><mo>∗</mo><mi mathvariant="normal">Φ</mi><mo>(</mo><mi>Z</mi><mo>)</mo><mo>]</mo><mo>+</mo><msub><mi>σ</mi><mi>t</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo><mi>ϕ</mi><mo>(</mo><mi>Z</mi><mo>)</mo><mo separator="true">,</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>0</mn><mo separator="true">,</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>&nbsp;</mtext><msub><mi>σ</mi><mi>t</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mn>0</mn></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">
        EI(x)= \begin{cases}
          [(\mu_t(x) - f(x^+) - \epsilon) &amp; \ \sigma_t(x) &gt; 0 \\
          \quad * \Phi(Z)] + \sigma_t(x)\phi(Z),\\
          0, &amp; \ \sigma_t(x) = 0
        \end{cases}
      </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:2.41em;"></span><span class="strut bottom" style="height:4.32em;vertical-align:-1.9100000000000001em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.05764em;">E</span><span class="mord mathit" style="margin-right:0.07847em;">I</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mrel">=</span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.35002em;"><span style="top:-2.19999em;"><span class="pstrut" style="height:3.15em;"></span><span class="delimsizinginner delim-size4"><span>⎩</span></span></span><span style="top:-2.19999em;"><span class="pstrut" style="height:3.15em;"></span><span class="delimsizinginner delim-size4"><span>⎪</span></span></span><span style="top:-3.1500100000000004em;"><span class="pstrut" style="height:3.15em;"></span><span class="delimsizinginner delim-size4"><span>⎨</span></span></span><span style="top:-4.30001em;"><span class="pstrut" style="height:3.15em;"></span><span class="delimsizinginner delim-size4"><span>⎪</span></span></span><span style="top:-4.60002em;"><span class="pstrut" style="height:3.15em;"></span><span class="delimsizinginner delim-size4"><span>⎧</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.8500199999999998em;"></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.41em;"><span style="top:-4.41em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mopen">[</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">μ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mbin">−</span><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.771331em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">+</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mbin">−</span><span class="mord mathit">ϵ</span><span class="mclose">)</span></span></span><span style="top:-2.97em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord"><span class="mspace quad"></span><span class="mbin">∗</span></span><span class="mord mathrm">Φ</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.07153em;">Z</span><span class="mclose">)</span><span class="mclose">]</span><span class="mbin">+</span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mord mathit">ϕ</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.07153em;">Z</span><span class="mclose">)</span><span class="mpunct">,</span></span></span><span style="top:-1.5300000000000002em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathrm">0</span><span class="mpunct">,</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.9099999999999997em;"></span></span></span></span><span class="arraycolsep" style="width:1em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.41em;"><span style="top:-4.41em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord"><span class="mspace">&nbsp;</span><span class="mord mathit" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mrel">&gt;</span><span class="mord mathrm">0</span></span></span><span style="top:-1.5299999999999998em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord"><span class="mspace">&nbsp;</span><span class="mord mathit" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord mathrm">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.9100000000000001em;"></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></span>
    </div>
    <span><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Z</mi><mo>=</mo><mfrac><mrow><msub><mi>μ</mi><mi>t</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo><mo>−</mo><mi>f</mi><mo>(</mo><msup><mi>x</mi><mo>+</mo></msup><mo>)</mo><mo>−</mo><mi>ϵ</mi></mrow><mrow><msub><mi>σ</mi><mi>t</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">Z= \frac{\mu_t(x) - f(x^+) - \epsilon}{\sigma_t(x)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.448331em;"></span><span class="strut bottom" style="height:2.384331em;vertical-align:-0.936em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.07153em;">Z</span><span class="mrel">=</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.448331em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathit">μ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mbin">−</span><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.771331em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">+</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mbin">−</span><span class="mord mathit">ϵ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></span>
    <p>
      where <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">Φ</mi><mo>(</mo><mo>⋅</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">\Phi(\cdot)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mord mathrm">Φ</span><span class="mopen">(</span><span class="mord">⋅</span><span class="mclose">)</span></span></span></span></span> indicates CDF and <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϕ</mi><mo>(</mo><mo>⋅</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">\phi(\cdot)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mord mathit">ϕ</span><span class="mopen">(</span><span class="mord">⋅</span><span class="mclose">)</span></span></span></span></span> indicates pdf.
    </p>
    <p>From the above expression, we can see that <em>Expected Improvement</em> will be high when: i) the expected value of <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>μ</mi><mi>t</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo><mo>−</mo><mi>f</mi><mo>(</mo><msup><mi>x</mi><mo>+</mo></msup><mo>)</mo></mrow><annotation encoding="application/x-tex">\mu_t(x) - f(x^+)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.771331em;"></span><span class="strut bottom" style="height:1.021331em;vertical-align:-0.25em;"></span><span class="base"><span class="mord"><span class="mord mathit">μ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mbin">−</span><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.771331em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">+</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> is high, or, ii) when the uncertainty <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>σ</mi><mi>t</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">\sigma_t(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span></span> around a point is high.


    </p><p> Like the PI acquisition function, we can moderate the amount of exploration of the EI acquisition function by modifying <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">ϵ</span></span></span></span></span>.
    </p>

    <figure class="gif-slider">
      <d-figure><img src="Exploring%20Bayesian%20Optimization_files/0_002.png"></d-figure>
    <div class="controls"><div><button onclick="changePng(this.parentNode.parentNode.parentNode, false)" class="stepper button-left"><svg width="10" height="10" viewBox="0 0 10 10" style="margin-top: 3px; margin-left: -1px;"><path d="M 10 0 L 0 5 L 10 10 z" fill="#888"></path></svg></button></div><div class="slidecontainer"><input oninput="changePng(this.parentNode.parentNode.parentNode, false, this.value);" type="range" min="0" max="9" value="0" class="slider"></div><div><button onclick="changePng(this.parentNode.parentNode.parentNode, true)" class="stepper button-right"><svg width="10" height="10" viewBox="0 0 10 10" style="margin-top: 3px;"><path d="M 0 0 L 10 5 L 0 10 z" fill="#888"></path></svg></button></div></div></figure>

    <p>
      For <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi><mo>=</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>0</mn><mn>1</mn></mrow><annotation encoding="application/x-tex">\epsilon = 0.01</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">ϵ</span><span class="mrel">=</span><span class="mord mathrm">0</span><span class="mord mathrm">.</span><span class="mord mathrm">0</span><span class="mord mathrm">1</span></span></span></span></span> we come close to the global maxima in a few iterations.  </p>

    <p>
      We now increase <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">ϵ</span></span></span></span></span> to explore more.
    </p>

    <figure class="gif-slider">
      <d-figure><img src="Exploring%20Bayesian%20Optimization_files/0_010.png"></d-figure>
    <div class="controls"><div><button onclick="changePng(this.parentNode.parentNode.parentNode, false)" class="stepper button-left"><svg width="10" height="10" viewBox="0 0 10 10" style="margin-top: 3px; margin-left: -1px;"><path d="M 10 0 L 0 5 L 10 10 z" fill="#888"></path></svg></button></div><div class="slidecontainer"><input oninput="changePng(this.parentNode.parentNode.parentNode, false, this.value);" type="range" min="0" max="9" value="0" class="slider"></div><div><button onclick="changePng(this.parentNode.parentNode.parentNode, true)" class="stepper button-right"><svg width="10" height="10" viewBox="0 0 10 10" style="margin-top: 3px;"><path d="M 0 0 L 10 5 L 0 10 z" fill="#888"></path></svg></button></div></div></figure>
    <p>
      As we expected, increasing the value to <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi><mo>=</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>3</mn></mrow><annotation encoding="application/x-tex">\epsilon = 0.3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">ϵ</span><span class="mrel">=</span><span class="mord mathrm">0</span><span class="mord mathrm">.</span><span class="mord mathrm">3</span></span></span></span></span>
 makes the acquisition function explore more. Compared to the earlier 
evaluations, we see less exploitation. We see that it evaluates only two
 points near the global maxima.
    </p>

    <p>
      Let us increase <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">ϵ</span></span></span></span></span> even more.
    </p>

    <figure class="gif-slider">
      <d-figure><img src="Exploring%20Bayesian%20Optimization_files/0_005.png"></d-figure>
    <div class="controls"><div><button onclick="changePng(this.parentNode.parentNode.parentNode, false)" class="stepper button-left"><svg width="10" height="10" viewBox="0 0 10 10" style="margin-top: 3px; margin-left: -1px;"><path d="M 10 0 L 0 5 L 10 10 z" fill="#888"></path></svg></button></div><div class="slidecontainer"><input oninput="changePng(this.parentNode.parentNode.parentNode, false, this.value);" type="range" min="0" max="9" value="0" class="slider"></div><div><button onclick="changePng(this.parentNode.parentNode.parentNode, true)" class="stepper button-right"><svg width="10" height="10" viewBox="0 0 10 10" style="margin-top: 3px;"><path d="M 0 0 L 10 5 L 0 10 z" fill="#888"></path></svg></button></div></div></figure>
    <p>
      Is this better than before? It turns out a yes and a no; we explored too much at <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">\epsilon = 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">ϵ</span><span class="mrel">=</span><span class="mord mathrm">3</span></span></span></span></span> and quickly reached near the global maxima. But unfortunately, we did not exploit to get more gains near the global maxima.
    </p>

    <div class="collapsible"><h4>PI vs Ei</h4><div class="collapsible-indicator"></div></div>
    <div class="content">
      <p>
        We have seen two closely related methods, The <em>Probability of Improvement</em> and the <em>Expected Improvement</em>.
      </p>

      <figure class="smaller-img">
        <d-figure><img src="Exploring%20Bayesian%20Optimization_files/0.svg"></d-figure>
      </figure>
      <p>
        The scatter plot above shows the policies’ acquisition functions evaluated on different points<d-footnote id="d-footnote-11">Each
 dot is a point in the search space. Additionally, the training set used
 while making the plot only consists of a single observation <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>5</mn><mo separator="true">,</mo><mi>f</mi><mo>(</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>5</mn><mo>)</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">(0.5, f(0.5))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mopen">(</span><span class="mord mathrm">0</span><span class="mord mathrm">.</span><span class="mord mathrm">5</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathrm">0</span><span class="mord mathrm">.</span><span class="mord mathrm">5</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span></d-footnote>.
        We see that <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>α</mi><mrow><mi>E</mi><mi>I</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\alpha_{EI}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base"><span class="mord"><span class="mord mathit" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight" style="margin-right:0.05764em;">E</span><span class="mord mathit mtight" style="margin-right:0.07847em;">I</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span></span></span></span></span> and <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>α</mi><mrow><mi>P</mi><mi>I</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\alpha_{PI}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base"><span class="mord"><span class="mord mathit" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight" style="margin-right:0.13889em;">P</span><span class="mord mathit mtight" style="margin-right:0.07847em;">I</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span></span></span></span></span> reach a maximum of 0.3 and around 0.47, respectively. Choosing a point with low <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>α</mi><mrow><mi>P</mi><mi>I</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\alpha_{PI}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base"><span class="mord"><span class="mord mathit" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight" style="margin-right:0.13889em;">P</span><span class="mord mathit mtight" style="margin-right:0.07847em;">I</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span></span></span></span></span> and high <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>α</mi><mrow><mi>E</mi><mi>I</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\alpha_{EI}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base"><span class="mord"><span class="mord mathit" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight" style="margin-right:0.05764em;">E</span><span class="mord mathit mtight" style="margin-right:0.07847em;">I</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span></span></span></span></span> translates to high risk<d-footnote id="d-footnote-12">Since “Probability of Improvement” is low</d-footnote> and high reward<d-footnote id="d-footnote-13">Since “Expected Improvement” is high</d-footnote>.
        In case of multiple points having the same <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>α</mi><mrow><mi>E</mi><mi>I</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\alpha_{EI}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base"><span class="mord"><span class="mord mathit" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight" style="margin-right:0.05764em;">E</span><span class="mord mathit mtight" style="margin-right:0.07847em;">I</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span></span></span></span></span>, we should prioritize the point with lesser risk (higher <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>α</mi><mrow><mi>P</mi><mi>I</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\alpha_{PI}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base"><span class="mord"><span class="mord mathit" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight" style="margin-right:0.13889em;">P</span><span class="mord mathit mtight" style="margin-right:0.07847em;">I</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span></span></span></span></span>). Similarly, when the risk is same (same <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>α</mi><mrow><mi>P</mi><mi>I</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\alpha_{PI}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base"><span class="mord"><span class="mord mathit" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight" style="margin-right:0.13889em;">P</span><span class="mord mathit mtight" style="margin-right:0.07847em;">I</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span></span></span></span></span>), we should choose the point with greater reward (higher <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>α</mi><mrow><mi>E</mi><mi>I</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\alpha_{EI}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base"><span class="mord"><span class="mord mathit" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight" style="margin-right:0.05764em;">E</span><span class="mord mathit mtight" style="margin-right:0.07847em;">I</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span></span></span></span></span>).
      </p>
    </div>

    <h3 id="thompsonsampling">Thompson Sampling</h3>

    <p>
      Another common acquisition function is Thompson Sampling <d-cite key="thompson"></d-cite>.
 At every step, we sample a function from the surrogate’s posterior and 
optimize it. For example, in the case of gold mining, we would sample a 
plausible distribution of the gold given the evidence and evaluate 
(drill) wherever it peaks.
    </p>

    <p>
      Below we have an image showing three sampled functions from the 
learned surrogate posterior for our gold mining problem. The training 
data constituted the point <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mo>=</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>5</mn></mrow><annotation encoding="application/x-tex">x = 0.5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">x</span><span class="mrel">=</span><span class="mord mathrm">0</span><span class="mord mathrm">.</span><span class="mord mathrm">5</span></span></span></span></span> and the corresponding functional value.
    </p>

    <figure>
      <d-figure>
        <img src="Exploring%20Bayesian%20Optimization_files/thompson.svg">
      </d-figure>
    </figure>

    <p>
      We can understand the intuition behind Thompson sampling by two observations:
      </p><ul>
        <li>
          <p>
            Locations with high uncertainty (<span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>σ</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex"> \sigma(x) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span></span>)
 will show a large variance in the functional values sampled from the 
surrogate posterior. Thus, there is a non-trivial probability that a 
sample can take high value in a highly uncertain region. Optimizing such
 samples can aid <strong>exploration</strong>.
          </p>
          <p>
            As an example, the three samples (sample #1, #2, #3) show a high variance close to <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mo>=</mo><mn>6</mn></mrow><annotation encoding="application/x-tex">x=6</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">x</span><span class="mrel">=</span><span class="mord mathrm">6</span></span></span></span></span>. Optimizing sample 3 will aid in exploration by evaluating <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mo>=</mo><mn>6</mn></mrow><annotation encoding="application/x-tex">x=6</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">x</span><span class="mrel">=</span><span class="mord mathrm">6</span></span></span></span></span>.
          </p>

        </li>
        <li>
          <p>
            The sampled functions must pass through the current max 
value, as there is no uncertainty at the evaluated locations. Thus, 
optimizing samples from the surrogate posterior will ensure <strong>exploiting</strong> behavior.
          </p>

          <p>
            As an example of this behavior, we see that all the sampled functions above pass through the current max at <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mo>=</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>5</mn></mrow><annotation encoding="application/x-tex">x = 0.5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">x</span><span class="mrel">=</span><span class="mord mathrm">0</span><span class="mord mathrm">.</span><span class="mord mathrm">5</span></span></span></span></span>. If <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi><mo>=</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>5</mn></mrow><annotation encoding="application/x-tex">x = 0.5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.64444em;"></span><span class="strut bottom" style="height:0.64444em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">x</span><span class="mrel">=</span><span class="mord mathrm">0</span><span class="mord mathrm">.</span><span class="mord mathrm">5</span></span></span></span></span> were close to the global maxima, then we would be able to <strong>exploit</strong> and choose a better maximum.
          </p>

        </li>
      </ul>



      <figure class="gif-slider">
        <d-figure><img src="Exploring%20Bayesian%20Optimization_files/0.png"></d-figure>
      <div class="controls"><div><button onclick="changePng(this.parentNode.parentNode.parentNode, false)" class="stepper button-left"><svg width="10" height="10" viewBox="0 0 10 10" style="margin-top: 3px; margin-left: -1px;"><path d="M 10 0 L 0 5 L 10 10 z" fill="#888"></path></svg></button></div><div class="slidecontainer"><input oninput="changePng(this.parentNode.parentNode.parentNode, false, this.value);" type="range" min="0" max="9" value="0" class="slider"></div><div><button onclick="changePng(this.parentNode.parentNode.parentNode, true)" class="stepper button-right"><svg width="10" height="10" viewBox="0 0 10 10" style="margin-top: 3px;"><path d="M 0 0 L 10 5 L 0 10 z" fill="#888"></path></svg></button></div></div></figure>

      <p>
The visualization above uses Thompson sampling for optimization. Again, 
we can reach the global optimum in relatively few iterations.
      </p>

      <h3>Random</h3>

      <p>
        We have been using intelligent acquisition functions until now.
        We can create a random acquisition function by sampling <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">x</span></span></span></span></span>
        randomly.      </p>

      <figure class="gif-slider">
        <d-figure><img src="Exploring%20Bayesian%20Optimization_files/0_014.png"></d-figure>
      <div class="controls"><div><button onclick="changePng(this.parentNode.parentNode.parentNode, false)" class="stepper button-left"><svg width="10" height="10" viewBox="0 0 10 10" style="margin-top: 3px; margin-left: -1px;"><path d="M 10 0 L 0 5 L 10 10 z" fill="#888"></path></svg></button></div><div class="slidecontainer"><input oninput="changePng(this.parentNode.parentNode.parentNode, false, this.value);" type="range" min="0" max="9" value="0" class="slider"></div><div><button onclick="changePng(this.parentNode.parentNode.parentNode, true)" class="stepper button-right"><svg width="10" height="10" viewBox="0 0 10 10" style="margin-top: 3px;"><path d="M 0 0 L 10 5 L 0 10 z" fill="#888"></path></svg></button></div></div></figure>
	<p> The visualization above shows that the performance of the random 
acquisition function is not that bad! However, if our optimization was 
more complex (more dimensions), then the random acquisition might 
perform poorly.
</p>
      <h3>Summary of Acquisition Functions</h3> <p>
        Let us now summarize the core ideas associated with acquisition 
functions: i) they are heuristics for evaluating the utility of a point;
 ii) they are a function of the surrogate posterior; iii) they combine 
exploration and exploitation; and iv) they are inexpensive to evaluate.</p>

      <div class="collapsible"><h4>Other Acquisition Functions</h4> <div class="collapsible-indicator"></div></div>
      <div class="content">

        <p>We have seen various acquisition functions until now. One 
trivial way to come up with acquisition functions is to have a 
explore/exploit combination.
        </p><p>

        </p><h3> Upper Confidence Bound (UCB) </h3>
        <p>
          One such trivial acquisition function that combines the 
exploration/exploitation tradeoff is a linear combination of the mean 
and uncertainty of our surrogate model. The model mean signifies 
exploitation (of our model’s knowledge) and model uncertainty signifies 
exploration (due to our model’s lack of observations).
          <span><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>α</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mi>μ</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>+</mo><mi>λ</mi><mo>×</mo><mi>σ</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">\alpha(x) = \mu(x) + \lambda \times \sigma(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.0037em;">α</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord mathit">μ</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mbin">+</span><span class="mord mathit">λ</span><span class="mbin">×</span><span class="mord mathit" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span></span></span>
        </p>

        <p>
          The intuition behind the UCB acquisition function is weighing 
of the  importance between the surrogate’s mean  vs. the surrogate’s 
uncertainty. The <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">λ</span></span></span></span></span> above is the hyperparameter that can control the preference between exploitation or exploration.
        </p>

        <p>
          We can further form acquisition functions by combining the 
existing acquisition functions though the physical interpretability of 
such combinations might not be so straightforward. One reason we might 
want to combine two methods is to overcome the limitations of the 
individual methods.
        </p>

        <h3>Probability of Improvement + <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>λ</mi><mtext>&nbsp;</mtext><mo>×</mo></mrow><annotation encoding="application/x-tex">\lambda \ \times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="base"><span class="mord mathit">λ</span><span class="mord"><span class="mspace">&nbsp;</span><span class="mbin">×</span></span></span></span></span></span> Expected Improvement (EI-PI)</h3>

        <p>
          One such combination can be a linear combination of PI and EI.

          We know PI focuses on the probability of improvement, whereas 
EI focuses on the expected improvement. Such a combination could help in
 having a tradeoff between the two based on the value of <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">λ</span></span></span></span></span>.
        </p>

        <h3>Gaussian Process Upper Confidence Bound (GP-UCB)</h3>

        <p>
          Before talking about GP-UCB, let us quickly talk about <strong>regret</strong>. Imagine if the maximum gold was <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">a</span></span></span></span></span> units, and our optimization instead samples a location containing <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>b</mi><mo>&lt;</mo><mi>a</mi></mrow><annotation encoding="application/x-tex">b &lt; a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.73354em;vertical-align:-0.0391em;"></span><span class="base"><span class="mord mathit">b</span><span class="mrel">&lt;</span><span class="mord mathit">a</span></span></span></span></span> units, then our regret is
              <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi><mo>−</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">a -
              b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="base"><span class="mord mathit">a</span><span class="mbin">−</span><span class="mord mathit">b</span></span></span></span></span>. If we accumulate the regret over <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">n</span></span></span></span></span> iterations, we get what is called <strong> cumulative regret. </strong> <br>

          GP-UCB’s<d-cite key="gpucb"></d-cite> formulation is given by:
        </p>

        <span><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>α</mi><mrow><mi>G</mi><mi>P</mi><mo>−</mo><mi>U</mi><mi>C</mi><mi>B</mi></mrow></msub><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><msub><mi>μ</mi><mi>t</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo><mo>+</mo><msqrt><mrow><msub><mi>β</mi><mi>t</mi></msub></mrow></msqrt><msub><mi>σ</mi><mi>t</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">
          \alpha_{GP-UCB}(x) = \mu_t(x) + \sqrt{\beta_t}\sigma_t(x)
        </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.983875em;"></span><span class="strut bottom" style="height:1.24em;vertical-align:-0.25612499999999994em;"></span><span class="base"><span class="mord"><span class="mord mathit" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.328331em;"><span style="top:-2.5500000000000003em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">G</span><span class="mord mathit mtight" style="margin-right:0.13889em;">P</span><span class="mbin mtight">−</span><span class="mord mathit mtight" style="margin-right:0.10903em;">U</span><span class="mord mathit mtight" style="margin-right:0.07153em;">C</span><span class="mord mathit mtight" style="margin-right:0.05017em;">B</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord"><span class="mord mathit">μ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mbin">+</span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist svg-align" style="height:0.983875em;"><span style="top:-3.2em;"><span class="pstrut" style="height:3.2em;"></span><span class="mord" style="padding-left:1em;"><span class="mord"><span class="mord mathit" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.05278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span></span></span><span style="top:-2.9438750000000002em;"><span class="pstrut" style="height:3.2em;"></span><span style="height:1.2em;"><svg width="100%" height="1.2em">
            <svg viewBox="0 0 400000 1200" preserveAspectRatio="xMinYMin
slice"><path d="M263 601c.667 0 18 39.667 52 119s68.167
 158.667 102.5 238 51.833 119.333 52.5 120C810 373.333 980.667 17.667 982 11
c4.667-7.333 11-11 19-11h398999v40H1012.333L741 607c-38.667 80.667-84 175-136
 283s-89.167 185.333-111.5 232-33.833 70.333-34.5 71c-4.667 4.667-12.333 7-23
 7l-12-1-109-253c-72.667-168-109.333-252-110-252-10.667 8-22 16.667-34 26-22
 17.333-33.333 26-34 26l-26-26 76-59 76-60zM1001 0h398999v40H1012z"></path></svg></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.25612499999999994em;"></span></span></span></span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span></span></span>
        <p>
          Where <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.61508em;"></span><span class="strut bottom" style="height:0.61508em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">t</span></span></span></span></span> is the timestep.
        </p>

        <p>
          Srinivas et. al.<d-cite key="gpucbBounds"></d-cite> developed a schedule for <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.05278em;">β</span></span></span></span></span> that they theoretically demonstrate to minimize cumulative regret.
        </p>

      </div>
      
      <h3>Comparison</h3>

      <p>
        We now compare the performance of different acquisition functions on the gold mining problem<d-footnote id="d-footnote-14">To know more about the difference between acquisition functions look at <a href="https://www.cs.ubc.ca/~nando/540-2013/lectures/l7.pdf">these</a> amazing
          slides from Nando De Freitas</d-footnote>. We have used the 
optimum hyperparameters for each acquisition function.

          We ran the random acquisition function several times with 
different seeds and plotted the mean gold sensed at every iteration.
      </p>

      <figure>
        <d-figure><img src="Exploring%20Bayesian%20Optimization_files/comp.svg"></d-figure>
      </figure>

      <p>
        The <em>random</em> strategy is initially comparable to or better than other acquisition functions<d-footnote id="d-footnote-15">UCB and GP-UCB have been mentioned in the collapsible</d-footnote>. However, the maximum gold sensed by <em>random</em>
 strategy grows slowly. In comparison, the other acquisition functions 
can find a good solution in a small number of iterations. In fact, most 
acquisition functions reach fairly close to the global maxima in as few 
as three iterations.
      </p>

      <h2>Hyperparameter Tuning</h2>

      <p>Before we talk about Bayesian optimization for hyperparameter tuning<d-cite key="Snoek2012,NIPS2011_4443,Bergstra"></d-cite>,
 we will quickly differentiate between hyperparameters and parameters: 
hyperparameters are set before learning and the parameters are learned 
from the data. To illustrate the difference, we take the example of 
Ridge regression.
      </p>
      <div class="desktop">
      <span><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mover accent="true"><mrow><mi>θ</mi></mrow><mo>^</mo></mover><mrow><mi>r</mi><mi>i</mi><mi>d</mi><mi>g</mi><mi>e</mi></mrow></msub><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>i</mi><msub><mi>n</mi><mrow><mi>θ</mi><mtext>&nbsp;</mtext><mo>∈</mo><mtext>&nbsp;</mtext><msup><mi mathvariant="double-struck">R</mi><mi>p</mi></msup></mrow></msub><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mrow><mo fence="true">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msubsup><mi>x</mi><mi>i</mi><mi>T</mi></msubsup><mi>θ</mi><mo fence="true">)</mo></mrow><mn>2</mn></msup><mo>+</mo><mi>λ</mi><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><msubsup><mi>θ</mi><mi>j</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">
        \hat{\theta}_{ridge} = argmin_{\theta\ \in \ \mathbb{R}^p} 
\sum\limits_{i=1}^{n} \left(y_i - x_i^T\theta \right)^2 + \lambda 
\sum\limits_{j=1}^{p} \theta^2_j
      </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.6985050000000006em;"></span><span class="strut bottom" style="height:3.1122820000000004em;vertical-align:-1.4137769999999998em;"></span><span class="base"><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9578799999999998em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span><span style="top:-3.26344em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="margin-left:0.16668em;"><span>^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight" style="margin-right:0.02778em;">r</span><span class="mord mathit mtight">i</span><span class="mord mathit mtight">d</span><span class="mord mathit mtight" style="margin-right:0.03588em;">g</span><span class="mord mathit mtight">e</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"></span></span></span></span></span><span class="mrel">=</span><span class="mord mathit">a</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit" style="margin-right:0.03588em;">g</span><span class="mord mathit">m</span><span class="mord mathit">i</span><span class="mord"><span class="mord mathit">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999985em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight" style="margin-right:0.02778em;">θ</span><span class="mrel mtight"><span class="mspace mtight"><span class="mtight">&nbsp;</span></span><span class="mrel mtight">∈</span></span><span class="mord mtight"><span class="mspace mtight"><span class="mtight">&nbsp;</span></span><span class="mord mtight"><span class="mord mathbb mtight">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.5935428571428571em;"><span style="top:-2.786em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathit mtight">p</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.17737em;"></span></span></span></span></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6513970000000002em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">i</span><span class="mrel mtight">=</span><span class="mord mathrm mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em;"></span></span></span></span><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mbin">−</span><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"></span></span></span></span></span><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.0953389999999998em;"><span style="top:-3.344231em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathrm mtight">2</span></span></span></span></span></span></span></span><span class="mbin">+</span><span class="mord mathit">λ</span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6985050000000006em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mathrm mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.347113em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">p</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.4137769999999998em;"></span></span></span></span><span class="mord"><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.864108em;"><span style="top:-2.4530000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight" style="margin-right:0.05724em;">j</span></span></span><span style="top:-3.1130000000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathrm mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.383108em;"></span></span></span></span></span></span></span></span></span></span>
    </div>
    <div class="mobile_device_480px">
      <span><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mtable><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msub><mover accent="true"><mrow><mi>θ</mi></mrow><mo>^</mo></mover><mrow><mi>r</mi><mi>i</mi><mi>d</mi><mi>g</mi><mi>e</mi></mrow></msub><mo>=</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>i</mi><msub><mi>n</mi><mrow><mi>θ</mi><mtext>&nbsp;</mtext><mo>∈</mo><mtext>&nbsp;</mtext><msup><mi mathvariant="double-struck">R</mi><mi>p</mi></msup></mrow></msub><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mrow><mo fence="true">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msubsup><mi>x</mi><mi>i</mi><mi>T</mi></msubsup><mi>θ</mi><mo fence="true">)</mo></mrow><mn>2</mn></msup></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>+</mo><mi>λ</mi><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><msubsup><mi>θ</mi><mi>j</mi><mn>2</mn></msubsup></mrow></mstyle></mtd></mtr></mtable></mrow><annotation encoding="application/x-tex">
        \begin{aligned}
        \hat{\theta}_{ridge} = &amp; argmin_{\theta\ \in \ \mathbb{R}^p}
 \sum\limits_{i=1}^{n} \left(y_i - x_i^T\theta \right)^2 \\
        &amp; + \lambda \sum\limits_{j=1}^{p} \theta^2_j
        \end{aligned}
      </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:3.5706740000000003em;"></span><span class="strut bottom" style="height:6.641348000000002em;vertical-align:-3.0706740000000012em;"></span><span class="base"><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.5706740000000003em;"><span style="top:-5.617782000000001em;"><span class="pstrut" style="height:3.698505000000001em;"></span><span class="mord"><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9578799999999998em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span><span style="top:-3.26344em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="margin-left:0.16668em;"><span>^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight" style="margin-right:0.02778em;">r</span><span class="mord mathit mtight">i</span><span class="mord mathit mtight">d</span><span class="mord mathit mtight" style="margin-right:0.03588em;">g</span><span class="mord mathit mtight">e</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"></span></span></span></span></span><span class="mrel">=</span></span></span><span style="top:-2.3416079999999995em;"><span class="pstrut" style="height:3.698505000000001em;"></span><span class="mord"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:3.0706740000000012em;"></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.5706740000000003em;"><span style="top:-5.617782000000001em;"><span class="pstrut" style="height:3.698505000000001em;"></span><span class="mord"><span class="mord"></span><span class="mord mathit">a</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit" style="margin-right:0.03588em;">g</span><span class="mord mathit">m</span><span class="mord mathit">i</span><span class="mord"><span class="mord mathit">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999985em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight" style="margin-right:0.02778em;">θ</span><span class="mrel mtight"><span class="mspace mtight"><span class="mtight">&nbsp;</span></span><span class="mrel mtight">∈</span></span><span class="mord mtight"><span class="mspace mtight"><span class="mtight">&nbsp;</span></span><span class="mord mtight"><span class="mord mathbb mtight">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.5935428571428571em;"><span style="top:-2.786em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathit mtight">p</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.17737em;"></span></span></span></span></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6513970000000002em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">i</span><span class="mrel mtight">=</span><span class="mord mathrm mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em;"></span></span></span></span><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mbin">−</span><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"></span></span></span></span></span><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.0953389999999998em;"><span style="top:-3.344231em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathrm mtight">2</span></span></span></span></span></span></span></span></span></span><span style="top:-2.3416079999999995em;"><span class="pstrut" style="height:3.698505000000001em;"></span><span class="mord"><span class="mord"></span><span class="mbin">+</span><span class="mord mathit">λ</span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6985050000000006em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mathrm mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.347113em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">p</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.4137769999999998em;"></span></span></span></span><span class="mord"><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.864108em;"><span style="top:-2.4530000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight" style="margin-right:0.05724em;">j</span></span></span><span style="top:-3.1130000000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathrm mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.383108em;"></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:3.0706740000000012em;"></span></span></span></span></span></span></span></span></span></span></span>
    </div>
      <p>
        In Ridge regression, the weight matrix <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span></span></span> is the parameter, and the regularization coefficient <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>λ</mi><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\lambda \geq 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.83041em;vertical-align:-0.13597em;"></span><span class="base"><span class="mord mathit">λ</span><span class="mrel">≥</span><span class="mord mathrm">0</span></span></span></span></span> is the hyperparameter. <br>
        If we solve the above regression problem via gradient descent 
optimization, we further introduce another optimization parameter, the 
learning rate <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.0037em;">α</span></span></span></span></span>.
      </p>

      <p>The most common use case of Bayesian Optimization is <em>hyperparameter tuning</em>: finding the best performing hyperparameters on machine learning models.</p>

      <p>When training a model is not expensive and time-consuming, we 
can do a grid search to find the optimum hyperparameters. However, grid 
search is not feasible if function evaluations are costly, as in the 
case of a large neural network that takes days to train. Further, grid 
search scales poorly in terms of the number of hyperparameters.

      </p><p>We turn to Bayesian Optimization to counter the expensive nature of evaluating our black-box function (accuracy).</p>

      <h3 id="example1">Example 1 — Support Vector Machine (SVM)</h3>

      <p>In this example, we use an SVM to classify on sklearn’s moons 
dataset and use Bayesian Optimization to optimize SVM hyperparameters.

      </p><p>
        </p><ul>
          <li>
            <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.05556em;">γ</span></span></span></span></span> — modifies the behavior of the SVM’s kernel. Intuitively it is a measure of the influence of a single training example<d-footnote id="d-footnote-16">StackOverflow <a href="https://stackoverflow.com/questions/35848210/support-vector-machine-what-are-c-gamma">answer</a> for intuition behind the hyperparameters.</d-footnote>.
          </li>
          <li>
            <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.07153em;">C</span></span></span></span></span> — modifies the slackness of the classification, the higher the <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.07153em;">C</span></span></span></span></span> is, the more sensitive is SVM towards the noise.
          </li>
        </ul>
      <p></p>

<p> Let us have a look at the dataset now, which has two classes and two features.</p>

      <figure class="smaller-img">
        <d-figure><img src="Exploring%20Bayesian%20Optimization_files/moons.svg"></d-figure>
      </figure>

      <p>
        Let us apply Bayesian Optimization to learn the best hyperparameters for this classification task<d-footnote id="d-footnote-17"> <strong>Note</strong>:
 the surface plots you see for the Ground Truth Accuracies below were 
calculated for each possible hyperparameter for showcasing purposes 
only. We do not have these values in real applications.
        </d-footnote>. The optimum values for &lt;<span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>C</mi><mo separator="true">,</mo><mtext>&nbsp;</mtext><mi>γ</mi></mrow><annotation encoding="application/x-tex">C, \ \gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.07153em;">C</span><span class="mpunct">,</span><span class="mord mathit"><span class="mspace">&nbsp;</span><span class="mord mathit" style="margin-right:0.05556em;">γ</span></span></span></span></span></span>&gt; have been found via running grid search at high granularity.
      </p>

      <figure class="gif-slider">
        <d-figure><img src="Exploring%20Bayesian%20Optimization_files/0_003.png"></d-figure>
      <div class="controls"><div><button onclick="changePng(this.parentNode.parentNode.parentNode, false)" class="stepper button-left"><svg width="10" height="10" viewBox="0 0 10 10" style="margin-top: 3px; margin-left: -1px;"><path d="M 10 0 L 0 5 L 10 10 z" fill="#888"></path></svg></button></div><div class="slidecontainer"><input oninput="changePng(this.parentNode.parentNode.parentNode, false, this.value);" type="range" min="0" max="9" value="0" class="slider"></div><div><button onclick="changePng(this.parentNode.parentNode.parentNode, true)" class="stepper button-right"><svg width="10" height="10" viewBox="0 0 10 10" style="margin-top: 3px;"><path d="M 0 0 L 10 5 L 0 10 z" fill="#888"></path></svg></button></div></div></figure>

      <p>Above we see a slider showing the work of the <em>Probability of Improvement</em> acquisition function in finding the best hyperparameters.</p>

      <figure class="gif-slider">
        <d-figure><img src="Exploring%20Bayesian%20Optimization_files/0_006.png"></d-figure>
      <div class="controls"><div><button onclick="changePng(this.parentNode.parentNode.parentNode, false)" class="stepper button-left"><svg width="10" height="10" viewBox="0 0 10 10" style="margin-top: 3px; margin-left: -1px;"><path d="M 10 0 L 0 5 L 10 10 z" fill="#888"></path></svg></button></div><div class="slidecontainer"><input oninput="changePng(this.parentNode.parentNode.parentNode, false, this.value);" type="range" min="0" max="9" value="0" class="slider"></div><div><button onclick="changePng(this.parentNode.parentNode.parentNode, true)" class="stepper button-right"><svg width="10" height="10" viewBox="0 0 10 10" style="margin-top: 3px;"><path d="M 0 0 L 10 5 L 0 10 z" fill="#888"></path></svg></button></div></div></figure>

      <p>Above we see a slider showing the work of the <em>Expected Improvement</em> acquisition function in finding the best hyperparameters.</p>

      <h3 id="comparison">Comparison</h3>

      <p>
        Below is a plot that compares the different acquisition functions. We ran the <em>random</em> acquisition function several times to average out its results.
      </p>

      <figure>
        <d-figure><img src="Exploring%20Bayesian%20Optimization_files/comp3d.svg"></d-figure>
      </figure>

    <p>
        All our acquisition beat the <em>random</em> acquisition function after seven iterations. We see the <em>random</em>
 method seemed to perform much better initially, but it could not reach 
the global optimum, whereas Bayesian Optimization was able to get fairly
 close. The initial subpar performance of Bayesian Optimization can be 
attributed to the initial exploration.
    </p>

 

      <div class="collapsible"><h4>Other Examples</h4><div class="collapsible-indicator"></div></div>
      <div class="content">
        <h3>Example 2 — Random Forest</h3>

        <p>
          Using Bayesian Optimization in a Random Forest Classifier.<d-cite key="scikit"></d-cite>
        </p>

        <p>
          We will continue now to train a Random Forest on the moons 
dataset we had used previously to learn the Support Vector Machine 
model. The primary hyperparameters of Random Forests we would like to 
optimize our accuracy are the <strong> number</strong> of
          Decision Trees we would like to have, the <strong>maximum depth</strong> for each of those decision trees.
        </p>
        <p>
          The parameters of the Random Forest are the individual trained Decision Trees models.
        </p>
        <p>
          We will be again using Gaussian Processes with Matern kernel 
to estimate and predict the accuracy function over the two 
hyperparameters.
        </p>

        <figure class="gif-slider">
          <d-figure><img src="Exploring%20Bayesian%20Optimization_files/0_007.png"></d-figure>
        <div class="controls"><div><button onclick="changePng(this.parentNode.parentNode.parentNode, false)" class="stepper button-left"><svg width="10" height="10" viewBox="0 0 10 10" style="margin-top: 3px; margin-left: -1px;"><path d="M 10 0 L 0 5 L 10 10 z" fill="#888"></path></svg></button></div><div class="slidecontainer"><input oninput="changePng(this.parentNode.parentNode.parentNode, false, this.value);" type="range" min="0" max="9" value="0" class="slider"></div><div><button onclick="changePng(this.parentNode.parentNode.parentNode, true)" class="stepper button-right"><svg width="10" height="10" viewBox="0 0 10 10" style="margin-top: 3px;"><path d="M 0 0 L 10 5 L 0 10 z" fill="#888"></path></svg></button></div></div></figure>

        <p>
          Above is a typical Bayesian Optimization run with the <em>Probability of Improvement</em> acquisition function.
        </p>

        <figure class="gif-slider">
          <d-figure><img src="Exploring%20Bayesian%20Optimization_files/0_015.png"></d-figure>
        <div class="controls"><div><button onclick="changePng(this.parentNode.parentNode.parentNode, false)" class="stepper button-left"><svg width="10" height="10" viewBox="0 0 10 10" style="margin-top: 3px; margin-left: -1px;"><path d="M 10 0 L 0 5 L 10 10 z" fill="#888"></path></svg></button></div><div class="slidecontainer"><input oninput="changePng(this.parentNode.parentNode.parentNode, false, this.value);" type="range" min="0" max="9" value="0" class="slider"></div><div><button onclick="changePng(this.parentNode.parentNode.parentNode, true)" class="stepper button-right"><svg width="10" height="10" viewBox="0 0 10 10" style="margin-top: 3px;"><path d="M 0 0 L 10 5 L 0 10 z" fill="#888"></path></svg></button></div></div></figure>

        <p>Above we see a run showing the work of the <em>Expected Improvement</em> acquisition function in optimizing the hyperparameters.</p>

        <figure class="gif-slider">
          <d-figure><img src="Exploring%20Bayesian%20Optimization_files/0_008.png"></d-figure>
        <div class="controls"><div><button onclick="changePng(this.parentNode.parentNode.parentNode, false)" class="stepper button-left"><svg width="10" height="10" viewBox="0 0 10 10" style="margin-top: 3px; margin-left: -1px;"><path d="M 10 0 L 0 5 L 10 10 z" fill="#888"></path></svg></button></div><div class="slidecontainer"><input oninput="changePng(this.parentNode.parentNode.parentNode, false, this.value);" type="range" min="0" max="9" value="0" class="slider"></div><div><button onclick="changePng(this.parentNode.parentNode.parentNode, true)" class="stepper button-right"><svg width="10" height="10" viewBox="0 0 10 10" style="margin-top: 3px;"><path d="M 0 0 L 10 5 L 0 10 z" fill="#888"></path></svg></button></div></div></figure>

        <p>
          Now using the <em>Gaussian Processes Upper Confidence Bound</em> acquisition function in optimizing the hyperparameters.
        </p>

        <figure class="gif-slider">
          <d-figure><img src="Exploring%20Bayesian%20Optimization_files/0_011.png"></d-figure>
        <div class="controls"><div><button onclick="changePng(this.parentNode.parentNode.parentNode, false)" class="stepper button-left"><svg width="10" height="10" viewBox="0 0 10 10" style="margin-top: 3px; margin-left: -1px;"><path d="M 10 0 L 0 5 L 10 10 z" fill="#888"></path></svg></button></div><div class="slidecontainer"><input oninput="changePng(this.parentNode.parentNode.parentNode, false, this.value);" type="range" min="0" max="9" value="0" class="slider"></div><div><button onclick="changePng(this.parentNode.parentNode.parentNode, true)" class="stepper button-right"><svg width="10" height="10" viewBox="0 0 10 10" style="margin-top: 3px;"><path d="M 0 0 L 10 5 L 0 10 z" fill="#888"></path></svg></button></div></div></figure>

        <p>Let us now use the Random acquisition function.</p>

        <figure>
          <d-figure><img src="Exploring%20Bayesian%20Optimization_files/RFcomp3d.svg"></d-figure>
        </figure>

        <p>
          The optimization strategies seemed to struggle in this 
example. This can be attributed to the non-smooth ground truth. This 
shows that the effectiveness of Bayesian Optimization depends on the 
surrogate’s efficiency to model the actual black-box function. It is 
interesting to notice that the Bayesian Optimization framework still 
beats the <em>random</em> strategy using various acquisition functions.
        </p>

        <h3>Example 3 — Neural Networks</h3>
        <p>
          Let us take this example to get an idea of how to apply 
Bayesian Optimization to train neural networks. Here we will be using <code>scikit-optim</code>,
 which also provides us support for optimizing function with a search 
space of categorical, integral, and real variables. We will not be 
plotting the ground truth here, as it is extremely costly to do so. 
Below are some code snippets that show the ease of using Bayesian 
Optimization packages for hyperparameter tuning.
        </p>

        <p>
          The code initially declares a search space for the 
optimization problem. We limit the search space to be the following:
          </p><ul>
            <li>
              batch_size — This hyperparameter sets the number of 
training examples to combine to find the gradients for a single step in 
gradient descent. <br> 
              Our search space for the possible batch sizes consists of integer values s.t. batch_size = <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mn>2</mn><mi>i</mi></msup><mtext>&nbsp;</mtext><mi mathvariant="normal">∀</mi><mtext>&nbsp;</mtext><mn>2</mn><mo>≤</mo><mi>i</mi><mo>≤</mo><mn>7</mn><mtext>&nbsp;</mtext><mi mathvariant="normal">&amp;</mi><mtext>&nbsp;</mtext><mi>i</mi><mo>∈</mo><mrow><mi mathvariant="double-struck">Z</mi></mrow></mrow><annotation encoding="application/x-tex">2^i \ \forall \ 2 \leq i \leq 7 \ \&amp; \ i \in \mathbb{Z}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.824664em;"></span><span class="strut bottom" style="height:0.960634em;vertical-align:-0.13597em;"></span><span class="base"><span class="mord"><span class="mord mathrm">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.824664em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">i</span></span></span></span></span></span></span></span><span class="mord mathrm"><span class="mspace">&nbsp;</span><span class="mord mathrm">∀</span></span><span class="mord mathrm"><span class="mspace">&nbsp;</span><span class="mord mathrm">2</span></span><span class="mrel">≤</span><span class="mord mathit">i</span><span class="mrel">≤</span><span class="mord mathrm">7</span><span class="mord mathrm"><span class="mspace">&nbsp;</span><span class="mord mathrm">&amp;</span></span><span class="mord mathit"><span class="mspace">&nbsp;</span><span class="mord mathit">i</span></span><span class="mrel">∈</span><span class="mord"><span class="mord mathbb">Z</span></span></span></span></span></span>.
            </li>
            <li>
              learning rate — This hyperparameter sets the stepsize with
 which we will perform gradient descent in the neural network. <br>
              We will be searching over all the real numbers in the range <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>[</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>6</mn></mrow></msup><mo separator="true">,</mo><mtext>&nbsp;</mtext><mn>1</mn><mo>]</mo></mrow><annotation encoding="application/x-tex">[10^{-6}, \ 1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="base"><span class="mopen">[</span><span class="mord mathrm">1</span><span class="mord"><span class="mord mathrm">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathrm mtight">6</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mord mathrm"><span class="mspace">&nbsp;</span><span class="mord mathrm">1</span></span><span class="mclose">]</span></span></span></span></span>.
            </li>
            <li>
              activation — We will have one categorical variable, i.e. 
the activation to apply to our neural network layers. This variable can 
take on values in the set <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>{</mo><mi>r</mi><mi>e</mi><mi>l</mi><mi>u</mi><mo separator="true">,</mo><mtext>&nbsp;</mtext><mi>s</mi><mi>i</mi><mi>g</mi><mi>m</mi><mi>o</mi><mi>i</mi><mi>d</mi><mo>}</mo></mrow><annotation encoding="application/x-tex">\{ relu, \ sigmoid \}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mopen">{</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit">e</span><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mord mathit">u</span><span class="mpunct">,</span><span class="mord mathit"><span class="mspace">&nbsp;</span><span class="mord mathit">s</span></span><span class="mord mathit">i</span><span class="mord mathit" style="margin-right:0.03588em;">g</span><span class="mord mathit">m</span><span class="mord mathit">o</span><span class="mord mathit">i</span><span class="mord mathit">d</span><span class="mclose">}</span></span></span></span></span>.
            </li>
          </ul>
        <p></p>

        <d-code block="" language="python">
          log_batch_size = Integer(
            low=2,
            high=7,
            name='log_batch_size'
          )
            lr = Real(
            low=1e-6,
            high=1e0,
            prior='log-uniform',
            name='lr'
          )
          activation = Categorical(
            categories=['relu', 'sigmoid'],
            name='activation'
          )

          dimensions = [
            dim_num_batch_size_to_base,
            dim_learning_rate,
            dim_activation
          ]
        </d-code>

        <p>
          Now import <code>gp-minimize</code><d-footnote id="d-footnote-18"><strong>Note</strong>: One will need to negate the accuracy values as we are using the minimizer function from <code>scikit-optim</code>.</d-footnote> from <code>scikit-optim</code> to perform the optimization. Below we show calling the optimizer using <em>Expected Improvement</em>, but of course we can select from a number of other acquisition functions.
        </p>

        <d-code block="" language="python">
          # initial parameters (1st point)
          default_parameters = 
            [4, 1e-1, 'relu']

          # bayesian optimization
            search_result = gp_minimize(
            func=train,
            dimensions=dimensions,
            acq_func='EI', # Expctd Imprv.
            n_calls=11,
            x0=default_parameters
          )
        </d-code>

        <figure class="smaller-img">
          <d-figure><img src="Exploring%20Bayesian%20Optimization_files/conv.svg"></d-figure>
        </figure>

        <p>
          In the graph above the y-axis denotes the best accuracy till then, <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mo fence="true">(</mo><mi>f</mi><mo>(</mo><msup><mi>x</mi><mo>+</mo></msup><mo>)</mo><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\left( f(x^+) \right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.771331em;"></span><span class="strut bottom" style="height:1.021331em;vertical-align:-0.25em;"></span><span class="base"><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.771331em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">+</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose delimcenter" style="top:0em;">)</span></span></span></span></span></span> and the x-axis denotes the evaluation number.
        </p>

        <p>
          Looking at the above example, we can see that incorporating 
Bayesian Optimization is not difficult and can save a lot of time. 
Optimizing to get an accuracy of nearly one in around seven iterations 
is impressive!<d-footnote id="d-footnote-19">The example above has been inspired by <a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/19_Hyper-Parameters.ipynb">Hvass Laboratories’ Tutorial Notebook</a> showcasing hyperparameter optimization in TensorFlow using <code>scikit-optim</code>.</d-footnote>
        </p>

        <p>
          Let us get the numbers into perspective. If we had run this 
optimization using a grid search, it would have taken around <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mn>5</mn><mo>×</mo><mn>2</mn><mo>×</mo><mn>7</mn><mo>)</mo></mrow><annotation encoding="application/x-tex">(5 \times 2 \times 7)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mopen">(</span><span class="mord mathrm">5</span><span class="mbin">×</span><span class="mord mathrm">2</span><span class="mbin">×</span><span class="mord mathrm">7</span><span class="mclose">)</span></span></span></span></span>
 iterations. Whereas Bayesian Optimization only took seven iterations. 
Each iteration took around fifteen minutes; this sets the time required 
for the grid search to complete around seventeen hours!
        </p>
      </div>

      <h1 id="conclusions">Conclusion and Summary</h1>

      <p>
        In this article, we looked at Bayesian Optimization for 
optimizing a black-box function. Bayesian Optimization is well suited 
when the function evaluations are expensive, making grid or exhaustive 
search impractical. We looked at the key components of Bayesian 
Optimization. First, we looked at the notion of using a surrogate 
function (with a prior over the space of objective functions) to model 
our black-box function. Next, we looked at the “Bayes” in Bayesian 
Optimization — the function evaluations are used as data to obtain the 
surrogate posterior. We look at acquisition functions, which are 
functions of the surrogate posterior and are optimized sequentially. 
This new sequential optimization is in-expensive and thus of utility of 
us. We also looked at a few acquisition functions and showed how these 
different functions balance exploration and exploitation. Finally, we 
looked at some practical examples of Bayesian Optimization for 
optimizing hyper-parameters for machine learning models.
      </p>

      <p>
        We hope you had a good time reading the article and hope you are ready to <strong>exploit</strong> the power of Bayesian Optimization. In case you wish to <strong>explore</strong> more, please read the <a href="#FurtherReading">Further Reading</a> section below. We also provide our <a href="https://github.com/distillpub/post--bayesian-optimization">repository</a> to reproduce the entire article.  
      </p>

  </d-article>

  <d-appendix>
<style>

d-appendix {
  contain: layout style;
  font-size: 0.8em;
  line-height: 1.7em;
  margin-top: 60px;
  margin-bottom: 0;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  color: rgba(0,0,0,0.5);
  padding-top: 60px;
  padding-bottom: 48px;
}

d-appendix h3 {
  grid-column: page-start / text-start;
  font-size: 15px;
  font-weight: 500;
  margin-top: 1em;
  margin-bottom: 0;
  color: rgba(0,0,0,0.65);
}

d-appendix h3 + * {
  margin-top: 1em;
}

d-appendix ol {
  padding: 0 0 0 15px;
}

@media (min-width: 768px) {
  d-appendix ol {
    padding: 0 0 0 30px;
    margin-left: -30px;
  }
}

d-appendix li {
  margin-bottom: 1em;
}

d-appendix a {
  color: rgba(0, 0, 0, 0.6);
}

d-appendix > * {
  grid-column: text;
}

d-appendix > d-footnote-list,
d-appendix > d-citation-list,
d-appendix > distill-appendix {
  grid-column: screen;
}

</style>


    <h3 id="embracebayesianoptimization">Embrace Bayesian Optimization</h3>

      <p>
        Having read all the way through, you might have been sold on the
 idea about the time you can save by asking Bayesian Optimizer to find 
the best hyperparameters for your fantastic model. There are a plethora 
of Bayesian Optimization libraries available. We have linked a few 
below. Do check them out.
      </p>

      <ul>
        <li><a href="https://scikit-optimize.github.io/">scikit-optimize</a>
          <d-footnote id="d-footnote-20">Really nice tutorial showcasing hyperparameter optimization on a neural network available at this <a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/19_Hyper-Parameters.ipynb">link</a>.
          </d-footnote>
        </li>

        <li><a href="https://app.sigopt.com/docs/overview/python">sigopt</a></li>

        <li><a href="http://hyperopt.github.io/hyperopt/">hyperopt</a></li>

        <li><a href="https://github.com/HIPS/Spearmint">spearmint</a></li>

        <li><a href="https://github.com/Yelp/MOE">MOE</a></li>

        <li><a href="https://botorch.org/tutorials/">BOTorch</a></li>
        <li><a href="https://github.com/SheffieldML/GPyOpt">GPyOpt</a></li>
        <li><a href="https://github.com/dragonfly/dragonfly">DragonFly</a></li>
      </ul>

      <h3 id="ack">Acknowledgments</h3>

      <p>
        This article was made possible with inputs from numerous people.
 Firstly, we would like to thank all the Distill reviewers for their 
punctilious and actionable feedback. These fantastic reviews immensely 
helped strengthen our article. We further express our gratitude towards 
the Distill Editors, who were extremely kind and helped us navigate 
various steps to publish our work. We would also like to thank <a href="https://sgarg87.github.io/">Dr. Sahil Garg</a> for his feedback on the flow of the article. We would like to acknowledge the help we received from <a href="http://initiatives.iitgn.ac.in/writingstudio/wp/">Writing Studio</a> to improve the script of our article. Lastly, we sincerely thank <a href="https://colah.github.io/">Christopher Olah</a>. His inputs, suggestions, multiple rounds of iterations made this article substantially better.  
      </p>

    <h3 id="FurtherReading">Further Reading</h3>
    <ol>
      <li>
        <p>Using gradient information when it is available.</p>
        <ul>
          <li>
            Suppose we have gradient information available, we should 
possibly try to use the information. This could result in a much faster 
approach to the global maxima. Please have a look at the paper by Wu, et
 al.<d-cite key="BOwtGD"></d-cite> to know more.
          </li>
        </ul>
      </li>
      <li>
        <p>
          To have a quick view of differences between Bayesian Optimization and Gradient Descent, one can look at <a href="https://stats.stackexchange.com/q/161936">this</a> amazing answer at StackOverflow.
        </p>
      </li>
      <li>
        <p>
          We talked about optimizing a black-box function here. If we 
are to perform over multiple objectives, how do these acquisition 
functions scale? There has been fantastic work in this domain too! We 
try to deal with these cases by having multi-objective acquisition 
functions. Have a look at <a href="https://gpflowopt.readthedocs.io/en/latest/notebooks/multiobjective.html">this excellent</a> notebook for an example using <code>gpflowopt</code>.
        </p>
      </li>
      <li>
        <p>
          One of the more interesting uses of hyperparameters 
optimization can be attributed to searching the space of neural network 
architecture for finding the architectures that give us maximal 
predictive performance. One might also want to consider nonobjective 
optimizations as some of the other objectives like memory consumption, 
model size, or inference time also matter in practical scenarios.
        </p>
      </li>
      <li>
        <p>
          When the datasets are extremely large, human experts tend to 
test hyperparameters on smaller subsets of the dataset and iteratively 
improve the accuracy for their models. There has been work in Bayesian 
Optimization, taking into account these approaches<d-cite key="hyperband,largeBO"></d-cite> when datasets are of such sizes.
        </p>
      </li>
      <li>
        <p>
          There also has been work on Bayesian Optimization, where one 
explores with a certain level of “safety”, meaning the evaluated values 
should lie above a certain security threshold functional value<d-cite key="SafeExplore"></d-cite>.
 One toy example is the possible configurations for a flying robot to 
maximize its stability. If we tried a point with terrible stability, we 
might crash the robot, and therefore we would like to explore the 
configuration space more diligently.
        </p>
      </li>
      <li>
        <p>
          We have been using GP in our Bayesian Optimization for getting
 predictions, but we can have any other predictor or mean and variance 
in our Bayesian Optimization.
        </p>
        <ul>
          <li>
            <p>
              One can look at <a href="http://aad.informatik.uni-freiburg.de/~hutter/ML3.pdf">this</a>
 slide deck by Frank Hutter discussing some limitations of a GP-based 
Bayesian Optimization over a Random Forest based Bayesian Optimization.
            </p>
          </li>
          <li>
            <p>
              There has been work on even using deep neural networks in Bayesian Optimization<d-cite key="NNbasedBO"></d-cite>
 for a more scalable approach compared to GP. The paper talks about how 
GP-based Bayesian Optimization scales cubically with the number of 
observations, compared to their novel method that scales linearly.
            </p>
          </li>
        </ul>
      </li>
      <li>
        <p>
          Things to take care when using Bayesian Optimization.
        </p>
        <ul>
          <li>
            <p>
              While working on the blog, we once scaled the accuracy from the range <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>[</mo><mn>0</mn><mo separator="true">,</mo><mtext>&nbsp;</mtext><mn>1</mn><mo>]</mo></mrow><annotation encoding="application/x-tex">[0, \ 1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mopen">[</span><span class="mord mathrm">0</span><span class="mpunct">,</span><span class="mord mathrm"><span class="mspace">&nbsp;</span><span class="mord mathrm">1</span></span><span class="mclose">]</span></span></span></span></span> to <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>[</mo><mn>0</mn><mo separator="true">,</mo><mtext>&nbsp;</mtext><mn>1</mn><mn>0</mn><mn>0</mn><mo>]</mo></mrow><annotation encoding="application/x-tex">[0, \ 100]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mopen">[</span><span class="mord mathrm">0</span><span class="mpunct">,</span><span class="mord mathrm"><span class="mspace">&nbsp;</span><span class="mord mathrm">1</span></span><span class="mord mathrm">0</span><span class="mord mathrm">0</span><span class="mclose">]</span></span></span></span></span>.
 This change broke havoc as the Gaussian Processes we were using had 
certain hyperparameters, which needed
              to be scaled with the accuracy to maintain scale 
invariance. We wanted to point this out as it might be helpful for the 
readers who would like to start using on Bayesian Optimization.
            </p>
          </li>
          <li>
            <p>
              We need to take care while using Bayesian Optimization. 
Bayesian Optimization based on Gaussian Processes Regression is highly 
sensitive to the kernel used. For example, if you are using <a href="https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.Matern.html">Matern</a> kernel, we are implicitly assuming that the function we are trying to optimize is first order differentiable.
            </p>
          </li>
          <li>
            <p>
              Searching for the hyperparameters, and the choice of the 
acquisition function to use in Bayesian Optimization are interesting 
problems in themselves. There has been amazing work done, looking at 
this problem. As mentioned previously in the post, there has
              been work done in strategies using multiple acquisition 
function<d-cite key="multiACQ"></d-cite> to deal with these interesting issues.
            </p>
          </li>
          <li>
            <p>
              A nice list of tips and tricks one should have a look at 
if you aim to use Bayesian Optimization in your workflow is from this 
fantastic post by Thomas on <a href="https://thuijskens.github.io/2016/12/29/bayesian-optimisation/">Bayesian
                Optimization with sklearn</a>.
            </p>
          </li>
        </ul>
      </li>
      <li>
        <p>
          Bayesian Optimization applications.
        </p>
        <ul>
          <li>
            <p>
              Bayesian Optimization has been applied to Optimal Sensor Set selection for predictive accuracy<d-cite key="sensorBO"></d-cite>.
            </p>
          </li>
          <li>
            <p>
              Peter Frazier in his <a href="https://www.youtube.com/watch?v=c4KKvyWW_Xk">talk</a> mentioned that Uber uses Bayesian Optimization for tuning algorithms via backtesting.
            </p>
          </li>
          <li>
            <p>Facebook<d-cite key="letham2019"></d-cite> uses Bayesian Optimization for A/B testing.
          </p></li>
          <li>
            <p>
              Netflix and <a href="https://engineeringblog.yelp.com/2014/10/using-moe-the-metric-optimization-engine-to-optimize-an-ab-testing-experiment-framework.html">Yelp</a> use Metrics Optimization software like <a href="http://github.com/Yelp/MOE">Metrics Optimization Engine (MOE)</a> which take advantage of Parallel Bayesian Optimization<d-cite key="yelpBO"></d-cite>.
            </p>
          </li>
        </ul>
      </li>
    </ol>

    <d-bibliography><script type="text/json">[["settles2009active",{"Author":"Burr Settles","Institution":"University of Wisconsin--Madison","Number":"1648","Title":"Active Learning Literature Survey","Type":"Computer Sciences Technical Report","Year":"2009","url":"http://burrsettles.com/pub/settles.activelearning.pdf","author":"Burr Settles","institution":"University of Wisconsin--Madison","number":"1648","title":"Active Learning Literature Survey","type":"techreport","year":"2009"}],["Tong2001",{"title":"Active learning: theory and applications","school":"Stanford University","author":"Simon Tong","year":"2001","url":"http://www.robotics.stanford.edu/~stong/papers/tong_thesis.pdf","type":"phdthesis"}],["humanOut",{"author":"B. Shahriari and K. Swersky and Z. Wang and R. P. Adams and N. de Freitas","journal":"Proceedings of the IEEE","title":"Taking the Human Out of the Loop: A Review of Bayesian Optimization","year":"2016","volume":"104","number":"1","pages":"148-175","keywords":"Bayes methods;Big Data;optimisation;storage allocation;Bayesian optimization;human productivity;product quality;storage architecture;large-scale heterogeneous computing;massive complex software system;Big data application;Big data;Bayes methods;Linear programming;Decision making;Design of experiments;Optimization;Genomes;Statistical analysis;decision making;design of experiments;optimization;response surface methodology;statistical learning;genomic medicine;Decision making;design of experiments;optimization;response surface methodology;statistical learning","doi":"10.1109/JPROC.2015.2494218","ISSN":"0018-9219","month":"Jan","issn":"0018-9219","type":"ARTICLE"}],["gpucb",{"author":"Auer, Peter","title":"Using Confidence Bounds for Exploitation-Exploration Trade-Offs","year":"2003","month":"Mar","publisher":"JMLR.org","volume":"3","number":"null","issn":"1532-4435","journal":"J. Mach. Learn. Res.","pages":"397–422","numpages":"26","keywords":"exploitation-exploration, linear value function, bandit problem, online Learning, reinforcement learning","type":"article"}],["gpucbBounds",{"author":"Srinivas, Niranjan and Krause, Andreas and Kakade, Sham M. and Seeger, Matthias","title":"Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design","journal":"arXiv e-prints","keywords":"Computer Science - Machine Learning","year":"2009","month":"Dec","eid":"arXiv:0912.3995","pages":"arXiv:0912.3995","archivePrefix":"arXiv","eprint":"0912.3995","primaryClass":"cs.LG","adsurl":"https://ui.adsabs.harvard.edu/abs/2009arXiv0912.3995S","adsnote":"Provided by the SAO/NASA Astrophysics Data System","archiveprefix":"arXiv","primaryclass":"cs.LG","type":"ARTICLE"}],["nandoBOLoop",{"author":"B. {Shahriari} and K. {Swersky} and Z. {Wang} and R. P. {Adams} and N. {de Freitas}","journal":"Proceedings of the IEEE","title":"Taking the Human Out of the Loop: A Review of Bayesian Optimization","year":"2016","volume":"104","number":"1","pages":"148-175","type":"ARTICLE"}],["Snoek2012",{"author":"Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P.","title":"Practical Bayesian Optimization of Machine Learning Algorithms","booktitle":"Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2","series":"NIPS'12","year":"2012","location":"Lake Tahoe, Nevada","pages":"2951--2959","numpages":"9","url":"http://dl.acm.org/citation.cfm?id=2999325.2999464","acmid":"2999464","publisher":"Curran Associates Inc.","address":"USA","type":"inproceedings"}],["nandoBOtut",{"author":"Brochu, Eric and M. Cora, Vlad and De Freitas, Nando","year":"2010","month":"12","title":"A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning","volume":"abs/1012.2599","journal":"CoRR","type":"article"}],["NIPS2011_4443",{"author":"Bergstra, James and Bardenet, R\\'{e}mi and Bengio, Yoshua and K\\'{e}gl, Bal\\'{a}zs","title":"Algorithms for Hyper-Parameter Optimization","year":"2011","isbn":"9781618395993","publisher":"Curran Associates Inc.","address":"Red Hook, NY, USA","booktitle":"Proceedings of the 24th International Conference on Neural Information Processing Systems","pages":"2546–2554","numpages":"9","location":"Granada, Spain","series":"NIPS’11","type":"inproceedings"}],["Bergstra",{"author":"Bergstra, J. and Yamins, D. and Cox, D. D.","title":"Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures","year":"2013","publisher":"JMLR.org","booktitle":"Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28","pages":"I–115–I–123","numpages":"9","location":"Atlanta, GA, USA","series":"ICML’13","url":"http://dl.acm.org/citation.cfm?id=3042817.3042832","acmid":"3042832","type":"inproceedings"}],["peterTutBO",{"author":"Frazier, Peter I.","title":"{A Tutorial on Bayesian Optimization}","journal":"arXiv e-prints","keywords":"Statistics - Machine Learning, Computer Science - Machine Learning, Mathematics - Optimization and Control","year":"2018","month":"Jul","eid":"arXiv:1807.02811","pages":"arXiv:1807.02811","archivePrefix":"arXiv","eprint":"1807.02811","primaryClass":"stat.ML","adsurl":"https://ui.adsabs.harvard.edu/abs/2018arXiv180702811F","adsnote":"Provided by the SAO/NASA Astrophysics Data System","archiveprefix":"arXiv","primaryclass":"stat.ML","type":"ARTICLE"}],["görtler2019a",{"author":"Görtler, Jochen and Kehlbeck, Rebecca and Deussen, Oliver","url":"https://distill.pub/2019/visual-exploration-gaussian-processes/","title":"A Visual Exploration of Gaussian Processes","journal":"Distill","year":"2019","note":"https://distill.pub/2019/visual-exploration-gaussian-processes","doi":"10.23915/distill.00017","type":"article"}],["Rasmussen2004",{"author":"Carl Edward Rasmussen","title":"Gaussian Processes in Machine Learning","booktitle":"Advanced Lectures on Machine Learning","publisher":"Springer Berlin Heidelberg","year":"2004","pages":"63--71","doi":"10.1007/978-3-540-28650-9_4","url":"http://www.gaussianprocess.org/gpml/chapters/RW.pdf","type":"InCollection"}],["thompTut",{"author":"Daniel J. Russo and Benjamin Van Roy and Abbas Kazerouni and Ian Osband and Zheng Wen","url":"http://dx.doi.org/10.1561/2200000070","year":"2018","volume":"11","journal":"Foundations and Trends® in Machine Learning","title":"A Tutorial on Thompson Sampling","doi":"10.1561/2200000070","issn":"1935-8237","number":"1","pages":"1-96","type":"article"}],["BOwtGD",{"title":"Bayesian Optimization with Gradients","author":"Wu, Jian and Poloczek, Matthias and Wilson, Andrew G and Frazier, Peter","booktitle":"Advances in Neural Information Processing Systems 30","editor":"I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett","pages":"5267--5278","year":"2017","publisher":"Curran Associates, Inc.","url":"http://papers.nips.cc/paper/7111-bayesian-optimization-with-gradients.pdf","type":"incollection"}],["mockusEI",{"author":"B. Mockus, J and Mockus, Linas","year":"1991","month":"07","pages":"157-172","title":"Bayesian approach to global optimization and application to multiobjective and constrained problems","volume":"70","journal":"Journal of Optimization Theory and Applications","doi":"10.1007/BF00940509","type":"article"}],["scikit",{"title":"Scikit-learn: Machine Learning in {P}ython","author":"Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.","journal":"Journal of Machine Learning Research","volume":"12","pages":"2825--2830","year":"2011","type":"article"}],["yelpBO",{"author":"Wang, Jialei and Clark, Scott C. and Liu, Eric and Frazier, Peter I.","title":"Parallel Bayesian Global Optimization of Expensive Functions","journal":"arXiv e-prints","keywords":"Statistics - Machine Learning, Mathematics - Optimization and Control","year":"2016","month":"Feb","eid":"arXiv:1602.05149","pages":"arXiv:1602.05149","archivePrefix":"arXiv","eprint":"1602.05149","primaryClass":"stat.ML","adsurl":"https://ui.adsabs.harvard.edu/abs/2016arXiv160205149W","adsnote":"Provided by the SAO/NASA Astrophysics Data System","archiveprefix":"arXiv","primaryclass":"stat.ML","type":"ARTICLE"}],["letham2019",{"author":"Letham, Benjamin and Karrer, Brian and Ottoni, Guilherme and Bakshy, Eytan","doi":"10.1214/18-BA1110","fjournal":"Bayesian Analysis","journal":"Bayesian Anal.","month":"06","number":"2","pages":"495--519","publisher":"International Society for Bayesian Analysis","title":"Constrained Bayesian Optimization with Noisy Experiments","url":"https://doi.org/10.1214/18-BA1110","volume":"14","year":"2019","type":"article"}],["sensorBO",{"author":"Garnett, R. and Osborne, M. A. and Roberts, S. J.","title":"Bayesian Optimization for Sensor Set Selection","booktitle":"Proceedings of the 9th ACM/IEEE International Conference on Information Processing in Sensor Networks","series":"IPSN '10","year":"2010","isbn":"978-1-60558-988-6","location":"Stockholm, Sweden","pages":"209--219","numpages":"11","url":"http://doi.acm.org/10.1145/1791212.1791238","doi":"10.1145/1791212.1791238","acmid":"1791238","publisher":"ACM","address":"New York, NY, USA","keywords":"Bayesian methods, Gaussian processes, experimental design, global optimization, sampling design, sensor networks, sensor selection, spatial learning","type":"inproceedings"}],["multiACQ",{"author":"Hoffman, Matthew and Brochu, Eric and de Freitas, Nando","title":"Portfolio Allocation for Bayesian Optimization","booktitle":"Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence","series":"UAI'11","year":"2011","isbn":"978-0-9749039-7-2","location":"Barcelona, Spain","pages":"327--336","numpages":"10","url":"http://dl.acm.org/citation.cfm?id=3020548.3020587","acmid":"3020587","publisher":"AUAI Press","address":"Arlington, Virginia, United States","type":"inproceedings"}],["NNbasedBO",{"author":"Snoek, Jasper and Rippel, Oren and Swersky, Kevin and Kiros, Ryan and Satish, Nadathur and Sundaram, Narayanan and Patwary, Md. Mostofa Ali and Prabhat, Prabhat and Adams, Ryan P.","title":"Scalable Bayesian Optimization Using Deep Neural Networks","booktitle":"Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37","series":"ICML'15","year":"2015","location":"Lille, France","pages":"2171--2180","numpages":"10","url":"http://dl.acm.org/citation.cfm?id=3045118.3045349","acmid":"3045349","publisher":"JMLR.org","type":"inproceedings"}],["hyperband",{"author":"Liam Li and Kevin Jamieson and Giulia DeSalvo and Afshin Rostamizadeh and Ameet Talwalkar","title":"Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization","year":"2018","URL":"http://www.jmlr.org/papers/volume18/16-558/16-558.pdf","journal":"Journal of Machine Learning Research","pages":"1-52","volume":"18-185","url":"http://www.jmlr.org/papers/volume18/16-558/16-558.pdf","type":"article"}],["largeBO",{"title":"Fast Bayesian Optimization of Machine Learning Hyperparameters on Large Datasets","author":"Aaron Klein and Stefan Falkner and Simon Bartels and Philipp Hennig and Frank Hutter","booktitle":"Proceedings of the 20th International Conference on Artificial Intelligence and Statistics","pages":"528--536","year":"2017","editor":"Aarti Singh and Jerry Zhu","volume":"54","series":"Proceedings of Machine Learning Research","address":"Fort Lauderdale, FL, USA","month":"20--22 Apr","publisher":"PMLR","pdf":"http://proceedings.mlr.press/v54/klein17a/klein17a.pdf","url":"http://proceedings.mlr.press/v54/klein17a.html","abstract":"Bayesian optimization has become a successful tool for hyperparameter optimization of machine learning algorithms, such as support vector machines or deep neural networks. Despite its success, for large datasets, training and validating a single configuration often takes hours, days, or even weeks, which limits the achievable performance. To accelerate hyperparameter optimization, we propose a generative model for the validation error as a function of training set size, which is learned during the optimization process and allows exploration of preliminary configurations on small subsets, by extrapolating to the full dataset. We construct a Bayesian optimization procedure, dubbed FABOLAS, which models loss and training time as a function of dataset size and automatically trades off high information gain about the global optimum against computational cost. Experiments optimizing support vector machines and deep neural networks show that FABOLAS often finds high-quality solutions 10 to 100 times faster than other state-of-the-art Bayesian optimization methods or the recently proposed bandit strategy Hyperband.","type":"InProceedings"}],["SafeExplore",{"author":"Sui, Yanan and Gotovos, Alkis and Burdick, Joel W. and Krause, Andreas","title":"Safe Exploration for Optimization with Gaussian Processes","booktitle":"Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37","series":"ICML'15","year":"2015","location":"Lille, France","pages":"997--1005","numpages":"9","url":"http://dl.acm.org/citation.cfm?id=3045118.3045225","acmid":"3045225","publisher":"JMLR.org","type":"inproceedings"}],["thompson",{"author":"Thompson, William R","title":"On The Likelihood That One Unknown Probability Exceeds Another In View Of The Evidence Of Two Samples","journal":"Biometrika","volume":"25","number":"3-4","pages":"285-294","year":"1933","month":"12","issn":"0006-3444","doi":"10.1093/biomet/25.3-4.285","url":"https://doi.org/10.1093/biomet/25.3-4.285","type":"article"}],["goldKridge",{"author":"Krige, D.G.","title":"A statistical approach to some basic mine valuation problems on the Witwatersrand ","journal":"Journal of the Southern African Institute of Mining and Metallurgy","year":"1951","volume":"52","number":"6","pages":"119-139","url":"https://journals.co.za/content/saimm/52/6/AJA0038223X_4792","publisher":"Southern African Institute of Mining and Metallurgy","issn":"0038-223X","type":"article","language":"English","abstract":"Certain fundamental concepts in the application of statistics to mine valuation on the Witwatersrand are discussed, and general conclusions are drawn regarding the application of the lognormal curve to the frequency distribution of gold values. An indication is given of the reliability of present valuation methods on the Rand. It is shown that the existing over- and under-valuation of blocks of ore listed as high-grade and low-grade, respectively, can be explained statistically. Suggestions are made for the elimination of such errors and for the improvement of the general standard of mine valuation by the use of statistical theory."}]]</script></d-bibliography>
  <d-footnote-list style="">
<style>

d-footnote-list {
  contain: layout style;
}

d-footnote-list > * {
  grid-column: text;
}

d-footnote-list a.footnote-backlink {
  color: rgba(0,0,0,0.3);
  padding-left: 0.5em;
}

</style>

<h3>Footnotes</h3>
<ol><li id="d-footnote-1-listing">Interestingly, our example is similar to one of the first use of Gaussian Processes (also called kriging)<d-cite key="goldKridge"></d-cite>, where Prof. Krige modeled the gold concentrations using a Gaussian Process.<a class="footnote-backlink" href="#d-footnote-1">[↩]</a></li><li id="d-footnote-2-listing">
          Gaussian Process supports setting of priors by using specific 
kernels and mean functions. One might want to look at this excellent 
Distill article<d-cite key="görtler2019a"></d-cite> on Gaussian Processes<d-cite key="Rasmussen2004"></d-cite> to learn more.<br><br>
          Please find <a href="https://youtu.be/EnXxO3BAgYk">this</a> amazing video from Javier González on Gaussian Processes. 
      <a class="footnote-backlink" href="#d-footnote-2">[↩]</a></li><li id="d-footnote-3-listing">
        Specifics: We use a Matern 5/2 kernel due to its property of favoring doubly differentiable functions. <!-- In contrast, Matern 3/2 favors singly differentiable functions. --> See <a href="http://www.gaussianprocess.org/gpml/">Rasmussen and Williams 2004</a> and <a href="https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.Matern.html">scikit-learn</a>, for details regarding the Matern kernel.
      <a class="footnote-backlink" href="#d-footnote-3">[↩]</a></li><li id="d-footnote-4-listing">More details on acquisition functions can be accessed at on this <a href="https://botorch.org/docs/acquisition">link</a>.<a class="footnote-backlink" href="#d-footnote-4">[↩]</a></li><li id="d-footnote-5-listing">The section below is based on the slides/talk from Peter Fraizer at Uber on Bayesian Optimization:
          <ul class="footnote-ul">
            <li> <a href="https://www.youtube.com/watch?v=c4KKvyWW_Xk">Youtube talk</a>,</li>
            <li> <a href="https://people.orie.cornell.edu/pfrazier/Presentations/2018.11.INFORMS.tutorial.pdf">slide deck</a></li>
          </ul>
        <a class="footnote-backlink" href="#d-footnote-5">[↩]</a></li><li id="d-footnote-6-listing">
        Please find <a href="https://www.cse.wustl.edu/~garnett/cse515t/spring_2015/files/lecture_notes/12.pdf">these</a> slides from Washington University in St. Louis to know more about acquisition functions.
      <a class="footnote-backlink" href="#d-footnote-6">[↩]</a></li><li id="d-footnote-7-listing">Ties are broken randomly.<a class="footnote-backlink" href="#d-footnote-7">[↩]</a></li><li id="d-footnote-8-listing">The proportion of uncertainty is identified by the grey translucent area.<a class="footnote-backlink" href="#d-footnote-8">[↩]</a></li><li id="d-footnote-9-listing">Points in the vicinity of current maxima<a class="footnote-backlink" href="#d-footnote-9">[↩]</a></li><li id="d-footnote-10-listing">A good introduction to the Expected Improvement acquisition function is by <a href="https://thuijskens.github.io/2016/12/29/bayesian-optimisation/" target="_blank">this post</a> by Thomas Huijskens and <a href="https://people.orie.cornell.edu/pfrazier/Presentations/2018.11.INFORMS.tutorial.pdf" target="_blank">these slides</a> by Peter Frazier<a class="footnote-backlink" href="#d-footnote-10">[↩]</a></li><li id="d-footnote-11-listing">Each
 dot is a point in the search space. Additionally, the training set used
 while making the plot only consists of a single observation <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>5</mn><mo separator="true">,</mo><mi>f</mi><mo>(</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>5</mn><mo>)</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">(0.5, f(0.5))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mopen">(</span><span class="mord mathrm">0</span><span class="mord mathrm">.</span><span class="mord mathrm">5</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathrm">0</span><span class="mord mathrm">.</span><span class="mord mathrm">5</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span><a class="footnote-backlink" href="#d-footnote-11">[↩]</a></li><li id="d-footnote-12-listing">Since “Probability of Improvement” is low<a class="footnote-backlink" href="#d-footnote-12">[↩]</a></li><li id="d-footnote-13-listing">Since “Expected Improvement” is high<a class="footnote-backlink" href="#d-footnote-13">[↩]</a></li><li id="d-footnote-14-listing">To know more about the difference between acquisition functions look at <a href="https://www.cs.ubc.ca/~nando/540-2013/lectures/l7.pdf">these</a> amazing
          slides from Nando De Freitas<a class="footnote-backlink" href="#d-footnote-14">[↩]</a></li><li id="d-footnote-15-listing">UCB and GP-UCB have been mentioned in the collapsible<a class="footnote-backlink" href="#d-footnote-15">[↩]</a></li><li id="d-footnote-16-listing">StackOverflow <a href="https://stackoverflow.com/questions/35848210/support-vector-machine-what-are-c-gamma">answer</a> for intuition behind the hyperparameters.<a class="footnote-backlink" href="#d-footnote-16">[↩]</a></li><li id="d-footnote-17-listing"> <strong>Note</strong>:
 the surface plots you see for the Ground Truth Accuracies below were 
calculated for each possible hyperparameter for showcasing purposes 
only. We do not have these values in real applications.
        <a class="footnote-backlink" href="#d-footnote-17">[↩]</a></li><li id="d-footnote-18-listing"><strong>Note</strong>: One will need to negate the accuracy values as we are using the minimizer function from <code>scikit-optim</code>.<a class="footnote-backlink" href="#d-footnote-18">[↩]</a></li><li id="d-footnote-19-listing">The example above has been inspired by <a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/19_Hyper-Parameters.ipynb">Hvass Laboratories’ Tutorial Notebook</a> showcasing hyperparameter optimization in TensorFlow using <code>scikit-optim</code>.<a class="footnote-backlink" href="#d-footnote-19">[↩]</a></li><li id="d-footnote-20-listing">Really nice tutorial showcasing hyperparameter optimization on a neural network available at this <a href="https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/19_Hyper-Parameters.ipynb">link</a>.
          <a class="footnote-backlink" href="#d-footnote-20">[↩]</a></li></ol>
</d-footnote-list><d-citation-list distill-prerendered="true"><style>
d-citation-list {
  contain: style;
}

d-citation-list .references {
  grid-column: text;
}

d-citation-list .references .title {
  font-weight: 500;
}
</style><h3 id="references">References</h3><ol id="references-list" class="references"><li id="goldKridge"><span class="title">A statistical approach to some basic mine valuation problems on the Witwatersrand </span>   <a href="https://journals.co.za/content/saimm/52/6/AJA0038223X_4792">[link]</a><br>Krige,
 D., 1951. Journal of the Southern African Institute of Mining and 
Metallurgy, Vol 52(6), pp. 119-139. Southern African Institute of Mining
 and Metallurgy.</li><li id="settles2009active"><span class="title">Active Learning Literature Survey</span>   <a href="http://burrsettles.com/pub/settles.activelearning.pdf">[PDF]</a><br>Settles, B., 2009. </li><li id="Tong2001"><span class="title">Active learning: theory and applications</span>   <a href="http://www.robotics.stanford.edu/~stong/papers/tong_thesis.pdf">[PDF]</a><br>Tong, S., 2001. </li><li id="humanOut"><span class="title">Taking the Human Out of the Loop: A Review of Bayesian Optimization</span> <br>Shahriari, B., Swersky, K., Wang, Z., Adams, R.P. and Freitas, N.d., 2016. Proceedings of the IEEE, Vol 104(1), pp. 148-175.  <a href="https://doi.org/10.1109/JPROC.2015.2494218" style="text-decoration:inherit;">DOI: 10.1109/JPROC.2015.2494218</a></li><li id="nandoBOtut"><span class="title">A
 Tutorial on Bayesian Optimization of Expensive Cost Functions, with 
Application to Active User Modeling and Hierarchical Reinforcement 
Learning</span> <br>Brochu, E., M. Cora, V. and De Freitas, N., 2010. CoRR, Vol abs/1012.2599. </li><li id="görtler2019a"><span class="title">A Visual Exploration of Gaussian Processes</span>   <a href="https://distill.pub/2019/visual-exploration-gaussian-processes/">[link]</a><br>Görtler, J., Kehlbeck, R. and Deussen, O., 2019. Distill.  <a href="https://doi.org/10.23915/distill.00017" style="text-decoration:inherit;">DOI: 10.23915/distill.00017</a></li><li id="Rasmussen2004"><span class="title">Gaussian Processes in Machine Learning</span>   <a href="http://www.gaussianprocess.org/gpml/chapters/RW.pdf">[PDF]</a><br>Rasmussen, C.E., 2004. Advanced Lectures on Machine Learning, pp. 63--71. Springer Berlin Heidelberg. <a href="https://doi.org/10.1007/978-3-540-28650-9_4" style="text-decoration:inherit;">DOI: 10.1007/978-3-540-28650-9_4</a></li><li id="mockusEI"><span class="title">Bayesian approach to global optimization and application to multiobjective and constrained problems</span> <br>B. Mockus, J. and Mockus, L., 1991. Journal of Optimization Theory and Applications, Vol 70, pp. 157-172.  <a href="https://doi.org/10.1007/BF00940509" style="text-decoration:inherit;">DOI: 10.1007/BF00940509</a></li><li id="thompson"><span class="title">On The Likelihood That One Unknown Probability Exceeds Another In View Of The Evidence Of Two Samples</span>   <a href="https://doi.org/10.1093/biomet/25.3-4.285">[link]</a><br>Thompson, W.R., 1933. Biometrika, Vol 25(3-4), pp. 285-294.  <a href="https://doi.org/10.1093/biomet/25.3-4.285" style="text-decoration:inherit;">DOI: 10.1093/biomet/25.3-4.285</a></li><li id="gpucb"><span class="title">Using Confidence Bounds for Exploitation-Exploration Trade-Offs</span> <br>Auer, P., 2003. J. Mach. Learn. Res., Vol 3(null), pp. 397–422. JMLR.org.</li><li id="gpucbBounds"><span class="title">Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design</span> <br>Srinivas, N., Krause, A., Kakade, S.M. and Seeger, M., 2009. arXiv e-prints, pp. arXiv:0912.3995. </li><li id="Snoek2012"><span class="title">Practical Bayesian Optimization of Machine Learning Algorithms</span>   <a href="http://dl.acm.org/citation.cfm?id=2999325.2999464">[link]</a><br>Snoek,
 J., Larochelle, H. and Adams, R.P., 2012. Proceedings of the 25th 
International Conference on Neural Information Processing Systems - 
Volume 2, pp. 2951--2959. Curran Associates Inc.</li><li id="NIPS2011_4443"><span class="title">Algorithms for Hyper-Parameter Optimization</span> <br>Bergstra,
 J., Bardenet, R., Bengio, Y. and K\'{e}gl, B., 2011. Proceedings of the
 24th International Conference on Neural Information Processing Systems,
 pp. 2546–2554. Curran Associates Inc.</li><li id="Bergstra"><span class="title">Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures</span>   <a href="http://dl.acm.org/citation.cfm?id=3042817.3042832">[link]</a><br>Bergstra,
 J., Yamins, D. and Cox, D.D., 2013. Proceedings of the 30th 
International Conference on International Conference on Machine Learning
 - Volume 28, pp. I–115–I–123. JMLR.org.</li><li id="scikit"><span class="title">Scikit-learn: Machine Learning in {P}ython</span> <br>Pedregosa,
 F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., 
Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., 
Passos, A., Cournapeau, D., Brucher, M., Perrot, M. and Duchesnay, E., 
2011. Journal of Machine Learning Research, Vol 12, pp. 2825--2830. </li><li id="BOwtGD"><span class="title">Bayesian Optimization with Gradients</span>   <a href="http://papers.nips.cc/paper/7111-bayesian-optimization-with-gradients.pdf">[PDF]</a><br>Wu,
 J., Poloczek, M., Wilson, A.G. and Frazier, P., 2017. Advances in 
Neural Information Processing Systems 30, pp. 5267--5278. Curran 
Associates, Inc.</li><li id="hyperband"><span class="title">Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization</span>   <a href="http://www.jmlr.org/papers/volume18/16-558/16-558.pdf">[PDF]</a><br>Li,
 L., Jamieson, K., DeSalvo, G., Rostamizadeh, A. and Talwalkar, A., 
2018. Journal of Machine Learning Research, Vol 18-185, pp. 1-52. </li><li id="largeBO"><span class="title">Fast Bayesian Optimization of Machine Learning Hyperparameters on Large Datasets</span>   <a href="http://proceedings.mlr.press/v54/klein17a.html">[HTML]</a><br>Klein,
 A., Falkner, S., Bartels, S., Hennig, P. and Hutter, F., 2017. 
Proceedings of the 20th International Conference on Artificial 
Intelligence and Statistics, Vol 54, pp. 528--536. PMLR.</li><li id="SafeExplore"><span class="title">Safe Exploration for Optimization with Gaussian Processes</span>   <a href="http://dl.acm.org/citation.cfm?id=3045118.3045225">[link]</a><br>Sui,
 Y., Gotovos, A., Burdick, J.W. and Krause, A., 2015. Proceedings of the
 32Nd International Conference on International Conference on Machine 
Learning - Volume 37, pp. 997--1005. JMLR.org.</li><li id="NNbasedBO"><span class="title">Scalable Bayesian Optimization Using Deep Neural Networks</span>   <a href="http://dl.acm.org/citation.cfm?id=3045118.3045349">[link]</a><br>Snoek,
 J., Rippel, O., Swersky, K., Kiros, R., Satish, N., Sundaram, N., 
Patwary, M.M.A., Prabhat, P. and Adams, R.P., 2015. Proceedings of the 
32Nd International Conference on International Conference on Machine 
Learning - Volume 37, pp. 2171--2180. JMLR.org.</li><li id="multiACQ"><span class="title">Portfolio Allocation for Bayesian Optimization</span>   <a href="http://dl.acm.org/citation.cfm?id=3020548.3020587">[link]</a><br>Hoffman,
 M., Brochu, E. and de Freitas, N., 2011. Proceedings of the 
Twenty-Seventh Conference on Uncertainty in Artificial Intelligence, pp.
 327--336. AUAI Press.</li><li id="sensorBO"><span class="title">Bayesian Optimization for Sensor Set Selection</span>   <a href="http://doi.acm.org/10.1145/1791212.1791238">[link]</a><br>Garnett,
 R., Osborne, M.A. and Roberts, S.J., 2010. Proceedings of the 9th 
ACM/IEEE International Conference on Information Processing in Sensor 
Networks, pp. 209--219. ACM. <a href="https://doi.org/10.1145/1791212.1791238" style="text-decoration:inherit;">DOI: 10.1145/1791212.1791238</a></li><li id="letham2019"><span class="title">Constrained Bayesian Optimization with Noisy Experiments</span>   <a href="https://doi.org/10.1214/18-BA1110">[link]</a><br>Letham,
 B., Karrer, B., Ottoni, G. and Bakshy, E., 2019. Bayesian Anal., Vol 
14(2), pp. 495--519. International Society for Bayesian Analysis. <a href="https://doi.org/10.1214/18-BA1110" style="text-decoration:inherit;">DOI: 10.1214/18-BA1110</a></li><li id="yelpBO"><span class="title">Parallel Bayesian Global Optimization of Expensive Functions</span> <br>Wang, J., Clark, S.C., Liu, E. and Frazier, P.I., 2016. arXiv e-prints, pp. arXiv:1602.05149. </li></ol></d-citation-list><distill-appendix>
<style>
  distill-appendix {
    contain: layout style;
  }

  distill-appendix .citation {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  distill-appendix > * {
    grid-column: text;
  }
</style>

    <h3 id="updates-and-corrections">Updates and Corrections</h3>
    <p>
    If you see mistakes or want to suggest changes, please <a href="https://github.com/distillpub/post--bayesian-optimization/issues/new">create an issue on GitHub</a>. </p>
    
    <h3 id="reuse">Reuse</h3>
    <p>Diagrams and text are licensed under Creative Commons Attribution <a href="https://creativecommons.org/licenses/by/4.0/">CC-BY 4.0</a> with the <a class="github" href="https://github.com/distillpub/post--bayesian-optimization">source available on GitHub</a>,
 unless noted otherwise. The figures that have been reused from other 
sources don’t fall under this license and can be recognized by a note in
 their caption: “Figure from …”.</p>
    
    <h3 id="citation">Citation</h3>
    <p>For attribution in academic contexts, please cite this work as</p>
    <pre class="citation short">Agnihotri &amp; Batra, "Exploring Bayesian Optimization", Distill, 2020.</pre>
    <p>BibTeX citation</p>
    <pre class="citation long">@article{agnihotri2020exploring,
  author = {Agnihotri, Apoorv and Batra, Nipun},
  title = {Exploring Bayesian Optimization},
  journal = {Distill},
  year = {2020},
  note = {https://distill.pub/2020/bayesian-optimization},
  doi = {10.23915/distill.00026}
}</pre>
    </distill-appendix></d-appendix>



<distill-footer>
<style>

:host {
  color: rgba(255, 255, 255, 0.5);
  font-weight: 300;
  padding: 2rem 0;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  background-color: hsl(180, 5%, 15%); /*hsl(200, 60%, 15%);*/
  text-align: left;
  contain: content;
}

.footer-container .logo svg {
  width: 24px;
  position: relative;
  top: 4px;
  margin-right: 2px;
}

.footer-container .logo svg path {
  fill: none;
  stroke: rgba(255, 255, 255, 0.8);
  stroke-width: 3px;
}

.footer-container .logo {
  font-size: 17px;
  font-weight: 200;
  color: rgba(255, 255, 255, 0.8);
  text-decoration: none;
  margin-right: 6px;
}

.footer-container {
  grid-column: text;
}

.footer-container .nav {
  font-size: 0.9em;
  margin-top: 1.5em;
}

.footer-container .nav a {
  color: rgba(255, 255, 255, 0.8);
  margin-right: 6px;
  text-decoration: none;
}

</style>

<div class="footer-container">

  <a href="https://distill.pub/" class="logo">
    <svg viewBox="-607 419 64 64">
  <path d="M-573.4,478.9c-8,0-14.6-6.4-14.6-14.5s14.6-25.9,14.6-40.8c0,14.9,14.6,32.8,14.6,40.8S-565.4,478.9-573.4,478.9z"></path>
</svg>

    Distill
  </a> is dedicated to clear explanations of machine learning

  <div class="nav">
    <a href="https://distill.pub/about/">About</a>
    <a href="https://distill.pub/journal/">Submit</a>
    <a href="https://distill.pub/prize/">Prize</a>
    <a href="https://distill.pub/archive/">Archive</a>
    <a href="https://distill.pub/rss.xml">RSS</a>
    <a href="https://github.com/distillpub">GitHub</a>
    <a href="https://twitter.com/distillpub">Twitter</a>
    &nbsp;&nbsp;&nbsp;&nbsp; ISSN 2476-0757
  </div>

</div>

</distill-footer><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-83741880-1', 'auto');
  ga('send', 'pageview');
</script></body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>